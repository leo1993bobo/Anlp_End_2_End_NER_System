Many O
state O
- O
of O
- O
the O
- O
art O
parsers O
use O
POS B-TaskName
tags O
and O
pre O
- O
trained O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
embeddings O
as O
a O
part O
of O
the O
word O
representation O
. O

Dozat O
and O
Manning O
( O
2018 O
) O
ﬁnd O
that O
characterbased O
LSTM B-MethodName
and O
lemma O
embeddings O
can O
further O
improve O
the O
performance O
of O
semantic O
dependency O
parser O
. O

Zhang O
et O
al O
. O
( O
2019 O
) O
use O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
embeddings O
for O
each O
token O
to O
improve O
the O
performance O
of O
AMR O
parsing O
. O

The O
word O
representation O
oiin O
our O
system O
is O
: O
oi= O
[ O
ow O
i;opos O
i;olemmas O
i;opw O
i;obw O
i;ochar O
i;one O
i O
] O
whereow O
iis O
word O
embedding O
with O
random O
initialization B-HyperparameterName
, O
opw O
iis O
pre O
- O
trained O
GloVe B-MethodName
embedding O
and O
obw O
iare O
BERT B-MethodName
embedding O
through O
average B-MetricName
pooling O
over O
subwords O
. O

P(u O
) O
= O
m O
/ O
productdisplay O
i=1P(ui|u O
< O
i O
, O
idx O
< O
i O
, O
w O
) O
To O
encode O
the O
input O
sentence O
, O
we O
use O
a O
multi O
- O
layer O
BiLSTM B-MethodName
fed O
with O
embeddings O
of O
the O
words O
: O
R= O
BiLSTM B-MethodName
( O
O O
) O
( O
1 O
) O
whereOrepresents O
[ O
o1, O
... O
,on],oiis O
the O
concatenation O
different O
types O
of O
embeddings O
for O
wi O
, O
and O
R= O
[ O
r1, O
... O
,rn]represents O
the O
output O
from O
the O
BiLSTM B-MethodName
. O

For O
the O
decoder O
, O
at O
each O
time O
step O
t O
, O
we O
use O
an O
l O
- O
layer O
LSTM B-MethodName
for O
generating O
hidden O
states O
zl O
tsequentially O
: O
zl O
t O
= O
fl(zl−1 O
t O
, O
zl O
t−1 O
) O
whereflis O
thel O
- O
th O
layer O
of O
LSTM B-MethodName
, O
zl O
0is O
the O
last O
hidden O
state O
rnin O
Eq O
. O

Suppose O
that O
we O
have O
a O
sequence O
of O
vector O
representations O
of O
the O
predicted O
nodes O
[ O
r′ O
1, O
... O
,r′ O
m O
] O
, O
which O
can O
be O
the O
BiLSTM B-MethodName
output O
riin O
Eq O
. O

4.4 O
Ablation O
Study O
BERT B-MethodName
with O
Other O
Embeddings O
We O
use O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
embedding O
in O
our O
model O
. O

We O
compared O
the O
performance O
of O
DM O
in O
the O
original O
SDP O
dataset O
with O
different O
subtoken O
pooling O
methods O
, O
and O
we O
also O
explored O
whether O
combining O
other O
embeddings O
such O
as O
pre O
- O
trained O
word O
embedding O
Glove O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
contextual O
embedding O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
will O
further O
improve O
the O
performance O
. O

ELMo B-MethodName
embedding O
is O
also O
helpful O
but O
can O
not O
outperform O
BERT B-MethodName
embedding O
. O

However O
, O
the O
performance O
dropped O
when O
ELMo B-MethodName
embedding O
and O
BERT B-MethodName
embedding O
are O
combined O
. O

We O
found O
that O
average B-MetricName
pooling O
is O
slightly O
better O
thanLF1 B-MetricName
Baseline O
93.41 B-MetricValue
Base-ﬁxed O
94.17 O
Base O
- O
tuned O
94.22 O
Base-ﬁxed O
+ O
Glove O
94.45 O
Base O
- O
tuned O
+ O
Glove O
94.48 O
Large-ﬁxed O
+ O
Glove O
94.62 O
Large O
- O
tuned O
+ O
Glove O
94.64 O
Large-ﬁxed O
+ O
Glove O
+ O
Lemma O
95.10 O
Large-ﬁxed O
+ O
Glove O
+ O
Lemma O
+ O
Char O
95.22 O
ELMo B-MethodName
+ O
Large-ﬁxed O
+ O
Glove O
+ O
Lemma O
94.78 O
ELMo B-MethodName
+ O
Glove O
+ O
Lemma O
+ O
Char O
95.06 O
BERT B-MethodName
- O
First O
95.22 O
BERT B-MethodName
- O
Avg O
95.28 O
BERT B-MethodName
- O
Avg O
+ O
dep O
- O
tree O
95.30 O
Table O
7 O
: O
Comparing O
Labeled O
F1 B-MetricName
scores B-MetricName
of O
models O
with O
different O
types O
of O
embedding O
combinations O
on O
the O
development O
set O
of O
the O
gold O
DM O
dataset O
. O

Baseline O
represents O
the O
parser O
of O
Wang O
et O
al O
. O
( O
2019 O
) O
.Base O
represents O
the O
pre O
- O
trained O
BERT B-MethodName
- O
Base O
uncased O
model O
and O
Large O
represents O
the O
pre O
- O
trained O
BERT B-MethodName
- O
Large O
uncased O
model O
. O

ﬁxed O
andtuned O
represents O
whether O
to O
ﬁne O
- O
tune O
the O
BERT B-MethodName
model O
. O

BERT B-MethodName
in O
the O
last O
block O
represents O
the O
last O
embedding O
combination O
( O
Large-ﬁxed O
+ O
Glove O
+ O
Lemma O
+ O
Char O
) O
in O
the O
ﬁrst O
block O
. O

Each O
model O
leverages O
character O
- O
level O
bidirectional O
LSTMs B-MethodName
as O
lexical O
feature O
extractors O
to O
encode O
morphological O
information O
. O

Bi O
- O
directional O
long O
- O
short O
term O
memory O
networks O
( O
Graves O
and O
Schmidhuber O
, O
2005 O
, O
biLSTMs B-MethodName
) O
have O
recently O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
syntactic O
parsing O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
; O
Cross O
and O
Huang O
, O
2016 O
; O
Dozat O
and O
Manning O
, O
2017 O
) O
. O

Our O
system O
leverages O
the O
representational O
power O
of O
bi O
- O
LSTMs B-MethodName
to O
generate O
compact O
features O
for O
both O
graph O
- O
based O
and O
transition O
- O
based O
parsing O
frameworks O
. O

With O
just O
two O
bi O
- O
LSTM B-MethodName
vectors O
as O
features O
, O
all O
three O
global O
parsing O
paradigms O
in O
our O
system O
have O
efﬁcient O
Opn3qimplementations O
. O

We O
address O
the O
ﬁrst O
challenge O
with O
characterlevel O
bi O
- O
LSTMs B-MethodName
, O
which O
have O
previously O
been O
shown O
to O
be O
effective O
in O
multi O
- O
lingual O
POS B-TaskName
tagging O
( O
Plank O
et O
al O
. O
, O
2016 O
) O
and O
dependency O
parsing O
( O
Ballesteros O
et O
al O
. O
, O
2015 O
; O
Alberti O
et O
al O
. O
, O
2017 O
) O
. O

Additionally O
, O
in O
the O
categories O
of O
small O
treebanks O
and O
surprise O
languages O
, O
we O
obtained O
the O
best O
average B-MetricName
performance.2 O
System O
Overview O
Character O
bi O
-LSTM B-MethodName
/ O
delexicalized O
featuresWord O
-level O
bi O
-LSTMAEDPEnsembleIV B-MetricName
. O

Unlabeled O
ParsingCharacter O
bi O
-LSTM B-MethodName
/ O
delexicalized O
featuresWord O
-level O
bi O
-LSTMAEDPEnsembleIV B-MetricName
. O

For O
most O
languages O
, O
we O
employ O
character O
- O
level O
biLSTMs B-MethodName
to O
capture O
morphological O
information O
. O

On O
top O
of O
the O
character O
- O
level O
representations O
, O
there O
is O
another O
layer O
of O
bi O
- O
LSTMs B-MethodName
processing O
at O
the O
word O
level O
, O
the O
output O
of O
which O
gives O
context O
- O
sensitive O
features O
associated O
with O
every O
word O
in O
the O
sentence O
. O

For O
the O
four O
surprise O
languages O
and O
a O
selected O
set O
of O
languages O
with O
small O
training O
treebanks O
, O
we O
substitute O
the O
character O
- O
level O
encodings O
of O
each O
word O
in O
Stage O
II O
with O
concatenation O
of O
part O
- O
of O
- O
speech O
( O
POS B-TaskName
) O
tag O
embeddings O
and O
morphological O
feature O
embeddings O
, O
but O
keep O
the O
wordlevel O
bi O
- O
LSTMs B-MethodName
. O

Based O
on O
the O
extracted O
LSTM B-MethodName
features O
and O
predicted O
unlabeled O
parse O
trees O
, O
this O
stage O
assigns O
the O
highest O
scoring O
label O
to O
each O
arc O
. O

These O
two O
feature O
extractors O
both O
leverage O
the O
representational O
power O
of O
bi O
- O
LSTMs B-MethodName
. O

3.1 O
Character O
LSTMs B-MethodName
Among O
the O
most O
straightforward O
ways O
for O
representing O
a O
word O
are O
through O
binary O
features O
or O
word O
embeddings O
. O

Our O
system O
adopts O
character O
- O
level O
bi O
- O
LSTMs B-MethodName
similar O
to O
Plank O
et O
al O
. O
( O
2016 O
) O
and O
Ballesteros O
et O
al O
. O
( O
2015 O
) O
. O

Formally O
, O
for O
a O
word O
wwith O
its O
character O
sequencerBOW O
, O
c1 O
, O
... O
, O
c O
m O
, O
EOWs O
, O
with O
two O
special O
begin O
- O
of O
- O
word O
( O
BOW O
, O
or O
c0 O
) O
and O
end O
- O
of O
- O
word O
( O
EOW O
, O
or O
cm 1 O
) O
symbols O
, O
we O
run O
a O
forward O
and O
abackward O
LSTM B-MethodName
at O
layer O
l O
: O
rÑ O
cl O
isLSTM B-MethodName
forwardprÑÐ O
cl1 O
isq O
rÐ O
cl O
isLSTM B-MethodName
backwardprÑÐ O
cl1 O
isq O
ÑÐ O
cl O
iÑ O
cl O
iÐ O
cl O
i O
eachcl O
idenotes O
the O
vector O
representation O
at O
layer O
lforci,denotes O
concatenation O
of O
vectors O
, O
and O
rsis O
a O
shorthand O
for O
a O
list O
of O
vectors O
. O

We O
take O
the O
concatenation O
ofÑcm 1andÐc0at O
the O
ﬁnal O
layer O
of O
the O
LSTMs B-MethodName
as O
the O
output O
vectors O
. O

We O
use O
twolayer O
bi O
- O
LSTMs B-MethodName
in O
our O
system O
. O

Efﬁciency O
Improvement O
Considering O
the O
Zipﬁan O
distribution O
for O
word O
frequencies O
, O
most O
of O
the O
time O
is O
spent O
on O
getting O
char O
bi O
- O
LSTM B-MethodName
representations O
for O
frequent O
words O
. O

On O
the O
other O
hand O
, O
for O
those O
words O
, O
it O
is O
considerably O
easier O
to O
train O
decent O
representations O
even O
without O
char O
bi O
- O
LSTMs B-MethodName
. O

We O
thus O
directly O
learn O
the O
dense O
word O
vectors O
for O
frequent O
words O
, O
as O
a O
proxy O
for O
character O
- O
level O
biLSTMs B-MethodName
and O
they O
can O
be O
considered O
as O
fast O
look O
- O
up O
tables O
without O
actually O
running O
the O
LSTMs1 B-MethodName
. O

3.2 O
Delexicalized O
Features O
For O
languages O
with O
small O
treebanks O
, O
the O
provided O
data O
is O
not O
adequate O
to O
learn O
character O
bi O
- O
LSTMs B-MethodName
. O

To O
get O
dense O
vectors O
for O
each O
word O
win O
the O
same O
form O
as O
the O
output O
of O
char O
bi O
- O
LSTMs B-MethodName
, O
we O
use O
the O
concatenation O
of O
UPOS B-TaskName
embeddingsÑpw O
and O
the O
bag O
- O
of O
- O
morphology O
( O
BOM O
) O
embeddings O
poolptÑmwuq O
. O

3.3 O
Word O
- O
level O
LSTMs B-MethodName
The O
character O
bi O
- O
LSTM B-MethodName
vector O
for O
each O
word O
is O
computed O
in O
isolation O
from O
other O
words O
in O
the O
sentence O
. O

In O
this O
module O
, O
we O
again O
leverage O
biLSTMs B-MethodName
for O
integration O
of O
contextual O
information O
. O

Inputs O
to O
the O
ﬁrst O
layer O
are O
character O
bi O
- O
LSTM B-MethodName
encodings O
, O
or O
concatenation O
of O
POS B-TaskName
- O
tag O
and O
BOM O
embeddings O
in O
the O
case O
of O
delexicalized O
models O
. O

Then O
Eisner O
’s O
algorithm O
is O
used O
to O
ﬁnd O
the O
maximum O
spanning O
tree O
among O
all O
possible O
projective O
trees O
: O
argmax O
valid O
parses O
y¸ O
ph O
, O
mqPyscoreMSTph B-MetricName
, O
mq O
Following O
Dozat O
and O
Manning O
( O
2017 O
) O
, O
we O
use O
a O
deep O
bi O
- O
afﬁne O
scoring O
function O
: O
scoreMSTph B-MetricName
, O
mqv O
/ O
intercal O
hUvm bhvh bmvm b O
where O
vhMLPMST O
- O
headpÑÐhq O
vmMLPMST O
- O
modpÑÐmq O
are O
representations O
transformed O
by O
two O
multi O
- O
layer O
perceptrons O
( O
MLPs O
) O
from O
their O
bi O
- O
LSTM B-MethodName
vectors O
. O

In O
our O
system O
, O
we O
only O
use O
two O
bi O
- O
LSTM B-MethodName
vectors O
, O
one O
from O
the O
top O
of O
the O
stack O
( O
ÑÐs0 O
) O
, O
and O
one O
from O
the O
top O
of O
the O
buffer O
( O
ÑÐb0 O
) O
. O

Our O
system O
performs O
relatively O
well O
on O
languages O
with O
high O
OOV O
ratios O
, O
such O
as O
hu O
, O
ko O
, O
lv O
andet O
, O
with O
the O
help O
of O
character O
bi O
- O
LSTMs B-MethodName
. O

For O
all O
languages O
and O
all O
treebanks O
, O
we O
trained O
models O
with2 O
- O
layer O
- O
deep O
and O
192 O
- O
unit O
- O
wide O
( O
96units O
for O
each O
direction O
) O
word O
- O
level O
bi O
- O
LSTMs B-MethodName
as O
feature O
extractors O
. O

Lexicalized O
character O
bi O
- O
LSTMs B-MethodName
are O
2layers O
deep O
and O
128units O
wide O
, O
with O
64 O
- O
dimensional O
input O
character O
embeddings O
. O

For O
languages O
without O
lexicalized O
feature O
extractors O
, O
we O
used O
concatenation O
of O
64 O
- O
dimensional O
UPOS B-TaskName
embeddings O
, O
and O
max O
pooling O
of O
64 O
- O
dimensional O
morphological O
embeddings O
as O
input O
to O
word O
- O
level O
bi O
- O
LSTMs B-MethodName
. O

The O
word O
- O
level O
bi O
- O
LSTM B-MethodName
feature O
vectors O
were O
passed O
through O
MLPs O
with O
1hidden O
layer O
and O
192 O
hidden B-HyperparameterName
units I-HyperparameterName
, O
before O
the O
bi O
- O
afﬁne O
scoring O
functions O
for O
MST O
, O
AHDP O
and O
AEDP O
unlabeled O
parsing O
. O

BiLSTMs B-MethodName
, O
both O
character O
- O
level O
and O
word O
- O
level O
, O
also O
had O
dropout B-HyperparameterName
rates I-HyperparameterName
of O
0.3for O
input O
and O
recurrent O
connections O
( O
Gal O
and O
Ghahramani O
, O
2016 B-HyperparameterValue
) O
. O

Further O
, O
we O
zeroed O
out O
input O
vectors O
to O
word O
- O
level O
LSTMs B-MethodName
for15 O
% O
of O
the O
time O
, O
to O
encourage O
the O
models O
gain O
more O
information O
from O
context O
. O

To O
speed O
up O
training O
, O
we O
simultaneously O
trained O
MST O
, O
AHDP O
, O
AEDP O
and O
arc O
labeling O
models O
with O
shared O
LSTM B-MethodName
feature O
extractors O
. O

In O
particular O
, O
the O
neural O
- O
probabilistic O
word O
representations O
produced O
by O
the O
Skip O
- O
gram O
and O
Continuous O
Bag O
- O
of O
- O
Words O
( O
CBOW B-MethodName
) O
architectures O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
implemented O
in O
the O
word2vec O
andfastText O
software O
packages O
have O
been O
extensively O
evaluated O
in O
recent O
years O
. O

In O
particular O
, O
the O
Skip O
- O
gram O
and O
CBOW B-MethodName
neural O
network O
architectures O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
implemented O
within O
the O
word2vec O
andfastText O
software O
packages O
provide O
scalable O
approaches O
to O
online O
training O
of O
word O
vector O
representations O
that O
have O
been O
shown O
to O
perform O
well O
across O
a O
number O
of O
tasks O
( O
Mikolov O
et O
al O
. O
, O
2013a O
; O
Levy O
et O
al O
. O
, O
2015 O
; O
Mikolov O
et O
al O
. O
, O
2017 O
) O
. O

Incorporating O
order O
in O
this O
way O
resulted O
in O
improvements O
in O
accuracy B-MetricName
over O
word2vec O
’s O
CBOW B-MethodName
implementation O
on O
proportional O
analogy O
problems O
, O
which O
were O
considerably O
enhanced O
by O
the O
incorporation O
of O
character O
- O
level O
embeddings O
. O

Pennington O
and O
his O
colleagues O
( O
2014 O
) O
report O
a O
best O
accuracy B-MetricName
of O
69.3 B-MetricValue
% O
after O
training O
Glove O
on O
a O
corpus O
of O
42 O
billion O
words O
, O
and O
Mikolov O
and O
colleagues O
( O
2017 O
) O
report O
an O
accuracy B-MetricName
of O
73 B-MetricValue
% O
when O
training O
a O
subword O
- O
sensitive O
CBOW B-MethodName
model O
for O
ﬁve O
iterations O
across O
a O
630 O
billion O
word O
corpus O
derived O
from O
Common O
Crawl O
. O

Regarding O
the O
performance O
of O
other O
ordersensitive O
approaches O
on O
this O
subset O
, O
Trask O
and O
his O
colleagues O
( O
2015 O
) O
report O
a O
1.41 O
% O
to O
3.07 O
% O
increase O
in O
absolute O
accuracy B-MetricName
over O
a O
standard O
CBOW B-MethodName
baseline O
with O
PENN O
, O
and O
Mikolov O
and O
his O
colleagues O
( O
2017 B-MetricValue
) O
report O
a O
4 O
% O
increase O
over O
a O
subword O
- O
sensitive O
CBOW B-MethodName
model O
with O
incorporation O
of O
position O
- O
dependent O
weights11 B-HyperparameterName
. O

4.2 O
Semantic O
similarity O
/ O
relatedness O
These O
improvements O
in O
analogy O
retrieval O
were O
not O
accompanied O
by O
better O
correlation B-MetricName
with O
human O
es11CBOW B-MethodName
baselines O
were O
around O
10 B-MetricValue
% O
higher O
than O
our O
SGNS O
baselines O
, O
attributable O
to O
differences O
in O
corpus O
size O
, O
composition O
and O
preprocessing O
; O
and O
perhaps O
architecture O
. O
Radius O
2 O
SW O
BATS O
MSR O
Google O
Gsem O
Gsyn O
Binf O
Bder O
Blex O
Benc O
SGNS O
fastText O
27.22 O
53.78 O
70.13 O
77.30 O
64.17 O
56.00 O
11.76 O
7.06 O
32.12 O
SGNS O
semVec O
27.46 O
53.72 O
69.53 O
77.30 O
63.07 O
56.28 O
11.65 O
7.25 O
32.66 O
EARP O
dir O
28.26 O
54.48 O
69.34 O
76.57 O
63.33 O
56.33 O
11.51 O
8.36 O
34.62 O
EARP O
pos O
28.80 O
54.38 O
67.71 O
71.85 O
64.28 O
57.22 O
12.75 O
8.47 O
34.70 O
EARP O
prox O
28.95 O
55.08 O
69.19 O
74.95 O
64.40 O
57.38 O
12.86 O
8.44 O
35.04 O
EARPx O
pos O
30.50 O
60.15 O
61.71 O
58.94 O
64.01 O
66.09 O
11.07 O
9.37 O
33.05 O
EARPx O
prox O
30.79 O
62.45 O
66.86 O
68.80 O
65.25 O
66.90 O
10.76 O
9.46 O
33.56 O
SGNS O
fastText O
X O
28.16 O
59.83 O
63.57 O
57.87 O
68.31 O
61.05 O
22.51 O
4.47 O
24.60 O
SGNS O
semVec O
X O
29.17 O
61.44 O
65.24 O
59.38 O
70.10 O
62.28 O
23.56 O
5.02 O
25.83 O
EARP O
dir O
X O
31.70 O
62.94 O
68.90 O
62.46 O
74.26 O
63.85 O
29.26 O
5.54 O
28.65 O
EARP O
pos O
X O
33.18 O
62.89 O
69.35 O
62.98 O
74.63 O
65.10 O
32.23 O
6.01 O
30.12 O
EARP O
prox O
X O
33.26 O
64.26 O
70.82 O
64.83 O
75.80 O
66.08 O
31.72 O
6.05 O
29.85 O
EARPx O
pos O
X O
36.89 O
68.90 O
61.12 O
44.05 O
75.29 O
71.32 O
44.14 O
6.81 O
27.62 O
EARPx O
prox O
X O
37.07 O
69.07 O
63.88 O
49.31 O
75.98 O
71.56 O
44.30 O
6.92 O
27.81 O
Radius O
5 O
SW O
BATS O
MSR O
Google O
Gsem O
Gsyn O
Binf O
Bder O
Blex O
Benc O
SGNS O
fastText O
25.54 O
48.21 O
69.29 O
78.47 O
61.66 O
49.93 O
11.95 O
7.00 O
31.51 O
SGNS O
semVec O
25.76 O
46.90 O
68.75 O
79.04 O
60.20 O
49.42 O
11.84 O
6.87 O
33.08 O
EARP O
dir O
27.07 O
49.20 O
68.66 O
76.85 O
61.85 O
50.58 O
12.38 O
8.30 O
35.04 O
EARP O
pos O
26.14 O
45.39 O
61.29 O
65.53 O
57.76 O
49.82 O
12.41 O
7.71 O
32.80 O
EARP O
prox O
28.40 O
51.06 O
69.17 O
77.05 O
62.62 O
53.59 O
14.05 O
8.51 O
35.56 O
EARPx O
pos O
28.30 O
52.67 O
59.18 O
59.80 O
58.67 O
57.27 O
12.90 O
8.78 O
32.35 O
EARPx O
prox O
30.94 O
58.15 O
69.37 O
73.51 O
65.93 O
59.86 O
15.36 O
10.07 O
36.52 O
SGNS O
fastText O
X O
26.27 O
55.04 O
66.32 O
64.56 O
67.79 O
56.23 O
18.56 O
4.43 O
25.38 O
SGNS O
semVec O
X O
27.69 O
55.93 O
67.96 O
67.37 O
68.44 O
57.90 O
20.32 O
5.12 O
26.98 O
EARP O
dir O
X O
30.12 O
58.20 O
70.22 O
69.42 O
70.88 O
59.98 O
24.46 O
5.78 O
30.11 O
EARP O
pos O
X O
31.02 O
54.78 O
65.67 O
60.41 O
70.03 O
60.28 O
28.21 O
5.96 O
29.94 O
EARP O
prox O
X O
32.67 O
60.77 O
72.23 O
69.79 O
74.27 O
64.01 O
28.71 O
6.46 O
31.67 O
EARPx O
pos O
X O
34.75 O
61.08 O
63.83 O
54.41 O
71.66 O
66.02 O
36.32 O
7.51 O
30.33 O
EARPx O
prox O
X O
36.67 O
67.44 O
72.22 O
66.78 O
76.74 O
69.16 O
38.72 O
7.56 O
32.52 O
Table O
1 O
: O
Analogical O
retrieval O
results O
at O
radius O
2 O
( O
top O
) O
and O
5 O
( O
bottom O
) O
. O

Tree O
- O
stack O
LSTM B-MethodName
in O
Transition O
Based O
Dependency O
Parsing O
. O

We O
introduce O
tree O
- O
stack O
LSTM B-MethodName
to O
model O
state O
of O
a O
transition O
based O
parser O
with O
recurrent O
neural B-MethodName
networks I-MethodName
. O

Tree O
- O
stack O
LSTM B-MethodName
does O
not O
use O
any O
parse O
tree O
based O
or O
hand O
- O
crafted O
features O
, O
yet O
performs O
better O
than O
models O
with O
these O
features O
. O

There O
are O
4 O
main O
components O
of O
this O
model O
: O
stack O
’s O
-LSTM B-MethodName
, O
buffer O
’s O
 O
LSTM B-MethodName
, O
actions O
’ O
LSTM B-MethodName
and O
tree O
- O
RNN O
. O

All O
LSTMs B-MethodName
use O
continuous O
dense O
feature O
vectors O
( O
embeddings O
) O
as O
an O
input O
. O

On O
the O
other O
hand O
, O
representational O
power O
of O
recurrent O
neural B-MethodName
networks I-MethodName
should O
allow O
a O
model O
both O
to O
summarize O
every O
action O
taken O
from O
the O
beginning O
to O
the O
current O
state O
and O
tree O
- O
fragments O
obtained O
until O
a O
current O
state O
. O

Ballesteros O
et O
al O
. O
usecharacter O
- O
based O
word O
representation O
for O
the O
stackLSTM B-MethodName
parser O
. O

Chen O
and O
Manning O
uses O
MLP O
, O
Dozat O
et O
al O
. O
applies O
BiLSTM B-MethodName
stacked O
with O
MLP O
as O
a O
decision O
module O
. O

We O
inspire O
from O
Dyer O
et O
al O
. O
’s O
stack O
- O
LSTM B-MethodName
which O
basically O
represents O
each O
component O
of O
a O
state O
( O
buffer O
, O
stack O
and O
actions O
) O
with O
an O
LSTM B-MethodName
. O

3 O
Model O
In O
this O
section O
, O
we O
describe O
MorphNet O
( O
Dayanık O
et O
al O
. O
, O
2018 O
) O
used O
for O
tagging O
and O
lemmatization O
; O
and O
Tree O
- O
stack O
LSTM B-MethodName
used O
for O
dependency O
parsing O
. O

Treestack O
LSTM B-MethodName
takes O
that O
for O
dependency O
parsing O
. O

The O
model O
operates O
with O
a O
unidirectional O
Long O
Short O
Term O
Memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
encoder O
to O
create O
a O
character O
- O
based O
word O
embeddings O
and O
a O
bidirectional O
LSTM B-MethodName
encoder O
to O
obtain O
context O
embeddings O
. O

The O
decoder O
consists O
of O
two O
layers O
LSTM B-MethodName
. O

hij O
= O
LSTM B-MethodName
( O
aij;hij 1 O
) O
( O
1 O
) O
hi0= O
0 O
( O
2 O
) O
ei O
= O
hiLi O
( O
3 O
) O
We O
model O
context O
encoder O
by O
using O
a O
bidirectional O
LSTM B-MethodName
. O

For O
a O
wordwiI O
deﬁne O
its O
corresponding O
context O
embeddingci2R2Has O
the O
concatenation O
of O
the O
forward  O
! O
ci2RHand O
the O
backward O
 ci2RHhidden O
states O
that O
are O
produced O
after O
the O
forward O
and O
backward O
LSTMs B-MethodName
process O
the O
word O
embedding O
ei O
. O

  O
! O
ci O
= O
LSTM B-MethodName
f(ei;  O
! O
ci 1 O
) O
( O
4 O
) O
 O
 ci O
= O
LSTM B-MethodName
b(ei O
; O
 ci+1 O
) O
( O
5 O
) O
  O
! O
c0= O
 cN+1= O
0 O
( O
6 O
) O
ci= O
[ O
  O
! O
ci O
; O
 ci O
] O
( O
7 O
) O
The O
decoder O
is O
implemented O
as O
a O
2 O
- O
Layer O
LSTM B-MethodName
network O
that O
outputs O
the O
correct O
tag O
for O
a O
single O
target O
word O
. O

3.7 O
Tree O
- O
stack O
LSTM B-MethodName
Tree O
- O
stack O
LSTM B-MethodName
has O
4 O
main O
components O
: O
buffer O
’s O
 O
-LSTM B-MethodName
, O
stack O
’s O
-LSTM B-MethodName
, O
actions’LSTM B-MethodName
and O
tree O
’s O
tree O
- O
RNN O
or O
t O
- O
RNN O
in O
short O
. O

We O
aim O
to O
represent O
each O
component O
of O
the O
transition O
system O
, O
c= O
( O
 O
; O
 O
; O
A O
) O
, O
with O
a O
distinct O
LSTM B-MethodName
similar O
to O
( O
Dyer O
et O
al O
. O
, O
2015 O
) O
. O

Buffer O
’s O
 O
-LSTM B-MethodName
is O
initialized O
with O
zero O
hidden O
state O
, O
and O
fed O
with O
input O
features O
from O
last O
word O
to O
the O
beginning O
. O

Similarly O
, O
stack O
’s O
-LSTM B-MethodName
is O
also O
initialized O
with O
zero O
hidden O
state O
and O
fed O
with O
input O
features O
from O
the O
beginning O
word O
to O
the O
last O
word O
of O
a O
stack O
. O

Actions O
’ O
LSTM B-MethodName
is O
also O
started O
with O
zero O
hidden O
state O
, O
and O
updated O
after O
each O
action O
. O

Inputs O
to O
-LSTM B-MethodName
and O
 O
-LSTM B-MethodName
are O
updated O
via O
tree O
- O
RNN O
. O

Concatenation O
of O
stack O
’s O
LSTM B-MethodName
, O
buffer O
’s O
LSTM B-MethodName
and O
actions O
’ O
LSTM B-MethodName
’s O
ﬁnal O
hidden B-HyperparameterName
layer I-HyperparameterName
becomes O
an O
input O
to O
MLP O
which O
outputs O
the O
probabilities O
for O
each O
transition O
in O
the O
next O
step O
. O

We O
do O
not O
change O
the O
LSTMs B-MethodName
’ O
hidden O
dimensions O
, O
but O
record O
the O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
took O
for O
convergence O
. O

We O
start O
with O
a O
dimension O
of O
32 O
and O
increase O
the O
dimension O
by O
powers O
of O
two O
until O
512 O
for O
LSTM B-MethodName
hiddens O
, O
1024 O
for O
LM O
matrix O
( O
explained O
in O
below O
) O
. O

All O
the O
LSTMs B-MethodName
and O
tree O
- O
RNN O
have O
hidden O
dimension O
of O
256 O
. O

In O
order O
to O
analyze O
our O
tree O
- O
stack O
LSTM B-MethodName
, O
we O
compare O
that O
model O
with O
Kırnap O
et O
al O
. O
sharing O
similar O
feature O
interests O
and O
transition O
system O
with O
our O
model O
. O

However O
, O
treestack O
LSTM B-MethodName
only O
needs O
raw O
features O
and O
previous O
parsing O
actions O
. O

As O
we O
deduce O
from O
Table O
3 O
, O
tree O
- O
stack O
LSTM B-MethodName
beneﬁts O
from O
morphological O
information O
with O
mid O
- O
resource O
languages O
. O

Language O
Parent O
Language O
enpud O
enewt O
jamodern O
jagsd O
cspud O
cspdt O
svpud O
svtalbanken O
ﬁpud O
ﬁtdt O
thpud O
idgsd O
pcm O
nsc O
enewt O
brkeb O
enewt O
Table O
4 O
: O
Our O
parent O
choices O
in O
languages O
without O
train O
data O
5 O
Discussion O
We O
use O
tree O
- O
stack O
LSTM B-MethodName
model O
in O
transition O
based O
dependency O
parsing O
framework O
. O

This O
study O
opens O
a O
question O
on O
adapting O
the O
tree O
- O
stack O
LSTM B-MethodName
in O
graph O
based O
dependency O
parsing O
. O

Following O
( O
Kingma O
and O
Welling O
, O
2014 O
; O
Kingma O
et O
al O
. O
, O
2014 O
) O
, O
the O
posterior O
approximation O
is O
regarded O
as O
a O
diagonal O
Gaussian O
N(;diag O
( O
2 O
) O
) O
, O
and O
its O
mean B-MetricName
and O
variance O
2are O
parameterized O
with O
deep O
neural B-MethodName
networks I-MethodName
. O

Left O
: O
linear O
mapping O
from O
BERT B-MethodName
( O
CC O
, O
see O
§ O
2 O
) O
color O
term O
embeddings O
to O
the O
CIELAB O
space O
. O

See O
§ O
2.We O
also O
show O
that O
part O
of O
this O
distributional O
signal O
is O
learnable O
by O
simple O
models O
— O
e.g. O
models O
based O
on O
pointwise O
mutual O
information O
( O
PMI O
) O
statistics O
— O
although O
large O
- O
scale O
language O
model O
pretraining O
( O
e.g. O
, O
BERT B-MethodName
) O
encodes O
the O
topology O
markedly O
better O
. O

In O
the O
ﬁrst O
conﬁguration O
( O
NC O
) O
, O
a O
color O
term O
is O
encoded O
without O
context O
, O
with O
the O
appropriate O
delimiter O
tokens O
attached O
( O
e.g. O
[ O
CLS O
] O
red O
[ O
SEP O
] O
for O
BERT B-MethodName
) O
. O

Linear O
mapping O
We O
train O
regularised O
linear B-MethodName
regression I-MethodName
models O
to O
map O
from O
color O
term O
embedding O
spaceX2RndLMto O
CIELAB O
space O
Y2Rn3 O
, O
minimisingL(W O
; O
 O
) O
= O
kXW Yk2 O
2 O
+ O
 O
kWk1 O
, O
whereW2R3dLMis O
a O
linear O
map O
and O
 O
is O
the O
lasso O
regularization O
hyper O
parameter O
. O

map O
max O
mean B-MetricName
max O
mean B-MetricName
max O
mean B-MetricName
max O
mean B-MetricName
max O
mean B-MetricName
max O
mean B-MetricName
BERT B-MethodName
0.160.010:09 O
0.75 B-MetricValue
0.730:01 O
0.26y0.200:03 O
0.74 O
0.730:08 O
0.24y0.190:03 O
0.76 O
0.750:05 O
RoBERTa B-MethodName
0.33x0.020:11 O
0.75 O
0.730:01 O
0.200.140:04 O
0.74 O
0.730:01 O
0.190.140:04 O
0.77 O
0.760:09 O
ELECTRA B-MethodName
0.13 O
0.010:08 O
0.75 O
0.640:13 O
0.25y0.190:05 O
0.75 O
0.730:01 O
0.23y0.160:04 O
0.78 O
0.760:01 O
Table O
1 O
: O
Results O
for O
the O
RSA O
experiments O
show O
max O
and O
mean B-MetricName
( O
across O
layers O
) O
Kendall O
’s O
 B-MetricValue
; O
correlations B-MetricName
that O
are O
signiﬁcantly O
non O
- O
zero O
are O
marked O
with O
* O
, O
yandxforp O
< O
0:05,<0:01and<0:001respectively O
. O

For O
instance O
, O
for O
both O
the O
CIELAB O
and O
CC O
BERT B-MethodName
RSMs O
, O
violet O
’s O
top O
nearestneighbors O
include O
purple O
, O
lavender O
, O
pink O
, O
and O
orange O
, O
and O
its O
furthest O
neighbors O
include O
aqua O
, O
olive O
, O
black O
, O
and O
gray O
. O

The O
mean B-MetricName
correlation B-MetricName
across O
layers O
in O
this O
setup O
is O
near O
zero O
, O
even O
though O
max O
correlations B-MetricName
for O
BERT B-MethodName
and O
RoBERTa B-MethodName
are O
signiﬁcant O
; O
this O
is O
unsurprising O
, O
however O
, O
as O
the O
LM O
has O
likely O
never O
encountered O
single O
color O
term O
tokens O
in O
isolation O
( O
cf O
. O

In O
terms O
of O
individual O
colors O
, O
Figure O
4a O
depicts O
the O
ranking O
of O
predicted O
CIELAB O
codes O
per O
Munsell O
color O
chip O
for O
BERT B-MethodName
( O
CC O
) O
. O

mean B-MetricName
BERT B-MethodName
- O
mini O
0.077 B-MetricValue
0.043 O
0.340 O
0.729 O
0.582 O
0.291 O
BERT B-MethodName
- O
small O
0.106 O
0.070 O
0.191 O
0.734 O
0.598 O
0.294 O
BERT B-MethodName
- O
medium O
0.097 O
0.057 O
0.035 O
0.739 O
0.654 O
0.221 O
BERT B-MethodName
- O
base O
0.1620.0920.058 O
0.740 O
0.677 O
0.182 O
Table O
3 O
: O
Results O
for O
the O
four O
smaller O
BERT B-MethodName
models O
. O

Effect O
of O
model O
size O
We O
also O
evaluate O
the O
effect O
of O
model O
size O
on O
alignment O
by O
testing O
four O
smaller O
BERT B-MethodName
( O
CC O
) O
models10using O
the O
same O
setup O
described O
above O
. O

Vision O
- O
and O
- O
Language O
models O
In O
a O
preliminary O
set O
of O
experiments O
, O
we O
evaluated O
multi O
- O
modal O
Vision O
- O
and O
- O
Language O
models O
( O
VisualBERT B-MethodName
( O
Li O
et O
al O
. O
, O
2019 O
) O
and O
VideoBERT B-MethodName
( O
Sun O
et O
al O
. O
, O
2019 O
) O
) O
, O
ﬁnding O
no O
major O
differences O
in O
results O
from O
the O
text O
- O
only O
models O
presented O
in O
this O
study.6 O
Related O
Work O
Distributional O
word O
representations O
have O
long O
been O
theorized O
to O
capture O
various O
types O
of O
information O
about O
the O
world O
( O
Schütze O
, O
1992 O
) O
. O

Core O
sets O
have O
been O
successfully O
applied O
to O
a O
variety O
of O
problems O
, O
from O
dimensionality O
reduction O
of O
massive O
data O
sets O
( O
Feldman O
et O
al O
. O
, O
2016 O
) O
, O
to O
vector O
summarization O
( O
Feldman O
et O
al O
. O
, O
2017 O
) O
or O
compression O
of O
neural B-MethodName
networks I-MethodName
( O
Baykal O
et O
al O
. O
, O
2018 O
) O
. O

We O
introduce O
a O
methodology O
originating O
from O
linguistics O
to O
analyse O
the O
representation O
learned O
by O
neural B-MethodName
networks I-MethodName
– O
the O
gating O
paradigm O
– O
and O
show O
that O
the O
correct O
representation O
of O
a O
word O
is O
only O
activated O
if O
the O
network O
has O
access O
to O
ﬁrst O
phoneme O
of O
the O
target O
word O
, O
suggesting O
that O
the O
network O
does O
not O
rely O
on O
a O
global O
acoustic O
pattern O
. O

Inspired O
by O
the O
researches O
for O
images O
( O
Su O
et O
al O
. O
, O
2019 O
) O
, O
efforts O
on O
attacking O
neural B-MethodName
networks I-MethodName
for O
NLP O
applications O
emerged O
recently O
( O
see O
Zhang O
et O
al O
. O
( O
2019 O
) O
for O
a O
survey O
) O
. O

Also O
, O
we O
introduced O
a O
methodology O
originating O
from O
linguistics O
to O
analyse O
the O
representation O
learned O
by O
neural B-MethodName
networks I-MethodName
: O
the O
gating O
paradigm O
. O

First O
, O
Lowe O
et O
al O
. O
( O
2015 O
) O
released O
the O
Ubuntu O
Dialogue O
dataset O
and O
proposed O
a O
neural O
model O
which O
matches O
the O
context O
and O
response O
with O
corresponding O
representations O
via O
RNNs O
and O
LSTMs B-MethodName
. O

Kadlec O
et O
al O
. O
( O
2015 O
) O
evaluate O
the O
performances O
of O
various O
models O
on O
the O
dataset O
, O
such O
as O
LSTMs B-MethodName
, O
Bi O
- O
LSTMs B-MethodName
, O
and O
CNNs O
. O

Further O
, O
Xu O
et O
al O
. O
( O
2017 O
) O
proposed O
a O
deep O
neural O
network O
to O
incorporate O
background O
knowledge O
for O
conversation O
by O
LSTM B-MethodName
with O
a O
specially O
designed O
recall B-MetricName
gate O
. O

In O
word O
- O
level O
, O
we O
use O
a O
shared O
LSTM B-MethodName
layer O
to O
obtain O
the O
word O
- O
level O
embedding O
for O
each O
word O
. O

At O
last O
, O
the O
utterance O
- O
level O
representation O
of O
each O
utterance O
is O
fed O
into O
another O
LSTM B-MethodName
layer O
to O
further O
model O
the O
information O
among O
different O
utterances O
, O
forming O
the O
contextlevel O
representation O
. O

We O
use O
a O
shared O
bi O
- O
directional O
LSTM B-MethodName
to O
get O
contextual O
word O
representations O
in O
each O
utterance O
, O
query O
, O
and O
the O
response O
. O

The O
representation O
of O
each O
word O
is O
formed O
by O
concatenating O
the O
forward O
and O
backward O
LSTM B-MethodName
hidden O
output O
. O

  h(x O
) O
= O
    O
 LSTM B-MethodName
( O
e(x O
) O
) O
( O
5 O
) O
  !h(x O
) O
= O
  O
    O
! O
LSTM B-MethodName
( O
e(x O
) O
) O
( O
6 O
) O
h(x O
) O
= O
[ O
  h(x);  !h(x O
) O
] O
( O
7 O
) O
whereh(x)is O
the O
representation O
of O
the O
word O
. O

We O
denote O
the O
word O
- O
level O
representation O
of O
the O
context O
ashu2Rmdwand O
the O
response O
as O
hr2Rmdw O
, O
wheredwis O
the O
dimension O
of O
Bi O
- O
LSTMs B-MethodName
. O

To O
further O
model O
the O
continuity O
and O
contextual O
information O
among O
the O
utterances O
, O
we O
fed O
the O
utterance O
- O
level O
representations O
into O
another O
bi O
- O
directional O
LSTM B-MethodName
layer O
to O
obtain O
the O
representation O
for O
each O
utterance O
in O
context O
perspective O
. O

ck O
= O
Bi O
- O
LSTM B-MethodName
( O
[ O
uk]n O
k=1 O
) O
( O
10 O
) O
whereck2Rdcis O
the O
context O
- O
level O
representation O
for O
thekth O
utterance O
in O
the O
context O
and O
dcis O
the O
output O
size O
of O
the O
Bi O
- O
LSTM B-MethodName
. O

M= O
[ O
M1;M2;M3;M4 O
] O
( O
26 O
) O
~m O
= O
MaxPoolingn+m O
i=0[Bi O
- O
LSTM B-MethodName
( O
mi)](27 O
) O
v O
= O
MaxPoolingm O
j=0[Bi O
- O
LSTM B-MethodName
( O
~mj O
) O
] O
( O
28 O
) O
Wheremiand O
~ O
mjare O
theith O
, O
jth O
row O
in O
the O
matrixmand O
~ O
m. O

We O
merge O
the O
results O
from O
different O
time O
steps O
in O
the O
outputs O
of O
LSTM B-MethodName
by O
max O
- O
pooling O
operation O
. O

To O
reduce O
the O
number O
of O
unknown O
words O
, O
Ubuntu O
Dialogue O
Corpus O
Douban O
Conversation O
Corpus O
R2@1 O
R O
10@1 O
R O
10@2 O
R O
10@5 O
MAP B-MetricName
MRR B-MetricName
P@1 O
R O
10@1 O
R O
10@2 O
R O
10@5 O
DualEncoder O
90.1 B-MetricValue
63.8 O
78.4 O
94.9 O
48.5 O
52.7 O
32.0 O
18.7 O
34.3 O
72.0 O
MV O
- O
LSTM B-MethodName
90.6 O
65.3 O
80.4 O
94.6 O
49.8 O
53.8 O
34.8 O
20.2 O
35.1 O
71.6 O
Match O
- O
LSTM B-MethodName
90.4 O
65.3 O
80.4 O
94.6 O
49.8 O
53.8 O
34.8 O
20.2 O
34.8 O
71.0 O
DL2R O
89.9 O
62.6 O
78.3 O
94.4 O
48.8 O
52.7 O
33.0 O
19.3 O
34.2 O
70.5 O
Multi O
- O
View O
90.8 O
66.2 O
80.1 O
95.1 O
50.5 O
54.3 O
34.2 O
20.2 O
35.0 O
72.9 O
SMN O
92.6 O
72.6 O
84.7 O
96.1 O
52.9 O
56.9 O
39.7 O
23.3 O
39.6 O
72.4 O
RNN O
- O
CNN O
91.1 O
67.2 O
80.9 O
95.6 O
- O
- O
- O
- O
- O
DUA O
- O
75.2 O
86.8 O
96.2 O
55.1 O
59.9 O
42.1 O
24.3 O
42.1 O
78.0 O
DAM O
93.8 O
76.7 O
87.4 O
96.9 O
55.0 O
60.1 O
42.7 O
25.4 O
41.0 O
75.7 O
TripleNet O
94.3 O
79.0 O
88.5 O
97.0 O
56.4 O
61.8 O
44.7 O
26.8 O
42.6 O
77.8 O
TripleNet O
elmo O
95.1 O
80.5 O
89.7 O
97.6 O
60.9 O
65.0 O
47.0 O
27.8 O
48.7 O
81.4 O
TripleNet O
ensemble O
95.6 O
82.1 O
90.9 O
98.0 O
63.2 O
67.8 O
51.5 O
31.3 O
49.4 O
83.2 O
Table O
1 O
: O
Experimental O
results O
on O
two O
public O
dialogue O
datasets O
. O

In O
the O
Embedding O
Layer O
, O
the O
word O
embeddings O
are O
pre O
- O
trained O
using O
the O
training O
set O
via O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
the O
weights B-HyperparameterName
of O
which O
are O
trainable O
. O

For O
all O
the O
Bidirectional O
LSTM B-MethodName
layers O
, O
we O
set O
their O
hidden B-HyperparameterName
size I-HyperparameterName
to O
200 B-HyperparameterValue
. O

The O
majority O
of O
the O
previous O
works O
on O
this O
task O
are O
designed O
without O
attention O
mechanisms O
, O
including O
the O
Sequential O
Matching O
Network O
( O
SMN O
) O
( O
Wu O
et O
al O
. O
, O
2017 O
) O
, O
Multi O
- O
View O
model O
( O
Zhou O
et O
al O
. O
, O
2016 O
) O
, O
Deep O
Learning O
to O
Respond O
( O
DL2R O
) O
( O
Yan O
et O
al O
. O
, O
2016 O
) O
, O
Match O
- O
LSTM B-MethodName
( O
Wang O
and O
Jiang O
, O
2016 O
) O
, O
MVLSTM B-MethodName
( O
Wan O
et O
al O
. O
, O
2016 O
) O
, O
and O
DualEncoder O
( O
Lowe O
et O
al O
. O
, O
2015 O
) O
. O

To O
further O
improve O
the O
performance O
, O
we O
utilize O
pre O
- O
trained O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
ﬁnetune O
it O
on O
the O
training O
set O
in O
the O
Ubuntu O
condition O
while O
we O
train O
ELMo B-MethodName
from O
scratch O
using O
the O
Douban O
training O
set O
. O

As O
the O
baseline O
of O
Douban O
corpus O
is O
relatively O
lower O
, O
we O
observe O
much O
bigger O
improvements O
in O
the O
corpus O
using O
ELMo B-MethodName
. O

The O
model O
ensemble O
has O
further O
improvements O
based O
on O
the O
single O
model O
with O
ELMo B-MethodName
; O
the O
score B-MetricName
of O
R10@1 O
in O
Ubuntu O
is O
close O
to O
the O
average B-MetricName
performance O
of O
human O
experts O
at O
83.8 B-MetricValue
( O
Lowe O
et O
al O
. O
, O
2016 O
) O
. O

While O
the O
attention O
of O
context O
focuses O
on O
the O
word O
‘ O
a O
’ O
which O
is O
near O
the O
key O
phrase O
‘ O
deb O
ﬁle O
, O
’ O
which O
may O
be O
because O
the O
representation O
of O
the O
word O
catches O
some O
information O
from O
the O
words O
nearby O
by O
BiLSTM B-MethodName
. O

We O
model O
the O
context O
from O
low O
( O
character O
) O
to O
high O
( O
context O
) O
level O
, O
update O
the O
representation O
by O
triple O
attention O
within O
hC;Q;Ri O
, O
match O
the O
triple O
focused O
on O
response O
, O
and O
fuse O
the O
matching O
results O
with O
hierarchical O
LSTM B-MethodName
for O
prediction O
. O

Inducing O
sparseness O
while O
training O
neural B-MethodName
networks I-MethodName
has O
been O
shown O
to O
yield O
models O
with O
a O
lower O
memory O
footprint O
but O
similar O
effectiveness O
to O
dense O
models O
. O

1 O
Introduction O
Many O
supervised O
learning O
problems O
today O
are O
solved O
with O
deep O
neural B-MethodName
networks I-MethodName
exploiting O
largescale O
labeled O
data O
. O

The O
contributions O
of O
this O
paper O
are O
( O
i O
) O
a O
predeﬁned O
sparseness O
model O
for O
recurrent O
neural B-MethodName
networks I-MethodName
, O
( O
ii O
) O
as O
well O
as O
for O
word O
embeddings O
, O
and O
( O
iii O
) O
proof O
- O
of O
- O
concept O
experiments O
on O
part O
- O
of O
- O
speech O
tagging O
and O
language O
modeling O
, O
including O
an O
analysis O
of O
the O
memorization O
capacity O
of O
dense O
vs. O
sparse O
networks O
. O

The O
code O
for O
running O
the O
presented O
experiments O
is O
publically O
available.2 O
2 O
Related O
Work O
A O
substantial O
body O
of O
work O
has O
explored O
the O
beneﬁts O
of O
using O
sparse O
neural B-MethodName
networks I-MethodName
. O

Whereas O
above O
- O
cited O
papers O
speciﬁcally O
explored O
convolutional O
networks O
, O
our O
work O
focuses O
on O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
. O

Note O
that O
our O
experiments O
make O
use O
of O
the O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
cell O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
but O
our O
discussion O
should O
hold O
for O
any O
type O
of O
recurrent O
network O
cell O
. O

For O
example O
, O
an O
LSTM B-MethodName
contains O
4 O
matricesWhhandWhi O
, O
whereas O
the O
Gated O
Recurrent O
Unit O
( O
GRU O
) O
( O
Chung O
et O
al O
. O
, O
2014 O
) O
only O
has O
3 O
. O

Yet O
, O
Merity O
et O
al O
. O
( O
2017 O
) O
showed O
that O
applying O
weight B-HyperparameterName
dropping O
( O
i.e. O
, O
DropConnect O
, O
Wan O
et O
al O
. O
( O
2013 B-HyperparameterValue
) O
) O
in O
an O
LSTM B-MethodName
’s O
Whh O
matrices O
has O
a O
stronger O
positive O
effect O
on O
language O
models O
than O
other O
ways O
to O
regularize O
them O
. O

Our O
baseline O
approach O
is O
the O
AWDLSTM B-MethodName
model O
introduced O
by O
Merity O
et O
al O
. O
( O
2017 O
) O
. O

The O
recurrent O
unit O
consists O
of O
a O
three O
- O
layer O
stacked O
LSTM B-MethodName
( O
Long O
Short O
- O
Term O
Memory O
network O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
) O
, O
with O
400 O
- O
dimensional O
inputs O
and O
outputs O
, O
and O
intermediate O
hidden O
state O
sizes O
of O
1150 O
. O

Speciﬁcally O
, O
each O
LSTM B-MethodName
layer O
is O
made O
sparse O
in O
such O
a O
way O
that O
the O
hidden O
dimension O
1150 O
is O
increased O
by O
a O
factor O
1.5 O
( O
chosen O
ad O
hoc O
) O
to O
1725 O
, O
but O
the O
embedding O
dimensions O
and O
total O
number O
of O
parameters O
remain O
the O
same O
( O
within O
error O
margins O
from O
rounding O
to O
integer O
dimensions O
for O
the O
dense O
blocks O
) O
. O

The O
number O
of O
parameters O
for O
the O
middle O
LSTM B-MethodName
layer O
can O
be O
calculated O
as:4 O
# O
params O
. O

LSTM B-MethodName
layer O
2 O
= O
4(hdid+h2 O
d+ O
2hd O
) O
( O
dense O
) O
= O
4N(hs O
N O
 O
is+h2 O
s O
N2 O
+ O
2hs O
N)(sparse O
) O
in O
which O
the O
ﬁrst O
expression O
represents O
the O
general O
case O
( O
e.g. O
, O
the O
dense O
case O
has O
input O
and O
state O
sizesid O
= O
hd= O
1150 O
) O
, O
and O
the O
second O
part O
is O
thesparse O
case O
composed O
of O
Nparallel O
LSTMs B-MethodName
3Alternative O
models O
could O
be O
designed O
for O
comparison O
, O
with O
modiﬁcations O
in O
both O
the O
embedding O
and O
output O
layer O
. O

Straightforward O
ideas O
include O
an O
ensemble O
of O
smaller O
independent O
models O
, O
or O
a O
mixture O
- O
of O
- O
softmaxes O
output O
layer O
to O
combine O
hidden O
states O
of O
the O
parallel O
LSTM B-MethodName
components O
, O
inspired O
by O
( O
Yang O
et O
al O
. O
, O
2017 O
) O
. O

4This O
follows O
from O
an O
LSTM B-MethodName
’s O
4 O
Whhand O
4Whimatrices O
, O
as O
well O
as O
bias O
vectors O
. O

We O
assume O
the O
standard O
Pytorch O
implementation O
( O
Paszke O
et O
al O
. O
, O
2017).ﬁnetune O
test O
perplexity B-MetricName
( O
Merity O
et O
al O
. O
, O
2017 B-MetricValue
) O
no O
58:8 O
baseline O
no O
58:80:3 O
sparse O
LSTM B-MethodName
no O
57:90:3 O
( O
Merity O
et O
al O
. O
, O
2017 O
) O
yes O
57:3 O
baseline O
yes O
56:60:2 O
sparse O
LSTM B-MethodName
yes O
57:00:2 O
Table O
1 O
: O
Language O
modeling O
for O
PTB O
( O
meanstdev B-MetricName
) O
. O

A O
dense O
model O
with O
hidden B-HyperparameterName
size I-HyperparameterName
h= O
1725 B-HyperparameterValue
would O
require O
46 O
M O
parameters O
, O
with O
24 O
M O
in O
the O
middle O
LSTM B-MethodName
alone O
. O

Given O
the O
strong O
hyperparameter O
dependence O
of O
the O
AWD O
- O
LSTM B-MethodName
model O
, O
and O
the O
known O
issues O
in O
objectively O
evaluating O
language O
models O
( O
Melis O
et O
al O
. O
, O
2017 O
) O
, O
we O
decided O
to O
keep O
all O
hyperparameters O
( O
i.e. O
, O
dropout B-HyperparameterName
rates I-HyperparameterName
and O
optimization O
scheme O
) O
as O
in O
the O
implementation O
from O
Merity O
et O
al O
. O
( O
2017)5 O
, O
including O
the O
weight B-HyperparameterName
dropping O
withp= O
0:5 O
in O
the O
sparse O
Whhmatrices O
. O

as O
input O
for O
a O
BiLSTM B-MethodName
layer O
with O
hidden O
state O
size O
10 O
for O
both O
forward O
and O
backward O
directions O
. O

7With O
LSTM B-MethodName
state O
sizes O
of O
50 O
, O
the O
careful O
tuning O
of O
dropout B-HyperparameterName
parameters O
gave O
an O
accuracy B-MetricName
of O
94.7 B-MetricValue
% O
when O
reducing O
the O
embedding O
size O
to O
k= O
2 O
, O
a O
small O
gap O
compared O
to O
96.8 O
% O
for O
embedding O
size O
50 O
. O

The O
reason O
for O
that O
is O
that O
with O
sparse O
20dimensional O
embeddings O
, O
the O
BiLSTM B-MethodName
still O
receives O
20 O
- O
dimensional O
inputs O
, O
from O
which O
a O
significant O
subset O
only O
transmits O
signals O
from O
a O
small O
set O
of O
frequent O
terms O
. O

In O
this O
case O
, O
sparsifying O
both O
the O
recurrent O
and O
embedding O
layer O
yields O
the O
best O
result O
, O
whereas O
the O
dense O
model O
works O
better O
than O
8It O
is O
likely O
that O
recurrent O
networks O
are O
not O
the O
best O
choice O
for O
this O
purpose O
, O
but O
here O
we O
only O
wanted O
to O
measure O
the O
LSTM B-MethodName
- O
based O
model O
’s O
capacity O
to O
memorize O
with O
and O
without O
predeﬁned O
sparseness.hyperparameter O
value(s O
) O
optimizer B-HyperparameterName
Adam O
( O
Kingma O
and O
Ba O
, O
2015 B-HyperparameterValue
) O
learning B-HyperparameterName
rate I-HyperparameterName
0.001 B-HyperparameterValue
epochs B-HyperparameterName
50 B-HyperparameterValue
word O
level O
embedding O
dropout B-HyperparameterName
y[0.0 O
, O
0.1 B-HyperparameterValue
, O
0.2 O
] O
variational O
embedding O
dropout B-HyperparameterName
y[0.0 O
, O
0.1 O
, O
0.2 O
, O
0.4 O
] O
DropConnect O
on O
Whhy O
[ O
0.0 O
, O
0.2 O
, O
0.4 O
] O
batch B-HyperparameterName
size I-HyperparameterName
20 B-HyperparameterValue
Table O
2 O
: O
Hyperparameters O
for O
POS B-TaskName
tagging O
model O
( O
yas O
introduced O
in O
( O
Merity O
et O
al O
. O
, O
2017 O
) O
) O
. O

Figure O
4(a O
) O
sketches O
the O
stacked O
3 O
- O
layer O
LSTM B-MethodName
network O
from O
the O
‘ O
dense O
RNN O
’ O
model O
( O
see O
Table O
4 O
) O
with O
k= O
200 O
andh= O
575 O
. O

As O
already O
mentioned O
, O
our O
proposed O
sparse O
LSTMs B-MethodName
are O
equivalent O
to O
a O
wellchosen O
composition O
of O
smaller O
dense O
LSTM B-MethodName
components O
with O
overlapping O
inputs O
and O
disjoint O
outputs O
. O

Figure O
4 O
: O
Schematic O
overview O
of O
3 O
- O
layer O
stacked O
( O
a O
) O
dense O
vs. O
( O
b O
) O
sparse O
LSTMs B-MethodName
with O
the O
same O
number O
of O
parameters O
( O
indicated O
with O
‘ O
par O
. O
’ O
) O
. O

Sparse O
layers O
are O
effectively O
composed O
of O
smaller O
dense O
LSTMs B-MethodName
. O

‘ O
Ri;j O
’ O
indicates O
component O
j O
within O
layer O
i O
, O
and O
‘ O
675!100 O
’ O
indicates O
an O
LSTM B-MethodName
compoment O
with O
input O
size O
675 O
and O
output O
size O
100 O
. O

Wang O
et O
al O
. O
( O
2016 O
) O
and O
Tang O
et O
al O
. O
( O
2016a O
) O
both O
concatenated O
the O
aspect O
embeddings O
and O
embeddings O
of O
each O
word O
as O
inputs O
to O
a O
LSTM B-MethodName
based O
model O
so O
as O
to O
introduce O
the O
information O
of O
the O
target O
into O
the O
model O
. O

Wang O
et O
al O
. O
, O
( O
2016 O
) O
proposed O
an O
attention O
based O
LSTM B-MethodName
which O
introduced O
the O
aspect O
clues O
by O
concatenating O
the O
aspect O
embeddings O
and O
the O
word O
representations O
. O

Tang O
et O
al O
. O
( O
2016a O
) O
developed O
two O
target O
- O
dependent O
LSTM B-MethodName
to O
model O
the O
left O
and O
right O
contexts O
with O
target O
, O
where O
the O
target O
information O
was O
automatically O
taken O
into O
account O
. O

Tay O
et O
al O
. O
( O
2017 O
) O
proposed O
an O
attention O
based O
LSTM B-MethodName
which O
learned O
to O
attend O
based O
on O
associative O
relationships O
between O
sentence O
words O
and O
aspect O
by O
adopting O
circular O
convolution O
and O
circular O
correlation B-MetricName
. O

Let O
  ∈ℝ  ×   O
be O
a O
word O
embedding O
lookup O
table O
generated O
by O
an O
unsupervised O
method O
such O
as O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
or O
CBOW B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
where O
   O
is O
the O
dimension O
of O
the O
word O
embeddings O
and O
   O
is O
the O
size O
of O
word O
vocabulary O
. O

Wang O
et O
al O
. O
( O
2016 O
) O
and O
Tang O
et O
al O
. O
( O
2016a O
) O
both O
concatenated O
the O
aspect O
embeddings O
and O
embeddings O
of O
each O
word O
as O
inputs O
to O
a O
LSTM B-MethodName
based O
model O
so O
as O
to O
introduce O
the O
information O
of O
the O
target O
into O
the O
model O
. O

Wang O
et O
al O
. O
, O
( O
2016 O
) O
proposed O
an O
attention O
based O
LSTM B-MethodName
which O
introduced O
the O
aspect O
clues O
by O
concatenating O
the O
aspect O
embeddings O
and O
the O
word O
representations O
. O

Tang O
et O
al O
. O
( O
2016a O
) O
developed O
two O
target O
- O
dependent O
LSTM B-MethodName
to O
model O
the O
left O
and O
right O
contexts O
with O
target O
, O
where O
the O
target O
information O
was O
automatically O
taken O
into O
account O
. O

Tay O
et O
al O
. O
( O
2017 O
) O
proposed O
an O
attention O
based O
LSTM B-MethodName
which O
learned O
to O
attend O
based O
on O
associative O
relationships O
between O
sentence O
words O
and O
aspect O
by O
adopting O
circular O
convolution O
and O
circular O
correlation B-MetricName
. O

Let O
  ∈ℝ  ×   O
be O
a O
word O
embedding O
lookup O
table O
generated O
by O
an O
unsupervised O
method O
such O
as O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
or O
CBOW B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
where O
   O
is O
the O
dimension O
of O
the O
word O
embeddings O
and O
   O
is O
the O
size O
of O
word O
vocabulary O
. O

We O
use O
300 O
- O
dimension O
word O
vectors O
pre O
- O
trained O
by O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
( O
whose O
vocabulary O
size O
is O
1.9 O
M O
) O
for O
our O
experiments O
, O
as O
previous O
works O
did O
( O
Tang O
et O
al O
. O
, O
2016b O
; O
Chen O
et O
al O
. O
, O
2017 O
; O
Zheng O
et O
al O
. O
, O
2018 O
) O
. O

 O
Bi O
- O
LSTM B-MethodName
and O
Bi O
- O
GRU O
adopt O
a O
Bi O
- O
LSTM B-MethodName
and O
a O
Bi O
- O
GRU O
network O
to O
model O
the O
sentence O
and O
use O
the O
hidden O
state O
of O
the O
final O
word O
for O
prediction O
respectively O
. O

 O
TD O
- O
LSTM B-MethodName
adopts O
two O
LSTMs B-MethodName
to O
model O
the O
left O
context O
with O
target O
and O
the O
right O
context O
with O
target O
respectively O
( O
Tang O
et O
al O
. O
, O
2016a O
) O
; O
It O
takes O
the O
hidden O
states O
of O
LSTM B-MethodName
at O
last O
time O
- O
step O
to O
represent O
the O
sentence O
for O
prediction O
. O

 O
LCR O
- O
Rot O
( O
Zheng O
et O
al O
. O
, O
2018 O
) O
employs O
three O
Bi O
- O
LSTMs B-MethodName
to O
model O
the O
left O
context O
, O
the O
target O
and O
the O
right O
context O
. O

 O
AOA O
- O
LSTM B-MethodName
( O
Huang O
et O
al O
. O
, O
2018 O
) O
introduces O
an O
attention O
- O
over O
- O
attention O
( O
AOA O
) O
based O
network O
to O
model O
aspects O
and O
sentences O
in O
a O
joint O
way O
and O
explicitly O
capture O
the O
interaction O
between O
aspects O
and O
context O
sentences O
. O

( O
2 O
) O
The O
TD O
- O
LSTM B-MethodName
model O
, O
which O
has O
been O
shown O
to O
be O
better O
than O
LSTM B-MethodName
( O
Tang O
et O
al O
. O
, O
2016a O
) O
, O
gets O
the O
worst O
performance O
of O
all O
RNN O
based O
models O
and O
the O
accuracy B-MetricName
achieved O
by O
TD O
- O
LSTM B-MethodName
is O
2.94 B-MetricValue
% O
and O
2.4 O
% O
lower O
than O
those O
by O
Bi O
- O
LSTM B-MethodName
on O
the O
two O
datasets O
respectively O
. O

Another O
noticeable O
observation O
is O
that O
Bi O
- O
GRU O
achieves O
80.27 O
% O
and O
73.35 O
% O
accuracies B-MetricName
which O
are O
1.7 B-MetricValue
% O
and O
2.82 O
% O
higher O
than O
those O
of O
Bi O
- O
LSTM B-MethodName
on O
the O
Restaurant O
and O
Laptop O
dataset O
respectively O
. O

It O
indicates O
that O
Bi O
- O
GRU O
is O
more O
suitable O
to O
this O
task O
than O
Bi O
- O
LSTM B-MethodName
. O

From O
the O
Table O
, O
we O
can O
observe O
that O
HAPN O
performs O
better O
Dataset O
Restaurant O
( O
% O
) O
Laptop O
( O
% O
) O
Majority O
65.00 O
53.45 O
Bi O
- O
LSTM B-MethodName
78.57 O
70.53 O
Bi O
- O
GRU O
80.27 O
73.35 O
TD O
- O
LSTM B-MethodName
75.63 O
* O
68.13 O
* O
MemNet O
79.98 O
70.33 O
IAN O
78.60 O
72.10 O
RAM O
80.23 O
74.49 O
LCR O
- O
Rot O
81.34 O
75.24 O
AOA O
- O
LSTM B-MethodName
81.20 O
74.50 O
HAPN O
82.23 O
77.27 O
Table O
2 O
: O
 O
Comparison O
with O
baselines O
on O
SemEval O
2014 O
dataset O
. O

GRU O
has O
been O
shown O
to O
achieve O
comparable O
performance O
with O
less O
parameters O
than O
LSTM B-MethodName
( O
Chung O
et O
al O
. O
, O
2014 O
; O
Jozefowicz O
et O
al O
. O
, O
2015 O
) O
. O

The O
model O
is O
deﬁned O
as O
follows O
: O
vb O
= O
X O
w2bew O
= O
jbj O
Zb O
= O
Wbillvb+dbill O
P(y O
= O
yeajb;V O
) O
= O
(ZbvV O
) O
Here O
, O
ew2Rdword O
is O
initialized O
to O
pre O
- O
trained O
GloVe B-MethodName
word O
embeddings1and O
ﬁnetuned O
as O
a O
model O
parameter O
. O

First O
we O
compute O
vN O
as O
the O
mean B-MetricName
GloVe B-MethodName
vectors O
of O
all O
unigrams O
in O
N O
1http://nlp.stanford.edu/data/glove O
. O

6B.zip(model O
denoted O
by O
N O
WGL O
): O
vGloVe B-MethodName
N O
= O
1P O
a2NjajX O
a2NX O
w2aew O
where O
ewrepresents O
the O
GloVe B-MethodName
vector O
for O
word O
w O
in O
articlea O
. O

The O
NWGLmodel O
additionally O
uses O
a O
mean B-MetricName
GloVe B-MethodName
vector O
( O
vGloVe B-MethodName
N O
) O
of O
size O
50 B-MetricValue
, O
whereas O
the O
N O
WFR O
model O
uses O
a O
word O
frequency O
vector O
( O
vFREQ O
N O
) O
of O
size O
2,000 O
. O

First O
, O
researchers O
could O
explore O
richer O
representations O
for O
text O
, O
both O
bill O
text O
and O
news O
text O
- O
for O
example O
, O
CNNs O
or O
contextual O
language O
models O
like O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

While O
the O
principles O
of O
federated O
learning O
are O
fairly O
generic O
, O
its O
methodology O
assumes O
that O
the O
underlying O
models O
are O
neural B-MethodName
networks I-MethodName
. O

These O
values O
lie O
within O
a O
range O
that O
provides O
good O
trade O
- O
off O
between O
the O
LSTM B-MethodName
embedding O
size O
and O
the O
richness O
of O
the O
language O
morphology O
. O

7 O
Experiments O
7.1 O
Neural O
language O
model O
LSTM B-MethodName
models O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
have O
been O
successfully O
used O
in O
a O
variety O
of O
sequence O
processing O
tasks O
. O

LSTM B-MethodName
models O
usually O
have O
a O
large O
number O
of O
parameters O
and O
are O
not O
suitable O
for O
on O
- O
device O
learning O
. O

We O
use O
a O
variant O
of O
LSTM B-MethodName
with O
a O
Coupled O
Input O
and O
Forget O
Gate O
( O
CIFG O
) O
( O
Greff O
et O
al O
. O
, O
2017 O
) O
for O
the O
federated O
neural O
language O
model O
. O

W O
and O
P O
refer O
to O
word O
and O
word O
- O
piece O
models O
, O
respectively O
. O
Nl O
, O
Nh O
, O
Ne O
, O
SeandStotal O
refer O
to O
the O
number O
of O
LSTM B-MethodName
layers O
, O
the O
number O
of O
hidden O
states O
in O
LSTM B-MethodName
, O
the O
embedding O
dimension O
size O
, O
the O
number O
of O
parameters O
in O
the O
embedding O
layer O
and O
in O
total O
, O
respectively O
. O

“ O
G O
” O
represents O
GLSTM B-MethodName
. O

the O
forget O
and O
input O
decisions O
together O
, O
which O
reduces O
the O
number O
of O
LSTM B-MethodName
parameters O
by O
25 O
% O
. O

We O
also O
use O
group O
- O
LSTM B-MethodName
( O
GLSTM B-MethodName
) O
( O
Kuchaiev O
and O
Ginsburg O
, O
2017 O
) O
to O
reduce O
the O
number O
of O
trainable O
variables O
of O
an O
LSTM B-MethodName
matrix O
by O
the O
number O
of O
feature O
groups O
, O
k. O
We O
setk= O
5 O
in O
experiments O
. O

For O
each O
vocabulary O
size O
, O
we O
ﬁrst O
start O
with O
a O
base O
architecture O
consisting O
of O
one O
LSTM B-MethodName
layer O
, O
a O
96 O
- O
dimensional O
embedding O
, O
and O
670hidden O
state O
units O
. O

We O
then O
attempt O
to O
increase O
the O
representational O
power O
of O
the O
LSTM B-MethodName
cell O
by O
increasing O
the O
number O
of O
hidden B-HyperparameterName
units I-HyperparameterName
and O
using O
multi O
- O
layer O
LSTM B-MethodName
cells O
( O
Sutskever O
et O
al O
. O
, O
2014 B-HyperparameterValue
) O
. O

Residual O
LSTM B-MethodName
( O
Kim O
et O
al O
. O
, O
2017 O
) O
and O
layer O
normalization O
( O
Lei O
Ba O
et O
al O
. O
, O
2016 O
) O
are O
used O
throughout O
experiments O
, O
as O
these O
techniques O
were O
observed O
to O
improve O
convergence O
. O

To O
avoid O
the O
restriction O
that O
Nh O
= O
Nein O
the O
output O
, O
we O
apply O
a O
projection O
step O
at O
the O
output O
gate O
of O
the O
LSTM B-MethodName
( O
Sak O
et O
al O
. O
, O
2014 O
) O
. O

This O
step O
reduces O
the O
dimension O
of O
the O
LSTM B-MethodName
hidden O
state O
from O
NhtoNe O
. O

We O
note O
that O
other O
recurrent O
neural O
models O
such O
as O
gated O
recurrent O
units O
( O
Chung O
et O
al O
. O
, O
2014 O
) O
can O
also O
be O
used O
instead O
of O
CIFG O
LSTMs B-MethodName
. O

For O
4 O
K O
word O
- O
piece O
models O
, O
GLSTM B-MethodName
is O
in O
general O
onpar O
with O
its O
P O
4K O
- O
L O
counterpart O
. O

We O
ﬁnd O
that O
there O
are O
two O
ingredients O
necessary O
for O
building O
a O
high O
- O
performing O
neural B-MethodName
QA I-MethodName
system O
: O
ﬁrst O
, O
the O
awareness O
of O
question O
words O
while O
processing O
the O
context O
and O
second O
, O
a O
composition O
function O
that O
goes O
beyond O
simple O
bag O
- O
of O
- O
words O
modeling O
, O
such O
as O
recurrent O
neural B-MethodName
networks I-MethodName
. O

To O
account O
for O
these O
shortcomings O
we O
introduce O
another O
baseline O
which O
relies O
on O
the O
application O
of O
a O
single O
bi O
- O
directional O
recurrent O
neural B-MethodName
networks I-MethodName
( O
BiRNN O
) O
followed O
by O
a O
answer O
layer O
that O
separates O
the O
prediction O
of O
the O
start O
and O
end O
of O
the O
answer O
span O
. O

7 O
Results O
7.1 O
Model O
Component O
Analysis O
Model O
Dev O
F1 B-MetricName
Exact O
Logistic B-MethodName
Regression151.0 I-MethodName
40 B-MetricValue
.0 O
Neural O
BoW O
Baseline O
56.2 O
43 O
.8 O
BiLSTM B-MethodName
58.2 O
48 O
.7 O
BiLSTM B-MethodName
+ O
wiqb71.8 O
62 O
.3 O
BiLSTM B-MethodName
+ O
wiqw73.8 O
64 O
.3 O
BiLSTM B-MethodName
+ O
wiqb+w(FastQA∗ O
) O
74.9 O
65 O
.5 O
FastQA∗+ O
intrafusion O
76.2 O
67 O
.2 O
FastQA∗+ O
intra O
+ O
inter O
( O
FastQAExt∗)77.5 O
68 O
.4 O
FastQA∗+ O
char O
- O
emb O
. O

( O
FastQAExt O
) O
78.3 O
69 O
.9 O
FastQA O
w/ O
beam O
- O
size O
5 O
76.3 O
67 O
.8 O
FastQAExt O
w/ O
beam O
- O
size O
5 O
78.5 O
70 O
.3 O
Table O
1 O
: O
SQuAD B-DatasetName
results O
on O
development O
set O
for O
increasingly O
complex O
architectures.1Rajpurkar O
et O
al O
. O
( O
2016 O
) O
Table O
1shows O
the O
individual O
contributions O
of O
each O
model O
component O
that O
was O
incrementally O
added O
to O
a O
plain O
BiLSTM B-MethodName
model O
without O
features O
, O
character O
embeddings O
and O
beam O
- O
search O
. O

For O
instance O
, O
it O
outperforms O
a O
feature O
rich O
logistic O
- O
regression O
baseline O
on O
the O
SQuAD B-DatasetName
development O
set O
( O
Table O
1 O
) O
and O
nearly O
reaches O
the O
BiLSTM B-MethodName
baseline O
system O
( O
i.e. O
, O
FastQA O
without O
character O
embeddings O
and O
features O
) O
. O

However O
, O
the O
gap O
to O
state O
- O
of O
- O
the O
- O
art O
systems O
is O
quite O
large O
( O
≈20%F1 B-MetricName
) O
which O
indicates O
that O
employing O
5We O
did O
not O
evaluate O
the O
BoW O
baseline O
on O
the O
SQuAD B-DatasetName
test O
set O
because O
it O
requires O
submitting O
the O
model O
to O
Rajpurkar O
et O
al O
. O
( O
2016 O
) O
and O
we O
ﬁnd O
that O
comparisons O
on O
NewsQA O
and O
the O
SQuAD B-DatasetName
development O
set O
give O
us O
enough O
insights O
. O
Model O
Test O
F1 B-MetricName
Exact O
Logistic B-MethodName
Regression151.0 I-MethodName
40 B-MetricValue
.4 O
Match O
- O
LSTM273.7 B-MethodName
64 O
.7 O
Dynamic O
Chunk O
Reader371.0 O
62 O
.5 O
Fine O
- O
grained O
Gating473.3 O
62 O
.5 O
Multi O
- O
Perspective O
Matching575.1 O
65 O
.5 O
Dynamic O
Coattention O
Networks675.9 O
66 O
.2 O
Bidirectional O
Attention O
Flow777.3 O
68 O
.0 O
r O
- O
net877.9 O
69 O
.5 O
FastQA O
w/ O
beam O
- O
size O
k= O
5 O
77 O
.1 O
68 O
.4 O
FastQAExt O
k= O
5 O
78.9 O
70 O
.8 O
Table O
2 O
: O
Ofﬁcial O
SQuAD B-DatasetName
leaderboard O
of O
singlemodel O
systems O
on O
test O
set O
from O
2016/12/29 O
, O
the O
date O
of O
submitting O
our O
model.1Rajpurkar O
et O
al O
. O
( O
2016 O
) O
, O
2Wang O
and O
Jiang O
( O
2017 O
) O
, O
3Yu O
et O
al O
. O
( O
2017 O
) O
, O
4Yang O
et O
al O
. O
( O
2017 O
) O
, O
5Wang O
et O
al O
. O
( O
2017 O
) O
, O
6Xiong O
et O
al O
. O
( O
2017 O
) O
, O
7Seo O
et O
al O
. O
( O
2017 O
) O
, O
8not O
published O
. O

Model O
Dev O
Test O
F1 B-MetricName
Exact O
F1 B-MetricName
Exact O
Match O
- O
LSTM148.9 B-MethodName
35 B-MetricValue
.2 O
48 O
.0 O
33 O
.4 O
BARB249.6 O
36 O
.1 O
48 O
.3 O
34 O
.1 O
Neural O
BoW O
Baseline O
37.6 O
25 O
.8 O
36 O
.6 O
24 O
.1 O
FastQA O
k= O
5 O
56.4 O
43.7 O
55.7 O
41 O
.9 O
FastQAExt O
k= O
5 O
56 O
.143.7 O
56.1 O
42.8 O
Table O
3 O
: O
Results O
on O
the O
NewsQA O
dataset O
. O

•VC O
- O
PCFG O
: O
The O
results O
of O
the O
visually O
- O
grounded O
compound O
PCFGs O
with O
the O
language O
modeling O
objective O
( O
Zhao O
and O
Titov O
, O
2020 O
) O
trained O
with O
30 O
non O
- O
terminals O
, O
60 O
preterminals O
, O
and O
512dimensional O
hidden O
states O
for O
the O
LSTM B-MethodName
inference O
network.•neural O
L O
- O
PCFG O
: O
The O
state O
- O
of O
- O
the O
- O
art O
neural O
LPCFG O
( O
Zhu O
et O
al O
. O
, O
2020 O
) O
, O
i.e. O
, O
the O
base O
model O
described O
in O
Section O
2 O
. O

We O
train O
the O
model O
with O
15 O
non O
- O
terminals O
, O
20 O
preterminals O
, O
and O
decrease O
the O
hidden O
states O
of O
the O
LSTM B-MethodName
inference O
network O
from O
512 O
dimensions O
to O
128 O
. O

They O
used O
a O
marginalized O
denoising O
autoencoder O
to O
obtain O
generalized O
feature O
representations O
across O
the O
source O
and O
target O
domains O
with O
a O
linear O
SVM B-MethodName
as O
the O
classiﬁcation O
model O
. O

Encoder O
The O
encoder O
generates O
a O
representation O
for O
each O
argument O
with O
an O
inner O
- O
attention O
Bidirectional O
LSTM B-MethodName
( O
Yang O
et O
al O
. O
, O
2016 O
) O
shared O
between O
the O
two O
arguments O
. O

Speciﬁcally O
, O
we O
encode O
each O
word O
in O
an O
argument O
into O
its O
word O
embeddings O
, O
which O
are O
fed O
into O
a O
BiLSTM B-MethodName
, O
to O
get O
the O
hidden O
representations O
ziusing O
a O
fully O
- O
connected O
layer O
Wcon O
top O
of O
the O
concatenated O
hidden O
states O
hi= O
[ O
~hi;~hi O
] O
. O

We O
use O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
for O
word O
embeddings O
with O
dimension O
300 O
. O

The O
encoder O
contains O
an O
inner O
- O
attention O
BiLSTM B-MethodName
with O
dimension O
50 O
, O
producing O
a O
representation O
with O
dimension O
200 O
for O
each O
example O
. O

Recently O
, O
with O
the O
success O
of O
deep O
learning O
on O
many O
NLP O
tasks O
, O
several O
pieces O
of O
work O
( O
Miwa O
and O
Bansal O
, O
2016 O
; O
Gupta O
et O
al O
. O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2017 O
) O
jointly O
model O
the O
interaction O
between O
the O
two O
subtasks O
based O
on O
neural B-MethodName
networks I-MethodName
, O
which O
they O
ﬁrstly O
apply O
RNN O
or O
CNN O
to O
encode O
the O
text O
, O
then O
treat O
entity O
recognition O
as O
a O
sequence O
labeling O
task O
and O
regard O
relation O
extraction O
as O
a O
multi O
- O
class O
classiﬁcation O
problem O
. O

It O
utilizes O
a O
pointer O
network O
( O
Vinyals O
et O
al O
. O
, O
2015 O
) O
alike O
way O
to O
generate O
the O
START O
/ O
END O
positions O
for O
all O
the O
candidate O
head O
and O
tail O
entities O
in O
a O
sequential O
fashion O
, O
which O
is O
tracked O
by O
an O
LSTM B-MethodName
decoder O
until O
it O
produces O
a O
word O
position O
beyond O
the O
text O
boundary O
. O

The O
encoder O
preprocesses O
the O
source O
text O
and O
extracts O
the O
sequence O
- O
level O
features O
using O
a O
Long O
Short O
Term O
Memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
the O
multiple O
relation O
classiﬁers O
predict O
all O
possible O
relations O
maintained O
in O
S O
, O
and O
the O
variable O
- O
length O
entity O
pair O
predictor O
sequentially O
generates O
all O
possible O
entity O
pairs O
for O
each O
possible O
relation O
type O
. O

Then O
an O
LSTM B-MethodName
is O
used O
to O
learn O
the O
token O
representation O
Xi O
for O
each O
word O
wi O
: O
Xi O
= O
LSTMencoder B-MethodName
( O
Ei;Xi 1 O
) O
; O
( O
1 O
) O
whereXi 1,Xi2Rddenotes O
word O
vectors O
for O
wordwi 1andwi O
, O
yielding O
the O
text O
representation O
matrixX= O
[ O
X1;:::Xn]2Rnd O
. O

The O
LSTM B-MethodName
maps O
the O
variable O
length O
input O
sequences O
to O
a O
ﬁxed O
- O
sized O
vector O
, O
and O
uses O
the O
last O
hidden O
state O
Xn2Rdas O
the O
representation O
vector O
of O
the O
text O
. O

In O
order O
to O
make O
better O
use O
of O
the O
features O
extracted O
by O
LSTM B-MethodName
and O
CNN O
, O
a O
concatenation O
operator O
is O
used O
between O
the O
last O
word O
representationXnin O
the O
encoder O
and O
text O
embedding O
Qfrom O
CNN O
to O
produce O
a O
fused O
vector O
H2Rm+d O
: O
H O
= O
Concat O
( O
Q;Xn O
): O
( O
4 O
) O
The O
binary O
classiﬁer O
for O
j O
- O
th O
relation O
type O
is O
shown O
as O
follows O
( O
We O
omit O
the O
bias O
bfor O
simpliﬁcation O
): O
Rj O
= O
HWH O
j O
; O
( O
5)Yj O
= O
softmax O
( O
RjWR O
j O
): O
( O
6 O
) O
As O
in O
Eq O
. O

The O
so O
- O
called O
paralleled O
mode O
draws O
the O
connections O
among O
the O
target O
relation O
, O
the O
token O
, O
and O
previous O
hidden O
state O
in O
a O
synchronous O
way O
: O
ai O
t O
= O
Watanh(WrRj+Wddt 1+WpPi O
) O
; O
( O
10 O
) O
whereis O
the O
element O
- O
wise O
multiplication O
operator O
. O
Rj2Rdisj O
- O
th O
relation O
embedding O
, O
Pi2Rd O
isi O
- O
th O
word O
of O
text O
representation O
P O
, O
dt 12Rdis O
hidden O
state O
of O
LSTM B-MethodName
decoder O
at O
time O
step O
t 1 O
which O
obtained O
by O
Eq O
. O

Once O
the O
attention O
weights B-HyperparameterName
are O
computed O
, O
the O
context O
vector O
ctis O
computed O
by O
: O
ct O
= O
nX O
i=1 O
 O
i O
tPi O
: O
( O
12 O
) O
Thencttogether O
with O
dt 1are O
fed O
into O
LSTM B-MethodName
decoder O
at O
time O
step O
t O
: O
dt O
= O
LSTMdecoder B-MethodName
( O
ct;dt 1 O
): O
( O
13 O
) O
Therefore O
, O
the O
LSTM B-MethodName
decoder O
is O
capable O
of O
tracking O
the O
state O
of O
the O
variable O
- O
length O
entity O
pair O
predictor O
. O

1http://lic2019.ccf.org.cn/kgHyperparameter O
value O
dropout B-HyperparameterName
rate I-HyperparameterName
0.5 B-HyperparameterValue
learning B-HyperparameterName
rate I-HyperparameterName
0.001 B-HyperparameterValue
batch B-HyperparameterName
size I-HyperparameterName
50 B-HyperparameterValue
hidden B-HyperparameterName
size I-HyperparameterName
of O
LSTM B-MethodName
100 B-HyperparameterValue
ﬁlter O
number O
of O
CNN O
100 O
window O
size O
of O
CNN O
3 O
head O
number O
4 O
 O
0.3 O
Table O
2 O
: O
Hyperparameter O
setting O
. O

Baseline O
: O
In O
our O
proposed O
baseline O
model O
, O
we O
design O
similar O
architecture O
with O
MrMep O
, O
in O
which O
the O
encoder O
and O
multiple O
relation O
classiﬁers O
are O
the O
same O
with O
MrMep O
, O
but O
we O
use O
Match O
- O
LSTM B-MethodName
( O
Wang O
and O
Jiang O
, O
2016 O
) O
as O
an O
implementation O
of O
variable O
- O
length O
entity O
pair O
predictor O
. O

The O
three O
main O
layers O
of O
Match O
- O
LSTM B-MethodName
in O
our O
baseline O
model O
are O
as O
follows O
: O
( O
1 O
) O
LSTM B-MethodName
Preprocessing O
Layer O
: O
we O
use O
output O
of O
the O
encoder O
as O
passage O
representation O
and O
relation O
embedding O
of O
multiple O
relation O
classiﬁers O
as O
the O
query O
representation O
; O
( O
2 O
) O
Match O
- O
LSTM B-MethodName
Layer O
: O
we O
make O
concatenation O
of O
query O
representation O
and O
each O
token O
embedding O
of O
passage O
representation O
to O
obtain O
query O
- O
aware O
token O
representation O
, O
and O
then O
fed O
it O
into O
a O
Bi O
- O
LSTM B-MethodName
layer O
; O
( O
3 O
) O
Answer O
Pointer O
Layer O
: O
this O
layer O
is O
same O
with O
Match O
- O
LSTM B-MethodName
( O
Wang O
and O
Jiang O
, O
2016 O
) O
and O
we O
adopt O
the O
sequence O
model O
to O
produce O
entity O
pairs O
. O

The O
cell O
unit O
number O
of O
LSTM B-MethodName
encoder O
and O
decoder O
is O
set O
to O
100 O
. O

The O
baseline O
model O
adopts O
Match O
- O
LSTM B-MethodName
( O
Wang O
and O
Jiang O
, O
2016 O
) O
as O
an O
implement O
of O
entity O
pair O
predictor O
, O
which O
sequentially O
aggregates O
the O
matching O
of O
the O
attention O
- O
weighted B-HyperparameterName
query O
to O
each O
token O
of O
the O
text O
. O

First O
, O
instead O
of O
using O
representations O
learnt O
by O
CNN O
, O
we O
use O
the O
last O
hidden O
state O
of O
the O
LSTM B-MethodName
encoder O
as O
text O
presentation O
, O
then O
feed O
it O
directly O
to O
the O
multiple O
binary O
classiﬁer O
( O
MrMep O
w/o O
CNN O
) O
. O

Second O
, O
instead O
of O
applying O
multi O
- O
head O
attention O
, O
we O
use O
the O
hidden O
state O
at O
each O
time O
step O
of O
the O
LSTM B-MethodName
encoder O
to O
represent O
each O
token O
, O
which O
is O
directly O
fed O
into O
the O
following O
triplet O
attention O
to O
extract O
entity O
pairs O
( O
MrMep O
w/o O
Multi O
- O
head O
) O
. O

9The O
FES O
representation O
space O
can O
be O
seen O
as O
entity O
and O
sentiment O
infused O
frame O
embedding O
space O
. O
CBOW B-MethodName
SG O
LBL O
Perplexity O
FES O
- O
LM O
133.8 O
135.8 O
126.0 O
Narrative O
Cloze O
Test O
( O
Recall@30 B-MetricName
) O
FES O
- O
LM O
38.9 O
37.3 O
43.2 O
FES O
- O
LM O
- O
Entity O
35.3 O
33.1 O
38.4 O
FES O
- O
LM O
- O
Sentiment O
34.9 O
32.8 O
36.3 O
Table O
3 O
: O
Quality O
comparison O
of O
neural O
language O
models O
. O

LBL O
outperforms O
CBOW B-MethodName
and O
SG O
on O
both O
tests O
. O

Here O
, O
we O
choose O
the O
Skip O
- O
Gram O
( O
SG O
) O
model O
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
and O
Continuous O
- O
Bag O
- O
of O
- O
Words O
( O
CBOW B-MethodName
) O
model O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
for O
comparison O
with O
the O
LBL O
model O
. O

We O
utilize O
the O
word2vec O
package O
to O
implement O
both O
SG O
and O
CBOW B-MethodName
. O

We O
set O
the O
context O
window O
size O
to O
be O
10 O
for O
SG O
and O
5 O
for O
CBOW B-MethodName
. O

In O
particular O
, O
we O
evaluate O
on O
identifying O
the O
correct O
sense O
of O
discourse O
connectives O
( O
both O
explicit O
and O
implicit O
10We O
also O
tried O
Neural O
- O
LSTM B-MethodName
( O
Pichotta O
and O
Mooney O
, O
2016a O
) O
and O
context2vec O
( O
Melamud O
et O
al O
. O
, O
2016 O
) O
model O
, O
but O
we O
can O
not O
get O
better O
results O
. O

We O
use O
an O
LSTM B-MethodName
encoder O
( O
300 O
hidden B-HyperparameterName
units I-HyperparameterName
) O
and O
decode O
with O
an O
LSTM B-MethodName
of O
the O
same O
size O
. O

Since O
it O
is O
operated O
on O
the O
word O
level O
, O
we O
use O
pre O
- O
trained O
300 O
- O
dimensional O
GloVe B-MethodName
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
keep O
them O
ﬁxed O
during O
training O
. O

Speciﬁcally O
, O
we O
use O
estimated O
human O
attention O
derived O
from O
eyetracking O
corpora O
to O
regularize O
attention O
functions O
in O
recurrent O
neural B-MethodName
networks I-MethodName
. O

Since O
learning O
good O
attention O
functions O
for O
recurrent O
neural B-MethodName
networks I-MethodName
requires O
large O
volumes O
of O
data O
( O
Zoph O
et O
al O
. O
, O
2016 O
; O
Britz O
et O
al O
. O
, O
2017 O
) O
, O
and O
errors O
in O
attention O
are O
known O
to O
propagate O
to O
classiﬁcation O
decisions O
( O
Alkhouli O
et O
al O
. O
, O
2016 O
) O
, O
we O
explore O
the O
idea O
of O
using O
human O
attention O
, O
as O
estimated O
from O
eye O
- O
tracking O
corpora O
, O
as O
an O
inductive O
bias O
on O
such O
attention O
functions O
. O

In O
other O
words O
, O
in O
multi O
- O
task O
learning O
, O
we O
optimize O
each O
task O
for O
a O
ﬁxed O
number O
of O
parameter O
updates O
( O
or O
mini O
- O
batches O
) O
before O
switching O
to O
the O
next O
task O
( O
Dong O
et O
al O
. O
, O
2015 O
) O
; O
in O
our O
case O
, O
we O
optimize O
for O
a O
target O
task O
( O
for O
a O
ﬁxed O
number O
of O
updates O
) O
, O
then O
improve O
our O
attention O
function O
based O
on O
human O
attention O
( O
for O
a O
ﬁxed O
number O
of O
updates O
) O
, O
then O
return O
to O
optimizing O
for O
the O
target O
task O
and O
continue O
iterating.2.1 O
Model O
Our O
architecture O
is O
a O
bidirectional O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
that O
encodes O
word O
representations O
xiinto O
forward O
and O
backward O
representations O
, O
and O
into O
combined O
hidden O
states O
hi(of O
slightly O
lower O
dimensionality O
) O
at O
every O
timestep O
. O

In O
fact O
, O
our O
model O
is O
a O
hierarchical O
model O
whose O
word O
representations O
are O
concatenations O
of O
the O
output O
of O
character O
- O
level O
LSTMs B-MethodName
and O
word O
embeddings O
, O
following O
Plank O
et O
al O
. O
( O
2016 O
) O
, O
but O
we O
ignore O
the O
character O
- O
level O
part O
of O
our O
architecture O
in O
the O
equations O
below O
: O
  O
! O
hi O
= O
LSTM B-MethodName
( O
xi;  !hi 1 O
) O
( O
1 O
) O
 O
 hi O
= O
LSTM B-MethodName
( O
xi O
; O
  hi+1 O
) O
( O
2 O
) O
ehi= O
[ O
  O
! O
hi O
; O
 hi O
] O
( O
3 O
) O
hi= O
tanh(Whehi+bh O
) O
( O
4 O
) O
The O
ﬁnal O
( O
reduced O
) O
hidden O
state O
is O
sometimes O
used O
as O
a O
sentence O
representation O
s O
, O
but O
we O
instead O
use O
attention O
to O
compute O
sby O
multiplying O
dynamically O
predicted O
attention O
weights B-HyperparameterName
with O
the O
hidden O
states O
for O
each O
time O
step O
. O

We O
are O
, O
to O
the O
best O
of O
our O
knowledge O
, O
the O
ﬁrst O
to O
use O
gaze O
to O
inform O
attention O
functions O
in O
recurrent O
neural B-MethodName
networks I-MethodName
. O
Human O
- O
inspired O
attention O
functions O
Ibraheem O
et O
al O
. O
( O
2017 O
) O
, O
however O
, O
uses O
optimal O
attention O
to O
simulate O
human O
attention O
in O
an O
interactive O
machine O
translation O
scenario O
, O
and O
Britz O
et O
al O
. O
( O
2017 O
) O
limit O
attention O
to O
a O
local O
context O
, O
inspired O
by O
ﬁndings O
in O
studies O
of O
human O
reading O
. O

Rei O
and O
Søgaard O
( O
2018 O
) O
use O
auxiliary O
data O
to O
regularize O
attention O
functions O
in O
recurrent O
neural B-MethodName
networks I-MethodName
; O
not O
from O
psycholinguistics O
data O
, O
but O
using O
small O
amounts O
of O
task O
- O
speciﬁc O
, O
token O
- O
level O
annotations O
. O

In O
a O
different O
context O
, O
Das O
et O
al O
. O
( O
2017 O
) O
investigated O
whether O
humans O
attend O
to O
the O
same O
regions O
as O
neural B-MethodName
networks I-MethodName
solving O
visual O
question B-TaskName
answering I-TaskName
problems O
. O

8 O
Conclusion O
We O
have O
shown O
that O
human O
attention O
provides O
a O
useful O
inductive O
bias O
on O
machine O
attention O
in O
recurrent O
neural B-MethodName
networks I-MethodName
for O
sequence O
classiﬁcation O
problems O
. O

In O
section O
3 O
, O
we O
describe O
the O
word O
similarity O
features O
used O
to O
train O
the O
SVM B-MethodName
model O
. O

We O
discuss O
the O
results O
by O
analyzing O
the O
effect O
of O
features O
on O
SVM B-MethodName
model O
, O
initial O
 O
values O
, O
and O
missing O
data O
on O
the O
performance O
of O
clustering O
in O
section O
7 O
. O

The O
authors O
trained O
a O
SVM B-MethodName
classiﬁer O
based O
on O
string O
similarity O
features O
to O
calculate O
word O
distances O
between O
all O
word O
pairs O
for O
a O
meaning B-MetricName
. O

3 O
Word O
similarity O
model O
In O
this O
section O
, O
we O
present O
the O
word O
similarity O
features O
used O
to O
train O
our O
SVM B-MethodName
model O
at O
the O
binarytask O
of O
classifying O
if O
a O
word O
pair O
is O
cognate O
or O
noncognate O
. O

String O
similarity O
features O
We O
use O
length O
normalized O
edit O
distance O
, O
number O
of O
common O
bigrams O
, O
common O
preﬁx O
length O
, O
individual O
word O
lengths O
, O
and O
absolute O
difference O
between O
the O
word O
lengths O
as O
features O
for O
training O
a O
SVM B-MethodName
classiﬁer O
( O
Hauer O
and O
Kondrak O
, O
2011 O
) O
. O

Point O
- O
wise O
Mutual O
Information O
( O
PMI O
) O
We O
include O
PMI O
weighted B-HyperparameterName
Needleman O
- O
Wunsch O
( O
Needleman O
and O
Wunsch O
, O
1970 O
) O
word O
similarity O
score B-MetricName
( O
J¨ager O
, O
2013 B-MetricValue
) O
as O
an O
additional O
feature O
for O
training O
the O
SVM B-MethodName
classiﬁer O
. O

SCA O
We O
experimented O
with O
SCA O
( O
Sound O
Class O
Based O
Phonetic O
Alignment O
) O
word O
distance O
score B-MetricName
( O
List O
et O
al O
. O
, O
2016 B-MetricValue
) O
as O
an O
additional O
feature O
in O
our O
SVM B-MethodName
model O
and O
found O
that O
inclusion O
of O
this O
feature O
improves O
the O
performance O
of O
cognate O
clustering O
systems O
. O

Our O
SVM B-MethodName
model O
is O
implemented O
using O
scikit O
- O
learn O
( O
Buitinck O
et O
al O
. O
, O
2013 O
) O
. O

The O
trained O
SVM B-MethodName
model O
is O
then O
used O
to O
predict O
the O
conﬁdence O
scores B-MetricName
for O
all O
the O
word O
pairs O
having O
the O
same O
meaning B-MetricName
. O

We O
extracted O
a O
total O
of O
48,389 O
cognate O
pairs O
( O
positive O
) O
and O
51,452 O
non O
- O
cognate O
pairs O
( O
negative O
) O
for O
training O
our O
SVM B-MethodName
model O
. O

7.1 O
Feature O
ablation O
To O
ascertain O
which O
word O
similarity O
features O
contribute O
the O
most O
to O
the O
performance O
of O
the O
ns O
- O
CRP O
algorithm O
, O
we O
trained O
three O
simpler O
SVM B-MethodName
models O
and O
evaluated O
the O
quality O
of O
the O
inferred O
clusters O
using O
these O
models O
. O

As O
future O
work O
, O
we O
plan O
to O
include O
language O
relatedness O
as O
features O
into O
SVM B-MethodName
training O
and O
also O
train O
the O
SVM B-MethodName
classiﬁer O
in O
an O
unsupervised O
fashion O
using O
the O
sd O
- O
CRP O
algorithms O
. O

SUDA O
– O
Alibaba O
at O
MRP O
2019 O
: O
Graph O
- O
Based O
Models O
with O
BERT B-MethodName
. O

External O
resources O
like O
BERT B-MethodName
are O
found O
helpful O
for O
all O
frameworks O
except O
AMR O
. O

Borrowing O
the O
idea O
of O
Dozat O
and O
Manning O
( O
2018 O
) O
, O
we O
encode O
the O
input O
word O
sequence O
with O
BiLSTMs B-MethodName
and O
predict O
the O
edges O
and O
labels O
between O
words O
with O
two O
MLPs O
. O

BERT B-MethodName
. O

We O
observe O
that O
using O
BERT B-MethodName
as O
our O
extra O
inputs O
is O
effective O
for O
all O
the O
models O
, O
except O
AMR O
. O

It O
is O
also O
interesting O
that O
BERTlarge B-MethodName
does O
not O
produce O
more O
improvements O
over O
BERT B-MethodName
- O
base O
based O
our O
preliminary O
experiments O
. O

xi O
= O
eword O
iechar O
i O
where O
echar O
i O
is O
extracted O
by O
the O
bidirectional O
character O
- O
level O
LSTM B-MethodName
( O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

They O
are O
then O
fed O
into O
a O
multilayer O
bidirectional O
wordlevel O
LSTM B-MethodName
to O
get O
contextualized O
representations O
. O

Given O
an O
input O
sentence O
X O
= O
fx0 O
; O
x1; O
; O
xng O
, O
each O
word O
xiis O
mapped O
into O
a O
dense O
vector O
xiand O
fed O
into O
bidirectional O
LSTM B-MethodName
layers O
. O

The O
top O
- O
layer O
output O
of O
each O
position O
are O
used O
to O
represent O
the O
span O
as O
ri;j= O
( O
fj fi)(bi bj O
) O
where O
fiandbiare O
the O
output O
vectors O
of O
the O
toplayer O
forward O
and O
backward O
LSTMs B-MethodName
. O

They O
share O
the O
same O
inputs O
and O
LSTM B-MethodName
encoder O
with O
the O
constituent O
parser O
under O
the O
MTL O
framework O
. O

Note O
that O
, O
we O
add O
one O
pseudo O
node O
like O
the O
” O
ROOT O
” O
node O
in O
the O
dependency O
parsing O
, O
so O
that O
we O
can O
get O
the O
top O
node O
of O
the O
graph O
( O
which O
is O
pointed O
by O
the O
” O
ROOT O
” O
node).For O
the O
edge O
prediction O
model O
, O
a O
multi O
- O
layered O
BiLSTM B-MethodName
is O
ﬁrstly O
used O
to O
encode O
the O
original O
input O
sentence O
, O
so O
that O
we O
get O
the O
representation O
of O
each O
word O
. O

We O
use O
hk(k21;2 O
; O
: O
: O
: O
; O
l O
) O
to O
denote O
the O
hidden O
states O
of O
BiLSTM B-MethodName
encoders O
of O
our O
model O
components O
, O
where O
lis O
the O
number O
of O
the O
BiLSTM B-MethodName
layers O
. O

The O
concept O
identiﬁcation O
model O
chooses O
a O
concept O
cconditioned O
on O
the O
aligned O
word O
kbased O
on O
the O
BiLSTM B-MethodName
state O
hk O
, O
which O
is O
deﬁned O
as O
P(cjhk O
; O
wk O
) O
. O

The O
alignment O
model O
is O
only O
used O
in O
training O
, O
and O
thus O
it O
only O
depends O
on O
the O
BiLSTM B-MethodName
hidden O
states O
h1;h2 O
; O
: O
: O
: O
; O
hnand O
the O
concept O
list O
c1 O
; O
c2 O
; O
: O
: O
: O
; O
cm O
. O

Given O
the O
concepts O
listc O
, O
the O
alignment O
model O
encodes O
cwith O
a O
BiLSTM B-MethodName
encoder O
, O
which O
deﬁnes O
the O
state O
of O
ciasgi O
, O
i21;2 O
; O
: O
: O
: O
; O
n O
. O
A O
globally O
- O
normalized O
alignmentmodel O
is O
used O
, O
which O
is O
deﬁned O
as O
Q O
( O
ajc;R;w O
) O
, O
and O
the O
score B-MetricName
of O
the O
alignment O
aiis O
also O
computed O
via O
a O
bilinear O
scorer B-MetricName
. O

3.1 O
Model O
Parameters O
In O
both O
SDP O
and O
UCCA O
tasks O
, O
we O
use O
100dimensional O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
as O
pretrained O
embedding O
and O
random O
initialized O
50dimensional O
char O
embedding O
. O

We O
also O
utilize O
the O
BERT B-MethodName
embeddings O
extracted O
from O
the O
last O
four O
transformer O
layers O
. O

The O
ﬁnal O
BERT B-MethodName
representation O
is O
their O
normalized O
weighted B-HyperparameterName
sum O
, O
which O
is O
concatenated O
with O
the O
word O
embeddings O
. O

InEDS O
task O
, O
external O
resources O
we O
use O
are O
: O
1 O
) O
word O
embeddings O
pre O
- O
trained O
with O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
on O
the O
Gigaword O
corpus O
for O
Chinese O
; O
and O
2 O
) O
BERT10(Devlin B-MethodName
et O
al O
. O
, O
2018 O
) O
, O
recently O
proposed O
effective O
deep O
contextualized O
word O
representation O
. O

InAMR O
task O
, O
we O
randomly O
choose O
the O
samples O
of O
the O
training O
data O
according O
to O
the O
proportion O
of O
9https://cogcomp.org/page/software O
_ O
view O
/ O
NETagger O
10We O
generate O
our O
pre O
- O
trained O
BERT B-MethodName
embedding O
with O
the O
released O
model O
in O
https://github.com/google-research/bert.DM O
PSD O
UCCA O
EDS O
AMR O
P O
R O
F1 B-MetricName
P O
R O
F1 B-MetricName
P O
R O
F1 B-MetricName
P O
R O
F1 B-MetricName
P O
R O
F1 B-MetricName
Node O
prediction O
labels O
89 B-MetricValue
90 O
89.26 O
83 O
85 O
83.80 O
# O
# O
# O
91 O
91 O
91.20 O
82 O
81 O
81.53 O
properties O
90 O
91 O
90.65 O
83 O
85 O
84.43 O
# O
# O
# O
89 O
91 O
89.72 O
77 O
73 O
74.96 O
anchors O
# O
# O
# O
# O
# O
# O
96 O
94 O
95.02 O
95 O
95 O
94.86 O
# O
# O
# O
Edge O
prediction O
top O
91 O
91 O
91.01 O
96 O
79 O
86.49 O
100 O
100 O
99.56 O
90 O
90 O
89.94 O
63 O
63 O
62.86 O
edges O
88 O
90 O
88.69 O
74 O
75 O
74.41 O
70 O
65 O
67.74 O
90 O
90 O
89.66 O
64 O
60 O
61.78 O
attributes O
# O
# O
# O
# O
# O
# O
54 O
33 O
40.80 O
# O
# O
# O
# O
# O
# O
Overall O
all O
90 O
92 O
91.26 O
84 O
86 O
84.81 O
81 O
76 O
78.43 O
92 O
92 O
91.85 O
73 O
70 O
71.72 O
Table O
2 O
: O
Experiment O
results O
on O
the O
provided O
test O
data O
from O
the O
shared O
task O
. O

We O
have O
also O
attempted O
to O
integrate O
BERT B-MethodName
representations O
into O
the O
basic O
model O
input O
, O
but O
it O
did O
not O
bring O
signiﬁcant O
improvements O
. O

After O
utilizing O
BERT B-MethodName
embeddings O
, O
the O
results O
rise O
to O
94.06 O
and O
88.79 O
respectively O
. O

As O
the O
improvements O
are O
not O
very O
signiﬁcant O
, O
we O
will O
explore O
better O
ways O
of O
integrating O
BERT B-MethodName
in O
the O
future O
. O

The O
MRP O
F1 B-MetricName
scores B-MetricName
on O
our O
deveplopment O
data O
are O
79.80 B-MetricValue
and O
73.41 O
, O
w/o O
BERT B-MethodName
respectively O
. O

Unlike O
SDP O
, O
the O
result O
is O
signiﬁcantly O
improved O
after O
using O
BERT B-MethodName
embeddings O
. O

External O
knowledge O
including O
BERT B-MethodName
and O
pretrained O
word O
embedding O
are O
effective O
; O
and O
postprocessing O
listed O
in O
can O
achieve O
about O
1 O
% O
improvement O
on O
our O
split O
dev O
/ O
test O
data O
. O

BERT B-MethodName
is O
also O
employed O
to O
boost O
the O
performance O
( O
except O
AMR O
) O
. O

A O
non O
- O
projective O
greedy O
dependency O
parser O
with O
bidirectional O
LSTMs B-MethodName
. O

The O
bidirectional O
LSTM B-MethodName
approach O
by O
Kiperwasser O
and O
Goldberg O
( O
2016 O
) O
is O
used O
to O
train O
a O
greedy O
parser O
with O
a O
dynamic O
oracle O
to O
mitigate O
error O
propagation O
. O

Here O
, O
we O
present O
such O
an O
implementation O
for O
the O
Covington O
( O
2001 O
) O
algorithm O
using O
bidirectional O
long O
short O
- O
term O
memory O
networks O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
which O
is O
the O
main O
contribution O
of O
this O
paper O
. O

They O
both O
rely O
on O
bidirectional O
LSTM B-MethodName
’s O
( O
BILSTM B-MethodName
’s O
) O
. O

Let O
LSTM B-MethodName
( O
x)be O
an O
abstraction O
of O
a O
standard O
long O
short O
- O
term O
memory O
network O
that O
processes O
the O
sequence O
x= O
[ O
x1, O
... O
,x|x| O
] O
, O
then O
a O
BILSTM B-MethodName
encoding O
of O
its O
ith O
element O
, O
BILSTM B-MethodName
( O
x O
, O
i)can O
be O
2Including O
some O
additional O
capabilities O
that O
we O
included O
especially O
for O
BIST O
- O
COVINGTON O
. O

3It O
might O
turn O
out O
that O
for O
some O
treebank O
/ O
language O
some O
of O
this O
information O
is O
not O
available O
, O
in O
which O
case O
the O
unavailable O
elements O
are O
considered O
as O
empty O
lists.deﬁned O
as O
: O
BILSTM B-MethodName
( O
x O
, O
i)=LSTM B-MethodName
( O
x1 O
: O
i) O
◦ O
LSTM B-MethodName
( O
x|x|:i O
) O
In O
the O
case O
of O
multilayer O
BILSTM B-MethodName
’ O
S(BIST O
parsers O
allow O
it O
) O
, O
given O
nlayers O
, O
the O
output O
of O
the O
BILSTM B-MethodName
mis O
fed O
as O
input O
to O
BILSTM B-MethodName
m+1 O
. O

From O
the O
BILSTM B-MethodName
network O
we O
take O
a O
hidden O
vector O
h O
, O
which O
can O
contain O
the O
output O
hidden O
vectors O
for O
: O
thexleftmost O
words O
in O
β O
, O
the O
rightmost O
yofλ1 O
, O
and O
thezleftmost O
and O
vrightmost O
words O
in O
λ2 O
. O

The O
size O
of O
the O
output O
of O
the O
stacked O
BILSTM B-MethodName
was O
set O
to O
512 O
. O

The O
number O
of O
BILSTM B-MethodName
layers O
is O
set O
to O
2 O
. O

⋆indicates O
the O
model O
was O
also O
trained O
with O
external O
word O
embeddings O
( O
E).•indicates O
the O
BILSTM B-MethodName
output O
dimension O
was O
256 O
. O

6 O
Conclusion O
This O
paper O
presented O
BIST O
- O
COVINGTON O
, O
a O
bidirectional O
LSTM B-MethodName
implementation O
of O
the O
Covington O
( O
2001 O
) O
algorithm O
for O
non O
- O
projective O
transitionbased O
dependency O
parsing O
. O

In O
this O
work O
, O
we O
compare O
the O
performance O
of O
an O
extensively O
pretrained O
model O
, O
OpenAI B-MethodName
GPT2 B-MethodName
- O
117 O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
to O
a O
state O
- O
of O
- O
the O
- O
art O
neural O
story O
generation O
model O
( O
Fan O
et O
al O
. O
, O
2018 O
) O
. O

We O
ﬁnd O
that O
although O
GPT2 B-MethodName
- O
117 O
conditions O
more O
strongly O
on O
context O
, O
is O
more O
sensitive O
to O
ordering O
of O
events O
, O
and O
uses O
more O
unusual O
words O
, O
it O
is O
just O
as O
likely O
to O
produce O
repetitive O
and O
under O
- O
diverse O
text O
when O
using O
likelihood O
- O
maximizing O
decoding O
algorithms O
. O

1 O
Introduction O
In O
2018 O
, O
large O
- O
scale O
neural O
models O
such O
as O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
OpenAI B-MethodName
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
) O
emerged O
as O
a O
dominant O
approach O
in O
NLP O
. O

In O
particular O
, O
the O
OpenAI B-MethodName
GPT2 B-MethodName
language O
model O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
achieves O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
several O
language O
modeling O
benchmarks O
, O
even O
in O
a O
zero O
- O
shot O
setting O
. O

While O
GPT2 B-MethodName
’s O
performance O
as O
a O
language O
model O
is O
undeniable O
, O
its O
performance O
as O
a O
text O
generator O
is O
much O
less O
clear O
. O

In O
this O
work O
, O
we O
perform O
an O
in O
- O
depth O
study O
of O
the O
properties O
of O
text O
generated O
by O
GPT2 B-MethodName
- O
117 O
( O
the O
smallest O
version O
of O
GPT2 B-MethodName
) O
in O
the O
context O
of O
story O
generation O
. O

To O
our O
knowledge O
, O
this O
work O
is O
the O
ﬁrst O
comprehensive O
analysis O
of O
the O
characteristics O
of O
GPT2 B-MethodName
- O
generated O
text O
. O

GPT2 B-MethodName
- O
117 O
GPT2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
is O
a O
large O
Transformer O
language O
model O
trained O
on O
WebText O
, O
a O
diverse O
corpus O
of O
internet O
text O
( O
not O
publicly O
released O
) O
containing O
over O
8 O
million O
documents O
equalling O
40 O
GB O
of O
text O
in O
total O
. O

The O
fullsize O
GPT2 B-MethodName
model O
, O
which O
has O
1542 O
million O
parameters O
, O
obtains O
state O
- O
of O
- O
the O
- O
art O
results O
on O
a O
variety O
of O
language O
modeling O
and O
other O
Natural O
Language O
Understanding O
benchmarks O
. O

At O
the O
time O
of O
our O
experiments O
, O
Radford O
et O
al O
. O
had O
only O
released O
the O
smallest O
of O
the O
models O
, O
known O
as O
GPT2117.2This B-MethodName
model O
, O
which O
we O
use O
for O
our O
experiments O
, O
has O
12 O
layers O
and O
117 O
million O
parameters O
. O

Like O
the O
full O
- O
size O
GPT2 B-MethodName
model O
, O
it O
has O
a O
vocabulary O
of O
50,257 O
byte O
- O
pair O
- O
encoding O
( O
BPE O
) O
tokens O
. O

At O
the O
time O
of O
writing O
, O
the O
full O
- O
size O
GPT2 B-MethodName
model O
has O
not O
been O
publicly O
released.generate O
stories O
, O
and O
Radford O
et O
al O
. O
show O
impressive O
samples O
of O
generated O
text O
( O
primarily O
from O
the O
full O
- O
size O
GPT2 B-MethodName
model O
) O
for O
k= O
40 O
. O

Due O
to O
the O
limited O
context O
size O
of O
GPT2117 B-MethodName
, O
we O
additionally O
exclude O
( O
prompt O
, O
story O
) O
examples O
that O
are O
longer O
than O
1024 O
BPE O
tokens O
when O
concatenated O
. O

For O
comparability O
with O
GPT2 B-MethodName
- O
117 O
, O
we O
evaluate O
the O
Fusion O
Model O
on O
WritingPrompts-1024 O
( O
see O
Table O
1 O
) O
, O
obtaining O
perplexities O
similar O
to O
those O
reported O
by O
Fan O
et O
al O
. O
on O
the O
full O
WritingPrompts O
dataset O
. O

GPT2 B-MethodName
- O
117 O
In O
order O
for O
the O
model O
to O
condition O
on O
prompts O
and O
generate O
stylistically O
correct O
stories O
, O
we O
ﬁnetune O
GPT2 B-MethodName
- O
117 O
on O
WritingPrompts1024.3We O
frame O
WritingPrompts O
as O
a O
language O
modeling O
task O
, O
representing O
the O
prompt O
and O
story O
as O
a O
single O
sequence O
separated O
by O
a O
delimiter O
token O
. O

We O
compute O
the O
word O
- O
level O
perplexity B-MetricName
of O
the O
ﬁnetuned O
GPT2 B-MethodName
- O
117 B-MetricValue
on O
the O
WritingPrompts-1024 O
dataset O
. O

The O
ﬁnetuned O
GPT2 B-MethodName
- O
117 O
obtains O
a O
test O
set O
wordperplexity B-MetricName
of O
31.544 O
– O
six O
points O
lower O
than O
the O
Fusion O
Model O
. O

Generation O
settings O
For O
both O
models O
, O
we O
generate O
stories O
using O
top- O
ksampling O
, O
obtaining O
1000 O
stories O
( O
from O
1000 O
different O
test O
set O
prompts O
) O
for O
3We O
use O
the O
PyTorch O
re O
- O
implementation O
of O
GPT2 B-MethodName
- O
117 O
available O
at O
https://github.com/huggingface/ O
pytorch O
- O
transformers O
4This O
is O
similar O
to O
other O
GPT2 B-MethodName
- O
117 O
WritingPrompts O
ﬁnetuning O
experiments O
( O
Mao O
et O
al O
. O
, O
2019 O
; O
Ziegler O
et O
al O
. O
, O
2019).Model O
Valid O
ppl O
Test O
ppl O
Fusion O
Model O
37.05 O
37.54 O
GPT2 B-MethodName
- O
117 O
31.13 O
31.54 O
Table O
1 O
: O
Word O
- O
level O
perplexities O
on O
WritingPrompts1024 O
for O
the O
Fusion O
Model O
and O
ﬁnetuned O
GPT2 B-MethodName
- O
117 O
. O

We O
ﬁnd O
that O
GPT2 B-MethodName
- O
117 O
scores B-MetricName
80.16 O
% O
on O
this O
task O
, O
while O
the O
Fusion O
Model O
scores B-MetricName
39.8 O
% O
.5Random O
chance O
scores B-MetricName
10 O
% O
. O

that O
GPT2 B-MethodName
- O
117 O
conditions O
on O
the O
prompt O
much O
more O
strongly O
than O
the O
Fusion O
Model O
. O

For O
all O
nandk O
, O
we O
ﬁnd O
that O
GPT2 B-MethodName
- O
117 O
has O
a O
higher O
overlap O
( O
i.e. O
copies O
more O
from O
the O
prompt O
) O
than O
the O
Fusion O
Model O
– O
see O
Figure O
6 O
in O
the O
Appendix O
. O

Furthermore O
, O
for O
k O
< O
100 O
, O
the O
GPT2 B-MethodName
- O
117 O
overlap O
is O
generally O
much O
higher O
than O
human O
levels O
. O

Both O
these O
phenomena O
can O
be O
seen O
in O
Table O
2 O
, O
where O
, O
for O
k= O
10 O
, O
GPT2 B-MethodName
- O
117 O
copies O
words O
such O
as O
queen O
more O
often O
than O
both O
the O
Fusion O
Model O
and O
the O
human O
- O
written O
story O
. O

Sentences O
are O
represented O
by O
the O
embedding O
method O
of O
Arora O
et O
al O
. O
( O
2017 O
) O
– O
a O
weighted B-HyperparameterName
average B-MetricName
of O
the O
GloVe B-MethodName
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 B-MetricValue
) O
of O
the O
words O
, O
with O
the O
ﬁrst O
principal O
component O
removed O
. O

As O
shown O
in O
Figure O
1 O
, O
we O
ﬁnd O
a O
similar O
pattern O
as O
for O
n O
- O
gram O
similarity O
: O
GPT2 B-MethodName
- O
117 O
generates O
sentences O
that O
are O
more O
similar O
to O
the O
prompt O
than O
the O
Fusion O
Model O
for O
all O
k O
, O
and O
both O
models O
’ O
prompt O
similarity O
decreases O
as O
kincreases O
. O

Using O
the O
spaCy O
named O
entity O
recognizer,6we O
measure O
the O
prompt O
entity O
usage O
rate O
, O
which O
is O
the O
percentage O
of O
all O
prompt O
named O
enti6https://spacy.ioties O
that O
appear O
in O
the O
story.7As O
shown O
in O
Figure O
7 O
in O
the O
Appendix O
, O
we O
ﬁnd O
that O
GPT2 B-MethodName
- O
117 O
uses O
more O
of O
the O
prompt O
named O
entities O
than O
the O
Fusion O
Model O
( O
as O
well O
as O
more O
named O
entities O
overall O
) O
, O
but O
both O
models O
use O
fewer O
named O
entities O
than O
humans O
when O
kis O
less O
than O
vocabulary O
size O
. O

These O
patterns O
can O
be O
seen O
in O
Table O
2 O
: O
GPT2117 B-MethodName
uses O
the O
prompt O
entities O
Queen O
andEngland O
whereas O
the O
Fusion O
Model O
does O
not O
( O
for O
either O
k O
) O
, O
and O
GPT2 B-MethodName
- O
117 O
uses O
speciﬁc O
time O
entities O
such O
as O
Thursday O
and3:26 O
PM O
. O

Conclusion O
In O
this O
section O
, O
we O
found O
that O
GPT2 B-MethodName
- O
117 O
conditions O
on O
the O
prompt O
much O
more O
strongly O
than O
the O
Fusion O
Model O
– O
a O
result O
which O
holds O
both O
in O
language O
modeling O
and O
generation O
settings O
. O

The O
latter O
result O
supports O
Radford O
et O
al O
. O
’s O
informal O
observation O
that O
GPT2 B-MethodName
has O
a O
‘ O
chameleonlike O
’ O
ability O
to O
‘ O
adapt O
to O
the O
style O
and O
content O
of O
the O
conditioning O
text’.8We O
speculate O
that O
GPT2 B-MethodName
- O
117 O
’s O
stronger O
conditioning O
ability O
may O
derive O
from O
its O
Transformer O
decoder O
architecture O
, O
whose O
powerful O
self O
- O
attention O
is O
used O
for O
story O
- O
prompt O
attention O
. O

Lastly O
, O
we O
note O
that O
very O
strong O
prompt O
- O
conditioning O
is O
not O
always O
a O
good O
thing O
– O
GPT2 B-MethodName
- O
117 O
often O
generates O
stories O
that O
copy O
too O
much O
or O
too O
literally O
from O
the O
prompt O
when O
kis O
small O
( O
this O
can O
be O
seen O
in O
Figure O
6 O
in O
the O
Appendix O
) O
. O

GPT2 B-MethodName
- O
117 O
shows O
this O
pattern O
more O
strongly O
, O
indicating O
greater O
use O
of O
context O
. O

Both O
models O
perform O
well O
on O
this O
task O
– O
the O
Fusion O
Model O
has O
an O
error O
rate O
of O
3.44 O
% O
and O
GPT2 B-MethodName
- O
117 O
an O
error O
rate O
of O
2.17 O
% O
. O

This O
36.92 O
% O
error O
reduction O
indicates O
that O
GPT2 B-MethodName
- O
117 O
is O
more O
sensitive O
to O
ordering O
of O
events O
. O

GPT2117 B-MethodName
assigns O
a O
much O
lower O
rank O
to O
the O
ﬁrst O
few O
swap O
positions O
( O
i.e. O
, O
rates O
them O
more O
probable O
) O
than O
the O
later O
positions O
. O

This O
shows O
that O
both O
models O
are O
less O
sensitive O
to O
out O
- O
of O
- O
order O
sentences O
that O
occur O
at O
the O
beginning O
of O
the O
text O
, O
than O
those O
occurring O
later.9The O
stronger O
pattern O
for O
GPT2 B-MethodName
- O
117 O
may O
be O
due O
to O
its O
stronger O
context O
conditioning O
( O
as O
shown O
in O
Section O
4 O
) O
– O
thus O
becoming O
more O
sensitive O
as O
context O
increases O
. O

However O
, O
even O
for O
the O
ﬁrst O
three O
swaps O
, O
GPT2 B-MethodName
- O
117 O
is O
more O
accurate O
than O
the O
Fusion O
Model O
at O
distinguishing O
the O
swapped O
text O
from O
the O
original O
. O

GPT2 B-MethodName
- O
117 O
is O
trained O
on O
45more O
data O
than O
the O
Fusion O
Model O
, O
but O
is O
similarly O
repetitive O
for O
all O
k. O

Though O
GPT2117 B-MethodName
has O
a O
slightly O
higher O
distinct- O
nthan O
the O
Fusion O
Model O
for O
most O
values O
of O
k O
, O
the O
difference O
is O
negligible O
compared O
to O
the O
inﬂuence O
of O
k. O
Wemake O
three O
conclusions O
from O
these O
patterns O
: O
( O
1 O
) O
Our O
ﬁndings O
support O
Holtzman O
et O
al O
. O
’s O
observation O
that O
repetition O
is O
strongly O
related O
to O
choice O
of O
decoding O
algorithm O
, O
and O
that O
likelihood O
maximizing O
algorithms O
( O
such O
as O
top- O
ksampling O
with O
low O
k O
) O
are O
a O
primary O
cause O
of O
repetition O
. O

( O
3 O
) O
Repetition O
is O
unlikely O
to O
be O
solved O
by O
more O
pretraining O
data O
alone O
– O
even O
though O
GPT2117 B-MethodName
is O
trained O
on O
45 O
times O
as O
much O
data O
as O
the O
Fusion O
Model O
, O
it O
produces O
text O
that O
is O
almost O
equally O
repetitive O
( O
for O
equal O
k O
) O
. O

As O
shown O
in O
Figure O
12 O
in O
the O
Appendix O
, O
word O
rareness O
is O
primarily O
governed O
by O
k O
– O
however O
, O
GPT2 B-MethodName
- O
117 O
has O
a O
lower O
mean B-MetricName
log O
unigram O
probability O
( O
i.e. O
, O
uses O
more O
rare O
words O
) O
than O
the O
Fusion O
Model O
for O
all O
equal O
values O
of O
k2 O
. O

This O
can O
be O
seen O
for O
example O
in O
Table O
2 O
, O
where O
GPT2117 B-MethodName
generates O
rarer O
words O
such O
as O
idleandcopiousfork= O
1000 O
. O

GPT2 B-MethodName
- O
117 O
also O
generates O
fewer O
stopwords O
than O
the O
Fusion O
Model O
, O
for O
all O
equal O
k. O
GPT2 B-MethodName
- O
117 O
’s O
slightly O
higher O
rare O
word O
usage O
( O
compared O
to O
the O
Fusion O
Model O
) O
might O
be O
explained O
by O
: O
( O
1 O
) O
its O
BPE O
encoding O
, O
which O
allows O
it O
to O
generate O
new O
words O
, O
not O
just O
those O
in O
a O
ﬁxed O
vocabulary O
; O
( O
2 O
) O
pretraining O
on O
a O
large O
amount O
of O
diverse O
text O
, O
allowing O
it O
to O
learn O
to O
produce O
a O
greater O
variety O
of O
words O
; O
( O
3 O
) O
stronger O
conditioning O
on O
the O
prompt O
as O
described O
in O
Section O
4 O
– O
which O
may O
inject O
more O
rareness O
into O
the O
generated O
text O
. O

Although O
GPT2 B-MethodName
- O
117 O
generates O
more O
rare O
words O
and O
is O
very O
slightly O
less O
repetitive O
than O
the O
Fusion O
Model O
, O
the O
difference O
is O
small O
compared O
to O
the O
effect O
of O
k O
, O
indicating O
that O
training O
data O
alone O
is O
unlikely O
to O
solve O
these O
problems O
. O

Both O
models O
( O
especially O
GPT2 B-MethodName
- O
117 O
) O
closely O
ﬁt O
the O
human O
POS B-TaskName
distribution O
as O
kapproaches O
vocabulary O
size.12This O
implies O
that O
, O
as O
with O
lexical O
diversity O
, O
the O
models O
have O
no O
difﬁculty O
ﬁtting O
the O
statistical O
distribution O
of O
human O
syntax O
. O

However O
, O
we O
note O
that O
as O
kincreases O
, O
lexical O
diversity O
reaches O
human O
level O
sooner O
than O
syntactic O
diversity O
– O
for O
example O
, O
GPT2 B-MethodName
- O
117 O
’s O
lexical O
distinct-3 O
reaches O
human O
level O
at O
k= O
600 O
( O
Figure O
9c O
) O
, O
but O
its O
POS B-TaskName
distinct11For O
example O
, O
the O
sentence O
I O
like O
cats O
has O
the O
POS B-TaskName
bigrams O
PRONOUN O
VERB O
and O
VERB O
NOUN O
. O

Despite O
the O
training O
, O
[ O
... O
] O
0.51.00 O
50 O
100 O
150 O
Token O
index0.00.51.0Token O
probability(c)GPT2 B-MethodName
- O
117 O
( O
k= O
2):I’ve O
always O
been O
a O
man O
of O
the O
people O
. O

The O
human O
text O
probabilities O
are O
measured O
with O
respect O
to O
the O
Fusion O
Model O
, O
but O
similar O
patterns O
hold O
for O
GPT2 B-MethodName
- O
117 O
. O

When O
generating O
with O
top- O
ksampling O
, O
probability O
increases O
faster O
, O
especially O
for O
smaller O
k. O
This O
plot O
is O
for O
the O
Fusion O
Model O
; O
similar O
patterns O
hold O
for O
GPT2 B-MethodName
- O
117 O
. O

Interestingly O
however O
, O
the O
same O
is O
not O
true O
for O
GPT2117 B-MethodName
, O
which O
converges O
to O
a O
story O
probability O
that O
islower O
than O
the O
probability O
it O
assigns O
the O
human O
stories O
. O

This O
means B-MetricName
that O
under O
full O
( O
non O
- O
truncated O
) O
sampling O
, O
the O
Fusion O
Model O
produces O
text O
that O
isequally O
surprising O
( O
to O
itself O
) O
as O
the O
WritingPrompts O
stories O
, O
whereas O
GPT2 B-MethodName
- O
117 O
produces O
text O
that O
is O
more O
surprising O
to O
itself O
. O

Explaining O
this O
observation O
is O
an O
open O
question O
– O
we O
speculate O
that O
GPT2 B-MethodName
- O
117 O
’s O
WebText O
pretraining O
may O
cause O
it O
to O
generate O
( O
under O
high O
k O
) O
text O
in O
a O
style O
or O
genre O
that O
is O
less O
predictable O
than O
WritingPrompts O
stories O
. O

We O
ﬁnd O
that O
, O
for O
the O
same O
k O
, O
GPT2 B-MethodName
- O
117 O
tends O
to O
generate O
more O
concrete O
words O
than O
the O
Fusion O
Model O
, O
and O
that O
for O
both O
models O
, O
concreteness O
converges O
to O
approximately O
human O
levels O
as O
kincreases O
. O

10 O
Conclusions O
The O
effect O
of O
massive O
pretraining O
In O
this O
study O
, O
we O
ﬁnd O
that O
GPT2 B-MethodName
- O
117 O
is O
a O
better O
story O
generation O
model O
than O
the O
Fusion O
Model O
in O
several O
speciﬁc O
ways O
: O
it O
conditions O
much O
more O
strongly O
on O
the O
provided O
context O
, O
is O
more O
sensitive O
to O
correct O
ordering O
of O
events O
, O
and O
generates O
text O
that O
is O
more O
contentful O
( O
using O
more O
rare O
words O
, O
concrete O
words O
, O
and O
named O
entities O
) O
. O

However O
, O
we O
ﬁnd O
that O
in O
other O
aspects O
, O
GPT2117 B-MethodName
performs O
no O
better O
than O
the O
Fusion O
Model O
: O
when O
kis O
small O
, O
the O
models O
generate O
text O
that O
is O
equally O
lexically O
under O
- O
diverse O
, O
syntactically O
under O
- O
complex O
, O
and O
repetitive O
– O
with O
a O
tendency O
to O
fall O
into O
a O
snowball O
effect O
of O
increasing O
overconﬁdence O
. O

She O
never O
talked O
to O
me O
about O
anything O
[ O
... O
] O
GPT2 B-MethodName
- O
117 O
( O
k= O
10 O
) O
:* O
” O
So O
what O
are O
we O
going O
to O
do O
? O

[ O
... O
] O
GPT2 B-MethodName
- O
117 O
( O
k= O
1000 O
): O
It O
was O
an O
odd O
occasion O
for O
the O
Queen O
of O
England O
to O
meet O
with O
her O
. O

A O
copious O
amount O
of O
curious O
glances O
from O
around O
the O
room O
until O
[ O
... O
] O
Table O
2 O
: O
A O
prompt O
and O
human O
story O
from O
the O
dataset O
, O
plus O
the O
models O
’ O
top- O
kgenerated O
stories O
, O
for O
two O
values O
of O
k. O
cally O
well O
- O
trained O
to O
match O
human O
text O
for O
these O
metrics O
) O
, O
nor O
caused O
by O
too O
little O
training O
data O
( O
as O
these O
problems O
are O
not O
improved O
by O
GPT2 B-MethodName
- O
117 O
’s O
extensive O
pretraining O
) O
. O

However O
, O
we O
also O
uncovered O
some O
less O
obvious O
characteristics O
of O
low- O
kgenerated O
text O
: O
compared O
to O
human O
- O
written O
text O
, O
it O
tends O
to O
copy O
more O
from O
the O
provided O
context O
( O
particularly O
GPT2 B-MethodName
- O
117 O
) O
; O
it O
contains O
more O
verbs O
and O
pronouns O
but O
fewer O
nouns O
and O
adjectives O
; O
its O
nouns O
are O
more O
concrete O
but O
its O
verbs O
are O
less O
concrete O
; O
and O
it O
uses O
a O
smaller O
range O
of O
syntactic O
patterns O
( O
a O
phenomenon O
that O
ca O
n’t O
be O
entirely O
attributed O
to O
n O
- O
gram O
repetition O
) O
. O

Limitations O
of O
this O
study O
This O
study O
uses O
only O
the O
smallest O
version O
of O
GPT2 B-MethodName
. O

It O
is O
likely O
that O
the O
larger O
versions O
of O
GPT2 B-MethodName
may O
exhibit O
stronger O
statistical O
differences O
for O
the O
metrics O
we O
examine O
. O

Speciﬁcally O
, O
we O
adopt O
parameter O
sharing O
in O
our O
bi O
- O
directional O
LSTM B-MethodName
model O
in O
the O
hope O
that O
the O
shared O
parameters O
will O
store O
information O
beneﬁcial O
to O
multiple O
tasks O
. O

3 O
Model O
In O
this O
section O
, O
we O
ﬁrst O
brieﬂy O
describe O
bidirectional O
LSTMs B-MethodName
. O

We O
then O
present O
our O
models O
which O
use O
bi O
- O
LSTMs B-MethodName
for O
ﬁne O
- O
grained O
Arabic O
POS B-TaskName
tagging4 O
. O

3.1 O
Bi O
- O
directional O
LSTMs B-MethodName
Recurrent O
neural B-MethodName
networks I-MethodName
( O
RNN O
) O
( O
Elman O
, O
1990 O
) O
are O
a O
class O
of O
neural B-MethodName
networks I-MethodName
that O
are O
capable O
of O
handling O
sequences O
of O
any O
length O
. O

Long O
short O
term O
memory O
( O
LSTM B-MethodName
) O
networks O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
address O
this O
issue O
by O
inprc3 O
) O
, O
and O
one O
enclitic O
( O
enc O
) O
. O

A O
bi O
- O
directional O
LSTM B-MethodName
network O
( O
Graves O
and O
Schmidhuber O
, O
2005 O
) O
is O
an O
extension O
of O
an O
LSTM B-MethodName
network O
that O
allows O
modeling O
of O
past O
and O
future O
dependencies O
in O
arbitrary O
- O
length O
input O
sequences O
. O

The O
output O
vector O
htof O
a O
bi O
- O
LSTM B-MethodName
is O
calculated O
by O
concatenating O
the O
output O
vector O
of O
the O
forward O
directional O
LSTM B-MethodName
that O
reads O
the O
sequence O
from O
beginning O
to O
end O
with O
the O
output O
vector O
of O
the O
backward O
directional O
LSTM B-MethodName
that O
reads O
the O
sequence O
in O
the O
reverse O
direction O
. O

3.2 O
Independent O
Prediction O
Model O
For O
our O
baseline O
method O
, O
we O
use O
a O
model O
that O
independently O
predicts O
each O
morphosyntactic O
category O
using O
bi O
- O
LSTMs B-MethodName
. O

The O
character O
- O
level O
embedding O
is O
computed O
by O
concatenating O
hidden O
states O
of O
the O
character O
- O
level O
forward O
LSTM B-MethodName
and O
those O
of O
the O
backward O
LSTM B-MethodName
as O
depicted O
in O
the O
bottom O
part O
of O
Figure O
1 O
. O

The O
vector O
representation O
rtis O
then O
fed O
into O
our O
bi O
- O
LSTM B-MethodName
model O
, O
giving O
the O
forward O
hidden O
state− O
→htand O
the O
backward O
hidden O
state← O
−ht O
. O

Speciﬁcally O
, O
we O
use O
parameter O
sharing O
in O
the O
hidden B-HyperparameterName
layers I-HyperparameterName
of O
our O
bi O
- O
LSTM B-MethodName
model O
so O
that O
we O
can O
generate O
a O
uniﬁed O
model O
that O
can O
carry O
information O
beneﬁcial O
to O
each O
task O
. O

The O
output O
vectors O
of O
the O
bi O
- O
LSTMs B-MethodName
are O
fed O
into O
multiple O
output O
layers O
, O
each O
performing O
a O
corresponding O
morphosyntactic O
prediction O
task O
. O

4.1 O
Experimental O
Setup O
4.1.1 O
Implementation O
Details O
We O
implement O
all O
bi O
- O
LSTM B-MethodName
models O
using O
the O
DyNet O
library O
( O
Neubig O
et O
al O
. O
, O
2017 O
) O
. O

CamelParser O
is O
an O
improved O
version O
of O
the O
previous O
state O
- O
ofthe O
- O
art O
tagger O
MADAMIRA O
( O
Pasha O
et O
al O
. O
, O
2014 O
) O
, O
which O
ranks O
the O
possible O
analyses O
provided O
by O
a O
morphological O
analyzer O
using O
SVMs B-MethodName
. O

5 O
Related O
Work O
Diab O
et O
al O
. O
( O
2004 O
) O
proposed O
a O
segmentationbased O
approach O
, O
in O
which O
they O
tag O
each O
cliticsegmented O
token O
using O
SVMs B-MethodName
. O

Another O
related O
line O
of O
work O
tackles O
sequential O
labeling O
problems O
using O
multi O
- O
task O
learning O
with O
deep O
neural B-MethodName
networks I-MethodName
and O
investigates O
situations O
where O
multi O
- O
task O
learning O
leads O
to O
improvements O
in O
performance O
( O
Søgaard O
and O
Goldberg O
, O
2016 O
; O
Bingel O
and O
Søgaard O
, O
2017 O
; O
Mart O
´ O
ınez O
Alonso O
and O
Plank O
, O
2017 O
) O
. O

Shen O
et O
al O
. O
( O
2016 O
) O
proposed O
an O
approach O
in O
which O
they O
encode O
a O
sequence O
of O
possible O
morphosyntactic O
tags O
provided O
by O
a O
morphological O
analyzer O
using O
bi O
- O
directional O
LSTMs B-MethodName
. O

Our O
model O
predicts O
predicate O
- O
argument O
dependencies O
relying O
on O
states O
of O
a O
bidirectional O
LSTM B-MethodName
encoder O
. O

Neural O
SRL B-TaskName
models O
instead O
exploited O
feature O
induction O
capabilities O
of O
neural B-MethodName
networks I-MethodName
, O
largely O
eliminating O
the O
need O
for O
complex O
hand O
- O
crafted O
features O
. O

This O
suggests O
that O
our O
LSTM B-MethodName
model O
can O
largely O
implicitly O
capture O
syntactic O
information O
, O
and O
this O
information O
can O
, O
to O
a O
large O
extent O
, O
substitute O
treebank O
syntax O
. O

Similarly O
to O
the O
span O
- O
based O
model O
of O
Zhou O
and O
Xu O
( O
2015 O
) O
we O
use O
bidirectional O
LSTMs B-MethodName
to O
encode O
sentences O
and O
rely O
on O
their O
states O
when O
predicting O
arguments O
of O
each O
predicate.1We O
predict O
semantic O
dependency O
edges O
between O
predicates O
and O
arguments O
relying O
on O
LSTM B-MethodName
states O
corresponding O
to O
the O
predicate O
and O
the O
argument O
positions O
( O
i.e. O
both O
edge O
endpoints O
) O
. O

Consequently O
, O
as O
standard O
for O
dependency O
SRL B-TaskName
, O
we O
ignore O
this O
subtask O
in O
further O
discussion.87.7F1which B-MetricName
compares O
favorable O
to O
the O
best O
local O
model O
( O
86.7 O
% O
F O
1for O
PathLSTM B-MethodName
( O
Roth O
and O
Lapata O
, O
2016 O
) O
) O
and O
approaches O
the O
best O
results O
overall O
( O
87.9 O
% O
for O
an O
ensemble O
of O
3 O
PathLSTM B-MethodName
models O
with O
a O
reranker O
on O
top O
) O
. O

For O
English O
, O
this O
constitutes O
a O
2.4 O
% O
absolute O
improvement O
over O
the O
comparable O
previous O
model O
( O
75.3 O
% O
for O
the O
local O
PathLSTM B-MethodName
) O
and O
substantially O
outperforms O
any O
previous O
method O
( O
76.5 O
% O
for O
the O
ensemble O
of O
3 O
PathLSTMs B-MethodName
) O
. O

In O
order O
to O
identify O
and O
classify O
arguments O
, O
we O
propose O
a O
model O
composed O
of O
three O
components O
: O
•a O
word O
representation O
component O
that O
from O
a O
wordwiin O
a O
sentence O
wbuild O
a O
word O
representationxi O
; O
•a O
Bidirectional O
LSTM B-MethodName
( O
BiLSTM B-MethodName
) O
encoder O
which O
takes O
as O
input O
the O
word O
representation O
xiand O
provide O
a O
dynamic O
representation O
of O
the O
word O
and O
its O
context O
in O
a O
sentence O
; O
•a O
classiﬁer O
which O
takes O
as O
an O
input O
the O
BiLSTM B-MethodName
representation O
of O
the O
candidate O
argument O
and O
the O
BiLSTM B-MethodName
representation O
of O
the O
predicate O
to O
predict O
the O
role O
associated O
to O
the O
candidate O
argument O
. O

2.2 O
Bidirectional O
LSTM B-MethodName
encoder O
One O
of O
the O
most O
effective O
ways O
to O
model O
sequences O
are O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNN O
) O
( O
Elman O
, O
1990 O
) O
, O
more O
precisely O
their O
gated O
versions O
, O
for O
example O
, O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
networks O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

Formally O
, O
we O
can O
deﬁne O
an O
LSTM B-MethodName
as O
a O
function O
LSTMθ(x1 B-MethodName
: O
i)that O
takes O
as O
input O
the O
sequence O
x1 O
: O
i O
and O
returns O
a O
hidden O
state O
hi∈Rdh O
. O

Bidirectional O
LSTMs B-MethodName
make O
use O
of O
two O
LSTMs B-MethodName
: O
one O
for O
the O
forward O
pass O
, O
and O
another O
for O
the O
backward O
pass O
, O
LSTMFandLSTMB B-MethodName
, O
respectively O
. O

In O
this O
way O
the O
concatenation O
of O
forward O
and O
backward O
LSTM B-MethodName
states O
encodes O
both O
left O
and O
right O
contexts O
of O
a O
word O
, O
BiLSTM B-MethodName
( O
x1 O
: O
n O
, O
i O
) O
= O
LSTMF(x1 B-MethodName
: O
i) O
◦ O
LSTMB(xn B-MethodName
: O
i O
) O
. O

In O
this O
work O
we O
stackklayers O
of O
bidirectional O
LSTMs B-MethodName
, O
each O
layer O
takes O
the O
lower O
layer O
as O
its O
input O
. O

2.3 O
Predicate O
- O
speciﬁc O
encoding O
As O
we O
will O
show O
in O
the O
ablation O
studies O
in O
Section O
3 O
, O
encoding O
a O
sentence O
with O
a O
bidirectional O
LSTM B-MethodName
in O
one O
shot O
and O
using O
it O
to O
predict O
the O
entire O
semantic O
dependency O
graph O
does O
not O
result O
in O
competitive O
SRL B-TaskName
performance O
. O

This O
contrasts O
with O
most O
other O
applications O
of O
LSTM B-MethodName
encoders O
( O
for O
example O
, O
in O
syntactic O
parsing O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
; O
Cross O
and O
Huang O
, O
2016 O
) O
or O
machine O
translation O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
) O
, O
where O
sentences O
are O
typically O
encoded O
once O
and O
then O
used O
to O
predict O
the O
entire O
structured O
output O
( O
e.g. O
, O
a O
syntactic O
tree O
or O
a O
target O
sentence O
) O
. O

In O
this O
way O
, O
sentences O
with O
more O
than O
one O
predicate O
will O
be O
re O
- O
encoded O
by O
bidirectional O
LSTMs B-MethodName
multiple O
times.2.4 O
Role O
classiﬁer O
Our O
goal O
is O
to O
predict O
and O
label O
arguments O
for O
a O
given O
predicate O
. O

2.4.1 O
Basic O
role O
classiﬁer O
The O
basic O
role O
classiﬁer O
takes O
the O
hidden O
state O
of O
the O
top O
- O
layer O
bidirectional O
LSTM B-MethodName
corresponding O
to O
the O
considered O
word O
at O
position O
iand O
uses O
it O
to O
estimate O
the O
probability O
of O
the O
role O
r. O
Though O
we O
experimented O
with O
multilayer O
perceptrons O
, O
we O
obtained O
the O
best O
results O
with O
a O
simple O
log O
- O
linear O
model O
: O
p(r|vi O
, O
p)∝exp(Wrvi O
) O
, O
( O
1 O
) O
whereviis O
the O
hidden O
state O
calculated O
by O
BiLSTM B-MethodName
( O
x1 O
: O
n O
, O
i),prefers O
to O
the O
predicate O
and O
the O
symbol∝signiﬁes O
proportionality O
. O

Moreover O
, O
predicting O
dependency O
edges O
relying O
on O
LSTM B-MethodName
states O
of O
endpoints O
was O
shown O
effective O
in O
the O
context O
of O
syntactic O
dependency O
3Since O
they O
considered O
span O
- O
based O
SRL B-TaskName
, O
they O
used O
BIO O
encoding O
( O
Ramshaw O
and O
Marcus O
, O
1995 O
) O
and O
ensured O
the O
consistency O
of O
B O
, O
I O
and O
O O
labels O
with O
a O
1 O
- O
order O
Markov O
CRF O
. O

Semantic O
role O
labeler O
dw(English O
word O
embeddings O
) O
100 O
dw(Chinese O
word O
embeddings O
) O
128 O
dw(Czech O
word O
embeddings O
) O
300 O
dw(Spanish O
word O
embeddings O
) O
300 O
dpos(POS B-TaskName
embeddings O
) O
16 O
dl(lemma O
embeddings O
) O
100 O
dh(LSTM B-MethodName
hidden O
states O
) O
512 O
dr(role O
representation O
) O
128 O
d O
/ O
prime O
l(output O
lemma O
representation O
) O
128 O
k(BiLSTM B-MethodName
depth O
) O
4 O
α(word O
dropout B-HyperparameterName
) O
.25 B-HyperparameterValue
learning B-HyperparameterName
rate I-HyperparameterName
.01 B-HyperparameterValue
Table O
1 O
: O
Hyperparameter O
values O
. O

For O
English O
, O
our O
model O
signiﬁcantly O
outperformed O
all O
the O
local O
counter O
- O
parts O
( O
i.e. O
, O
models O
which O
do O
not O
perform O
global O
inference O
) O
on O
the O
in O
- O
domain O
tests O
( O
see O
Table O
2 O
) O
with O
87.6 O
% O
F O
1for O
our O
model O
vs. O
86.7 O
% O
for O
PathLSTM B-MethodName
( O
Roth O
and O
Lapata O
, O
2016 O
) O
. O

When O
compared O
with O
global O
models O
, O
our O
model O
performed O
on O
- O
par O
with O
the O
state O
- O
of O
- O
the O
- O
art O
global O
version O
of O
PathLSTM B-MethodName
. O

For O
Chinese O
( O
Table O
4 O
) O
, O
the O
proposed O
model O
outperformed O
the O
best O
previous O
model O
( O
PathLSTM B-MethodName
) O
with O
an O
improvement O
of O
1.8 O
% O
F O
1 O
. O

Finally O
, O
for O
Spanish O
( O
Table O
6 O
) O
, O
our O
system O
, O
though O
again O
achieved O
competitive O
results O
, O
did O
not O
outperform O
the O
best O
CoNLL-2009 O
model O
and O
yielded O
results O
very O
similar O
to O
those O
of O
PathLSTM B-MethodName
. O

the O
ensemble O
of O
PathLSTMs B-MethodName
) O
. O

Since O
our O
model O
does O
not O
use O
predicted O
parse O
trees O
and O
instead O
relies O
on O
the O
ability O
of O
LSTMs B-MethodName
to O
capture O
long O
distance O
dependencies O
and O
syntactic O
phenomena O
( O
Linzen O
et O
al O
. O
, O
2016 O
) O
, O
it O
is O
less O
brittle O
in O
this O
setting O
. O

This O
is O
somewhat O
surprising O
given O
that O
one O
- O
pass O
LSTM B-MethodName
encoders O
performed O
competitively O
for O
syntactic O
dependencies O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
; O
Cross O
and O
Huang O
, O
2016 O
) O
and O
suggests O
that O
major O
differences O
between O
the O
two O
problems O
require O
the O
use O
of O
different O
modeling O
approaches O
. O

We O
compared O
our O
approach O
to O
the O
global O
PathLSTM B-MethodName
model O
: O
PathLSTM B-MethodName
is O
a O
natural O
reference O
point O
as O
it O
is O
the O
most O
accurate O
previous O
model O
, O
exploits O
similar O
modeling O
and O
representation O
techniques O
( O
e.g. O
, O
word O
embeddings O
, O
LSTMs B-MethodName
) O
but O
, O
unlike O
our O
approach O
, O
relies O
on O
predicted O
syntax O
. O

Contrary O
to O
our O
expectations O
, O
syntactically O
- O
driven O
and O
global O
PathLSTM B-MethodName
was O
weaker O
for O
longer O
distances O
. O

Ours O
PathLSTM B-MethodName
Freq O
. O

( O
% O
) O
VerbalA0 O
90.5 O
90.4 O
15 O
% O
A1 O
92.0 O
91.8 O
21 O
% O
A2 O
80.3 O
80.2 O
5 O
% O
AM- O
* O
77.9 O
77.0 O
16 O
% O
All O
86.4 O
86.1 O
61 O
% O
Ours O
PathLSTM B-MethodName
Freq O
. O

In O
contrast O
, O
PathLSTM B-MethodName
will O
have O
access O
only O
to O
the O
single O
( O
top O
scoring O
) O
parse O
tree O
and O
, O
thus O
, O
may O
be O
more O
brittle O
. O

In O
Table O
8 O
, O
we O
break O
down O
F O
1results O
on O
the O
English O
test O
set O
into O
verbal O
and O
nominal O
predicates O
, O
and O
again O
compare O
our O
results O
with O
PathLSTM B-MethodName
. O

For O
verbal O
predicates O
, O
our O
model O
slightly O
outperformed O
PathLSTM B-MethodName
in O
core O
roles O
( O
A0 O
- O
2 O
) O
and O
performed O
much O
better O
( O
0.9 O
% O
F O
1 O
) O
in O
predicting O
modiﬁers O
( O
AM- O
* O
) O
. O

This O
is O
very O
surprising O
as O
some O
information O
about O
modiﬁers O
is O
actually O
explicitly O
encoded O
in O
syntactic O
dependen O
- O
System O
P O
R O
F O
1VerbalPathLSTM B-MethodName
93.4 O
87.8 O
90.5 O
Ours O
92.7 O
89.8 O
91.2 O
System O
P O
R O
F O
1Nom O
. O
PathLSTM B-MethodName
92.0 O
83.9 O
87.8 O
Ours O
89.4 O
86.6 O
88.0 O
Table O
9 O
: O
Argument O
recognition O
results O
broken O
down O
into O
verbal O
and O
nominal O
predicates O
. O

cies O
exploited O
by O
PathLSTM B-MethodName
( O
e.g. O
, O
the O
syntactic O
dependency O
TMP O
is O
predictive O
of O
the O
modiﬁer O
role O
AM O
- O
TMP O
) O
. O

For O
nominal O
predicates O
, O
PathLSTM B-MethodName
was O
more O
accurate O
than O
our O
model O
for O
all O
roles O
excluding O
A0 O
. O

On O
one O
hand O
, O
Figure O
4 O
shows O
that O
PathLSTM B-MethodName
is O
more O
accurate O
on O
roles O
one O
syntactic O
arc O
away O
from O
the O
nominal O
predicate O
. O

This O
again O
suggests O
that O
PathLSTM B-MethodName
struggles O
with O
harder O
cases O
. O
System O
Example O
Manual O
Most O
of O
the O
stock O
[ O
A2]selling O
[ O
\A2]pressure O
came O
[ O
A0]from O
[ O
\A0]Wall O
Street O
professionals O
. O

PathLSTM B-MethodName
Most O
of O
the O
stock O
[ O
A2]selling O
[ O
\A2]pressure O
came O
from O
Wall O
Street O
professionals O
. O

Consequently O
, O
we O
hypothesized O
that O
our O
model O
should O
be O
weaker O
than O
PathLSTM B-MethodName
in O
recognizing O
arguments O
but O
should O
be O
on O
par O
with O
PathLSTM B-MethodName
in O
assigning O
their O
roles O
. O

In O
contrast O
, O
PathLSTM B-MethodName
does O
not O
make O
any O
mistake O
with O
the O
labeling O
of O
the O
argument O
selling O
but O
fails O
to O
recognize O
from O
as O
an O
argument O
. O

FitzGerald O
et O
al O
. O
( O
2015 O
) O
used O
handcrafted O
features O
within O
an O
MLP O
for O
calculating O
potentials O
of O
a O
CRF O
model O
; O
Roth O
and O
Lapata O
( O
2016 O
) O
extended O
the O
features O
of O
a O
non O
- O
neural O
SRL B-TaskName
model O
with O
LSTM B-MethodName
representations O
of O
syntactic O
paths O
between O
arguments O
and O
predicates O
; O
Lei O
et O
al O
. O
( O
2015 O
) O
relied O
on O
low O
- O
rank O
tensor O
factorization O
that O
captured O
interactions O
between O
arguments O
, O
predicate O
, O
their O
syntactic O
path O
and O
semantic O
roles O
; O
while O
Collobert O
et O
al O
. O
( O
2011 O
) O
and O
Foland O
and O
Martin O
( O
2015 O
) O
used O
convolutional O
networks O
as O
sentence O
encoder O
and O
a O
CRF O
as O
a O
role O
classiﬁer O
, O
both O
approaches O
employed O
a O
rich O
set O
of O
features O
as O
input O
of O
the O
convolutional O
encoder O
. O

Finally O
, O
Swayamdipta O
et O
al O
. O
( O
2016 O
) O
jointly O
modeled O
syntactic O
and O
semantic O
structures O
; O
they O
extended O
one O
of O
the O
earliest O
neural O
approaches O
for O
SRL B-TaskName
( O
Henderson O
et O
al O
. O
, O
2008 O
; O
Titov O
et O
al O
. O
, O
2009 O
; O
Gesmundo O
et O
al O
. O
, O
2009 O
) O
, O
with O
more O
sophisticated O
modeling O
techniques O
, O
for O
example O
, O
using O
LSTMs B-MethodName
instead O
of O
vanilla O
RNNs O
. O

It O
would O
be O
interesting O
to O
see O
if O
explicit O
modeling O
of O
latent O
syntax O
is O
also O
beneﬁcial O
when O
used O
in O
conjunction O
with O
LSTMs B-MethodName
. O

The O
state O
- O
of O
- O
the O
- O
art O
model O
leverages O
the O
autoregressive O
decoder O
such O
as O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
and O
Long O
- O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
to O
generate O
the O
target O
sequence O
( O
representing O
the O
parse O
) O
from O
left O
to O
right O
. O

We O
use O
the O
RoBERTa B-MethodName
base O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
( O
we O
refer O
to O
this O
model O
as O
RoBERTa B-MethodName
) O
as O
our O
query O
encoder O
to O
fairly O
compare O
with O
the O
previous O
method O
. O

This O
model O
has O
the O
same O
architecture O
as O
BERT B-MethodName
base O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
with O
several O
modiﬁcations O
during O
pretraining O
. O

RoBERTa B-MethodName
is O
also O
trained O
with O
longer O
sentences O
and O
larger O
batch B-HyperparameterName
sizes I-HyperparameterName
with O
more O
training O
samples O
. O

For O
the O
multilingual O
zeroshot O
and O
few O
- O
shot O
semantic B-TaskName
parsing I-TaskName
task O
, O
we O
use O
XLM O
- O
R O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
and O
multilingual O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
which O
are O
trained O
on O
text O
for O
more O
than O
100 O
languages O
. O

4.2 O
Baseline O
Models O
Monolingual O
Baselines O
: O
For O
monolingual O
experiments O
, O
we O
select O
the O
algorithms O
reported O
in O
Rongali O
et O
al O
. O
( O
2020 O
) O
as O
baselines O
for O
ATIS O
and O
SNIPS.TOP O
ATIS O
SNIPSMethodEM B-MetricName
IC O
EM B-MetricName
IC O
EM B-MetricName
IC O
Joint O
BiRNN O
( O
Hakkani O
- O
T O
¨ur O
et O
al O
. O
, O
2016 B-MetricValue
) O
- O
- O
80.70 O
92.60 O
73.20 O
96.90 O
Attention O
BiRNN O
( O
Liu O
and O
Lane O
, O
2016 O
) O
- O
- O
78.90 O
91.10 O
74.10 O
96.70 O
Slot O
Gated O
Full O
Attention O
( O
Goo O
et O
al O
. O
, O
2018 O
) O
- O
- O
82.20 O
93.60 O
75.50 O
97.00 O
CapsuleNlU O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
- O
- O
83.40 O
95.00 O
80.90 O
97.30 O
SR(S)+ELMO+SVMRank B-MethodName
( O
Gupta O
et O
al O
. O
, O
2018 O
) O
83.93 O
- O
- O
- O
- O
SR(E)+ELMO+SVMRank B-MethodName
( O
Gupta O
et O
al O
. O
, O
2018 O
) O
87.25 O
- O
- O
- O
- O
AR O
- O
S2S O
- O
PTR O
( O
paper O
) O
( O
Rongali O
et O
al O
. O
, O
2020 O
) O
86.67 O
98.13 O
87.12 O
97.42 O
87.14 O
98.00 O
AR O
- O
S2S O
- O
PTR O
( O
reproduce O
) O
( O
Rongali O
et O
al O
. O
, O
2020 O
) O
85.67 O
98.17 O
88.91 O
97.09 O
90.71 O
98.43 O
IT O
- O
S2S O
- O
PTR O
( O
 O
= O
1 O
) O
86.74 O
98.47 O
89.14 O
97.31 O
91.00 O
98.43 O
IT O
- O
S2S O
- O
PTR O
( O
input O
- O
src O
, O
uniform O
) O
85.41 O
98.71 O
- O
- O
- O
Table O
1 O
: O
Exact O
Match O
and O
Intent O
Classiﬁcation O
scores B-MetricName
for O
on O
the O
test O
set O
. O

For O
all O
monolingual O
experiments O
, O
we O
use O
RoBERTa B-MethodName
as O
our O
pretrained O
encoder O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O

Cross O
lingual O
Baselines O
: O
For O
multilingual O
experiments O
( O
zero O
- O
shot O
and O
few O
- O
shot O
) O
, O
we O
use O
a O
sequence O
labelling O
model O
based O
on O
multilingual O
BERT B-MethodName
and O
an O
autoregressive O
sequence O
to O
sequence O
model O
( O
Rongali O
et O
al O
. O
, O
2020 O
) O
as O
our O
baseline O
. O

4.3 O
Results O
4.3.1 O
Model O
Conﬁguration O
We O
use O
the O
pretrained O
RoBERTa B-MethodName
and O
mBERT B-MethodName
as O
the O
encoder O
for O
our O
model O
. O

By O
using O
EM B-MetricName
, O
the O
entire O
parsing O
sequence O
predicted O
by O
the O
model O
has O
to O
match O
the O
reference O
sequence O
, O
since O
it O
’s O
not O
easy O
to O
apply O
the O
F1 B-MetricName
score B-MetricName
or O
semantic O
error O
rate O
( O
Thomson O
et O
al O
. O
, O
2012 B-MetricValue
) O
toen O
es O
pt O
de O
fr O
hi O
zh O
jaavg O
IT O
- O
S2S O
- O
PTR O
87.23 O
50.06 O
39.30 O
39.46 O
46.78 O
11.42 O
28.72 O
12.60 O
32.69 O
AR O
- O
S2S O
- O
PTR O
86.83 O
40.72 O
33.38 O
34.00 O
17.22 O
7.45 O
23.74 O
10.04 O
23.77 O
mBERT B-MethodName
86.33 O
48.46 O
38.56 O
39.12 O
42.98 O
15.22 O
21.89 O
23.29 O
32.78 O
Table O
3 O
: O
Zero O
- O
shot O
cross O
lingual O
EM B-MetricName
scores B-MetricName
by O
our O
approach O
( O
IT O
) O
, O
autoregressive O
baseline O
( O
AR O
) O
and O
sequence O
labeling O
baseline O
( O
mBERT B-MethodName
) O
. O

We O
also O
test O
on O
the O
multilingual O
TOP O
dataset O
( O
Xia O
and O
Monti O
, O
2021 O
) O
, O
which O
extends O
the O
TOP O
datasets O
1Chinese O
is O
tokenized O
at O
the O
character O
level O
in O
mBERT B-MethodName
, O
while O
Katakana O
/ O
Hiragana O
are O
tokenized O
with O
whitespace O
. O

Deep O
learning O
algorithms O
boost O
the O
performance O
of O
semantic B-TaskName
parsing I-TaskName
, O
especially O
using O
recurrent O
neural B-MethodName
networks I-MethodName
( O
Liu O
and O
Lane O
, O
2016 O
; O
Hakkani O
- O
T O
¨ur O
et O
al O
. O
, O
2016 O
) O
. O

Other O
architectures O
are O
also O
explored O
, O
such O
as O
convolutional O
neural B-MethodName
networks I-MethodName
( O
Kim O
, O
2014 O
) O
and O
capsule O
networks O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
. O

First O
, O
we O
implement O
the O
algorithm O
of O
 O
metaphor O
detection O
based O
on O
a O
Support O
Vector O
 O
Machine O
( O
SVM B-MethodName
) O
 O
classifier O
. O

Second O
, O
we O
apply O
 O
the O
SVM B-MethodName
classifier O
to O
predict O
the O
senses O
, O
either O
li O
teral O
or O
metaphoric O
, O
of O
each O
verb O
in O
Baidu O
Baike O
 O
corpus O
, O
which O
has O
1,543,669 O
million O
entries O
and O
 O
7.6 O
billion O
tokens.1 O
We O
then O
measure O
the O
semantic O
 O
similarities O
among O
different O
radical O
groups O
by O
the O
 O
vector O
representation O
 O
according O
to O
each O
sense O
of O
 O
each O
character O
. O

SVM B-MethodName
are O
well O
performed O
in O
higher O
dimension O
, O
 O
particularly O
when O
 O
targeted O
instances O
only O
hold O
a O
 O
small O
portion O
in O
a O
dataset O
. O

Since O
our O
design O
focu O
ses O
on O
the O
effectiveness O
of O
syn O
tactic O
conditions O
in O
 O
metaphor O
detection O
rather O
than O
on O
a O
classifier O
, O
we O
 O
choose O
SVM B-MethodName
with O
linear O
kernel O
as O
our O
classifier O
for O
its O
linear O
binary O
classification O
and O
use O
LibSVM B-MethodName
 O
( O
Chang O
and O
Lin O
, O
2011 O
) O
as O
the O
SVM B-MethodName
tool O
. O

4.2 O
Model O
and O
analysis O
 O
We O
evaluate O
the O
17 O
syntactic O
conditions O
using O
the O
 O
SVM B-MethodName
classification O
model O
in O
the O
dataset O
intr O
oduced O
in O
Section O
4.1 O
. O

have O
demonstrated O
how O
Recurrent O
Neural O
Networks O
, O
specially O
, O
Long O
- O
Short O
- O
Term O
- O
Memory O
( O
LSTM B-MethodName
) O
is O
effective O
insolving O
various O
text O
generation O
tasks O
( O
Mao O
et O
al O
. O
, O
2014 O
) O
. O

Most O
recently O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
and O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
have O
shown O
very O
promising O
performance O
. O

For O
the O
experiments O
described O
in O
this O
paper O
, O
we O
chose O
recurrent O
neural O
network O
with O
LSTM B-MethodName
cells O
as O
the O
Sequence O
Generator O
due O
to O
its O
recent O
promising O
results O
obtained O
for O
language O
modeling O
tasks O
( O
Kiros O
et O
al O
. O
, O
2014 O
; O
Kim O
et O
al O
. O
, O
2016 O
) O
. O

Given O
the O
previous O
iwords O
, O
i.e. O
, O
x1 O
: O
i O
, O
the O
recurrent O
neural O
network O
based O
language O
models O
compute O
the O
conditional O
probability O
for O
the O
next O
wordyi O
= O
vforv2V O
, O
the O
vocabulary O
set O
, O
by O
computing O
a O
hidden O
state O
hiand O
passing O
it O
through O
a O
Softmax O
function O
: O
P(yi O
= O
vjx1 O
: O
i)P(yi O
= O
vjhi O
) O
( O
1 O
) O
P(yijhi)/exp(W O
hi+B O
) O
( O
2 O
) O
hi= O
 O
( O
hi 1;xi O
) O
( O
3 O
) O
Here O
, O
 O
can O
be O
a O
standard O
RNN O
cell O
or O
more O
complicated O
cell O
like O
LSTM B-MethodName
, O
GRU O
etc O
and O
Wand O
Bare O
linear O
transformation O
coefﬁcients O
. O

The O
ﬁrst O
one O
is O
called O
RILSTM B-MethodName
which O
is O
identical O
to O
TILM O
except O
that O
the O
inﬂuence O
vector O
of O
RILSTM B-MethodName
is O
generated O
randomly O
as O
opposed O
to O
generating O
it O
by O
the O
Inﬂuence O
Generator O
of O
TILM O
. O

The O
second O
baseline O
is O
called O
IILSTM B-MethodName
where O
we O
do O
not O
inject O
the O
inﬂuence O
vector O
as O
a O
bias O
into O
2All O
the O
codes O
and O
evaluation O
scripts O
for O
experimentation O
can O
be O
found O
at O
the O
following O
link O
: O
( O
https:// O
bitbucket.org/karmake2/tilm/src/master/Acronym O
Details O
Nature O
Bigram O
Bigram O
Language O
Model O
Static O
LSTM B-MethodName
Long O
short O
- O
term O
memory O
Static O
RILSTM B-MethodName
LSTM B-MethodName
with O
Random O
Inﬂuence O
Dynamic O
IILSTM B-MethodName
Sampling O
from O
Joint O
LSTMInﬂuence B-MethodName
DistributionDynamic O
TILM O
Topical O
Inﬂuence O
LM O
Dynamic O
Table O
2 O
: O
Baselines O
for O
Quantitative O
Comparison O
. O

the O
vector O
representation O
of O
words O
, O
rather O
, O
the O
Inﬂuence O
Generator O
directly O
computes O
a O
probability O
distribution O
for O
sampling O
the O
next O
word O
and O
this O
probability O
is O
multiplied O
with O
the O
probability O
computed O
independently O
by O
LSTM B-MethodName
. O

To O
answer O
the O
ﬁrst O
question O
, O
i.e. O
, O
to O
verify O
the O
beneﬁt O
of O
modeling O
the O
evolution O
of O
topical O
inﬂuence O
, O
we O
also O
compared O
TILM O
against O
two O
baselines O
representing O
static O
models O
: O
simple O
bigram O
language O
model O
and O
Long O
- O
Short O
Term O
Memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Gers O
et O
al O
. O
, O
1999 O
) O
. O

For O
example O
, O
BLEU-4 B-MetricName
score B-MetricName
obtained O
by O
TILM O
on O
KDD O
Dataset O
is O
0:57 O
, O
while O
LSTM B-MethodName
obtained O
only O
a O
score B-MetricName
of O
0:22 O
. O

ROUGE B-MetricName
- O
L O
score B-MetricName
obtained O
by O
TILM O
is O
0:63 O
, O
while O
it O
is O
0:31 O
for O
LSTM B-MethodName
. O

This O
clearly O
indicates O
that O
TILM O
can O
indeed O
capture O
the O
temporal O
evolution O
of O
KDD O
paper O
titles O
over O
time O
and O
given O
a O
input O
timestamp O
T O
, O
can O
generate O
text O
relevant O
to O
T. O
Also O
note O
that O
, O
RILSTM B-MethodName
performs O
signiﬁcantly O
worse O
compared O
to O
LSTM B-MethodName
for O
most O
datasets O
which O
implies O
that O
the O
inﬂuence O
vector O
plays O
the O
key O
role O
in O
helping O
TILM O
capture O
the O
evolution O
of O
the O
text O
stream O
. O

Itis O
also O
noteworthy O
that O
IILSTM B-MethodName
is O
the O
second O
best O
performing O
method O
which O
conﬁrms O
that O
injecting O
inﬂuence O
vector O
as O
a O
bias O
into O
the O
word O
representation O
works O
better O
than O
using O
the O
Joint O
LSTMInﬂuence B-MethodName
distribution O
obtained O
by O
simply O
multiplying O
inﬂuence O
probabilities O
with O
LSTM B-MethodName
probabilities O
. O

Our O
model O
extends O
the O
well O
- O
known O
BIST O
graph O
- O
based O
dependency O
parser O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
) O
by O
incorporating O
a O
BiLSTM B-MethodName
- O
based O
tagging O
component O
to O
produce O
automatically O
predicted O
POS B-TaskName
tags O
for O
the O
parser O
. O

Our O
joint O
model O
extends O
the O
well O
- O
known O
BIST O
graph O
- O
based O
dependency O
parser O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
) O
with O
an O
additional O
lower O
- O
level O
BiLSTM B-MethodName
- O
based O
tagging O
component O
. O

Given O
word O
tokens O
in O
an O
input O
sentence O
, O
the O
tagging O
component O
uses O
a O
BiLSTM B-MethodName
to O
learn O
“ O
latent O
” O
feature O
vectors O
representing O
these O
word O
tokens O
. O

The O
parsing O
component O
then O
uses O
another O
BiLSTM B-MethodName
to O
learn O
another O
set O
of O
latent O
feature O
representations O
, O
based O
on O
both O
the O
input O
word O
tokens O
and O
the O
predicted O
POS B-TaskName
tags O
. O

We O
obtain O
eiby O
concatenating O
word O
embedding O
e(W O
) O
wiand O
character O
- O
level O
word O
embedding O
e(C O
) O
wi O
: O
ei O
= O
e(W O
) O
wie(C O
) O
wi(1 O
) O
Here O
, O
each O
word O
type O
win O
the O
training O
data O
is O
represented O
by O
a O
real O
- O
valued O
word O
embedding O
e(W O
) O
w. O
Given O
the O
word O
type O
wconsisting O
of O
kcharactersw O
= O
c1c2:::ckwhere O
each O
jthcharacter O
in O
w O
is O
represented O
by O
a O
character O
embedding O
cj O
, O
we O
use O
a O
sequence O
BiLSTM B-MethodName
( O
BiLSTM B-MethodName
seq O
) O
to O
learn O
its O
character O
- O
level O
vector O
representation O
( O
Ballesteros O
et O
al O
. O
, O
2015 O
; O
Plank O
et O
al O
. O
, O
2016 O
) O
. O

The O
input O
to O
BiLSTM B-MethodName
seqis O
the O
sequence O
of O
kcharacter O
embeddings O
c1 O
: O
k O
, O
and O
the O
output O
is O
a O
concatenation O
of O
outputs O
of O
a O
forward O
LSTM B-MethodName
( O
LSTM B-MethodName
f O
) O
readingthe O
input O
in O
its O
regular O
order O
and O
a O
reverse O
LSTM B-MethodName
( O
LSTM B-MethodName
r O
) O
reading O
the O
input O
in O
reverse O
: O
e(C O
) O
w O
= O
BiLSTM B-MethodName
seq(c1 O
: O
k O
) O
= O
LSTM B-MethodName
f(c1 O
: O
k)LSTM B-MethodName
r(ck:1 O
) O
2.2 O
Tagging O
component O
We O
feed O
the O
sequence O
of O
vectors O
e1 O
: O
nwith O
an O
additional O
context O
position O
index O
iinto O
another O
BiLSTM B-MethodName
( O
BiLSTM B-MethodName
pos O
) O
, O
resulting O
in O
latent O
feature O
vectorsv(pos O
) O
i O
each O
representing O
the O
ithword O
wiins O
: O
v(pos O
) O
i= O
BiLSTM B-MethodName
pos(e1 O
: O
n O
; O
i O
) O
( O
2 O
) O
We O
use O
a O
MLP O
with O
softmax O
output O
( O
MLP O
pos O
) O
on O
top O
of O
the O
BiLSTM B-MethodName
posto O
predict O
POS B-TaskName
tag O
of O
each O
word O
in O
s. O
The O
number O
of O
nodes O
in O
the O
output O
layer O
of O
this O
MLP O
posis O
the O
number O
of O
POS B-TaskName
tags O
. O

We O
then O
create O
a O
sequence O
of O
vectors O
x1 O
: O
nin O
which O
each O
xiis O
produced O
by O
concatenating O
the O
POS B-TaskName
tag O
embedding O
e(P O
) O
piand O
the O
word O
vector O
representation O
ei O
: O
xi O
= O
e(P O
) O
piei O
= O
e(P O
) O
pie(W O
) O
wie(C O
) O
wi(4 O
) O
We O
feed O
the O
sequence O
of O
vectors O
x1 O
: O
nwith O
an O
additional O
index O
iinto O
a O
BiLSTM B-MethodName
( O
BiLSTM B-MethodName
dep O
) O
, O
resulting O
in O
latent O
feature O
vectors O
vias O
follows O
: O
vi= O
BiLSTM B-MethodName
dep(x1 O
: O
n O
; O
i O
) O
( O
5 O
) O
Based O
on O
latent O
feature O
vectors O
vi O
, O
we O
follow O
a O
common O
arc O
- O
factored O
parsing O
approach O
to O
decode O
dependency O
arcs O
( O
McDonald O
et O
al O
. O
, O
2005 O
) O
. O

Here O
, O
we O
score B-MetricName
an O
arc O
by O
using O
a O
MLP O
with O
a O
one O
- O
node O
output O
layer O
( O
MLP O
arc O
) O
on O
top O
of O
the O
BiLSTM B-MethodName
dep O
: O
score B-MetricName
arc(i O
; O
j O
) O
( O
6 B-MetricValue
) O
= O
MLP O
arc  O
vivj(vivj)jvi vjj O
where O
( O
vivj)andjvi vjjdenote O
the O
elementwise O
product O
and O
the O
absolute O
element O
- O
wise O
difference O
, O
respectively O
; O
and O
viandvjare O
correspondingly O
the O
latent O
feature O
vectors O
associating O
to O
the O
ithandjthwords O
in O
s O
, O
computed O
by O
Equation O
5 O
. O

For O
predicting O
dependency O
relation O
type O
of O
a O
head O
- O
modiﬁer O
arc O
, O
we O
use O
another O
MLP O
with O
softmax O
output O
( O
MLP O
rel O
) O
on O
top O
of O
the O
BiLSTM B-MethodName
dep O
. O

2.4 O
Joint O
model O
training O
The O
training O
objective O
loss O
of O
our O
joint O
model O
is O
the O
sum O
of O
the O
POS B-TaskName
tagging O
loss O
LPOS B-TaskName
, O
the O
structure O
lossLARCand O
the O
relation O
labeling O
loss O
LREL O
: O
L O
= O
LPOS+LARC+LREL B-TaskName
( O
9)The O
model O
parameters O
, O
including O
word O
embeddings O
, O
character O
embeddings O
, O
POS B-TaskName
embeddings O
, O
three O
one O
- O
hidden O
- O
layer O
MLPs O
and O
three O
BiLSTMs B-MethodName
, O
are O
learned O
to O
minimize O
the O
sum O
Lof O
the O
losses O
. O

However O
, O
unlike O
our O
model O
, O
jPTDP O
v1.0 O
uses O
a O
BiLSTM B-MethodName
to O
learn O
“ O
shared O
” O
latent O
feature O
vectors O
which O
are O
then O
used O
for O
both O
POS B-TaskName
tagging O
and O
dependency O
parsing O
tasks O
, O
rather O
than O
using O
two O
separate O
layers O
. O

For O
learning O
character O
- O
level O
word O
embeddings O
, O
we O
use O
one O
- O
layer O
BiLSTM B-MethodName
seq O
, O
and O
set O
the O
size O
of O
LSTM B-MethodName
hidden O
states O
to O
be O
equal O
to O
the O
vector O
size O
of O
character O
embeddings O
. O

We O
apply O
dropout B-HyperparameterName
( O
Srivastava O
et O
al O
. O
, O
2014 B-HyperparameterValue
) O
with O
a O
67 O
% O
keep O
probability O
to O
the O
inputs O
of O
BiLSTMs B-MethodName
and O
MLPs O
. O

Due O
to O
limited O
computational O
resource O
, O
for O
experiments O
presented O
in O
Section O
3 O
, O
we O
perform O
a O
minimal O
grid O
search O
of O
hyper O
- O
parameters O
to O
select O
the O
number O
ofBiLSTM B-MethodName
posandBiLSTM B-MethodName
deplayers O
fromf1;2 O
g O
and O
the O
size O
of O
LSTM B-MethodName
hidden O
states O
in O
each O
layer O
fromf128;256 O
g. O

For O
experiments O
presented O
in O
sections O
4 O
and O
5 O
, O
we O
ﬁx O
the O
number O
of O
BiLSTM B-MethodName
layers O
at O
2 O
and O
the O
size O
of O
hidden O
states O
at O
128 O
. O

Word O
embeddings O
are O
initialized O
by O
100dimensional O
GloVe B-MethodName
word O
vectors O
pre O
- O
trained O
on O
Wikipedia O
and O
Gigaword O
( O
Pennington O
et O
al O
. O
, O
2014).2As O
mentioned O
in O
Section O
2.5 O
, O
we O
perform O
a O
minimal O
grid O
search O
of O
hyper O
- O
parameters O
and O
ﬁnd O
that O
the O
highest O
mixed O
accuracy B-MetricName
on O
the O
development O
set O
is O
obtained O
when O
using O
2 B-MetricValue
BiLSTM B-MethodName
layers O
and O
256 O
- O
dimensional O
LSTM B-MethodName
hidden O
states O
( O
in O
Table O
1 O
, O
we O
present O
scores B-MetricName
obtained O
on O
the O
development O
set O
when O
using O
2 O
BiLSTM B-MethodName
layers O
) O
. O

” O
denote O
the O
size O
of O
LSTM B-MethodName
hidden O
states O
and O
the O
scores B-MetricName
computed O
without O
punctuations O
, O
respectively O
. O

While O
also O
a O
BiLSTM- B-MethodName
and O
graph O
- O
based O
model O
, O
it O
uses O
a O
more O
sophisticated O
attention O
mechanism O
“ O
biafﬁne O
” O
for O
better O
decoding O
dependency O
arcs O
and O
relation O
types O
. O

We O
used O
the O
ﬁxed O
set O
of O
hyper O
- O
parameters O
as O
used O
for O
the O
CoNLL O
2018 O
shared O
task O
as O
mentioned O
in O
Section O
2.5.11 O
We O
then O
submitted O
the O
parsing O
outputs O
by O
run11Word O
embeddings O
are O
initialized O
by O
the O
100 O
- O
dimensional O
pre O
- O
trained O
GloVe B-MethodName
word O
vectors O
. O
TaskDevelopment O
set O
Evaluation O
set O
Pre O
. O

Speciﬁc O
approaches O
include O
: O
dynamic O
embedding O
models O
using O
a O
probabilistic O
Bayesian O
version O
of O
Word2Vec B-MethodName
( O
Bamler O
and O
Mandt O
, O
2017 O
) O
, O
pointwise O
mutual O
information O
( O
PMI O
) O
( O
Yao O
et O
al O
. O
, O
2018 O
) O
, O
and O
exponential O
family O
embeddings O
( O
Rudolph O
and O
Blei O
, O
2018 O
) O
. O

NN O
k(w)is O
the O
set O
of O
k O
- O
nearest B-MethodName
neighbors I-MethodName
( O
kNN O
) O
of O
a O
word O
w. O
Notation O
3.4 O
. O

NNt O
k(w)is O
the O
set O
of O
k O
- O
nearest B-MethodName
neighbors I-MethodName
( O
kNN O
) O
of O
a O
word O
wduring O
time O
t. O
Notation O
3.5 O
. O
cosis O
cosine O
similarity O
, O
which O
we O
use O
as O
a O
similarity O
function O
between O
embeddings O
. O

Formally O
: O
dt(w O
) O
= O
1  O
 O
NNt O
k(w)\NNt 1 O
k(w O
) O
 O
k(1 O
) O
where O
NNt O
k(w)is O
the O
set O
of O
k O
- O
nearest B-MethodName
neighbors I-MethodName
( O
kNN O
) O
of O
wduring O
time O
t. O

Speciﬁcally O
, O
we O
look O
at O
the O
nearest B-MethodName
neighbors I-MethodName
of O
wat O
times O
tandt 1 O
, O
and O
denote O
the O
set O
of O
descriptors O
by O
Dt O
: O
Dt(w O
) O
= O
NNt O
k(w)nNNt 1 O
k(w O
) O
( O
3 O
) O
For O
example O
, O
using O
this O
method O
for O
‘ O
Russia O
’ O
results O
in O
Soviet O
Union O
andSoviet O
for O
1989 O
, O
when O
the O
Soviet O
Union O
was O
dissolving O
, O
and O
Ukraine O
, O
Kyrgyzstan O
, O
and O
Latvia O
for O
1990 O
, O
when O
these O
countries O
attempted O
to O
gain O
independence O
from O
the O
Union O
. O

Similarly O
, O
W(t O
) O
nyt2RjV(t O
) O
nytjd(t O
) O
nytis O
the O
matrix O
of O
embeddings O
learned O
from O
the O
NYT O
at O
timet O
, O
where O
d(t O
) O
nytis O
the O
embedding O
size O
and O
jV(t O
) O
nytj O
is O
the O
vocabulary O
size O
of O
the O
NYT O
at O
time O
t. O
We O
seek O
a O
matrix O
W(t)2RjVwikijd(t O
) O
nytthat O
will O
contain O
the O
transformation O
of O
WwikitoW(t O
) O
nytfor O
time O
t. O
By O
making O
an O
additional O
simplifying O
assumption O
that O
the O
vector O
spaces O
are O
equivalent O
under O
a O
linear B-MethodName
transformation I-MethodName
, O
we O
are O
able O
to O
ﬁnd O
W(t)by O
optimizing O
the O
following O
linear B-MethodName
regression I-MethodName
model O
: O
arg O
min O
TX O
wi2Vwiki\V(t O
) O
nyt O
 O
Wwiki(wi)T W(t O
) O
nyt(wi O
) O
 O
2 O
2(4 O
) O
where O
T2Rdwikid(t O
) O
nyt O
. O

Machine O
Learning O
Approach O
We O
consider O
several O
supervised O
machine O
learning O
approaches O
, O
experimenting O
with O
random B-MethodName
forest I-MethodName
, O
SVM B-MethodName
, O
neural B-MethodName
networks I-MethodName
, O
etc O
. O
We O
also O
devise O
several O
features O
leveraged O
by O
our O
classiﬁers O
: O
veandvw O
, O
i.e. O
, O
the O
embeddings O
of O
the O
event O
e O
and O
the O
word O
w. O
Any O
embedding O
model O
( O
Sections O
3 O
, O
6.1 O
) O
can O
be O
used O
. O

7.1 O
Implementation O
Details O
Embeddings O
: O
The O
global O
embeddings O
were O
created O
based O
on O
the O
Wikipedia O
dump O
of O
May O
2016 O
, O
using O
Word2Vec B-MethodName
’s O
skip O
- O
gram O
with O
negative O
sampling O
, O
with O
a O
window O
size O
of O
10 O
. O

For O
each O
year O
of O
content O
, O
we O
created O
embeddings O
using O
Word2Vec B-MethodName
’s O
skip O
- O
gram O
with O
negative O
sampling O
, O
with O
a O
window O
size O
of O
5 O
and O
a O
dimensionality O
of O
140 O
, O
using O
the O
Gensim O
library O
( O
Rehurek O
and O
Sojka O
, O
2010 O
) O
. O

SVM B-MethodName
with O
RBF O
kernel O
and O
C=1.0 O
produced O
0.97 O
. O

Random B-MethodName
Forest I-MethodName
classiﬁer O
with O
800 O
trees O
produced O
0.97 O
as O
well O
. O

To O
investigate O
the O
limitations O
of O
our O
model O
as O
well O
as O
the O
behavioral O
difference O
between O
convolutional O
and O
recurrent O
neural B-MethodName
networks I-MethodName
, O
we O
generate O
adversarial O
examples O
to O
confuse O
the O
model O
and O
compare O
to O
human O
performance O
. O

This O
motivates O
our O
proposed O
sequential O
aggregation O
function O
based O
on O
a O
singlelayer O
unidirectional O
RNN O
with O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
units O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

aggregateRNN O
- O
LSTM B-MethodName
= O
RNN O
( O
[ O
z1:::z O
m O
] O
) O
( O
5 O
) O
By O
performing O
1 O
- O
max O
pooling O
over O
the O
outputs O
of O
aggregateCNNor O
aggregateRNN O
- O
LSTM4we B-MethodName
obtain O
a O
single O
vector O
r(representing O
rq;rpij;rajon O
the O
word O
level O
, O
or O
rs O
jon O
the O
sentence O
level O
): O
r O
= O
max O
pool(aggregate O
( O
[ O
z1:::z O
m O
] O
) O
) O
( O
6 O
) O
We O
share O
the O
weights B-HyperparameterName
between O
the O
comparison O
and O
aggregation O
operations O
within O
the O
word O
and O
sentence O
level O
but O
not O
across O
levels O
. O

While O
the O
dataset O
contains O
multiple O
sources O
of O
information O
about O
the O
movie O
contents O
such O
as O
4Using O
only O
the O
last O
RNN O
output O
for O
aggregateRNN O
- O
LSTM B-MethodName
did O
not O
provide O
convincing O
results.videos O
, O
subtitles O
, O
and O
movie O
scripts O
, O
here O
we O
focus O
on O
answering O
the O
questions O
only O
from O
plot O
synopses O
. O

4 O
Results O
We O
train O
11 O
models O
with O
different O
random O
initializations B-HyperparameterName
for O
both O
the O
CNN O
and O
RNN O
- O
LSTM B-MethodName
aggregation O
function O
and O
form O
majority O
- O
vote O
ensembles O
of O
the O
nine O
models O
with O
the O
highest O
validation O
accuracy B-MetricName
. O

With O
a O
test O
accuracy B-MetricName
of O
85.12 B-MetricValue
, O
the O
RNN O
- O
LSTM B-MethodName
ensemble O
achieves O
a O
new O
state O
of O
the O
art O
that O
is O
more O
than O
ﬁve O
percentage O
points O
above O
the O
previous O
best O
result O
. O

Test O
Wang O
and O
Jiang O
( O
2016 O
) O
72.10 O
72.90 O
Liu O
et O
al O
. O
( O
2017 O
) O
79.00 O
79.99 O
Dzendzik O
et O
al O
. O
( O
2017 O
) O
- O
80.02 O
Proposed O
models O
CNN O
word O
level O
only O
76.51 O
CNN O
79.62 O
CNN O
ensemble O
82.58 O
82.73 O
RNN O
- O
LSTM B-MethodName
83.14 O
RNN O
- O
LSTM B-MethodName
ensemble O
84.37 O
85.12 O
CNN O
RNN O
- O
LSTM B-MethodName
ensemble O
84.78 O
84.70 O
Table O
1 O
: O
MovieQA O
accuracies B-MetricName
for O
previously O
published O
results O
and O
our O
proposed O
single O
models O
( O
best O
out O
of O
11 B-MetricValue
) O
and O
ensembles O
( O
nine O
best O
out O
of O
11 O
) O
. O

Furthermore O
, O
the O
RNN O
- O
LSTM B-MethodName
aggregation O
function O
is O
superior O
to O
aggregation O
via O
CNNs O
, O
improving O
the O
validation O
accuracy B-MetricName
by O
1.5 B-MetricValue
percentage O
points O
. O

While O
this O
improvement O
is O
statistically O
signiﬁcant,6combining O
both O
aggregation O
functions O
by O
ensembling O
the O
nine O
best O
CNN O
and O
RNN O
- O
LSTM B-MethodName
models O
each O
, O
yields O
a O
small O
but O
statistically O
insigniﬁcant O
improvement O
of O
0.41 O
percentage O
points O
over O
the O
RNN O
- O
LSTM B-MethodName
ensemble O
on O
the O
validation O
set O
. O

This O
might O
explain O
why O
the O
RNN O
- O
LSTM B-MethodName
ensemble O
even O
outperforms O
the O
CNN O
RNN O
- O
LSTM B-MethodName
ensemble O
on O
the O
test O
set O
by O
a O
small O
margin O
. O

6McNemar O
test O
( O
McNemar O
, O
1947 O
) O
, O
p O
< O
0:05.Systems O
CNN O
RNN O
- O
LSTM B-MethodName
All O
questions O
71.45 O
71.31 O
- O
Correctly O
solved O
80.86 O
79.35 O
- O
Incorrectly O
solved O
35.73 O
34.49 O
Table O
2 O
: O
Percentage O
of O
questions O
in O
which O
the O
plot O
sentences O
containing O
the O
clues O
for O
the O
answer O
are O
ranked O
highest O
according O
to O
the O
model O
’s O
sentence O
attention O
distribution O
( O
relative O
to O
its O
selected O
answer O
) O
on O
the O
validation O
set O
( O
averaged B-MetricName
results O
of O
nine O
models O
) O
. O

5.1 O
Word O
- O
level O
Black O
- O
box O
Attack O
Adversarial O
examples O
for O
image O
recognition O
are O
typically O
created O
by O
adding O
some O
imperceptible O
noise O
( O
Szegedy O
et O
al O
. O
, O
2014 O
; O
Goodfellow O
et O
al O
. O
, O
2015 O
) O
, O
yet O
this O
is O
difﬁcult O
to O
do O
for O
natural O
lan O
- O
Systems O
Average B-MetricName
Ensemble O
CNN O
78.74 B-MetricValue
81.72 O
RNN O
- O
LSTM B-MethodName
81.53 O
83.76 O
CNN O
RNN O
- O
LSTM B-MethodName
81.14 O
84.27 O
Table O
3 O
: O
Adversarial O
accuracies B-MetricName
on O
the O
validation O
set O
under O
the O
word O
- O
level O
black O
- O
box O
attack O
based O
on O
manual O
lexical O
substitutions O
in O
questions O
. O

Although O
the O
differences O
are O
small O
, O
the O
RNN O
- O
LSTM B-MethodName
and O
CNN O
RNN O
- O
LSTM B-MethodName
ensembles O
are O
even O
less O
affected O
by O
lexical O
substitutions O
than O
the O
CNN O
ensemble O
. O

The O
robustness O
of O
the O
models O
against O
this O
attack O
can O
probably O
be O
attributed O
to O
the O
pretrained O
GloVe B-MethodName
embeddings O
, O
which O
allow O
it O
to O
generalize O
for O
semantically O
equivalent O
lexical O
choices O
. O

7We O
only O
substituted O
with O
words O
contained O
in O
the O
pretrained O
GloVe B-MethodName
embeddings O
used O
by O
the O
models O
to O
avoid O
introducing O
unknown O
words O
. O

As O
Figure O
3 O
reveals O
, O
already O
modifying O
the O
single O
most O
important O
word O
in O
the O
most O
important O
sentence O
has O
a O
large O
effect O
on O
the O
average B-MetricName
performance O
of O
both O
the O
CNN O
and O
RNN O
- O
LSTM B-MethodName
models O
. O

For O
increasing O
k O
, O
the O
RNN O
- O
LSTM B-MethodName
versions O
appeared O
to O
be O
a O
bit O
more O
robust O
against O
the O
attack O
, O
but O
for O
k10the O
difference O
shrinks O
and O
the O
accuracy B-MetricName
of O
both O
models O
drops O
to O
only O
about O
30 B-MetricValue
% O
. O

AddC O
AddQ O
AddQA O
Without O
optimization O
CNN O
76.87 O
76.67 O
76.66 O
76.33 O
RNN O
- O
LSTM B-MethodName
81.11 O
81.11 O
81.05 O
81.05 O
After O
two O
optimization O
epochs B-HyperparameterName
CNN O
N O
/ O
A O
73.38 B-HyperparameterValue
57.39 O
13.61 O
RNN O
- O
LSTM B-MethodName
N O
/ O
A O
79.94 O
68.05 O
23.22 O
Table O
4 O
: O
Adversarial O
accuracies B-MetricName
on O
200 B-MetricValue
random O
validation O
questions O
under O
the O
sentence O
- O
level O
black O
- O
box O
attacks O
( O
averaged B-MetricName
results O
of O
nine O
models O
) O
. O

While O
composing O
the O
sentence O
of O
just O
common O
English O
words O
( O
AddC O
) O
does O
not O
affect O
the O
models O
too O
much O
, O
adding O
words O
from O
the O
question O
8As O
this O
attack O
is O
computationally O
very O
expensive O
we O
only O
ran O
it O
on O
a O
random O
subset O
of O
200 O
validation O
questions O
for O
two O
optimization O
epochs B-HyperparameterName
of O
the O
distractor O
sentence O
. O
Attack O
optimized O
for O
Evaluated O
systems O
CNN O
RNN O
- O
LSTM B-MethodName
CNN O
13.61 B-HyperparameterValue
21.50 O
RNN O
- O
LSTM B-MethodName
22.06 O
23.22 O
Table O
5 O
: O
AddQA O
attack O
results O
when O
testing O
models O
on O
adversarial O
examples O
optimized O
to O
fool O
another O
model O
( O
averaged B-MetricName
results O
of O
nine O
models O
) O
. O

Another O
observation O
is O
that O
the O
RNN O
- O
LSTM B-MethodName
models O
outperform O
the O
CNN O
models O
by O
a O
large O
margin O
under O
all O
attacks O
. O

Therefore O
, O
Systems O
Average B-MetricName
Ensemble O
CNN O
31.59 B-MetricValue
32.07 O
RNN O
- O
LSTM B-MethodName
32.61 O
32.17 O
Table O
6 O
: O
Adversarial O
accuracies B-MetricName
on O
the O
validation O
set O
under O
the O
sentence O
- O
level O
white O
- O
box O
attack O
based O
on O
removal O
of O
the O
plot O
sentence O
with O
highest O
attention O
( O
averaged B-MetricName
results O
of O
nine O
models O
) O
. O

This O
highlights O
the O
weakness O
of O
the O
model O
to O
give O
answers O
in O
more O
complex O
scenarios O
where O
the O
answer O
is O
less O
obvious.6 O
Human O
vs. O
Machine O
Processing O
In O
order O
to O
gain O
insights O
how O
to O
further O
improve O
machine O
reading O
comprehension O
, O
we O
performed O
a O
case O
study O
in O
which O
a O
human O
was O
asked O
to O
answer O
difﬁcult O
questions O
that O
none O
of O
11 O
CNN O
or O
RNNLSTM B-MethodName
models O
solved O
correctly O
. O

The O
same O
comparison O
was O
conducted O
between O
the O
hierarchical O
CNN O
and O
RNN O
- O
LSTM B-MethodName
models O
. O

Although O
there O
are O
improvements O
, O
which O
indicate O
that O
sequential O
processing O
is O
better O
suited O
for O
QA O
tasks O
, O
the O
RNN O
- O
LSTM B-MethodName
models O
exhibit O
the O
same O
fundamental O
drawbacks O
. O

Then O
, O
we O
explored O
the O
limitations O
of O
our O
models O
and O
the O
behavioral O
difference O
between O
CNNs O
and O
RNN O
- O
LSTMs B-MethodName
with O
adversarial O
examples O
generated O
at O
different O
linguistic O
levels O
( O
word O
vs. O
sentence O
level O
) O
and O
from O
different O
adversary O
’s O
knowledge O
( O
black O
- O
box O
vs. O
white O
- O
box O
) O
. O

In O
general O
, O
RNN O
- O
LSTM B-MethodName
models O
outperformed O
CNN O
models O
, O
but O
our O
results O
for O
sentence O
- O
level O
black O
- O
box O
attacks O
indicate O
they O
might O
share O
the O
same O
weaknesses O
. O

D O
ESCRIPTION O
WORD O
EMBEDDING B-MetricName
HYPOTHESIS O
METHOD O
1 O
Find O
the O
top O
- O
k O
neighbors O
near O
the O
cueBERT B-MethodName
not O
in O
context O
, O
lemmatized O
and O
unlemmatizedTop O
- O
k O
near O
cue O
in O
BERTare B-MethodName
associates O
in O
human O
associations.kNN O
, O
median O
rank O
and O
P@K O
2.1 O
Asymmetry O
of O
countriesBERTin B-MethodName
context O
Non O
- O
prominent O
countries O
are O
more O
similar O
to O
prominent O
countries O
than O
vice O
versa O
. O
Cosine O
2.2 O
Asymmetries O
as O
frequenciesOutput O
of O
Exp.1 O
Frequent O
words O
are O
more O
often O
associates O
of O
less O
frequent O
words O
than O
vice O
versa O
. O
Cues O
and O
targets O
from O
Nelson O
, O
cues O
and O
targets O
from O
Exp O
. O

2.3 O
Asymmetries O
of O
hypernyms O
and O
hyponymsBERTin B-MethodName
context O
If O
salience O
= O
‘ O
more O
general O
’ O
, O
hyponyms O
are O
more O
similar O
to O
hypernyms O
than O
the O
reverse O
. O

2.4 O
Asymmetry O
as O
neighbourhood O
densityBERT B-MethodName
not O
in O
context O
unlemmatized O
; O
FastText O
( O
only O
for O
countries)A O
semantically O
richer O
word O
elicits O
a O
greater O
number O
of O
close O
neighbours O
than O
a O
fainter O
word O
. O
Extraction O
of O
asymmetric O
pairs O
from O
Nelson O
; O
kNN O
; O
cosine O
( O
threshold B-MetricName
0.2 O
) O
3 B-MetricValue
Violation O
of O
triangle O
inequalityBERTin B-MethodName
context O
BERTembedding B-MethodName
space O
violates O
the O
triangle O
inequality O
. O
Extraction O
of O
asymmetric O
triples O
; O
cosine O
, O
 O
threshold B-MetricName
. O

3.1 O
Word O
embedding O
data O
BERT B-MethodName
Devlin O
et O
al O
. O
( O
2018 O
) O
propose O
BERT B-MethodName
, O
a O
transformer O
- O
based O
model O
that O
uses O
an O
attention O
mechanism O
to O
extract O
the O
context O
of O
words O
and O
subwords O
from O
text O
. O

The O
innovation O
of O
BERT B-MethodName
is O
the O
application O
of O
a O
bi O
- O
directional O
training O
to O
the O
Transformer O
, O
achieving O
a O
better O
use O
of O
context O
from O
text O
than O
systems O
with O
unidirectional O
training O
. O

BERT B-MethodName
is O
pretrained O
on O
the O
BookCorpus O
( O
800 O
M O
words O
) O
and O
English O
Wikipedia O
( O
2500 O
M O
words O
) O
. O

In O
all O
experiments O
below O
, O
we O
use O
the O
Huggin O
- O
face O
version O
of O
BERT1 B-MethodName
, O
speciﬁcally O
the O
“ O
bert O
- O
baseuncased O
” O
model O
that O
we O
expressly O
do O
not O
modify O
. O

Method O
As O
human O
associations O
in O
Nelson O
are O
lemmatized O
, O
we O
tested O
both O
the O
lemmatized O
and O
unlemmatized O
versions O
on O
the O
“ O
raw O
” O
BERT B-MethodName
word O
embeddings O
obtained O
from O
the O
vocabulary O
of O
the O
model.6 O
The O
median O
rank O
is O
a O
measure O
of O
central O
tendency O
of O
the O
median O
rank O
in O
BERTfor B-MethodName
the O
n O
- O
ranked O
associate O
target O
in O
Nelson O
. O

For O
example O
, O
in O
Figure O
2 O
for O
the O
unlemmatized O
data O
, O
we O
would O
have O
to O
take O
the O
median O
across O
ranks O
of O
the O
ranks O
given O
in O
BERT B-MethodName
( O
1 O
, O
1 O
, O
2 O
, O
32 O
, O
22 O
) O
for O
the O
ﬁrst O
ranked O
target O
in O
Nelson O
. O

For O
each O
cue O
in O
Nelson O
, O
we O
extract O
the O
top- O
knearest O
neighbours O
in O
the O
BERT B-MethodName
space O
and O
rank O
the O
results O
by O
their O
cosine O
similarity O
in O
descending O
order O
. O

Then O
, O
for O
each O
same O
cue O
and O
each O
one O
of O
its O
targets O
in O
Nelson O
, O
we O
calculate O
the O
target O
’s O
rank O
in O
BERT(see B-MethodName
Figure O
2 O
) O
. O

P@K O
tells O
us O
if O
the O
ﬁrst O
associate O
in O
the O
human O
associa6These O
raw O
vectors O
come O
with O
the O
pre O
- O
trained O
model O
of O
BERT B-MethodName
. O

tions O
appears O
in O
the O
top- O
kassociates O
of O
BERT B-MethodName
. O

The O
median O
BERT B-MethodName
ranking O
for O
the O
ﬁrst O
human O
associate O
in O
unlemmatized O
and O
lemmatized O
associations O
is O
respectively O
4 O
and O
3 O
. O

For O
the O
unlemmatized O
version O
, O
the O
ﬁrst O
associate O
in O
the O
human O
word O
associations O
is O
the O
word O
with O
the O
highest O
ranking O
in O
BERTin B-MethodName
13.02 O
% O
of O
cases O
and O
in O
the O
top O
5 O
ranks O
of O
BERTin B-MethodName
61.62 O
% O
of O
the O
cases O
. O

As O
can O
be O
seen O
in O
Table O
2 O
, O
the O
results O
in O
BERT B-MethodName
are O
convincing O
. O

For O
the O
unlemmatized O
data O
, O
the O
ﬁrst O
three O
columns O
show O
examples O
where O
BERT B-MethodName
ranks O
the O
right O
associate O
at O
or O
near O
the O
top O
of O
its O
list O
. O

The O
last O
two O
columns O
are O
examples O
of O
not O
very O
good O
association O
rankings O
in O
BERT B-MethodName
. O

Notice O
the O
third O
column O
, O
which O
shows O
the O
limitations O
of O
an O
unlemmatized O
approach O
as O
BERTdoes B-MethodName
not O
correctly O
distinguish O
between O
forms O
of O
the O
same O
word O
. O

For O
the O
lemmatized O
data O
, O
we O
see O
an O
improvement O
inthe O
prediction O
of O
BERT B-MethodName
: O
the O
rankings O
of O
the O
human O
associations O
are O
in O
general O
lower O
than O
the O
rankings O
of O
the O
unlemmatized O
version O
. O

On O
this O
basis O
, O
we O
used O
the O
sentences O
as O
input O
for O
BERT B-MethodName
, O
we O
extracted O
the O
word O
vectors O
( O
in O
context O
) O
8The O
word O
lists O
are O
shown O
in O
the O
supplementary O
materials O
. O
Context O
cos(B O
; O
A O
) O
cos(A O
; O
B O
) O
A O
is O
similar O
to O
B O
76.19 O
% O
A O
is O
essentially O
B O
57.14 O
% O
A O
is O
roughly O
B O
66.67 O
% O
Table O
2 O
: O
Results O
of O
Experiment O
2.1 O
: O
percent O
of O
times O
the O
cosine O
similarity O
in O
B O
ERTis O
higher O
when O
the O
more O
prominent O
country O
is O
in O
second O
position O
( O
cos(B O
; O
A O
) O
) O
, O
matching O
people O
’s O
preferences O
, O
compared O
to O
when O
it O
is O
in O
ﬁrst O
position O
( O
cos(A O
; O
B O
) O
) O
. O

Since O
BERT B-MethodName
spaces O
take O
context O
into O
account O
, O
they O
should O
be O
able O
to O
detect O
the O
differences O
between O
the O
order O
of O
the O
words O
in O
the O
context O
and O
if O
they O
replicate O
human O
associations O
, O
we O
should O
ﬁnd O
thatcos(B O
= O
NK;A O
= O
China O
) O
cos(A O
= O
China;B O
= O
NK O
) O
more O
often O
than O
the O
reverse O
. O

9http://www.natcorp.ox.ac.uk/Rank O
Avg O
Frequency O
Avg O
Frequency O
Word O
Association O
BERT B-MethodName
1 O
1369 O
8716 O
2 O
1942 O
8432 O
3 O
2804 O
9691 O
4 O
2696 O
10619 O
5 O
2546 O
13136 O
Table O
3 O
: O
Results O
of O
Experiment O
2.2 O
for O
unlemmatized O
associations O
. O

For O
each O
rank O
, O
we O
compute O
the O
average B-MetricName
frequency O
of O
the O
targets O
in O
human O
associations O
and O
BERTpredictions B-MethodName
for O
a O
given O
cue O
. O

Could O
a O
frequency O
explanation O
, O
though O
, O
be O
extended O
to O
all O
words O
in O
BERTembeddings B-MethodName
? O

Then O
, O
we O
extract O
from O
the O
corpus O
both O
the O
pairs O
of O
cues O
and O
targets O
from O
Nelson O
’s O
human O
associations O
and O
the O
pairs O
from O
BERT B-MethodName
, O
ordered O
by O
the O
rankings O
obtained O
in O
Experiment O
1 O
. O

Results O
As O
shown O
in O
Table O
3 O
, O
the O
average B-MetricName
frequency O
for O
unlemmatized O
( O
and O
lemmatized O
) O
associations O
in O
BERT B-MethodName
is O
higher O
than O
in O
human O
word O
associations O
. O

In O
contrast O
, O
if O
we O
compute O
the O
frequency O
of O
the O
targets O
found O
by O
BERT B-MethodName
, O
we O
ﬁnd O
that O
only O
30 O
% O
of O
associations O
have O
a O
target O
with O
higher O
frequency O
. O

This O
indicates O
that O
, O
in O
general O
, O
if O
we O
were O
to O
ﬁnd O
human O
- O
like O
asymmetric O
judgments O
of O
similarity O
inBERT B-MethodName
spaces O
it O
would O
not O
be O
a O
frequency O
effect O
. O

This O
conﬁrms O
that O
BERT B-MethodName
spaces O
encode O
salience O
in O
terms O
of O
richness O
of O
specifying O
features O
. O

But O
in O
BERT B-MethodName
’s O
vectors O
corresponding O
to O
human O
asymmetric O
associations O
( O
those O
where O
a O
cue O
elicits O
a O
target O
but O
not O
the O
reverse O
) O
, O
the O
target O
is O
denser O
than O
the O
cue O
in O
only O
26.58 O
% O
of O
cases.11 O
11As O
a O
control O
, O
we O
also O
extract O
bi O
- O
directional O
associations O
: O
the O
cue O
word O
produce O
a O
certain O
target O
and O
this O
speciﬁc O
targetWe O
also O
tested O
the O
country O
data O
described O
in O
section O
5.1 O
with O
the O
same O
procedure O
. O

human O
associations O
as O
a O
cue O
- O
target O
pair.13To O
calculate O
the O
BERTword B-MethodName
embeddings O
, O
we O
contextualise O
each O
pair O
in O
the O
three O
different O
contexts O
already O
shown O
for O
previous O
experiments O
. O

To O
determine O
if O
the O
two O
pairs O
of O
words O
violate O
the O
triangle O
inequality O
in O
BERTspace B-MethodName
, O
we O
follow O
a O
procedure O
similar O
to O
Grifﬁths O
et O
al O
. O
( O
2007 O
) O
’s O
. O

If O
the O
pair O
exists O
and O
( O
w1 O
; O
w3 O
) O
is O
greater O
than O
 O
, O
then O
BERT B-MethodName
’s O
embeddings O
also O
show O
transitivity O
; O
if O
the O
pair O
exists O
, O
but O
( O
w1 O
; O
w3 O
) O
is O
less O
than O
 O
, O
then O
( O
w1 O
; O
w3 O
) O
inBERT B-MethodName
does O
not O
reﬂect O
the O
transitivity O
found O
in O
humans O
; O
if O
the O
pair O
does O
not O
exist O
and O
( O
w1 O
; O
w3 O
) O
is O
lesser O
than O
 O
, O
then O
there O
is O
a O
lack O
of O
transitivity O
both O
in O
human O
associations O
and O
in O
B O
ERTembeddings O
. O

Results O
Figure O
3 O
shows O
some O
comparisons O
, O
in O
terms O
of O
precision B-MetricName
and O
recall B-MetricName
, O
of O
the O
BERTspaces B-MethodName
against O
the O
gold O
human O
associations O
for O
different O
values O
of O
 B-MetricValue
. O

The O
results O
show O
that O
the O
BERTspace B-MethodName
has O
many O
more O
triples O
for O
which O
there O
is O
triangle O
inequality O
( O
transitive O
triples O
) O
, O
especially O
for O
low O
values O
of O
 O
. O

This O
is O
expected O
, O
as O
BERT B-MethodName
is O
trained O
on O
a O
much O
larger O
vocabulary O
space O
. O

As O
the O
results O
show O
, O
for O
increasing O
values O
of O
 O
and O
more O
and O
more O
stringent O
deﬁnitions O
of O
similarity O
, O
13Samples O
of O
the O
word O
lists O
are O
shown O
in O
the O
supplementary O
materials.the O
agreement O
of O
BERTwith B-MethodName
the O
human O
norms O
on O
violations O
of O
triangle O
inequality O
is O
high O
, O
and O
this O
despite O
a O
clear O
tendency O
to O
overestimate O
triangle O
inequalities O
( O
transitivity O
) O
, O
as O
the O
TI O
values O
show O
. O

7 O
Discussion O
We O
have O
found O
converging O
evidence O
for O
BERTbeing B-MethodName
like O
human O
word O
associations O
in O
ranks O
of O
associations O
, O
quantitatively O
and O
qualitatively O
. O

In O
studying O
whether O
BERT B-MethodName
similarity O
spaces O
are O
asymmetric O
, O
we O
ﬁnd O
converging O
evidence O
to O
human O
experiments O
, O
using O
country O
names O
, O
both O
in O
the O
fact O
that O
the O
notion O
of O
salience O
inﬂuences O
the O
calculation O
of O
similarity O
and O
also O
in O
the O
fact O
that O
frequency O
correlates O
with O
the O
preferential O
direction O
of O
similarity O
. O

So O
do O
BERTembedding B-MethodName
spaces O
, O
for O
reasonably O
high O
values O
of O
the O
similarity O
measure O
. O

A O
recent O
comparison O
of O
word O
embeddings O
, O
Word2vec O
and O
Glove O
, O
to O
Nelson O
’s O
norms O
indicates O
that O
vectorial O
representations O
that O
do O
not O
take O
context O
into O
account O
, O
unlike O
BERT B-MethodName
, O
still O
are O
unable O
to O
capture O
the O
triangle O
inequality O
( O
Nematzadeh O
et O
al O
. O
, O
2017 O
) O
. O

With O
distributed O
representation O
for O
words O
and O
sentences O
and O
the O
powerful O
non O
- O
linear O
calculation O
ability O
of O
the O
neural B-MethodName
networks I-MethodName
, O
we O
could O
explore O
deeper O
syntactic O
and O
maybe O
semantic O
meaning B-MetricName
in O
text O
analysis O
, O
and O
both O
graph O
- O
based O
( O
Pei O
et O
al O
. O
, O
2015 O
; O
Wang O
and O
Chang O
, O
2016 O
) O
and O
transition O
- O
based O
( O
Chen O
and O
Manning O
, O
2014 O
; O
Weiss O
et O
al O
. O
, O
2015 O
; O
Dyer O
et O
al O
. O
, O
2015 O
; O
Andor O
et O
al O
. O
, O
2016 O
) O
parsing O
have O
beneﬁted O
a O
lot O
from O
neural O
representation O
learnings O
. O

In O
contrast O
, O
‘ O
slate O
’ O
is O
more O
strongly O
associated O
with O
the O
adjective O
’s O
antonym O
‘ O
clean O
’ O
, O
likely O
owing O
the O
widespread O
collocation O
‘ O
clean O
slate’.2.2.2 O
Vector O
embedding O
cosine O
distance O
Global O
Vectors O
for O
Word O
Representation O
( O
GloVe B-MethodName
) O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
skip O
- O
gram O
model O
trained O
vectors O
( O
Word2Vec B-MethodName
) O
provide O
vector O
representations O
for O
words O
that O
encompass O
semantic O
and O
linguistic O
similarity O
. O

We O
examine O
the O
Twitter O
GloVe B-MethodName
set O
( O
d= O
200 O
) O
, O
the O
Wiki O
- O
GigaWord O
GloVe B-MethodName
set O
of O
( O
d= O
200 O
) O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
and O
Google O
News O
Word2Vec B-MethodName
vectors O
( O
d= O
300 O
) O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
. O

2.3 O
Quantile O
normalization O
and O
correlations B-MetricName
between O
metrics O
Across O
these O
seven O
different O
semantic O
association O
metrics O
, O
distributions O
of O
scores B-MetricName
varies O
from O
Gaussian O
( O
GloVe B-MethodName
, O
Word2Vec B-MethodName
, O
ConceptNet5 O
) O
to O
exponential O
( O
Bigram O
) O
. O

This O
resulted O
in O
a O
choice O
of O
four O
measures O
, O
Bigram O
( O
Googe O
Ngram O
) O
, O
Word2Vec B-MethodName
( O
Google O
News O
) O
, O
ConceptNet O
( O
ConceptNet5 O
) O
, O
and O
LDA O
, O
as O
candidate O
semantic O
models O
of O
human O
word O
choices.4Unless O
otherwise O
noted O
, O
we O
will O
use O
‘ O
Bigram O
’ O
and O
‘ O
Word2Vec B-MethodName
’ O
to O
refer O
to O
those O
metrics O
based O
on O
Google O
Ngram O
and O
Google O
News O
, O
respectively O
. O

We O
can O
seeAnswer O
: O
 O
emptyAnswer O
: O
 O
insaneAnswer O
: O
 O
rough O
Clue O
( O
Adjective O
) O
Answer0.00.20.40.60.81.0Answer O
probabilityTarget O
Codenames O
: O
heart O
, O
phone O
Non O
- O
Target O
Codenames O
: O
relationship O
Bigram O
Conceptnet O
Word2Vec B-MethodName
Human O
empty O
insane O
roughheart O
phone O
relationshipCodenames O
( O
nouns)Bigram O
empty O
insane O
roughConceptnet O
empty O
insane O
roughWord2Vec B-MethodName
0.00.20.40.60.81.0Figure O
2 O
: O
This O
example O
speaker O
conﬁguration O
shows O
how O
different O
clues O
are O
preferred O
by O
different O
models O
: O
‘ O
empty O
’ O
most O
often O
co O
- O
occurs O
with O
‘ O
heart O
’ O
and O
‘ O
phone O
’ O
and O
is O
thus O
favored O
by O
the O
Bigram O
model O
. O

Similarly O
, O
since O
‘ O
insane O
’ O
appears O
most O
often O
in O
the O
context O
windows O
for O
both O
‘ O
heart O
’ O
and O
‘ O
phone O
’ O
, O
it O
is O
the O
top O
prediction O
for O
Word2Vec B-MethodName
. O

Chance O
performance O
is O
zero O
for O
this O
measure O
. O
Top O
answer O
Rank O
correlation B-MetricName
Mean O
SEM B-MetricName
Mean O
SEM B-MetricName
Listener O
Bigram O
0.4160.056 O
0.3840.037 O
ConceptNet O
0.2080.046 O
0.1960.046 O
Word2Vec B-MethodName
0.2470.055 O
0.2530.037 O
LDA O
0.0520.050 O
-0.0530.045 O
Speaker O
Bigram O
0.4180.055 O
0.5160.033 O
ConceptNet O
0.4050.055 O
0.3460.045 O
Word2Vec B-MethodName
0.2780.050 O
0.2790.043 O
LDA O
0.0890.032 O
0.0550.045 O
Table O
2 B-MetricValue
: O
Comparison O
of O
semantic O
association O
measures O
in O
matching O
human O
responses O
in O
Experiment O
1 O
( O
No O
OED O
) O
. O

The O
procedure O
was O
run O
for O
the O
four O
designated O
models O
( O
Bigram O
, O
Word2Vec B-MethodName
, O
ConceptNet O
and O
LDA O
) O
, O
separately O
for O
the O
listener O
and O
the O
speaker O
side O
, O
for O
100;000sampling O
iterations O
. O

For O
the O
listener O
task O
, O
the O
Bigram O
association O
metric O
scores B-MetricName
marginally O
higher O
than O
Word2Vec B-MethodName
in O
top O
answer O
but O
strongly O
outperforms O
other O
models O
in O
rank O
correlation B-MetricName
. O

While O
ConceptNet O
( O
top O
answer O
) O
and O
Word2Vec B-MethodName
( O
rank O
correlation B-MetricName
) O
win O
on O
the O
speaker O
side O
, O
surprisingly O
, O
Bigram O
performs O
considerably O
worse O
than O
in O
experiment O
1 B-MetricValue
. O

In O
terms O
of O
task O
difﬁ-Top O
answer O
Rank O
correlation B-MetricName
Mean O
SEM B-MetricName
Mean O
SEM B-MetricName
Listener O
Bigram O
0.5610.092 O
0.6180.044 O
ConceptNet O
0.4240.080 O
0.1640.092 O
Word2Vec B-MethodName
0.5450.091 O
0.4080.084 O
LDA O
0.1060.040 O
-0.4610.074 O
Speaker O
Bigram O
0.1300.044 O
-0.0060.068 O
ConceptNet O
0.5640.098 O
0.1700.076 O
Word2Vec B-MethodName
0.4910.092 O
0.2000.077 O
LDA O
0.0910.040 O
-0.0830.069 O
Table O
3 B-MetricValue
: O
Comparison O
of O
semantic O
association O
measures O
to O
human O
data O
from O
Experiment O
2 O
( O
separate O
speaker O
and O
listener O
OED O
) O
. O

This O
hypothesis O
was O
directly O
addressed O
in O
the O
next O
experiment O
. O
Top O
answer O
Rank O
Correlation B-MetricName
Mean O
SEM B-MetricName
Mean O
SEM B-MetricName
Listener O
Bigram O
0.5860.072 O
0.4960.056 O
ConceptNet O
0.2070.043 O
-0.0500.063 O
Word2Vec B-MethodName
0.4410.063 O
0.2420.064 O
Speaker O
Bigram O
0.5050.047 O
0.2800.062 O
ConceptNet O
0.2900.051 O
-0.0610.066 O
Word2Vec B-MethodName
0.3830.059 O
0.0410.069 O
Table O
4 B-MetricValue
: O
Comparison O
of O
semantic O
association O
measures O
to O
human O
data O
from O
Experiment O
3 O
( O
joint O
speaker O
- O
listener O
OED O
) O
. O

This O
difference O
is O
more O
pronounced O
for O
the O
Rank O
correlation B-MetricName
measure O
, O
where O
other O
models O
perform O
at O
chance O
with O
the O
exception O
of O
Word2Vec B-MethodName
in O
the O
listener O
task O
. O

As O
many O
deep O
learning O
algorithms O
( O
including O
the O
recurrent O
neural B-MethodName
networks I-MethodName
used O
here O
) O
natively O
capture O
long O
- O
distance O
dependencies O
, O
direct O
inclusion O
of O
syntactic O
features O
is O
likely O
not O
productive O
. O

For O
both O
sections O
of O
the O
task O
( O
identifying O
all O
metaphoric O
words O
and O
identifying O
verbs O
) O
, O
four O
of O
the O
top O
ﬁve O
systems O
use O
some O
form O
of O
long O
shortterm O
memory O
network O
( O
LSTM B-MethodName
) O
. O

The O
system O
of O
Wu O
et O
al O
. O
( O
2018 O
) O
performed O
best O
on O
both O
tasks O
( O
F1 B-MetricName
of O
.651 B-MetricValue
on O
all O
parts O
of O
speech O
, O
and O
.672 O
for O
verbs O
) O
using O
a O
combination O
of O
a O
convolutional O
neural O
network O
( O
CNN O
) O
and O
bidirectional O
LSTM B-MethodName
. O

Most O
recently O
, O
the O
work O
of O
Gao O
et O
al O
. O
( O
2018 O
) O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
the O
shared O
task O
data O
as O
well O
as O
a O
variety O
of O
other O
datasets O
, O
including O
the O
TroFi O
( O
Birke O
and O
Sarkar O
, O
2006 O
) O
and O
Mohammad O
et O
al O
. O
( O
2016 O
) O
datasets O
, O
using O
Bi O
- O
LSTM B-MethodName
models O
coupled O
with O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
embeddings O
. O

They O
represent O
each O
word O
as O
the O
concatenation O
of O
a O
300 O
dimension O
GloVe B-MethodName
embeddings O
with O
an O
ELMo B-MethodName
vector O
. O

These O
are O
then O
input O
to O
a O
bidirectional O
LSTM B-MethodName
. O

As O
with O
thesequential O
model O
, O
they O
are O
input O
to O
a O
bidirecitonal O
LSTM B-MethodName
using O
GloVe B-MethodName
and O
ELMo B-MethodName
vectors O
. O

This O
paper O
remedies O
this O
state O
of O
affairs O
by O
training O
a O
Long O
Short O
- O
Term O
Memory O
network O
( O
LSTM B-MethodName
) O
over O
a O
realistically O
sized O
subset O
of O
child O
- O
directed O
input O
. O

We O
show O
that O
the O
LSTM B-MethodName
indeed O
abstracts O
new O
structures O
as O
learning O
proceeds O
. O

In O
contrast O
with O
previous O
models O
: O
( O
i O
) O
we O
train O
a O
vanilla O
char O
- O
LSTM B-MethodName
on O
a O
more O
realistic O
variety O
and O
amount O
of O
data O
, O
focusing O
on O
a O
limited O
amount O
of O
child O
- O
directed O
language O
; O
( O
ii O
) O
we O
do O
not O
rely O
on O
extrinsic O
evaluations O
or O
downstream B-TaskName
tasks I-TaskName
, O
instead O
we O
introduce O
a O
methodology O
to O
evaluate O
how O
the O
distribution O
of O
grammatical O
items O
, O
over O
time O
, O
comes O
to O
approximate O
the O
one O
in O
the O
input O
, O
through O
a O
continuous O
process O
and O
( O
iii O
) O
we O
tentatively O
explore O
the O
interaction O
between O
meaning B-MetricName
representations O
and O
the O
abstraction O
abilities O
of O
the O
network O
, O
blurring O
the O
distinction O
between O
lexicon O
and O
syntax O
, O
in O
a O
way O
more O
akin O
to O
Construction O
Grammar O
( O
CxG O
, O
Fillmore O
, O
1988 O
; O
Goldberg O
, O
1995 O
; O
Kay O
and O
Fillmore O
, O
1999 O
) O
. O

2 O
Related O
Work O
A O
considerable O
amount O
of O
literature O
has O
investigated O
the O
ability O
of O
ANNs B-MethodName
to O
acquire O
grammar O
, O
and O
the O
list O
we O
present O
here O
is O
by O
no O
means B-MetricName
exhaustive O
. O

The O
analysis O
of O
the O
syntactic O
abilities O
of O
LSTMs B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
and O
ANN O
- O
based O
language O
models O
dates O
back O
quite O
a O
few O
years O
( O
McClelland O
, O
1992 O
; O
Lewis O
and O
Elman O
, O
2001 O
) O
. O

Similarly O
, O
it O
has O
been O
shown O
that O
LSTMs B-MethodName
( O
McCoy O
et O
al O
. O
, O
2018 O
; O
Wilcox O
et O
al O
. O
, O
2018 O
) O
can O
learn O
tricky O
syntactic O
rules O
like O
the O
English O
auxiliary O
inversion O
and O
ﬁller O
- O
gap O
dependencies O
, O
although O
, O
in O
later O
work O
, O
McCoy O
et O
al O
. O
( O
2020 O
) O
ﬁnd O
that O
only O
models O
with O
an O
explicit O
inductive O
bias O
( O
Shen O
et O
al O
. O
, O
2018 O
) O
learn O
to O
generalize O
the O
MOVE O
-MAIN O
rule O
with O
respect O
to O
auxiliary O
inversion O
. O

Further O
studies O
have O
shown O
that O
networks O
carrying O
explicit O
inductive O
bias O
perform O
better O
than O
vanilla O
LSTMs B-MethodName
. O

In O
a O
recent O
paper O
, O
Lepori O
et O
al O
. O
( O
2020 O
) O
show O
that O
a O
constituency O
- O
based O
network O
generalizes O
more O
robustly O
than O
a O
dependencybased O
one O
, O
and O
that O
both O
outperform O
a O
more O
basic O
BiLSTM B-MethodName
. O

Lastly O
, O
we O
mention O
the O
study O
carried O
out O
by O
Kuncoro O
et O
al O
. O
( O
2018 O
) O
who O
perform O
their O
study O
using O
a O
character O
- O
based O
LSTM B-MethodName
– O
a O
choice O
we O
will O
follow O
in O
this O
work O
. O

That O
is O
, O
ANNs B-MethodName
are O
expected O
to O
gain O
abstract O
grammatical O
abilities O
through O
compositional O
generalization O
, O
where O
compositionality O
is O
understood O
as O
the O
ability O
to O
produce O
an O
unbounded O
number O
of O
sentences O
by O
means B-MetricName
of O
aset O
of O
algebraic O
rules O
( O
Baroni O
, O
2020 O
) O
. O

In O
contrast O
with O
this O
approach O
, O
usage O
- O
based O
models O
encourage O
us O
to O
adopt O
a O
different O
perspective O
, O
and O
to O
analyze O
LSTMs B-MethodName
’ O
grammatical O
abilities O
with O
respect O
to O
the O
kind O
of O
representations O
( O
more O
in O
§ O
3.3 O
) O
posited O
by O
theories O
such O
as O
Construction O
Grammar O
( O
CxG O
, O
Fillmore O
, O
1988 O
; O
Goldberg O
, O
1995 O
; O
Kay O
and O
Fillmore O
, O
1999 O
) O
. O

3.1 O
Computational O
complexity O
of O
the O
acquisition O
mechanism O
( O
C O
) O
Our O
aim O
is O
to O
test O
how O
much O
grammatical O
structure O
can O
be O
induced O
from O
linguistic O
input O
through O
a O
pattern-ﬁnding O
mechanism O
such O
as O
that O
provided O
by O
ANNs B-MethodName
. O

Therefore O
, O
we O
ﬁx O
the O
level O
of O
computational O
complexity O
to O
a O
vanilla O
, O
character O
- O
based O
LSTM B-MethodName
, O
which O
we O
train O
exploring O
different O
sources O
of O
input O
in O
a O
speciﬁc O
range O
fIig O
, O
selected O
based O
on O
their O
complexity O
level O
. O

( O
LSTM;I B-MethodName
i)a  O
! O
` O
i O
( O
1 O
) O
Our O
choice O
of O
model O
has O
consequences O
from O
a O
theoretical O
point O
of O
view O
. O

LSTMs B-MethodName
, O
under O
this O
perspective O
, O
can O
be O
seen O
as O
a O
domain O
- O
general O
attention O
and O
memory O
mecha O
- O
nism O
, O
without O
any O
explicitly O
hard O
- O
coded O
grammatical O
knowledge O
. O

On O
the O
continuum O
between O
specialized O
devices O
and O
general O
purpose O
associative O
mechanisms O
, O
LSTMs B-MethodName
place O
themselves O
on O
the O
latter O
side O
, O
with O
their O
recurrent O
structure O
seeming O
to O
be O
crucial O
in O
the O
linguistic O
abstraction O
process O
( O
Tran O
et O
al O
. O
, O
2018 O
) O
. O

In O
this O
sense O
, O
the O
speciﬁc O
features O
of O
the O
language O
on O
which O
ANNs B-MethodName
are O
trained O
can O
not O
be O
overlooked O
when O
it O
comes O
to O
describing O
their O
acquired O
grammatical O
abilities O
. O

Compared O
to O
what O
a O
child O
is O
exposed O
to O
during O
the O
most O
crucial O
months O
of O
language O
acquisition O
, O
ANNs B-MethodName
are O
trained O
on O
an O
input O
that O
is O
often O
unrealistic O
in O
size O
: O
the O
LSTM B-MethodName
introduced O
in O
Gulordava O
et O
al O
. O
( O
2018 O
) O
is O
for O
example O
exposed O
to O
90 O
M O
tokens O
, O
and O
sees O
them O
multiple O
times O
over O
training O
. O

4.2 O
Language O
models O
For O
each O
of O
the O
considered O
corpora O
, O
we O
train O
a O
character O
- O
based O
LSTM B-MethodName
on O
the O
tokenized O
, O
raw O
text O
. O

To O
do O
so O
, O
we O
slightly O
modify O
the O
PyTorch O
implementation O
of O
a O
vanilla O
LSTM.6 B-MethodName
, O
adapting O
it O
to O
a O
character O
- O
based O
setting O
. O

Part O
of O
Speech O
are O
preﬁxed O
by O
“ O
” O
and O
syntactic O
relations O
are O
preﬁxed O
by O
“ O
@ O
” O
by O
the O
LSTM B-MethodName
, O
which O
is O
only O
fed O
with O
raw O
text O
and O
is O
therefore O
completely O
agnostic O
about O
the O
linguistic O
categories O
superimposed O
by O
the O
parser O
. O

5 O
What O
do O
ANNs B-MethodName
approximate O
? O

Our O
ﬁrst O
analysis O
demonstrates O
that O
the O
language O
generated O
by O
the O
LSTM B-MethodName
reproduces O
the O
distribution O
of O
the O
input O
, O
and O
that O
this O
happens O
well O
beyond O
the O
lexical O
level O
: O
in O
other O
words O
, O
the O
network O
has O
acquired O
statistical O
regularities O
at O
the O
level O
of O
grammatical O
patterns O
, O
and O
is O
able O
to O
use O
them O
productively O
to O
generate O
novel O
language O
fragments O
that O
adhere O
to O
the O
same O
distribution O
as O
the O
input O
. O

In O
this O
paper O
, O
we O
have O
reviewed O
relevant O
work O
concerning O
the O
assessment O
of O
grammatical O
abilities O
in O
neural O
language O
models O
and O
noted O
the O
lack O
of O
variety O
in O
both O
the O
input O
data O
fed O
to O
ANNs B-MethodName
( O
I O
) O
and O
the O
theoretical O
framework O
used O
in O
analysing O
the O
output O
language O
( O
 O
) O
. O

In O
line O
with O
the O
existing O
usagebased O
computational O
accounts O
, O
we O
have O
introduced O
a O
methodology O
to O
evaluate O
the O
level O
of O
productivity O
of O
an O
LSTM B-MethodName
trained O
on O
limited O
, O
child O
- O
directed O
data O
, O
using O
inspirations O
from O
constructionist O
approaches O
. O

We O
have O
been O
able O
to O
show O
that O
neural B-MethodName
networks I-MethodName
approximate O
the O
distribution O
of O
constructions O
at O
a O
quite O
reﬁned O
level O
when O
trained O
over O
a O
bare O
3 O
M O
10p= O
6:988142426844016 O
e-28forcat1andp= O
7:420868598608134 O
e-32forcat2cat1 O
cat2 O
input O
BM O
5 O
10 O
15 O
20 O
25 O
30 O
35 O
distributional O
shift O
a O
minute O
a O
NOUN O
0.28 O
0.32 O
0.71 O
0.51 O
0.44 O
0.39 O
0.38 O
0.37 O
0.34 O
0.37 O
a O
minute O
a O
@root O
0.13 O
0.19 O
0.49 O
0.37 O
0.26 O
0.20 O
0.21 O
0.22 O
0.20 O
0.30 O
you O
VERB O
it O
PRON O
@root O
@expl O
0.10 O
0.19 O
0.46 O
0.28 O
0.25 O
0.25 O
0.19 O
0.17 O
0.21 O
0.25 O
you O
VERB O
you O
you O
VERB O
@iobj O
0.28 O
0.40 O
0.68 O
0.56 O
0.47 O
0.49 O
0.39 O
0.42 O
0.43 O
0.25 O
we O
can O
VERB O
PRON O
can O
@root O
0.51 O
0.54 O
0.79 O
0.74 O
0.59 O
0.54 O
0.55 O
0.61 O
0.57 O
0.22 O
goVERB O
@obj O
VERB O
@conj O
@obj O
0.64 O
0.72 O
0.56 O
0.74 O
0.70 O
0.74 O
0.72 O
0.72 O
0.72 O
-0.16 O
AUX O
hungry O
@cop O
@conj O
0.68 O
0.52 O
0.36 O
0.39 O
0.44 O
0.45 O
0.47 O
0.42 O
0.59 O
-0.24 O
can O
get O
can O
@advcl O
0.55 O
0.54 O
0.24 O
0.36 O
0.45 O
0.48 O
0.43 O
0.39 O
0.52 O
-0.28 O
Table O
4 O
: O
Pairs O
of O
catenae O
( O
cat1;cat O
2 O
) O
, O
their O
cosine O
similarity O
in O
the O
space O
obtained O
from O
CHILDES O
, O
in O
the O
space O
obtained O
from O
the O
best O
model O
( O
BM O
) O
and O
in O
all O
the O
intermediate O
models O
. O

Most O
importantly O
, O
the O
introduced O
methodology O
, O
despite O
being O
preliminary O
, O
presents O
a O
number O
of O
features O
that O
make O
our O
study O
ﬁt O
in O
the O
usage O
- O
based O
theoretical O
framework O
while O
also O
using O
neural B-MethodName
networks I-MethodName
as O
language O
modeling O
tools O
, O
more O
specifically O
: O
( O
i O
) O
it O
posits O
no O
sharp O
distinction O
between O
lexicon O
and O
grammar O
: O
fully O
lexicalized O
, O
partially O
ﬁlled O
and O
purely O
syntactic O
patterns O
are O
all O
part O
of O
our O
constructicon O
and O
can O
play O
a O
similar O
role O
in O
production O
. O

Hedderich O
and O
Klakow O
( O
2018 O
) O
, O
following O
Goldberger O
and O
Ben O
- O
Reuven O
( O
2017 O
) O
, O
add O
a O
noise O
adaptation O
layer O
on O
top O
of O
an O
LSTM B-MethodName
, O
which O
learns O
how O
to O
correct O
noisy O
labels O
, O
given O
a O
small O
amount O
of O
training O
data O
. O

4.3.2 O
Neural O
Model O
A O
common O
neural O
model O
for O
NER B-TaskName
is O
the O
BiLSTM B-MethodName
- O
CRF O
model O
( O
Ma O
and O
Hovy O
, O
2016 O
) O
. O

4Separate O
experiments O
show O
that O
omitting O
gazetteers O
impacts O
performance O
only O
slightly O
. O
When O
using O
a O
standard O
BiLSTM B-MethodName
- O
CRF O
model O
, O
the O
loss O
of O
a O
dataset O
( O
D O
) O
composed O
of O
sentences O
( O
s O
) O
is O
calculated O
as O
: O
L= X O
s2DlogP(y(s)jx(s O
) O
) O
( O
13 O
) O
WhereP(y(s)jx(s))is O
calculated O
by O
the O
CRF O
over O
outputs O
from O
the O
BiLSTM B-MethodName
. O

We O
use O
pretrained O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
word O
vectors O
for O
English O
, O
and O
the O
same O
pretrained O
vectors O
used O
in O
Lample O
et O
al O
. O
( O
2016 O
) O
for O
Dutch O
, O
German O
, O
and O
Spanish O
. O

As O
with O
our O
BiLSTM B-MethodName
experiments O
, O
we O
use O
pretrained O
GloVe B-MethodName
word O
vectors O
for O
English O
, O
and O
the O
same O
pretrained O
vectors O
used O
in O
Lample O
et O
al O
. O
( O
2016 O
) O
for O
Dutch O
, O
German O
, O
and O
Spanish O
. O

6The O
code O
was O
kindly O
provided O
by O
the O
authors O
. O
MethodnLanguage O
Tool O
eng O
deu O
esp O
ned O
amh O
ara O
hin O
som O
avg O
GoldCogcomp O
89.1 O
72.5 O
82.5 O
82.6 O
67.2 O
53.4 O
74.4 O
80.3 O
75.3 O
BiLSTM B-MethodName
- O
CRF O
90.3 O
77.3 O
85.2 O
81.1 O
69.2 O
52.8 O
73.8 O
82.3 O
76.5 O
Oracle O
WeightingCogcomp O
83.7 O
65.7 O
76.2 O
76.4 O
54.3 O
42.0 O
56.3 O
68.5 O
65.4 O
BiLSTM B-MethodName
- O
CRF O
87.8 O
70.2 O
78.5 O
70.4 O
60.4 O
43.4 O
57.6 O
73.2 O
67.7 O
Noise O
Adaptation O
( O
Hedderich O
, O
2018 O
) O
61.5 O
46.1 O
57.3 O
41.5 O
– O
– O
– O
– O
– O
Self O
- O
training O
( O
Jie O
et O
al O
. O
, O
2019 O
) O
82.3 O
65.2 O
76.3 O
65.5 O
52.1 O
40.1 O
55.1 O
65.3 O
62.7 O
Raw O
AnnotationsCogcomp O
54.8 O
36.9 O
49.5 O
47.9 O
31.0 O
32.6 O
30.9 O
44.0 O
40.9 O
BiLSTM B-MethodName
- O
CRF O
73.3 O
57.7 O
61.9 O
58.3 O
42.2 O
36.8 O
47.5 O
54.9 O
54.1 O
CBL O
- O
RawCogComp O
74.7 O
63.0 O
68.7 O
67.0 O
45.0 O
37.8 O
50.6 O
67.9 O
59.3 O
BiLSTM B-MethodName
- O
CRF O
84.6 O
67.9 O
79.6 O
70.0 O
52.9 O
42.1 O
55.2 O
70.4 O
65.3 O
Combined O
WeightingCogcomp O
75.2 O
56.6 O
70.8 O
70.8 O
46.5 O
44.1 O
57.5 O
60.2 O
60.2 O
BiLSTM B-MethodName
- O
CRF O
73.5 O
60.3 O
64.9 O
61.9 O
48.0 O
38.0 O
49.0 O
56.6 O
56.5 O
CBL O
- O
CombinedCogcomp O
77.3 O
61.8 O
74.0 O
72.4 O
49.2 O
43.7 O
58.2 O
67.6 O
63.0 O
BiLSTM B-MethodName
- O
CRF O
81.1 O
64.9 O
74.9 O
63.4 O
52.2 O
39.8 O
52.0 O
67.0 O
61.9 O
Table O
2 O
: O
F1 B-MetricName
scores B-MetricName
on O
English O
, O
German O
, O
Spanish O
, O
Dutch O
, O
Amharic O
, O
Arabic O
, O
Hindi O
, O
and O
Somali O
. O

Each O
section O
shows O
performance O
of O
both O
Cogcomp O
( O
non O
- O
neural O
) O
and O
BiLSTM B-MethodName
( O
neural O
) O
systems O
. O

On O
average B-MetricName
, O
our O
best O
results O
are O
found O
in O
the O
uninitialized O
( O
Raw O
) O
CBL O
from O
BiLSTM B-MethodName
- O
CRF O
. O

Our O
best O
model O
, O
CBL O
- O
Raw O
BiLSTMCRF B-MethodName
, O
improves O
over O
the O
Raw O
Annotations O
BiLSTMCRF B-MethodName
baseline O
by O
11.2 O
points O
F1 B-MetricName
, O
and O
the O
Self O
- O
training O
prior O
work O
by O
2.6 B-MetricValue
points O
F1 B-MetricName
, O
showing O
that O
it O
is O
an O
effective O
way O
to O
address O
the O
problem O
of O
partial O
annotation O
. O

We O
leave O
it O
to O
future O
work O
to O
discover O
improved O
ways O
of O
incorporating O
instance O
weights B-HyperparameterName
in O
a O
BiLSTM B-MethodName
- O
CRF O
. O

We O
show O
results O
in O
Table O
5 O
, O
using O
the O
BiLSTM B-MethodName
- O
CRF O
model O
. O

We O
used O
www.isi.edu/ O
˜ulf O
/ O
uroman.html O
8From O
translate.google.comTest O
Scheme O
P O
R O
F1 B-MetricName
( O
Zhang O
et O
al O
. O
, O
2016 B-MetricValue
) O
- O
- O
34.8 O
( O
Tsai O
et O
al O
. O
, O
2016 O
) O
- O
- O
43.3 O
( O
Pan O
et O
al O
. O
, O
2017 O
) O
- O
- O
44.0 O
( O
Mayhew O
et O
al O
. O
, O
2017 O
) O
- O
- O
46.2 O
BILSTM B-MethodName
- O
CRF O
Train O
on O
Gold O
71.6 O
70.2 O
70.9 O
Raw O
annotations O
73.0 O
23.8 O
35.9 O
Combined O
Weighting O
65.9 O
34.2 O
45.0 O
CBL O
- O
Raw O
57.8 O
47.3 O
52.0 O
CBL O
- O
Combined O
58.3 O
44.2 O
50.2 O
Table O
5 O
: O
Bengali O
manual O
annotation O
results O
. O

All O
classiﬁcation O
tasks O
are O
implemented O
using O
the O
SVMmulticlassclassiﬁer B-MethodName
( O
Joachims O
, O
1999 O
) O
. O

All O
three O
downstream B-TaskName
systems O
employ O
‘ O
vintage O
’ O
classiﬁers O
( O
CRFs O
and O
SVMs B-MethodName
) O
for O
which O
regularization O
techniques O
and O
best O
practices O
are O
well O
established O
, O
such O
that O
one O
can O
hope O
for O
a O
certain O
degree O
of O
feature O
selection O
during O
training O
. O

Based O
on O
the O
long O
- O
held O
observation O
that O
language O
exhibits O
hierarchical O
structure O
, O
previous O
work O
has O
proposed O
coupling O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
with O
di O
 O
erentiable O
stack O
data O
structures O
( O
Joulin O
and O
Mikolov O
, O
2015 O
; O
Grefenstette O
et O
al O
. O
, O
2015 O
) O
to O
give O
them O
some O
of O
the O
computational O
power O
of O
pushdown O
automata O
( O
PDAs O
) O
, O
the O
class O
of O
automata O
that O
recognize O
context O
- O
free O
languages O
( O
CFLs O
) O
. O

com O
/ O
bdusell O
/ O
nondeterministic O
- O
stack O
- O
rnn O
.2 O
Background O
and O
Motivation O
In O
all O
di O
 O
erentiable O
stack O
- O
augmented O
networks O
that O
we O
are O
aware O
of O
( O
including O
ours O
) O
, O
a O
network O
called O
thecontroller O
, O
which O
is O
some O
kind O
of O
RNN O
( O
typically O
an O
LSTM B-MethodName
) O
, O
is O
augmented O
with O
a O
di O
 O
erentiable O
stack O
, O
which O
has O
no O
parameters O
of O
its O
own O
. O

4.1 O
Model O
The O
controller O
can O
be O
any O
type O
of O
RNN O
; O
in O
our O
experiments O
, O
we O
used O
a O
LSTM B-MethodName
RNN O
. O

5.4 O
Baselines O
We O
compare O
our O
NS O
- O
RNN O
against O
three O
baselines O
: O
an O
LSTM B-MethodName
, O
the O
Stack O
LSTM B-MethodName
of O
Joulin O
and O
Mikolov O
( O
2015 O
) O
( O
“ O
JM O
” O
) O
, O
and O
the O
Stack O
LSTM B-MethodName
of O
Grefenstette O
et O
al O
. O
( O
2015 O
) O
( O
“ O
Gref O
” O
) O
. O

For O
all O
three O
stack O
models O
, O
we O
use O
an O
LSTM B-MethodName
controller O
whose O
initial O
hidden O
state O
is O
ﬁxed O
to O
0 O
, O
and O
we O
use O
only O
one O
stack O
for O
the O
JM O
and O
Gref O
models O
. O

We O
encode O
all O
input O
symbols O
as O
one O
- O
hot O
vectors O
; O
there O
are O
no O
embedding O
layers.5.5 O
Hyperparameters O
For O
all O
models O
, O
we O
use O
a O
single O
- O
layer O
LSTM B-MethodName
with O
20 O
hidden B-HyperparameterName
units I-HyperparameterName
. O

We O
selected O
this O
number O
because O
we O
found O
that O
an O
LSTM B-MethodName
of O
this O
size O
could O
not O
completely O
solve O
the O
marked O
reversal O
task O
, O
indicating O
that O
the O
hidden O
state O
is O
a O
memory O
bottleneck O
. O

As O
noted O
by O
Grefenstette O
et O
al O
. O
( O
2015 O
) O
, O
initialization B-HyperparameterName
can O
play O
a O
large O
role O
in O
whether O
a O
Stack O
LSTM B-MethodName
converges O
on O
algorithmic O
behavior O
or O
becomes O
trapped O
in O
a O
local O
optimum O
. O

We O
initialize O
all O
fullyconnected O
layers O
except O
for O
the O
recurrent O
LSTM B-MethodName
layer O
with O
Xavier O
uniform O
initialization B-HyperparameterName
( O
Glorot O
and O
Bengio O
, O
2010 B-HyperparameterValue
) O
, O
and O
all O
other O
parameters O
uniformly O
from O
[ O
 0:1;0:1 O
] O
. O

For O
all O
tasks O
, O
stackbased O
models O
outperform O
the O
LSTM B-MethodName
baseline O
, O
indicating O
that O
the O
tasks O
are O
e O
 O
ective O
benchmarks O
for O
di O
 O
erentiable O
stacks O
. O

Although O
the O
NS O
- O
RNN O
outperforms O
the O
LSTM B-MethodName
baseline O
, O
the O
JM O
model O
solves O
the O
task O
most O
e O
 O
ectively O
, O
though O
still O
imperfectly O
. O

We O
showed O
that O
it O
o O
 O
ers O
improved O
trainability O
and O
modeling O
power O
over O
previous O
stack O
- O
based O
neural O
language O
models O
; O
the O
NSRNN O
learns O
to O
solve O
some O
deterministic O
tasks O
more O
e O
 O
ectively O
than O
other O
stack O
- O
LSTMs B-MethodName
, O
and O
achieves O
the O
best O
results O
on O
a O
challenging O
nondeterministic O
context O
- O
free O
language O
. O

However O
, O
we O
note O
that O
the O
NS O
- O
RNN O
struggled O
on O
a O
task O
where O
signals O
in O
the O
data O
were O
distant O
, O
and O
did O
not O
generalize O
to O
longer O
lengths O
as O
well O
as O
other O
stack O
- O
LSTMs B-MethodName
; O
we O
hope O
to O
address O
these O
shortcomings O
in O
future O
work O
. O

We O
explore O
implications O
of O
our O
results O
on O
machine O
translation O
and O
synthetic O
tasks.1 O
2 O
Related O
Work O
Computational O
Power O
of O
neural B-MethodName
networks I-MethodName
has O
been O
studied O
since O
the O
foundational O
paper O
McCulloch O
and O
Pitts O
( O
1943 O
) O
; O
in O
particular O
, O
among O
sequence O
- O
to O
- O
sequence O
models O
, O
this O
aspect O
of O
RNNs O
has O
long O
been O
studied O
( O
Kolen O
and O
Kremer O
, O
2001 O
) O
. O

A O
class O
of O
seq O
- O
to O
- O
seq O
neural B-MethodName
networks I-MethodName
is O
Turingcomplete O
if O
the O
class O
of O
languages O
recognized O
by O
the O
networks O
is O
exactly O
the O
class O
of O
languages O
recognized O
by O
Turing O
machines O
. O

Transformer O
Networks O
with O
positional O
encodings O
are O
not O
necessarily O
equivalent O
in O
terms O
of O
their O
computational O
expressiveness O
( O
Yun O
et O
al O
. O
, O
2020 O
) O
to O
those O
with O
only O
positional O
masking O
when O
considering O
the O
encoder O
only O
model O
( O
as O
used O
in O
BERT B-MethodName
and O
GPT-2 B-MethodName
) O
. O

Although O
our O
proofs O
rely O
on O
arbitrary O
precision B-MetricName
, O
which O
is O
common O
practice O
while O
studying O
the O
computational O
power O
of O
neural B-MethodName
networks I-MethodName
in O
theory O
( O
Siegelmann O
and O
Sontag O
, O
1992 B-MetricValue
; O
P O
´ O
erez O
et O
al O
. O
, O
2019 O
; O
Hahn O
, O
2020 O
; O
Yun O
et O
al O
. O
, O
2020 O
) O
, O
implementations O
in O
practice O
work O
over O
ﬁxed O
precision B-MetricName
settings O
. O

For O
 O
the O
two O
models O
using O
neural B-MethodName
networks I-MethodName
, O
our O
pr O
oposed O
model O
outperformed O
the O
Vo O
 O
& O
Zhang O
 O
model O
, O
and O
the O
result O
was O
statistically O
significant O
 O
at O
p=0.01 O
 O
using O
t O
- O
test O
. O

Recent O
work O
on O
Chinese O
and O
Arabic O
word O
segmentation O
uses O
bidirectional O
neural O
network O
models O
to O
predict O
word O
boundaries O
, O
e.g. O
based O
on O
biLSTMs B-MethodName
( O
Ma O
et O
al O
. O
, O
2018 O
; O
Almuhareb O
et O
al O
. O
, O
2019 O
) O
or O
a O
pre O
- O
trained O
BERT B-MethodName
model O
( O
Huang O
et O
al O
. O
, O
2020 O
) O
. O

These O
models O
are O
not O
directly O
applicable O
for O
our O
task O
of O
tokenization O
repair O
for O
the O
English O
language O
, O
since O
our O
inputs O
contain O
spurious O
and O
missing O
spaces O
and O
the O
English O
BERT B-MethodName
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
is O
pre O
- O
trained O
on O
text O
with O
correct O
tokenization O
. O

Following O
Graves O
( O
2013 O
) O
, O
we O
implement O
these O
models O
as O
recurrent O
neural B-MethodName
networks I-MethodName
, O
using O
LSTM B-MethodName
cells O
. O

Our O
architecture O
consists O
of O
an O
LSTM B-MethodName
cell O
with O
1024 O
units O
, O
followed O
by O
a O
dense O
layer O
( O
with O
1024 O
units O
and O
ReLU O
activation B-HyperparameterName
) O
and O
a O
softmax O
output O
layer O
for O
character O
classiﬁcation O
. O

Our O
architecture O
consists O
of O
a O
bidirectional O
LSTM B-MethodName
cell O
with O
1024 O
units O
, O
followed O
by O
a O
dense O
layer O
( O
with O
1024 O
units O
and O
ReLU O
activation B-HyperparameterName
) O
and O
a O
sigmoid O
output O
unit O
. O

With O
the O
advances O
in O
computing O
power O
and O
the O
rebirth O
of O
neural B-MethodName
networks I-MethodName
, O
the O
dominant O
paradigm O
has O
become O
the O
use O
of O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
( O
Mikolov O
et O
al O
. O
, O
2010 O
) O
. O

We O
investigate O
the O
proposed O
method O
using O
a O
simple O
LSTM B-MethodName
model O
and O
a O
small O
- O
size O
Transformer O
model O
on O
the O
IEMOCAP B-MetricName
dataset O
( O
Busso O
et O
al O
. O
, O
2008 O
) O
, O
composed O
of O
ﬁve O
acted O
sessions O
, O
for O
a O
fourclass O
emotions O
classiﬁcation O
and O
we O
compare O
to O
the O
state O
of O
the O
art O
( O
Mirsamadi O
et O
al O
. O
, O
2017 O
) O
model O
, O
a O
local O
attention O
based O
BiLSTM B-MethodName
. O

5.1 O
TOI O
in O
Language O
Modelling O
For O
language O
modeling O
, O
we O
use O
three O
different O
methods O
: O
A O
simple O
LSTM B-MethodName
that O
does O
not O
beneﬁt O
from O
extensive O
hyper O
- O
parameter O
optimization O
. O

An O
Average B-MetricName
Stochastic O
Gradient O
Descent O
Weight O
- O
Dropped O
LSTM B-MethodName
( O
AWD O
- O
LSTM B-MethodName
) O
as O
described O
in O
Merity O
et O
al O
. O
( O
2017 B-MetricValue
) O
, O
with O
the O
same O
hyper O
- O
parameters O
. O

For O
the O
Simple O
LSTM B-MethodName
and O
AWD O
- O
LSTM B-MethodName
, O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
20 B-HyperparameterValue
and O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
400 B-HyperparameterValue
. O

AWD O
- O
LSTM B-MethodName
and O
MoS O
are O
trained O
on O
1000 O
epochs B-HyperparameterName
, O
and O
the O
Simple O
LSTM B-MethodName
on O
100 B-HyperparameterValue
epochs B-HyperparameterName
. O

5.2 O
TOI O
in O
Speech O
Emotion O
Recognition O
For O
Speech O
Emotion O
Recognition O
( O
SER O
) O
we O
use O
two O
different O
models O
: O
the O
encoder O
of O
the O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
followed O
by O
convolutional O
layers O
, O
and O
the O
simple O
LSTM B-MethodName
used O
in O
text O
domain O
case O
. O

Since O
the O
Transformer O
is O
stateless O
and O
uses O
self O
- O
attention O
instead O
, O
we O
are O
able O
to O
investigate O
the O
effect O
of O
Alleviated O
TOI O
( O
P O
) O
independently O
of O
LSTM B-MethodName
cells O
. O

We O
apply O
the O
methodology O
used O
in O
text O
on O
the O
SER O
task O
, O
using O
the O
simple O
LSTM B-MethodName
and O
a O
window O
size O
of O
300 O
frames O
. O

Finally O
, O
to O
investigate O
the O
effect O
of O
the O
Alleviated O
TOI O
( O
P O
) O
strategy O
independently O
of O
LSTM B-MethodName
cells O
, O
we O
design O
a O
ﬁnal O
experiment O
in O
the O
SER O
task O
. O

Comparison O
with O
State O
of O
the O
Art O
and O
Simple O
LSTM B-MethodName
. O

Table O
3 O
demonstrates O
how O
models O
can O
be O
improved O
by O
applying O
our O
Alleviated O
TOI O
method O
on O
2 O
latest O
state O
- O
of O
- O
the O
- O
art O
models O
: O
AWD O
- O
LSTM B-MethodName
( O
Merity O
et O
al O
. O
, O
2017 O
) O
and O
AWDLSTM B-MethodName
- O
MoS O
( O
Yang O
et O
al O
. O
, O
2017 O
) O
, O
and O
the O
Simple O
LSTM B-MethodName
model O
. O

To O
ensure O
fairness B-MetricName
, O
we O
allocate O
the O
same O
computational O
resources O
for O
the O
base O
model O
as O
well O
the O
model O
with O
Alleviated O
TOI O
, O
i.e. O
we O
train O
with O
the O
equivalent O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
. O
Model O
test O
ppl O
AWD O
- O
LSTM B-MethodName
( O
Merity O
et O
al O
. O
, O
2017 B-MetricValue
) O
58.8 O
AWD O
- O
LSTM B-MethodName
+ O
Alleviated O
TOI O
56.46 O
AWD O
- O
LSTM B-MethodName
- O
MoS O
( O
Yang O
et O
al O
. O
, O
2017 O
) O
55.97 O
AWD O
- O
LSTM B-MethodName
- O
MoS O
+ O
Alleviated O
TOI O
54.58 O
Simple O
- O
LSTM B-MethodName
75.36 O
Simple O
- O
LSTM B-MethodName
+ O
Alleviated O
TOI O
74.44 O
Table O
3 O
: O
Comparison O
between O
state O
- O
of O
- O
the O
- O
art O
models O
( O
Merity O
et O
al O
. O
, O
2017 O
; O
Yang O
et O
al O
. O
, O
2017 O
) O
and O
a O
Simple O
LSTM B-MethodName
, O
and O
the O
same O
models O
with O
Alleviated O
TOI O
. O

The O
matrix O
with O
P= O
7 O
shows O
a O
good O
distribution O
— O
corresponding O
to O
the O
strong O
performance O
— O
and O
the O
matrix O
with O
P= O
10 O
shows O
that O
each O
row O
contains O
a O
low O
diversity O
of O
data O
points O
. O
Experiment O
WA O
UA O
Extreme O
TOI O
( O
15k O
steps O
) O
0.475 O
0.377 O
Inter O
- O
batch O
TOI O
( O
15k O
steps O
) O
0.478 O
0.386 O
Standard O
TOI O
( O
15k O
steps O
) O
0.486 O
0.404 O
Alleviated O
TOI O
( O
15k O
steps O
) O
0.553 O
0.489 O
Alleviated O
TOI O
( O
60 O
epochs B-HyperparameterName
) O
0.591 B-HyperparameterValue
0.523 O
Table O
5 O
: O
Token O
order O
imbalance O
( O
TOI O
) O
comparison O
for O
the O
IEMOCAP B-MetricName
dataset O
on O
a O
SER O
task O
using O
angry O
, O
happy O
, O
neutral O
andsadclasses O
with O
a O
simple O
LSTM B-MethodName
model O
. O

Table O
6 O
reports O
the O
result O
of O
a O
ﬁnal O
experiment O
that O
aims O
to O
investigate O
the O
effect O
of O
Alleviated O
TOI O
( O
P O
) O
independently O
of O
LSTM B-MethodName
cells O
. O

We O
also O
experimented O
with O
an O
encoder O
comprising O
of O
three O
separate O
LSTMs B-MethodName
whose O
output O
states O
are O
concatenated O
and O
used O
to O
predict O
the O
MSD O
for O
the O
lemma O
to O
be O
inﬂected O
. O

Architecture O
The O
encoder O
uses O
three O
separate O
single O
- O
layer O
bidirectional O
LSTMs B-MethodName
( O
LSTM B-MethodName
left O
, O
LSTM B-MethodName
baseandLSTM B-MethodName
right O
) O
to O
encode O
the O
input O
into O
a O
ﬁxed O
length O
vector O
c. O
It O
is O
based O
on O
Vylomova O
et O
al O
. O
( O
2017 O
) O
. O

LSTM B-MethodName
lefttakes O
as O
input O
the O
sequence O
that O
is O
to O
the O
left O
of O
the O
current O
lemma O
in O
the O
sentence O
and O
computes O
the O
hidden O
states O
h(t O
) O
l∈RH O
. O

In O
a O
similar O
fashion O
, O
LSTM B-MethodName
right O
takes O
as O
input O
the O
sequence O
that O
is O
to O
the O
right O
of O
the O
current O
lemma O
in O
the O
sentence O
. O

h(t O
) O
r O
= O
fr(x(t O
) O
, O
h(t−1 O
) O
r O
) O
LSTM B-MethodName
basetakes O
as O
input O
the O
sequence O
of O
character O
embeddings O
, O
e(ct O
) O
, O
concatenated O
with O
the O
lemma O
embedding O
, O
e(l O
) O
, O
for O
every O
character O
cin O
the O
lemma O
. O

h(t O
) O
b O
= O
fb(x(t O
) O
b O
, O
h(t−1 O
) O
b O
) O
The O
input O
to O
the O
decoder O
is O
the O
vector O
c∈R3H O
which O
is O
the O
concatenation O
of O
the O
ﬁnal O
states O
from O
the O
three O
encoder O
LSTMs B-MethodName
. O

Existing O
language O
modeling O
( O
LM O
) O
based O
approaches O
such O
as O
Para2Vec O
( O
Le O
and O
Mikolov O
, O
2014 O
) O
rely O
on O
deep O
neural B-MethodName
networks I-MethodName
to O
generate O
vector O
representations O
for O
the O
paragraph O
- O
level O
. O

( O
Dai O
and O
Le O
, O
2015 O
) O
proposed O
an O
approach O
to O
improve O
the O
vector O
representations O
with O
pre O
- O
trained O
recurrent O
neural B-MethodName
networks I-MethodName
. O

We O
then O
apply O
Principal O
Component O
Analysis O
( O
PCA B-MethodName
) O
( O
Wold O
et O
al O
. O
, O
1987 O
) O
to O
reduce O
the O
dimensionality O
of O
this O
matrix O
. O

On O
the O
right O
hand O
, O
we O
show O
the O
corresponding O
SimMatrix O
( O
before O
we O
apply O
PCA B-MethodName
) O
. O

Then O
we O
use O
PCA B-MethodName
to O
reduce O
the O
number O
of O
columns O
, O
loosely O
speaking O
, O
creating O
a O
set O
of O
representative O
utterances O
in O
a O
datadriven O
manner O
. O

Thus O
, O
the O
resulting O
vector O
is O
closer O
to O
neighbors O
of O
both O
points O
, O
shrinking O
the O
distances O
between O
neighbors O
of O
both O
utterances O
. O
Moreover O
, O
because O
each O
feature O
( O
dimension O
) O
in O
the O
space O
is O
a O
representative O
utterance O
, O
after O
the O
collapsing O
of O
two O
utterances O
, O
we O
perform O
PCA B-MethodName
to O
come O
up O
with O
the O
new O
dimensions O
after O
the O
user O
is O
done O
with O
the O
labeling O
. O

For O
all O
corpora O
, O
a O
linear O
SVM B-MethodName
classiﬁer O
is O
trained O
per O
intent O
in O
one O
- O
vs O
- O
all O
fashion O
. O

To O
start O
, O
the O
authors O
reported O
that O
binary O
SVM B-MethodName
classiﬁcation O
of O
creole O
and O
non O
- O
creole O
languages O
failed O
to O
distinguish O
the O
two O
classes O
, O
even O
though O
their O
underlying O
distributions O
are O
quite O
different O
. O

Chau O
et O
al O
. O
( O
2020 O
) O
used O
this O
dataset O
as O
a O
low O
- O
resource O
language O
test O
case O
for O
their O
method O
of O
pretraining O
mBERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

We O
conduct O
all O
the O
experiments O
on O
both O
English O
BERT B-MethodName
and O
multilingual O
mBERT B-MethodName
models O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

As O
our O
baseline O
, O
we O
consider O
pretrained O
BERT B-MethodName
Base O
and O
mBERT B-MethodName
models O
, O
and O
evaluate O
them O
on O
our O
development O
splits O
for O
the O
creoles O
. O

These O
are O
obtained O
in O
BERT B-MethodName
6Nigerian O
Pidgin O
: O
http://naijalingo.com/ O
. O

html O
.Nigerian O
Pidgin O
Singlish O
Haitian O
Creole O
BERT B-MethodName
P@1 O
P O
D@1 O
PLL O
P@1 O
P O
D@1 O
PLL O
P@1 O
P O
D@1 O
PLL O
Pretrained O
22.79 O
10.92 O
142.65 O
23.94 O
21.09 O
76.01 O
18.84 O
5.65 O
177.40MIXEDERM O
63.83 O
59.97 O
42.41 O
46.77 O
42.89 O
41.06 O
68.09 O
43.35 O
55.04 O
DRO O
- O
One O
60.99 O
56.76 O
52.51 O
44.23 O
40.73 O
49.18 O
57.04 O
36.73 O
121.51 O
DRO O
- O
Random O
60.40 O
56.33 O
52.69 O
43.33 O
39.07 O
49.14 O
57.65 O
36.16 O
119.17 O
DRO O
- O
Language O
60.40 O
54.80 O
54.17 O
43.19 O
39.57 O
48.88 O
57.55 O
36.69 O
118.85C O
- O
O O
NLYERM O
73.72 O
71.38 O
28.14 O
53.80 O
51.26 O
34.22 O
73.15 O
55.50 O
55.51 O
DRO O
- O
One O
64.28 O
59.86 O
61.81 O
45.34 O
43.59 O
66.53 O
58.16 O
36.91 O
144.46 O
DRO O
- O
Random O
63.72 O
59.31 O
60.31 O
45.73 O
42.40 O
64.16 O
57.65 O
37.41 O
142.04 O
DRO O
- O
Language O
63.58 O
59.74 O
56.82 O
44.73 O
40.57 O
53.72 O
56.94 O
35.50 O
138.60 O
Table O
3 O
: O
Intrinsic O
evaluation O
: O
Precision@1 B-MetricName
( O
P@1 O
) O
, O
Precision@1 B-MetricName
for O
words O
in O
our O
creole O
dictionary O
( O
P O
D@1 O
) O
, O
and O
average B-MetricName
Pseudo O
- O
log O
- O
likelihood O
score B-MetricName
( O
PLL O
) O
. O

6 O
Results O
and O
Analyses O
Intrinsic O
evaluation O
The O
main O
ﬁnding O
of O
the O
intrinsic O
evaluation O
is O
that O
ERM O
outperforms O
DRONigerian O
Pidgin O
Singlish O
BERT B-MethodName
NER B-TaskName
[ O
F O
1 O
] O
UPOS B-TaskName
[ O
Acc O
] O
UPOS B-TaskName
[ O
Acc]MIXEDERM O
87.86 O
98.00 O
91.24 O
DRO O
- O
Language O
88.40 O
98.06 O
90.22C O
- O
O O
NLYERM O
87.98 O
98.04 O
91.17 O
DRO O
- O
Language O
87.12 O
97.98 O
90.44 O
Table O
4 O
: O
Extrinsic O
evaluation O
. O

§ O
A O
for O
full O
results O
with O
both O
BERT B-MethodName
and O
mBERT B-MethodName
) O
. O

While O
we O
only O
report O
results O
for O
BERT B-MethodName
here O
, O
we O
observe O
the O
same O
patters O
with O
mBERT B-MethodName
( O
see O
App O
. O

We O
explore O
their O
effects O
below O
. O
Nigerian O
Pidgin O
BERT B-MethodName
Size O
P@1 O
P O
D@1 O
PLL O
ERMTiny O
31.31 O
26.12 O
110.23 O
Small O
47.39 O
46.75 O
77.47 O
Base O
63.83 O
59.97 O
42.41 O
DRO O
- O
LanguageTiny O
31.00 O
23.09 O
99.70 O
Small O
43.00 O
37.75 O
82.50 O
Base O
60.40 O
54.80 O
54.17 O
Table O
5 O
: O
Over O
- O
parameterization O
experiments O
with O
MIXED O
-LANGUAGE O
Nigerian O
Pidgin O
English O
data O
. O

In O
order O
to O
investigate O
the O
role O
of O
over O
- O
parameterization O
in O
our O
experiments O
, O
we O
ran O
additional O
MIXED O
-LANGUAGE O
experiments O
on O
Nigerian O
Pidgin O
English O
, O
with O
different O
sized O
BERT B-MethodName
models O
, O
namely O
BERT B-MethodName
Tiny O
, O
BERT B-MethodName
Small O
( O
Jiao O
et O
al O
. O
, O
2020 O
) O
, O
and O
BERT B-MethodName
Base O
. O

5 O
demonstrate O
that O
over O
- O
parameterization O
was O
not O
a O
leading O
cause O
for O
DRO O
failure O
, O
otherwise O
we O
would O
expect O
for O
smaller O
BERT B-MethodName
versions O
to O
have O
relative O
better O
performance O
compared O
to O
the O
corresponding O
ERM O
runs O
. O

Instead O
, O
we O
see O
that O
standard O
BERT B-MethodName
works O
ﬁne O
for O
this O
task O
, O
and O
over O
- O
parameterization O
is O
not O
the O
cause O
of O
poor O
performance O
of O
DRO O
in O
our O
experiments O
. O

To O
investigate O
this O
potential O
weakness O
in O
our O
experiments O
, O
we O
run O
additional O
experiments O
using O
BERT B-MethodName
Small O
on O
MIXED O
-LANGUAGE O
data O
for O
Nigerian O
Pidgin O
English O
, O
trying O
different O
weight B-HyperparameterName
decay O
values O
in O
each O
Tab O
. O

ToNigerian O
Pidgin O
BERT B-MethodName
Weight O
Decay O
P@1 O
P O
D@1 O
PLL O
ERM O
0.01 O
47.39 O
46.75 O
77.47 O
DRO O
- O
Language0.01 O
43.00 O
37.75 O
82.50 O
0.05 O
42.86 O
38.47 O
83.03 O
0.10 O
43.00 O
38.74 O
81.80 O
0.30 O
42.70 O
39.53 O
81.94 O
Table O
6 O
: O
Regularization O
experiments O
on O
M O
IXED O
LANGUAGE O
Nigerian O
Pidgin O
data O
, O
based O
on O
BERT B-MethodName
Small O
. O

Speciﬁcally O
, O
we O
train O
an O
SVM B-MethodName
on O
the O
BERT B-MethodName
encodings.7OurA O
- O
distance O
results O
suggest O
that O
creole O
languages O
do O
notexhibit O
more O
drift O
than O
English O
when O
the O
data O
are O
comparable O
. O

In O
our O
system O
, O
we O
manually O
and O
explicitly O
share O
the O
universal O
annotations O
via O
a O
shared O
LSTM B-MethodName
component O
. O

Biafﬁne O
BiLSTM B-MethodName
. O

Similar O
to O
Shi O
et O
al O
. O
( O
2017 O
) O
; O
Sato O
et O
al O
. O
( O
2017 O
) O
; O
Vania O
et O
al O
. O
( O
2017 O
) O
, O
we O
use O
last O
year O
’s O
ﬁrst O
- O
place O
model O
( O
Dozat O
et O
al O
. O
, O
2017 O
) O
, O
the O
graph O
- O
based O
biafﬁne O
bizLSTM B-MethodName
model O
as O
our O
backbone O
. O

Given O
a O
sentence O
of O
Nwords O
, O
the O
input O
is O
ﬁrst O
fed O
to O
a O
bi O
- O
directional O
LSTM B-MethodName
and O
obtain O
the O
feature O
of O
each O
word O
wi O
. O

3.2 O
Joint O
Training O
For O
a O
joint O
training O
model O
of O
Nlanguages O
, O
we O
have O
N+1Biaffne O
Bi O
- O
LSTMs B-MethodName
( O
called O
LSTMs B-MethodName
) O
, O
see O
Figure O
1 O
. O

For O
each O
language O
, O
we O
have O
a O
languagespeciﬁc O
LSTM B-MethodName
to O
process O
the O
lexical O
information O
such O
as O
word- O
or O
character- O
level O
embedding O
, O
and O
the O
output O
is O
wl O
i;j O
. O
For O
all O
languages O
we O
have O
a O
shared O
LSTM B-MethodName
which O
takes O
delexicalized O
information O
such O
as O
morphology O
and O
POS B-TaskName
tags O
as O
input O
and O
the O
output O
is O
wd O
i;j O
. O
Inspired O
by O
Sato O
et O
al O
. O
( O
2017 O
) O
, O
we O
use O
a O
gating O
mechanism O
to O
combine O
these O
two O
set O
of O
features O
. O

For O
LSTM B-MethodName
, O
we O
use O
hidden B-HyperparameterName
size I-HyperparameterName
equals O
to O
400 B-HyperparameterValue
and O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
is O
3 B-HyperparameterValue
. O

0.33 O
% O
dropout B-HyperparameterName
rate I-HyperparameterName
is O
applied O
to O
the O
input O
and O
LSTM B-MethodName
hidden B-HyperparameterName
layer I-HyperparameterName
. O

We O
use O
Bayesian O
dropout B-HyperparameterName
( O
Gal O
and O
Ghahramani O
, O
2016 B-HyperparameterValue
) O
in O
the O
LSTM B-MethodName
layers O
. O

And O
then O
we O
only O
keep O
LSTM B-MethodName
1 O
and O
the O
shared O
LSTM B-MethodName
part O
to O
ﬁne O
tune O
the O
models O
for O
language O
1 O
. O

It O
utilizednovel O
transition O
set O
and O
features O
based O
on O
bidirectional O
LSTMs B-MethodName
and O
was O
developed O
to O
deal O
with O
speciﬁc O
features O
of O
UCCA O
graphs O
, O
such O
as O
DAG O
structure O
of O
the O
graph O
, O
discontinuous O
structures O
, O
and O
non O
- O
terminal O
nodes O
corresponding O
to O
complex O
semantic O
units O
. O

The O
parser O
architecture O
utilizes O
arcfactored O
inference O
and O
a O
bidirectional O
- O
LSTM B-MethodName
composed O
with O
a O
multi O
- O
layer O
perceptron O
. O

• O
Download O
pre O
- O
trained O
word O
embeddings O
( O
GloVe B-MethodName
, O
Pennington O
et O
al O
. O
2014 O
) O
. O

Recent O
studies O
showed O
that O
models O
based O
on O
neural B-MethodName
networks I-MethodName
can O
outperform O
conventional O
models O
( O
e.g. O
, O
na O
¨ıve O
Bayes O
) O
on O
text O
classiﬁcation O
tasks O
( O
Kim O
, O
2014 O
; O
Iyyer O
et O
al O
. O
, O
2015 O
; O
Tang O
et O
al O
. O
, O
2015 O
; O
Dai O
and O
Le O
, O
2015 O
; O
Jin O
et O
al O
. O
, O
2016 O
; O
Joulin O
et O
al O
. O
, O
2017 O
; O
Shen O
et O
al O
. O
, O
2018 O
) O
. O

They O
typically O
use O
words O
in O
the O
target O
documents O
as O
inputs O
, O
map O
words O
into O
continuous O
vectors O
( O
embeddings O
) O
, O
and O
capture O
the O
semantics O
in O
documents O
by O
using O
compositional O
functions O
over O
word O
embeddings O
such O
as O
averaging O
or O
summation O
of O
word O
embeddings O
, O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNN O
) O
, O
and O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNN).Apart O
from O
the O
aforementioned O
approaches O
, O
past O
studies O
attempted O
to O
use O
entities O
in O
a O
knowledge O
base O
( O
KB O
) O
( O
e.g. O
, O
Wikipedia O
) O
to O
capture O
the O
semantics O
in O
documents O
. O

Moreover O
, O
we O
derive O
a O
representation O
based O
both O
on O
entities O
and O
words O
by O
simply O
adding O
zentity O
andzword1 O
: O
zfull O
= O
zentity O
+ O
zword O
: O
( O
4 O
) O
We O
then O
solve O
the O
task O
using O
a O
multiclass O
logistic B-MethodName
regression I-MethodName
classiﬁer O
with O
the O
computed O
representation O
( O
i.e. O
, O
with O
zentity O
orzfull O
) O
as O
features O
. O

4.2 O
Baselines O
We O
used O
the O
following O
models O
as O
our O
baselines O
: O
BoW O
- O
SVM B-MethodName
( O
Jin O
et O
al O
. O
, O
2016 O
): O
This O
model O
is O
based O
on O
a O
conventional O
linear O
support O
vector O
machine O
( O
SVM B-MethodName
) O
with O
bag O
of O
words O
( O
BoW O
) O
features O
. O

5We O
used O
the O
by O
- O
date O
version O
downloaded O
from O
the O
author O
’s O
web O
site O
: O
http://qwone.com/ O
˜jason/ O
20Newsgroups/.The O
performance O
of O
this O
model O
was O
superior O
to O
that O
of O
many O
state O
- O
of O
- O
the O
- O
art O
models O
, O
including O
those O
based O
on O
the O
skip O
- O
gram O
and O
CBOW B-MethodName
models O
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
, O
and O
the O
paragraph O
vector O
model O
( O
Le O
and O
Mikolov O
, O
2014 O
) O
. O

SWEM B-MetricName
- O
concat O
( O
Shen O
et O
al O
. O
, O
2018 O
): O
This O
model O
is O
based O
on O
a O
neural O
network O
model O
with O
simple O
pooling O
operations O
( O
i.e. O
, O
average B-MetricName
and O
max O
pooling O
) O
over O
pretrained O
word O
embeddings.6Despite O
its O
simplicity O
, O
it O
outperformed O
many O
neural O
network O
- O
based O
models O
such O
as O
the O
word O
- O
based O
CNN O
model O
( O
Kim O
, O
2014 B-MetricValue
) O
and O
RNN O
model O
with O
LSTM B-MethodName
units O
( O
Shen O
et O
al O
. O
, O
2018 O
) O
. O

.842 O
.836 O
.942 O
.865 O
TAGME O
( O
NABoE O
- O
full O
) O
.860 O
.853 O
.958 O
.889 O
BoW O
- O
SVM B-MethodName
.790 O
.783 O
.947 O
.851 O
BoE O
.831 O
.827 O
.965 O
.886 O
SWEM B-MetricName
- O
concat O
.853 O
.855 O
.967 O
.898 O
TextEnt O
.845 O
.839 O
.967 O
.910 O
Table O
1 O
: O
Results O
of O
the O
text O
classiﬁcation O
task O
on O
the O
20NG O
and O
R8 O
datasets O
. O

5.2 O
Baselines O
We O
used O
the O
following O
baseline O
models O
: O
BoW O
( O
Xu O
and O
Li O
, O
2016 O
) O
This O
model O
is O
based O
on O
a O
logistic B-MethodName
regression I-MethodName
classiﬁer O
with O
conventional O
binary O
BoW O
features O
. O

It O
uses O
the O
logistic B-MethodName
regression I-MethodName
classiﬁer O
with O
the O
features O
derived O
by O
the O
RNN O
. O

NMT O
Model O
Our O
baseline O
model O
consists O
of O
a O
2 O
- O
layer O
bi O
- O
directional O
LSTM B-MethodName
encoder O
with O
an O
embeddings O
size O
of O
512 B-HyperparameterValue
and O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
512 O
. O

The O
1 O
- O
layer O
LSTM B-MethodName
decoder O
with O
512 O
hidden B-HyperparameterName
units I-HyperparameterName
uses O
an O
attention O
network O
with O
128 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
. O

Then O
, O
we O
ﬁt O
a O
linear B-MethodName
regression I-MethodName
model O
based O
on O
the O
sentences O
and O
their O
scores B-MetricName
, O
in O
which O
the O
response O
variable O
is O
the O
preference O
score B-MetricName
and O
the O
predictor O
variables O
are O
extracted O
features O
or O
heuristics O
based O
on O
the O
sentences O
. O

Furthermore O
, O
we O
showed O
that O
pretrained O
transformer O
models O
( O
Dual O
- O
Source O
RoBERTa B-MethodName
and O
T5 B-MethodName
) O
beat O
all O
other O
baselines O
. O

For O
instance O
, O
it O
takes O
50 O
hours O
to O
process O
the O
test O
set O
using O
a O
Transformer O
model O
such O
as O
T5 B-MethodName
SMALL O
on O
a O
single O
NVidia O
V100 O
GPU O
. O

A O
straightforward O
approach O
to O
single O
- O
property O
extraction O
is O
to O
use O
an O
LSTM B-MethodName
sequence O
- O
to O
- O
sequence O
model O
where O
the O
input O
consists O
of O
a O
property O
name O
concatenated O
with O
the O
considered O
input O
text O
. O

Dual O
- O
Source O
RoBERTa B-MethodName
. O

Therefore O
, O
we O
experimented O
with O
the O
pretrained O
RoBERTa B-MethodName
language O
model O
as O
an O
encoder O
. O

RoBERTa B-MethodName
models O
were O
developed O
as O
a O
hyperoptimized O
version O
of O
BERT B-MethodName
with O
a O
byte O
- O
level O
BPE O
and O
a O
considerably O
larger O
dictionary O
( O
Liu O
et O
al O
. O
, O
2019 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

All O
the O
model O
parameters O
, O
including O
the O
RoBERTa B-MethodName
weights B-HyperparameterName
, O
were O
further O
optimized O
on O
the O
WikiReading O
Recycled O
task O
. O

T5 B-MethodName
. O

Recently O
proposed O
T5 B-MethodName
model O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
is O
a O
Transformer O
model O
pretrained O
on O
a O
cleaned O
version O
of O
CommonCrawl O
. O

T5 B-MethodName
is O
famous O
for O
achieving O
excellent O
performance O
on O
the O
SuperGLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
. O

In O
the O
case O
of O
MPE O
, O
we O
reduce O
the O
dataset O
to O
the O
single O
property O
setting O
, O
asBasic O
seq2seqVanilla O
TransformerVanilla O
Dual O
- O
SourceDual O
- O
Source O
RoBERTaT5 B-MethodName
Numer O
of O
inputs O
1 O
1 O
2 O
2 O
1 O
Pretrained O
encoder O
  O
  O
  O
+ O
+ O
Pretrained O
decoder O
  O
  O
  O
  O
+ O
Number O
of O
parameters O
32 O
M O
46 O
M O
25 O
M O
234 O
M O
60 O
M O
Table O
5 O
: O
Comparison O
of O
evaluated O
models O
. O

The O
T5 B-MethodName
model O
can O
be O
considered O
as O
a O
pretrained O
equivalent O
of O
Vanilla O
Transformer O
, O
and O
our O
RoBERTa B-MethodName
- O
based O
model O
can O
be O
viewed O
as O
a O
partially O
- O
pretrained O
Vanilla O
Dual O
- O
Source O
Transformer O
. O

Basic O
seq2seq O
is O
an O
RNN O
counterpart O
of O
both O
T5 B-MethodName
and O
Vanilla O
Transformer O
. O

used O
by O
the O
T5 B-MethodName
model O
’s O
authors O
. O

For O
example O
, O
we O
supposed O
that O
the O
model O
consisted O
of O
unidirectional O
LSTMs B-MethodName
and O
truecasing B-TaskName
was O
applied O
to O
the O
output O
. O

Dual O
- O
Source O
RoBERTa B-MethodName
model O
is O
initialized O
with O
RoBERTa B-MethodName
BASE O
( O
consisting O
of O
12 O
encoder O
layers O
and O
a O
dictionary O
of O
50,000 O
subword O
units O
) O
. O

In O
the O
case O
of O
the O
T5 B-MethodName
model O
, O
we O
keep O
hyperparameters O
as O
close O
as O
possible O
to O
those O
used O
during O
pretraining O
. O

On O
WikiReading O
Recycled O
, O
we O
supplement O
the O
evaluation O
with O
pretrained O
models O
: O
Dual O
- O
Source O
RoBERTa B-MethodName
and O
T5 B-MethodName
. O

Its O
pretrained O
version O
, O
Dual O
- O
Source O
RoBERTa B-MethodName
, O
improves O
the O
result O
by1:4points O
. O

As O
the O
T5 B-MethodName
model O
beats O
the O
Vanilla O
Dual O
- O
Source O
Transformer O
, O
we O
may O
conclude O
that O
even O
though O
the O
WikiReading O
Recycled O
dataset O
is O
very O
large O
, O
the O
pretraining O
is O
crucial O
for O
this O
MPE O
task O
. O

The O
best O
result O
was O
achieved O
by O
the O
T5 B-MethodName
model O
( O
10:9points O
) O
, O
albeit O
it O
still O
does O
not O
meet O
expectations O
. O

Both O
RoBERTa B-MethodName
and O
Vanilla O
refer O
to O
Dual O
- O
Source O
Transformers O
. O
Model O
unseen O
rare O
categorical O
relational O
exact O
match O
long O
test O
- O
B O
Basic O
seq2seq O
2.0 O
30.2 O
84.9 O
50.2 O
71.1 O
56.4 O
75.2 O
Vanilla O
Dual O
- O
Source O
0.0 O
40.7 O
83.9 O
70.8 O
80.5 O
63.1 O
77.5 O
Dual O
- O
Source O
RoBERTa B-MethodName
0.0 O
50.7 O
86.0 O
76.8 O
84.3 O
68.2 O
80.9 O
Finetuned O
T5 B-MethodName
10.9 O
53.8 O
86.3 O
73.4 O
83.4 O
65.9 O
80.3 O
Table O
7 O
: O
Results O
on O
WikiReading O
Recycled O
human O
- O
annotated O
test O
set O
supplemented O
with O
scores B-MetricName
on O
diagnostics O
subsets O
. O

Both O
RoBERTa B-MethodName
and O
Vanilla O
refer O
to O
Dual O
- O
Source O
Transformers O
. O

The O
use O
of O
recently O
introduced O
models O
like O
LongFormer O
( O
Beltagy O
et O
al O
. O
, O
2020 O
) O
and O
BigBird B-MethodName
( O
Zaheer O
et O
al O
. O
, O
2020 O
) O
might O
decrease O
the O
gap O
in O
scores B-MetricName
between O
long O
and O
average B-MetricName
- O
length O
articles O
. O

While O
in O
the O
works O
that O
apply O
neural B-MethodName
networks I-MethodName
, O
the O
user O
information O
is O
embedded O
separately O
( O
Chen O
et O
al O
. O
, O
2016b O
) O
or O
added O
at O
the O
attention O
layer O
( O
Chen O
et O
al O
. O
, O
2016a O
; O
Wu O
et O
al O
. O
, O
2018 O
) O
in O
order O
to O
make O
user O
- O
speciﬁc O
predictions O
at O
the O
output O
. O

Neil O
et O
al O
. O
( O
2016 O
) O
have O
proposed O
a O
phased O
LSTM B-MethodName
that O
utilizes O
an O
addi O
- O
tional O
time O
gate O
to O
control O
the O
passing O
of O
the O
information O
. O

3 O
Personalized O
Sentiment O
Model O
Motivated O
by O
the O
diversity O
in O
individuality O
, O
we O
introduce O
a O
model O
that O
considers O
both O
textual O
and O
contextual O
information O
and O
applies O
hierarchical O
neural B-MethodName
networks I-MethodName
to O
facilitate O
the O
aspect O
of O
personalization O
in O
sentiment B-TaskName
analysis I-TaskName
. O

After O
extracting O
the O
components O
, O
an O
embedding O
layer O
is O
applied O
to O
generate O
a O
representation O
vector O
for O
the O
text O
. O
Representation O
with O
Pre O
- O
trained O
Word O
Embeddings O
( O
WE O
) O
Pre O
- O
trained O
word O
vectors O
such O
as O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
generate O
embeddings O
according O
to O
the O
cooccurrences O
of O
the O
words O
. O

Representation O
with O
Deep O
Contextualization O
( O
DC O
) O
Peters O
et O
al O
. O
( O
2018 O
) O
proposed O
a O
deep O
contextualized O
representation O
( O
ELMo B-MethodName
) O
that O
takes O
a O
ﬁner O
granular O
level O
( O
characters O
) O
to O
generate O
embeddings O
for O
the O
text O
by O
leveraging O
a O
deep O
bidirectional O
language O
model O
. O

We O
apply O
ELMo B-MethodName
for O
each O
post O
, O
and O
then O
combine O
the O
representation O
of O
the O
post O
and O
the O
encoded O
user O
index O
at O
each O
time O
point O
. O

3.4 O
Recurrent O
Neural O
Network O
with O
Input O
Selection O
from O
Post O
History O
We O
apply O
a O
deep O
recurrent O
neural O
network O
with O
long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
, O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
on O
the O
input O
sequences O
constructed O
with O
one O
of O
the O
input O
formulations O
. O

InWEandCW O
, O
100 O
dimensional O
Twitter O
word O
vectors O
are O
taken O
from O
GloVe6 B-MethodName
. O

The O
ELMo B-MethodName
word O
representations O
in O
the O
DCinput O
formulation O
are O
supported O
by O
TensorFlow O
Hub7 O
, O
which O
are O
later O
re O
- O
trained O
with O
other O
weights B-HyperparameterName
in O
the O
model O
. O

The O
list O
of O
concepts O
provided O
by O
SenticNet O
5 O
is O
more O
restricted O
than O
the O
word O
vectors O
by O
GloVe B-MethodName
, O
and O
is O
far O
less O
ﬂexible O
than O
the O
character O
- O
based O
representation O
. O

Note O
that O
using O
the O
character O
- O
based O
DCformulation O
alone O
offers O
better O
performance O
than O
using O
the O
combination O
of O
word O
- O
and O
concept O
- O
level O
representations O
; O
however O
the O
ELMo B-MethodName
representation O
has O
a O
more O
complex O
structure O
, O
a O
higher O
dimensional O
output O
, O
and O
it O
takes O
longer O
time O
to O
re O
- O
train O
the O
weights B-HyperparameterName
in O
the O
network O
. O

Technically O
, O
it O
also O
provides O
an O
alternative O
for O
studying O
various O
time O
gaps O
in O
temporal O
sequences O
with O
neural B-MethodName
networks I-MethodName
. O

Unlike O
previous O
approaches O
which O
usually O
inject O
wordlevel O
information O
at O
the O
input O
of O
a O
long O
shortterm O
memory O
( O
LSTM B-MethodName
) O
network O
, O
we O
inject O
it O
into O
the O
softmax O
function O
. O

Previous O
work O
usually O
combines O
word O
- O
level O
information O
and O
character O
- O
level O
information O
at O
the O
input O
of O
LSTM B-MethodName
layers O
through O
a O
gating O
mechanism O
, O
or O
averaging O
or O
concatenation O
of O
word O
vectors O
. O

Our O
model O
can O
predict O
the O
next O
word O
using O
the O
embeddings O
of O
the O
words O
in O
the O
currentn O
- O
gram O
window O
, O
in O
addition O
to O
the O
hidden O
state O
of O
the O
LSTM B-MethodName
layer O
. O

After O
that O
, O
we O
combine O
the O
gated O
wordlevel O
information O
with O
the O
output O
of O
LSTM B-MethodName
. O

Luong O
and O
Manning O
( O
2016 O
) O
introduced O
a O
characterword O
neural O
machine O
translation O
model O
that O
only O
consults O
character O
- O
level O
representations O
for O
rare O
words O
encoded O
with O
a O
deep O
LSTM B-MethodName
. O

Miyamoto O
and O
Cho O
( O
2016 O
) O
introduced O
a O
gate O
mechanism O
between O
word O
embeddings O
and O
character O
embeddings O
obtained O
from O
a O
bidirectional O
LSTM B-MethodName
( O
BiLSTM B-MethodName
) O
for O
English O
. O

Although O
there O
are O
a O
number O
of O
research O
efforts O
for O
using O
both O
character O
- O
level O
and O
word O
- O
level O
information O
, O
they O
feed O
the O
two O
types O
of O
information O
only O
to O
LSTM B-MethodName
, O
while O
our O
model O
also O
injects O
the O
word O
- O
level O
information O
into O
the O
softmax O
function O
. O

3 O
Model O
Description O
For O
language O
modeling O
, O
we O
basically O
use O
a O
LSTM B-MethodName
network O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

We O
denote O
the O
hidden O
state O
of O
LSTM B-MethodName
for O
the O
t O
- O
th O
word O
wtasht2Rd O
, O
wheredis O
the O
embedding O
size O
. O

Symbols O
ˆ O
and O
$ O
respectively O
represent O
the O
start O
and O
the O
end O
of O
a O
word.3.1 O
Input O
Word O
Representations O
We O
use O
BiLSTM B-MethodName
to O
encode O
character O
n O
- O
grams O
to O
obtain O
character O
- O
level O
representation O
. O

This O
is O
because O
BiLSTM B-MethodName
over O
character O
3 O
- O
grams O
obtained O
best O
results O
on O
most O
LM O
datasets O
in O
the O
work O
of O
Vania O
and O
Lopez O
( O
2017 O
) O
, O
but O
Japanese O
and O
Chinese O
are O
more O
ideographic O
than O
the O
others O
, O
and O
it O
is O
expected O
that O
a O
smallernworks O
better O
. O

We O
compute O
the O
character O
- O
level O
representation O
of O
wtas O
follows O
: O
ct O
= O
Wfhfw O
l+Wbhbw O
0+b O
; O
( O
1 O
) O
where O
hfw O
l O
, O
hbw O
02Rdare O
the O
last O
states O
of O
the O
forward O
and O
backward O
LSTMs B-MethodName
respectively O
. O

Several O
baseline O
models O
and O
our O
models O
are O
listed O
as O
follows O
: O
-Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
: O
We O
use O
BiLSTM B-MethodName
to O
encode O
characters O
without O
injecting O
wordlevel O
information O
. O

2We O
have O
tested O
the O
above O
other O
methods O
, O
such O
as O
avg O
, O
add O
and O
cat O
, O
for O
combining O
htandwt O
, O
in O
place O
of O
gate O
, O
and O
found O
these O
methods O
did O
not O
work O
well O
. O
Embedding O
size O
d O
650 O
LSTM B-MethodName
layers O
2 O
Dropout O
0.5 O
Optimizer O
SGD O
Learning O
rate O
20 O
Learning O
rate O
decay O
4 O
Parameter O
init O
: O
rand O
uniform O
[ O
-0.1,0.1 O
] O
Batch O
size O
20 O
LSTM B-MethodName
sequence O
length O
35 O
Gradient O
clipping O
0.25 O
Epochs O
40 O
Table O
1 O
: O
Hyper O
- O
parameters O
of O
our O
model O
. O

We O
use O
dfor O
the O
sizes O
of O
the O
character O
/ O
word O
embeddings O
and O
for O
the O
number O
of O
hidden B-HyperparameterName
units I-HyperparameterName
of O
LSTM B-MethodName
and O
BiLSTM B-MethodName
. O

-Word O
- O
LSTM B-MethodName
: O
Standard O
word O
- O
level O
LSTM B-MethodName
model O
. O

-Char O
- O
BiLSTM B-MethodName
- O
gate O
/ O
avg O
/ O
add O
/ O
cat O
- O
WordLSTM B-MethodName
: O
We O
combine O
character O
- O
level O
and O
word O
- O
level O
information O
at O
the O
input O
of O
LSTM B-MethodName
through O
gate O
/ O
avg O
/ O
add O
/ O
cat O
methods O
, O
mentioned O
in O
Sec O
. O

-Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
: O
We O
inject O
word O
- O
level O
information O
only O
into O
the O
softmax O
function O
. O

-Char O
- O
BiLSTM B-MethodName
- O
gate O
/ O
avg O
/ O
add O
/ O
cat O
- O
WordLSTM B-MethodName
- O
Word O
: O
We O
combine O
our O
injection O
method O
and O
previous O
injection O
methods O
, O
which O
means B-MetricName
we O
inject O
word O
- O
level O
information O
both O
at O
the O
input O
of O
LSTM B-MethodName
and O
into O
the O
softmax O
function O
. O

For O
both O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
and O
CharBiLSTM B-MethodName
- O
gate O
/ O
avg O
/ O
add O
/ O
cat O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
, O
we O
useg= O
0:5 O
= O
adaptive O
and O
n= O
1=2=3to O
represent O
our O
speciﬁc O
injection O
method O
. O

For O
example O
, O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
2 O
) O
represents O
that O
we O
use O
a O
ﬁxed O
gate O
value O
on O
wordlevel O
information O
in O
Eq O
. O

5.2 O
Comparison O
of O
Baseline O
Models O
The O
results O
of O
Word O
- O
LSTM B-MethodName
and O
Char O
- O
BiLSTMLSTM B-MethodName
are O
shown O
in O
Table O
3 O
. O

We O
also O
showed O
the O
results O
of O
Word O
- O
LSTM B-MethodName
and O
Char O
- O
CNN O
- O
LSTM B-MethodName
from O
the O
work O
of O
Gerz O
et O
al O
. O
( O
2018 O
) O
. O

The O
embedding O
size O
and O
the O
number O
of O
LSTM B-MethodName
layers O
are O
the O
same O
as O
those O
for O
the O
models O
in O
Gerz O
et O
al O
. O
( O
2018 O
) O
. O

As O
shown O
in O
the O
table O
, O
both O
the O
Word O
- O
LSTM B-MethodName
and O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
baselines O
are O
better O
than O
the O
Word O
- O
LSTM B-MethodName
and O
Char O
- O
CNNLSTM B-MethodName
from O
the O
work O
of O
Gerz O
et O
al O
. O
( O
2018 O
) O
on O
all O
the O
datasets4 O
. O

Both O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
and O
Char O
- O
CNN O
- O
LSTM B-MethodName
from O
Gerz O
et O
al O
. O
( O
2018 O
) O
are O
better O
than O
their O
respective O
Word O
- O
LSTM B-MethodName
on O
all O
the O
datasets O
. O

One O
possible O
reason O
is O
that O
all O
the O
unseen O
words O
in O
the O
test O
set O
in O
the O
14 O
datasets O
can O
not O
be O
handled O
by O
Word O
- O
LSTM B-MethodName
in O
the O
testing O
phase O
. O

It O
is O
also O
shown O
that O
as O
TTR O
increases O
, O
CharBiLSTM B-MethodName
- O
LSTM B-MethodName
achieves O
the O
better O
result O
than O
Word O
- O
LSTM B-MethodName
. O

vi O
zh O
ja O
pt O
en O
ms O
he O
ar O
de O
cs O
es O
et O
ru O
ﬁ O
Word O
- O
LSTM B-MethodName
( O
Gerz O
et O
al O
. O
, O
2018 O
) O
190 O
826 O
156 O
272 O
494 O
725 O
2189 O
2587 O
903 O
2200 O
366 O
2564 O
1309 O
4263 O
Char O
- O
CNN O
- O
LSTM B-MethodName
( O
Gerz O
et O
al O
. O
, O
2018 O
) O
158 O
797 O
136 O
214 O
371 O
525 O
1519 O
1659 O
602 O
1252 O
275 O
1478 O
812 O
2236 O
Our O
Word O
- O
LSTM B-MethodName
137 O
582 O
113 O
201 O
348 O
476 O
1480 O
1610 O
609 O
1278 O
271 O
1295 O
839 O
2128 O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
134 O
578 O
107 O
178 O
302 O
463 O
1170 O
1337 O
483 O
973 O
230 O
967 O
620 O
1648 O
Char O
- O
BiLSTM B-MethodName
- O
gate O
- O
Word O
- O
LSTM B-MethodName
136 O
582 O
112 O
195 O
328 O
483 O
1340 O
1619 O
551 O
1149 O
264 O
1189 O
704 O
1987 O
Char O
- O
BiLSTM B-MethodName
- O
cat O
- O
Word O
- O
LSTM B-MethodName
133 O
565 O
105 O
183 O
314 O
432 O
1239 O
1360 O
504 O
1052 O
245 O
993 O
614 O
1602 O
Char O
- O
BiLSTM B-MethodName
- O
avg O
- O
Word O
- O
LSTM B-MethodName
133 O
609 O
110 O
177 O
307 O
461 O
1181 O
1340 O
478 O
963 O
225 O
996 O
611 O
1574 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
127 O
551 O
103 O
171 O
298 O
423 O
1091 O
1302 O
481 O
938 O
218 O
967 O
606 O
1578 O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g O
= O
adaptive O
, O
n= O
1)126 O
567 O
104 O
175 O
314 O
424 O
1133 O
1279 O
491 O
920 O
235 O
949 O
605 O
1592 O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1)123 O
523 O
101 O
171 O
292 O
415 O
1068 O
1247 O
479 O
934 O
217 O
906 O
601 O
1590 O
Table O
3 O
: O
Perplexity O
of O
several O
baseline O
models O
and O
Char O
- O
CNN O
- O
LSTM B-MethodName
on O
14 O
language O
modeling O
datasets O
. O

In O
our O
experiments O
, O
CharBiLSTM B-MethodName
- O
gate O
- O
Word O
- O
LSTM B-MethodName
underperforms O
CharBiLSTM B-MethodName
- O
LSTM B-MethodName
on O
all O
the O
datasets O
. O

Char O
- O
BiLSTM B-MethodName
- O
cat O
- O
Word O
- O
LSTM B-MethodName
achieves O
better O
results O
than O
Char O
- O
BiLSTM B-MethodName
- O
gateWord O
- O
LSTM B-MethodName
on O
all O
the O
datasets O
, O
but O
still O
underperforms O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
on O
8 O
out O
of O
14 O
datasets O
. O

Char O
- O
BiLSTM B-MethodName
- O
avg O
- O
Word O
- O
LSTM B-MethodName
outperforms O
Char O
- O
BiLSTM B-MethodName
- O
cat O
- O
Word O
- O
LSTM B-MethodName
on O
9 O
out O
of O
14 O
datasets O
, O
which O
indicates O
the O
simple O
average B-MetricName
method O
is O
better O
than O
the O
gating O
mechanismand O
the O
concatenation O
method O
in O
our O
tasks O
. O

However O
, O
Char O
- O
BiLSTM B-MethodName
- O
avg O
- O
Word O
- O
LSTM B-MethodName
still O
has O
no O
obvious O
improvements O
, O
compared O
with O
CharBiLSTM B-MethodName
- O
LSTM B-MethodName
on O
most O
datasets O
. O

Kim O
et O
al O
. O
( O
2016 O
) O
used O
a O
Char O
- O
CNN O
- O
LSTM B-MethodName
model O
without O
injecting O
word O
- O
level O
information O
. O

They O
reported O
that O
some O
basic O
methods O
( O
e.g. O
, O
concatenation O
, O
averaging O
and O
adaptive O
weighting B-HyperparameterName
schemes O
) O
for O
injecting O
word O
- O
level O
information O
degraded O
the O
performance O
of O
their O
Char O
- O
CNN O
- O
LSTM B-MethodName
. O

Miyamoto O
and O
Cho O
( O
2016 O
) O
showed O
the O
concatenation O
method O
for O
injecting O
word O
- O
level O
information O
into O
their O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
also O
degraded O
their O
WordLSTM B-MethodName
model O
. O

Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
achieves O
more O
improved O
results O
than O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
on O
13 O
out O
of O
14 O
datasets O
and O
also O
performs O
best O
in O
general O
among O
Char O
- O
BiLSTM B-MethodName
- O
avg O
/ O
add O
/ O
gate O
/ O
catWord O
- O
LSTM B-MethodName
. O

Our O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g O
= O
0:5;n O
= O
1 O
) O
and O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g O
= O
adaptive;n O
= O
1 O
) O
work O
effectively O
, O
and O
both O
of O
them O
achieve O
better O
results O
than O
CharBiLSTM B-MethodName
- O
LSTM B-MethodName
. O

Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1 O
) O
works O
better O
than O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g O
= O
adaptive;n= O
1 O
) O
on O
most O
datasets O
. O

When O
compared O
with O
other O
injection O
methods O
, O
CharBiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1 O
) O
achieves O
the O
best O
results O
on O
most O
datasets O
( O
bold O
scores B-MetricName
in O
Table O
3 O
) O
. O

This O
suggests O
that O
our O
injection O
method O
, O
aiming O
at O
the O
different O
position O
from O
the O
input O
of O
LSTM B-MethodName
, O
the O
softmax O
function O
, O
makes O
good O
use O
of O
word O
- O
level O
information O
. O

5.4 O
Combination O
of O
Injection O
Methods O
To O
avoid O
too O
many O
combinations O
of O
our O
injection O
method O
and O
other O
previous O
methods O
, O
we O
only O
chose O
to O
combine O
our O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1 O
) O
with O
the O
other O
previous O
injection O
methods O
, O
because O
Char O
- O
BiLSTM B-MethodName
- O
LSTMWord B-MethodName
( O
g= O
0:5;n= O
1 O
) O
performs O
better O
than O
CharBiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g O
= O
adaptive;n= O
1 O
) O
, O
as O
mentioned O
above O
. O

The O
results O
of O
the O
combination O
of O
our O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1 O
) O
and O
the O
previous O
injection O
methods O
are O
shown O
in O
Table O
4 O
. O

Among O
them O
, O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTMWord B-MethodName
( O
g= O
0:5;n= O
1 O
) O
obtained O
the O
best O
results O
on O
most O
datasets O
( O
bold O
scores B-MetricName
in O
Table O
4 O
) O
. O

The O
number O
of O
words O
used O
in O
our O
injection O
method O
is O
denoted O
by O
n. O
In O
our O
experiments O
, O
we O
only O
set O
nto O
1 O
, O
2 O
and O
3 O
, O
as O
we O
observed O
no O
obvious O
improvements O
when O
using O
a O
larger O
n. O
Since O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
WordLSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1 O
) O
performs O
best O
in O
general O
on O
most O
datasets O
, O
as O
mentioned O
above O
, O
we O
only O
changed O
nfor O
this O
model O
. O

Note O
that O
our O
CharBiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
2=3 O
) O
does O
not O
need O
extra O
parameters O
as O
we O
just O
reuse O
the O
word O
embeddings O
from O
the O
lookup O
tableWinto O
compute O
word O
- O
level O
information O
. O

In O
general O
, O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTMWord B-MethodName
( O
g= O
0:5;n= O
2 O
) O
achieves O
the O
best O
result O
on O
most O
datasets O
. O

Char O
- O
BiLSTM B-MethodName
- O
add O
- O
WordLSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
3 O
) O
does O
not O
obtain O
further O
improvements O
on O
most O
datasets O
. O

We O
only O
use O
its O
character O
- O
level O
representation O
obtained O
through O
our O
BiLSTM B-MethodName
over O
characters O
to O
perform O
the O
lan O
- O
vi O
zh O
ja O
pt O
en O
ms O
he O
ar O
de O
cs O
es O
et O
ru O
ﬁ O
Char O
- O
BiLSTM B-MethodName
- O
gate O
- O
Word O
- O
LSTM B-MethodName
136 O
582 O
112 O
195 O
328 O
483 O
1340 O
1619 O
551 O
1149 O
264 O
1189 O
704 O
1987 O
Char O
- O
BiLSTM B-MethodName
- O
gate O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1)125 O
538 O
105 O
182 O
316 O
430 O
1339 O
1474 O
536 O
1116 O
260 O
1103 O
659 O
1728 O
Char O
- O
BiLSTM B-MethodName
- O
cat O
- O
Word O
- O
LSTM B-MethodName
133 O
565 O
105 O
183 O
314 O
432 O
1239 O
1360 O
504 O
1052 O
245 O
993 O
614 O
1602 O
Char O
- O
BiLSTM B-MethodName
- O
cat O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1)122 O
541 O
103 O
180 O
305 O
426 O
1158 O
1316 O
530 O
1031 O
241 O
1012 O
607 O
1561 O
Char O
- O
BiLSTM B-MethodName
- O
avg O
- O
Word O
- O
LSTM B-MethodName
133 O
609 O
110 O
177 O
307 O
461 O
1181 O
1340 O
478 O
963 O
225 O
996 O
611 O
1574 O
Char O
- O
BiLSTM B-MethodName
- O
avg O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1)121 O
495 O
99 O
165 O
293 O
398 O
1044 O
1224 O
488 O
890 O
218 O
898 O
569 O
1510 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
127 O
551 O
103 O
171 O
298 O
423 O
1091 O
1302 O
481 O
938 O
218 O
967 O
606 O
1578 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1)116 O
481 O
98 O
160 O
291 O
387 O
1038 O
1172 O
462 O
874 O
215 O
870 O
568 O
1494 O
Table O
4 O
: O
Perplexity O
of O
the O
combination O
of O
our O
injection O
method O
with O
the O
previous O
methods O
on O
14 O
language O
modeling O
datasets O
. O

vi O
zh O
ja O
pt O
en O
ms O
he O
ar O
de O
cs O
es O
et O
ru O
ﬁ O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1)116 O
481 O
98 O
160 O
291 O
387 O
1038 O
1172 O
462 O
874 O
215 O
870 O
568 O
1494 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
2)117 O
489 O
95 O
163 O
277 O
376 O
998 O
1179 O
452 O
867 O
213 O
884 O
548 O
1456 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
3)118 O
475 O
96 O
162 O
285 O
391 O
1041 O
1162 O
463 O
877 O
215 O
913 O
563 O
1471 O
Table O
5 O
: O
Perplexity O
of O
our O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
including O
word O
- O
level O
information O
for O
previous O
words O
on O
14 O
language O
modeling O
datasets O
. O

We O
refer O
the O
model O
that O
discards O
infrequent O
words O
as O
Char O
- O
BiLSTMadd B-MethodName
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1,= O
5=15=25 O
) O
. O

When O
discarding O
the O
words O
whose O
frequency O
is O
less O
than O
or O
equal O
to O
15 O
, O
the O
model O
obtains O
better O
results O
only O
on O
2 O
out O
of O
14 O
datasets O
than O
CharBiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1 O
) O
. O

When O
we O
increase O
the O
frequency O
threshold B-MetricName
further O
to O
25 B-MetricValue
, O
the O
performance O
of O
the O
model O
has O
dropped O
compared O
with O
Char O
- O
BiLSTM B-MethodName
- O
addWord O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1;= O
15 O
) O
as O
more O
frequent O
words O
are O
discarded O
. O

Char O
- O
BiLSTMadd B-MethodName
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1;= O
5 O
) O
achieves O
better O
results O
than O
Char O
- O
BiLSTM B-MethodName
- O
addWord O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1 O
) O
on O
7 O
out O
of O
14 O
datasets O
. O

We O
used O
CharBiLSTM B-MethodName
- O
LSTM B-MethodName
and O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
WordLSTM B-MethodName
as O
baseline O
models O
. O

The O
language O
models O
used O
in O
the O
previous O
work O
are O
improved O
at O
different O
aspects O
, O
and O
most O
of O
them O
are O
also O
based O
on O
standard O
LSTM B-MethodName
, O
like O
ours O
. O

Botha O
5http://www.statmt.org/wmt13/translation-task.htmlPTB O
CS O
DE O
ES O
FR O
RU O
MLBL O
( O
Botha O
and O
Blunsom O
, O
2014 O
) O
- O
465 O
296 O
200 O
225 O
304 O
MorphSum O
( O
Kim O
et O
al O
. O
, O
2016 O
) O
- O
398 O
263 O
177 O
196 O
271 O
CharCNN O
( O
Kim O
et O
al O
. O
, O
2016 O
) O
78.9 O
371 O
239 O
165 O
184 O
261 O
SkipGram B-MethodName
initialization B-HyperparameterName
( O
Bojanowski O
et O
al O
. O
, O
2017 B-HyperparameterValue
) O
- O
312 O
206 O
145 O
159 O
206 O
MorphSum+RE+RW(Assylbekov O
and O
Takhanov O
, O
2018 O
) O
72.2 O
338 O
222 O
157 O
172 O
210 O
Char O
- O
BiLSTM B-MethodName
- O
LSTM B-MethodName
85.5 O
311 O
198 O
144 O
164 O
223 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
79.1 O
300 O
199 O
138 O
155 O
213 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1;= O
5)75.9 O
287 O
192 O
135 O
152 O
201 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
2;= O
5)76.1 O
284 O
193 O
137 O
150 O
202 O
Table O
6 O
: O
Perplexity O
of O
our O
models O
and O
previous O
work O
on O
6 O
language O
modeling O
datasets O
. O

vi O
zh O
ja O
pt O
en O
ms O
he O
ar O
de O
cs O
es O
et O
ru O
ﬁ O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1)116 O
481 O
98 O
160 O
291 O
387 O
1038 O
1172 O
462 O
874 O
215 O
870 O
568 O
1494 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1;= O
5)116 O
495 O
98 O
166 O
285 O
397 O
1016 O
1153 O
463 O
863 O
214 O
877 O
547 O
1492 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1;= O
15 O
) O
117 O
502 O
99 O
164 O
286 O
397 O
1046 O
1185 O
467 O
883 O
215 O
924 O
570 O
1492 O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1;= O
25 O
) O
118 O
502 O
101 O
167 O
292 O
405 O
1053 O
1202 O
471 O
896 O
215 O
929 O
573 O
1526 O
Table O
7 O
: O
Perplexity O
of O
Char O
- O
BiLSTM B-MethodName
- O
add O
- O
Word O
- O
LSTM B-MethodName
- O
Word O
( O
g= O
0:5;n= O
1 O
) O
with O
different O
frequency O
thresholds B-MetricName
on O
14 O
language O
modeling O
datasets O
. O

Kim O
et O
al O
. O
( O
2016 O
) O
used O
CNN O
as O
their O
character O
encoder O
, O
and O
also O
trained O
an O
LSTM B-MethodName
language O
model O
, O
where O
the O
input O
representation O
of O
a O
word O
is O
the O
sum O
of O
the O
morpheme O
embeddings O
of O
the O
word O
. O

As O
shown O
in O
the O
table O
, O
Char O
- O
BiLSTMLSTM B-MethodName
underperforms O
the O
previous O
work O
on O
PTB O
. O

As O
we O
can O
see O
, O
CharBiLSTM B-MethodName
- O
LSTM B-MethodName
achieves O
better O
results O
than O
most O
previous O
work O
on O
non O
- O
English O
datasets O
. O

7 O
Conclusion O
In O
addition O
to O
combining O
character O
- O
level O
and O
word O
- O
level O
information O
at O
the O
input O
of O
LSTM B-MethodName
, O
which O
is O
a O
widely O
used O
combination O
manner O
, O
we O
proposed O
to O
also O
inject O
word O
- O
level O
information O
into O
the O
softmax O
function O
in O
a O
character O
- O
aware O
neural O
language O
model O
. O

The O
architecture O
is O
inspired O
by O
Google O
’s O
PARSEY O
SAURUS O
( O
Alberti O
et O
al O
. O
, O
2017 O
) O
, O
with O
a O
ﬁrst O
left O
- O
to O
- O
right O
char O
LSTM B-MethodName
covering O
the O
wholesentence O
and O
( O
artiﬁcial O
) O
whitespaces O
introduced O
to O
separate O
tokens.16The O
output O
vectors O
of O
the O
char O
LSTM B-MethodName
at O
the O
token O
separations O
are O
used O
as O
( O
learned O
) O
word O
embeddings O
that O
are O
concatenated O
( O
when O
present O
) O
with O
both O
the O
pre O
- O
trained O
ones O
provided O
for O
the O
task O
and O
the O
UPOS B-TaskName
tags O
predicted O
by O
the O
external O
tagger O
. O

The O
concatenated O
vectors O
serve O
as O
input O
to O
a O
word O
bi O
- O
LSTM B-MethodName
that O
is O
also O
used O
to O
predict O
UPOS B-TaskName
tags O
as O
a O
joint O
task O
( O
training O
with O
the O
gold O
tags O
provided O
as O
oracle O
) O
. O

For O
a O
given O
word O
wi O
, O
its O
ﬁnal O
vector O
representation O
is O
the O
concatenation O
of O
the O
output O
of O
the O
bi O
- O
LSTM B-MethodName
layers O
at O
positioniwith O
the O
LSTM B-MethodName
- O
predicted O
UPOS B-TaskName
tag O
. O

The O
deployment O
of O
the O
LSTMs B-MethodName
is O
done O
once O
for O
a O
given O
sentence O
. O

Many O
hyperparameters O
are O
however O
available O
as O
options O
, O
such O
as O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
of O
the O
char O
and O
word O
LSTMs B-MethodName
, O
the O
size O
of O
input O
, O
hidden O
and O
output O
dimensions O
for O
the O
LSTMs B-MethodName
and O
feedforward O
layers O
. O

The O
model O
architecture O
and O
training O
closely O
follow O
that O
of O
the O
medium O
- O
sized O
LSTM B-MethodName
model O
presented O
by O
Zaremba O
et O
al O
. O
( O
2014 O
) O
which O
we O
build O
using O
Tensorﬂow O
. O

The O
network O
consists O
of O
two O
LSTM B-MethodName
layers O
with O
hidden O
states O
of O
size O
650 O
, O
with O
a O
fully O
- O
connected O
softmax O
output O
and O
word O
embedding O
look O
- O
up O
table O
input O
. O

The O
LSTMs B-MethodName
are O
initialized O
with O
a O
uniform O
random O
distribution O
between O
[ O
 0:05;0:05 O
] O
, O
and O
are O
unrolled O
for O
35time O
steps O
, O
and O
train O
on O
contiguous O
sequences O
of O
text O
, O
passing O
the O
hidden O
state O
to O
the O
proceeding O
batch B-HyperparameterName
with I-HyperparameterName
the O
batch B-HyperparameterName
size I-HyperparameterName
set O
to O
20 B-HyperparameterValue
. O

A O
dropout B-HyperparameterName
rate I-HyperparameterName
of O
0:5was O
applied O
to O
the O
embedding O
vector O
, O
and O
outputs O
of O
both O
LSTM B-MethodName
layers O
during O
training O
. O

As O
proposed O
previously O
by O
Gulordava O
et O
al O
. O
( O
2018 O
) O
, O
we O
add O
a O
linear O
projection O
layer O
between O
the O
ﬁnal O
LSTM B-MethodName
layer O
and O
the O
softmax O
output O
, O
which O
increases O
the O
dimensionality O
to O
the O
appropriate O
length O
. O

V O
ocab O
Splits O
Train O
( O
90 O
% O
) O
Test O
( O
10 O
% O
) O
Output O
Embeddings O
0.290 O
0.301 O
AM O
Embeddings O
0.257 O
0.195 O
Concatenated O
0.218 O
0.182 O
Table O
1 O
: O
Mean O
squared O
error O
scores B-MetricName
of O
linear B-MethodName
regression I-MethodName
models O
trained O
to O
predict O
bias O
term O
for O
each O
embedding O
model O
. O

Our O
study O
reveals O
the O
weak O
and O
strong O
points O
of O
the O
Stanford O
, O
CMU O
, O
FLAIR O
, O
ELMO O
and O
BERT B-MethodName
models O
, O
as O
well O
as O
their O
shared O
limitations O
. O

Many O
of O
these O
solutions O
beneﬁt O
from O
large O
available O
data O
sets O
or O
from O
recent O
developments O
in O
deep O
neural B-MethodName
networks I-MethodName
. O

These O
models O
are O
: O
Stanford O
NER B-TaskName
( O
Finkel O
et O
al O
. O
, O
2005 O
) O
, O
the O
model O
made O
by O
the O
NLP O
team O
from O
Carnegie O
Mellon O
University O
( O
CMU O
) O
( O
Lample O
et O
al O
. O
, O
2016 O
) O
, O
ELMO O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
FLAIR O
( O
Akbik O
et O
al O
. O
, O
2018 O
) O
and O
BERT B-MethodName
- O
Base O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

Lample O
and O
the O
team O
( O
at O
CMU O
) O
used O
an O
LSTM B-MethodName
deep O
neural O
network O
with O
an O
output O
with O
CRF O
for O
the O
ﬁrst O
time O
. O

ELMO O
and O
FLAIR O
are O
new O
language O
modeling O
techniques O
as O
an O
encoder O
, O
and O
LSTM B-MethodName
with O
a O
CRF O
layer O
as O
an O
output O
decoder O
. O

A O
team O
from O
Google O
used O
a O
ﬁne O
- O
tuning O
approach O
with O
the O
BERT B-MethodName
model O
in O
a O
NER B-TaskName
problem O
for O
the O
ﬁrst O
time O
, O
based O
on O
a O
Bi O
- O
diREctional O
Transformer O
language O
model O
( O
LM O
) O
. O

The O
LSTM B-MethodName
layer O
put O
forward O
by O
Lample O
from O
Carnegie O
Mellon O
University O
( O
CMU O
) O
was O
the O
ﬁrst O
deep O
learning O
architecture O
with O
a O
CRF O
output O
layer O
( O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

The O
following O
: O
a O
token O
- O
based O
language O
model O
( O
LM O
) O
1The O
details O
of O
the O
model O
parameters O
are O
described O
in O
our O
supplementary O
materials O
. O
Model O
F1 B-MetricName
Ensemble O
of O
HMM O
, O
TBL O
, O
MaxEnt O
, O
RRM O
( O
Florian O
et O
al O
. O
, O
2003)88.76 O
Semi O
- O
supervised O
learning O
( O
Ando O
and O
Zhang O
, O
2005)89.31 O
Stanford O
CRF O
( O
Finkel O
et O
al O
. O
, O
2005 B-MetricValue
) O
87.94 O
Neural O
network O
( O
Collobert O
et O
al O
. O
, O
2011)89.59 O
CRF O
& O
lexicon O
embeddings O
( O
Passos O
et O
al O
. O
, O
2014)90.90 O
CMU O
LSTM B-MethodName
- O
CRF O
( O
Lample O
et O
al O
. O
, O
2016)90.94 O
Bi O
- O
LSTM B-MethodName
- O
CNNs O
- O
CRF O
( O
Ma O
and O
Hovy O
, O
2016)91.21 O
ELMO O
: O
Token O
based O
LM O
BiLSTM B-MethodName
- O
CRF O
( O
Peters O
et O
al O
. O
, O
2018)92.22 O
BERT B-MethodName
- O
base O
: O
Fine O
tune O
BiTransformer O
LM O
with O
BPE O
token O
encoding O
( O
Devlin O
et O
al O
. O
, O
2018)92.4 O
( O
* O
) O
CVT O
: O
Cross O
- O
view O
training O
with O
BiLSTM B-MethodName
- O
CRF O
( O
Clark O
et O
al O
. O
, O
2018)92.61 O
BERT B-MethodName
- O
large O
: O
Fine O
tune O
BiTransformer O
LM O
with O
BPE O
token O
encoding O
( O
Devlin O
et O
al O
. O
, O
2018)92.8 O
( O
* O
) O
FLAIR O
: O
Char O
based O
LM O
+ O
Glove O
with O
Bi O
- O
LSTM B-MethodName
- O
CRF O
( O
Akbik O
et O
al O
. O
, O
2018)93.09 O
( O
* O
* O
) O
Fine O
tune O
Bi O
- O
Transformer O
LM O
with O
CNN O
token O
encoding O
( O
Baevski O
et O
al O
. O
, O
2019)93.5 O
Table O
1 O
: O
Results O
reported O
in O
authors O
’ O
publications O
about O
NER B-TaskName
models O
on O
the O
original O
CoNLL O
2003 O
test O
set O
. O

See O
a O
discussion O
at O
( O
Flair O
, O
2018 O
) O
and O
the O
reported O
results O
at O
( O
Akbik O
et O
al O
. O
, O
2019 O
) O
with O
bi O
- O
LSTM B-MethodName
with O
CRF O
( O
ELMO O
) O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
a O
character O
- O
based O
LM O
with O
the O
same O
output O
( O
FLAIR O
) O
( O
Akbik O
et O
al O
. O
, O
2018 O
) O
and O
a O
bi O
- O
directional O
language O
model O
based O
on O
an O
encoder O
block O
from O
the O
transformer O
architecture O
( O
BERT B-MethodName
) O
with O
a O
ﬁne O
tune O
classiﬁcation O
output O
layer O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
are O
very O
important O
techniques O
; O
and O
that O
not O
only O
in O
the O
domain O
of O
NER B-TaskName
. O

StanfordCMU O
ELMO O
FLAIR O
BERT B-MethodName
ALL O
- O
O O
88.13 O
89.78 O
92.39 O
92.83 O
91.62 O
ALL O
- O
C O
88.73 O
90.39 O
93.21 O
93.79 O
92.33 O
PER O
- O
O O
93.31 O
95.74 O
97.07 O
97.49 O
96.14 O
PER O
- O
C O
93.94 O
96.49 O
97.81 O
98.08 O
96.88 O
ORG O
- O
O O
84.23 O
86.90 O
90.68 O
91.34 O
90.61 O
ORG O
- O
C O
84.89 O
87.53 O
91.61 O
92.64 O
91.44 O
LOC O
- O
O O
90.83 O
92.02 O
93.87 O
94.01 O
92.85 O
LOC O
- O
C O
91.58 O
92.62 O
94.92 O
94.72 O
93.59 O
MISC O
- O
O O
79.10 O
77.31 O
82.31 O
82.89 O
80.81 O
MISC O
- O
C O
79.37 O
77.58 O
82.47 O
84.40 O
81.10 O
Table O
4 O
: O
Results O
for O
selected O
models O
on O
the O
original O
( O
designated O
as O
ending O
’ O
... O
-O O
’ O
) O
and O
re O
- O
annotated O
/ O
corrected O
( O
’ O
... O
-C O
’ O
) O
CoNLL O
2003 O
test O
set O
concerning O
NE O
classes O
( O
ALL O
comprise O
PER O
, O
ORG O
, O
LOC O
, O
MISC O
) O
. O

ELMO O
, O
FLAIR O
and O
BERT B-MethodName
, O
which O
use O
contextualised O
language O
models O
, O
performed O
much O
better O
. O

StanfordCMU O
ELMO O
FLAIR O
BERT B-MethodName
DE O
- O
WT O
10 O
6 O
9 O
8 O
10 O
DE O
- O
BS O
38 O
39 O
33 O
33 O
40 O
SL O
- O
S O
46 O
21 O
13 O
16 O
11 O
SL O
- O
C O
448 O
378 O
250 O
223 O
300 O
DL O
- O
CR O
372 O
316 O
198 O
184 O
263 O
DL O
- O
S O
202 O
107 O
97 O
100 O
117 O
DL O
- O
C O
247 O
175 O
144 O
146 O
170 O
G O
- O
A O
219 O
183 O
98 O
101 O
94 O
G O
- O
HC O
72 O
68 O
65 O
59 O
65 O
G O
- O
I O
19 O
20 O
21 O
20 O
20 O
Errors O
703 O
554 O
395 O
370 O
472 O
Unique O
errors O
235 O
93 O
23 O
12 O
79 O
Table O
5 O
: O
Number O
of O
errors O
for O
a O
particular O
model O
and O
a O
particular O
class O
of O
errors O
. O

Modern O
techniques O
using O
contextualized O
language O
models O
like O
ELMO O
, O
FLAIR O
and O
BERT B-MethodName
reduced O
a O
number O
of O
mistakes O
in O
SL O
- O
C O
category O
by O
more O
than O
50 O
% O
in O
comparison O
to O
the O
Stanford O
model O
. O

ELMO O
, O
FLAIR O
and O
BERT B-MethodName
are O
more O
affected O
by O
G O
- O
HC O
and O
G O
- O
I O
, O
FLAIR O
is O
also O
reduced O
with O
DL O
- O
C O
and O
DE O
- O
WT O
. O

It O
seems O
that O
by O
using O
ELMO O
embeddings O
we O
can O
outperform O
the O
FLAIR O
and O
BERT B-MethodName
- O
Base O
models O
in O
case O
of O
sentences O
about O
general O
topics O
, O
in O
which O
the O
context O
of O
a O
whole O
sentence O
is O
more O
important O
than O
properties O
of O
words O
composing O
entities O
. O

StanfordCMU O
ELMO O
FLAIR O
BERT B-MethodName
DCS O
( O
F1 B-MetricName
) O
45.37 B-MetricValue
61.86 O
76.36 O
71.89 O
68.90 O
DCS O
( O
P O
) O
43.66 O
58.07 O
73.11 O
69.35 O
59.06 O
DCS O
( O
R O
) O
47.21 O
66.17 O
79.92 O
74.63 O
82.66 O
TS O
- O
O O
( O
F1 B-MetricName
) O
68.96 B-MetricValue
79.66 O
89.45 O
88.51 O
83.47 O
TS O
- O
O O
( O
P O
) O
76.92 O
78.33 O
85.48 O
85.25 O
75.18 O
TS O
- O
O O
( O
R O
) O
62.50 O
81.03 O
93.81 O
92.04 O
93.81 O
TS O
- O
R O
( O
F1 B-MetricName
) O
63.06 B-MetricValue
72.86 O
85.01 O
86.63 O
79.66 O
TS O
- O
R O
( O
P O
) O
65.47 O
70.65 O
81.45 O
83.70 O
71.60 O
TS O
- O
R O
( O
R O
) O
60.83 O
75.21 O
88.91 O
89.77 O
89.77 O
RS O
( O
No O
) O
3571 O
3339 O
2096 O
1404 O
3086 O
Table O
6 O
: O
Diagnostic O
data O
sets O
results O
for O
selected O
models O
: O
’ O
DCS O
’ O
- O
Document O
Context O
Sentences O
, O
’ O
TS O
- O
O O
’ O
- O
Template O
Sentences O
with O
original O
entities O
, O
’ O
TS O
- O
R O
’ O
Template O
Sentences O
with O
replaced O
entities O
, O
’ O
RS O
’ O
- O
Random O
Sentences O
. O

For O
the O
CoNLL O
2003 O
data O
set O
and O
ﬁve O
important O
ML O
models O
( O
Stanford O
, O
CMU O
, O
ELMO O
, O
FLAIR O
, O
BERT B-MethodName
- O
base O
) O
we O
re O
- O
annotated O
all O
errors O
with O
respect O
to O
the O
newly O
proposed O
ontology O
. O

The O
word O
based O
sequence O
labelling O
approaches O
were O
further O
extended O
to O
use O
neural O
architectures O
, O
especially O
using O
RNNs O
and O
its O
variants O
such O
as O
LSTMs B-MethodName
and O
GRUs O
( O
Sankaran O
and O
Jawahar O
, O
2012 O
; O
Krishnan O
et O
al O
. O
, O
2014 O
; O
Saluja O
et O
al O
. O
; O
Adiga O
et O
al O
. O
, O
2018 O
; O
Mathew O
et O
al O
. O
, O
2016 O
) O
. O

In O
the O
case O
of O
providing O
a O
text O
line O
as O
input O
, O
we O
are O
essentially O
providing O
more O
context O
about O
the O
input O
in O
comparison O
to O
the O
word O
level O
models O
and O
the O
RNN O
( O
or O
LSTM B-MethodName
) O
cells O
are O
powerful O
enough O
to O
capture O
the O
long O
- O
term O
dependencies O
. O

We O
use O
3 O
stacked O
layers O
of O
LSTM B-MethodName
at O
the O
encoder O
and O
the O
decoder O
with O
the O
same O
settings O
as O
in O
Bah O
- O
danau O
et O
al O
. O
( O
2015 O
) O
. O

3.3 O
Baselines O
Character O
Tagger O
- O
Sequence O
Labelling O
using O
BiLSTMs B-MethodName
This O
is O
a O
sequence O
labelling O
model O
which O
uses O
BiLSTM B-MethodName
cells O
and O
input O
is O
a O
character O
sequence O
( O
Saluja O
et O
al O
. O
) O
. O

We O
henceforth O
refer O
to O
both O
the O
systems O
as O
BiLSTM B-MethodName
andBiLSTMCRF B-MethodName
, O
respectively O
. O

Encoder O
- O
Decoder O
Models O
: O
For O
the O
seq2seq O
model O
( O
Sutskever O
et O
al O
. O
, O
2014 O
) O
, O
we O
use O
3 O
stacked O
layers O
of O
LSTM B-MethodName
each O
at O
the O
encoder O
and O
the O
decoder O
. O

Error O
type O
analysis O
In O
Table O
5 O
, O
we O
analyse O
the O
reduction O
in O
speciﬁc O
error O
types O
for O
PCRFModel O
Bhagavad O
G O
¯it¯a O
Saha O
´ O
sran¯ama O
Combined O
CRR O
WRR O
Norm O
LP O
CRR O
WRR O
Norm O
LP O
CRR O
WRR O
Norm O
LP O
OCR O
84.81 O
% O
64.40 O
% O
– O
35.76 O
% O
0.65 O
% O
– O
77.88 O
% O
23.84 O
% O
– O
BiLSTM B-MethodName
93.79 O
% O
68.60 O
% O
-0.553 O
61.31 O
% O
7.28 O
% O
-1.292 O
85.23 O
% O
45.60 O
% O
-0.852 O
BiLSTM B-MethodName
- O
CRF O
94.68 O
% O
68.60 O
% O
-0.548 O
65.31 O
% O
7.28 O
% O
-1.281 O
85.82 O
% O
45.60 O
% O
-0.847 O
PCRF O
- O
seq2seq O
96.87 O
% O
70.56 O
% O
-0.227 O
81.77 O
% O
9.34 O
% O
-1.216 O
87.94 O
% O
57.17 O
% O
-0.803 O
EncDec+Char O
91.48 O
% O
68.00 O
% O
-0.542 O
63.63 O
% O
15.74 O
% O
-1.321 O
82.51 O
% O
47.37 O
% O
-0.865 O
EncDec+BPE O
90.92 O
% O
68.00 O
% O
-0.496 O
61.53 O
% O
15.74 O
% O
-1.384 O
83.14 O
% O
45.98 O
% O
-0.842 O
CopyNet+BPE O
97.01 O
% O
75.21 O
% O
-0.165 O
87.01 O
% O
33.47 O
% O
-0.856 O
89.65 O
% O
68.71 O
% O
-0.551 O
Table O
3 O
: O
Performance O
in O
terms O
of O
CRR O
, O
WRR O
and O
Norm O
LP O
( O
acceptability O
) O
for O
all O
the O
competing O
models O
CRR O
WRR O
Bhagavad O
G O
¯it¯a O
96.80 O
% O
71.23 O
% O
Saha O
´ O
sran¯ama O
82.81 O
% O
26.01 O
% O
Combined O
87.88 O
% O
60.91 O
% O
Table O
4 O
: O
Performance O
in O
terms O
of O
CRR O
, O
WRR O
for O
Google O
OCRModel O
Bhagavad O
G O
¯it¯a O
Saha O
´ O
sran¯ama O
System O
errors O
Ins O
Del O
Sub O
Ins O
Del O
Sub O
Ins O
Del O
Sub O
OCR O
23 O
63 O
1868 O
73 O
696 O
1596 O
– O
– O
– O
PCRF O
22 O
57 O
641 O
72 O
663 O
932 O
0 O
73 O
209 O
CopyNet O
22 O
45 O
629 O
72 O
576 O
561 O
10 O
5 O
52 O
Table O
5 O
: O
Insertion O
, O
Deletion O
and O
Substitution O
errors O
for O
OCR O
, O
PCRF O
and O
CopyNet O
modes O
for O
both O
the O
datasets O
. O

The O
system O
contains O
a O
BiLSTM B-MethodName
feature O
extractor O
for O
getting O
context O
- O
aware O
word O
representation O
and O
two O
biafﬁne O
classiﬁers O
to O
predict O
the O
head O
token O
of O
each O
word O
and O
the O
label O
between O
a O
head O
and O
its O
dependent O
. O

Deep O
Bi O
- O
LSTM B-MethodName
Encoder O
, O
which O
produces O
the O
context O
- O
aware O
representation O
of O
each O
token O
in O
the O
sentence O
based O
on O
context O
. O

Dozat O
and O
Manning O
( O
2016 O
) O
use O
the O
pre O
- O
trained O
GloVe B-MethodName
embeddings O
as O
an O
extra O
representation O
of O
the O
word O
. O

The O
lexicalized O
representation O
xl O
iof O
tokenwiis O
deﬁned O
as O
: O
xl O
i= O
[ O
ew O
i+el O
i;|{z O
} O
wordeu O
i+ex O
i|{z O
} O
POS;epw B-TaskName
i;ec O
i O
] O
( O
1 O
) O
and O
the O
delexicalized O
representation O
xd O
iof O
token O
wiis O
deﬁned O
as O
: O
xd O
i= O
[ O
eu O
i;ex O
i;ec O
i O
] O
( O
2 O
) O
In O
the O
following O
sections O
, O
we O
uses O
xito O
represent O
xl O
iorxd O
iwhen O
the O
context O
is O
clear.2.2 O
Deep O
Bi O
- O
LSTM B-MethodName
Encoder O
Generally O
, O
the O
token O
embeddings O
deﬁned O
above O
are O
context O
independent O
, O
which O
means B-MetricName
that O
the O
sentence O
- O
level O
information O
is O
ignored O
. O

In O
recent O
years O
, O
some O
work O
shows O
that O
the O
deep O
BiLSTM B-MethodName
can O
effectively O
capture O
the O
contextual O
information O
of O
words O
( O
Dyer O
et O
al O
. O
, O
2015 O
; O
Kiperwasser O
and O
Goldberg O
, O
2016 O
; O
Dozat O
and O
Manning O
, O
2016 O
; O
Ma O
et O
al O
. O
, O
2018 O
) O
. O

In O
order O
to O
encode O
context O
features O
, O
we O
use O
a O
3layer O
sentence O
level O
BiLSTM B-MethodName
on O
top O
of O
x1 O
: O
n O
: O
~ht= O
LSTM(~ht 1;xi;~ B-MethodName
) O
~ht= O
LSTM(~ht+1;xi;~ B-MethodName
) O
vi=~hi~hi O
~are O
the O
model O
parameters O
of O
the O
forward O
hidden O
sequence O
~ O
h.~are O
the O
model O
parameters O
of O
the O
backward O
hidden O
sequence O
~h O
. O

FLAS O
means B-MetricName
F1 B-MetricName
score B-MetricName
of O
LAS.word/lemma O
dropout B-HyperparameterName
0.33 B-MetricValue
upos O
/ O
xpos O
tag O
dropout B-HyperparameterName
0.33 O
char O
- O
CNN O
dropout B-HyperparameterName
0.33 O
BiLSTM B-MethodName
layers O
3 O
BiLSTM B-MethodName
hidden B-HyperparameterName
layer I-HyperparameterName
dimensions O
400 B-HyperparameterValue
Hidden O
units O
in O
MLP(arc)500 O
Hidden O
units O
in O
MLP(rel)100 O
Learning O
rate O
0.002 O
Optimization O
algorithm O
Adam O
Table O
5 O
: O
Hyper O
- O
parameter O
values O
used O
in O
shared O
task O
. O

3http://epe.nlpl.eu6 O
Conclusions O
In O
this O
paper O
, O
we O
present O
a O
graph O
- O
based O
dependency O
parsing O
system O
for O
the O
CoNLL O
2018 O
UD O
Shared O
Task O
, O
which O
composed O
of O
a O
BiLSTMs B-MethodName
feature O
extractor O
and O
a O
bi O
- O
afﬁne O
pointer O
networks O
. O

The O
results O
suggests O
that O
a O
deep O
BiLSTM B-MethodName
extractor O
and O
a O
bi O
- O
afﬁne O
pointer O
networks O
is O
a O
way O
to O
achieve O
competitive O
parsing O
performances O
. O

For O
the O
supertagging O
model O
the O
main O
contribution O
of O
Kasai O
et O
al O
. O
( O
2018 O
) O
was O
two O
- O
fold O
: O
the O
ﬁrst O
was O
to O
add O
a O
character O
CNN O
for O
modeling O
word O
embeddings O
using O
subword O
features O
, O
and O
the O
second O
was O
to O
add O
highway O
connections O
to O
add O
more O
layers O
to O
a O
standard O
bidirectional O
LSTM B-MethodName
. O

The O
model O
has O
three O
main O
components O
: O
the O
input O
layer O
, O
the O
bidirectional O
LSTM B-MethodName
component O
, O
and O
the O
output O
layer O
which O
computes O
a O
softmax O
over O
the O
set O
of O
supertags O
. O

a O
100/200/300 O
size O
word O
embedding O
which O
is O
initialized O
using O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

For O
words O
that O
do O
not O
appear O
in O
GloVe B-MethodName
, O
we O
randomly O
initialized O
the O
word O
embedding O
. O

3.2 O
BiLSTM B-MethodName
Layer O
The O
core O
of O
this O
base O
model O
is O
a O
bidirectional O
recurrent O
neural O
network O
, O
in O
particular O
a O
Long O
Short O
- O
Term O
Memory O
neural O
network O
( O
Graves O
and O
Schmidhuber O
, O
2005 O
) O
. O

We O
did O
experiment O
with O
the O
addition O
of O
highway O
connections O
but O
we O
found O
no O
improvement O
in O
accuracy B-MetricName
over O
the O
baseline O
BiLSTM B-MethodName
- O
only O
model O
with O
a O
signiﬁcant O
increase O
in O
training O
time O
. O

Dropout O
layers O
( O
Gal O
and O
Ghahramani O
, O
2016 O
; O
Srivastava O
et O
al O
. O
, O
2014 O
) O
are O
inserted O
between O
the O
input O
and O
BiLSTM B-MethodName
layer O
, O
between O
BiLSTM B-MethodName
layers O
, O
and O
between O
recurrent O
time O
steps O
. O

We O
used O
2 O
- O
3 O
BiLSTM B-MethodName
layers O
. O

3.3 O
Output O
Layer O
We O
concatenate O
hidden O
vectors O
from O
both O
directions O
of O
the O
last O
layer O
of O
BiLSTM B-MethodName
and O
pass O
it O
into O
a O
multilayer O
perceptron O
( O
MLP O
) O
. O

4 O
Deconstructing O
Supertags O
The O
error O
analysis O
of O
our O
baseline O
BiLSTM B-MethodName
model O
is O
shown O
in O
Fig O
. O

We O
also O
observed O
that O
the O
baseline O
BiLSTM B-MethodName
model O
can O
achieve O
over O
97 O
% O
3 O
- O
best O
accuracy B-MetricName
on O
the O
supertagging O
task O
. O

Rather O
than O
a O
re O
- O
ranking O
frame O
- O
Prediction O
Ground O
Truth O
# O
times O
in O
dev O
NP O
NP O
Nt2 O
: O
NP O
NP O
At36 O
: O
194 O
COtCO O
: O
VP O
PP O
NP1 O
# O
INVPt13 O
: O
156 O
NP O
PP O
NP1 O
# O
INNPt4 O
: O
VP O
PP O
NP1 O
# O
INVPt13 O
: O
144 O
Table O
1 O
: O
The O
top-3 O
errors O
made O
by O
the O
state O
- O
of O
- O
the O
- O
art O
Bi O
- O
LSTM B-MethodName
supertagger O
. O

We O
experiment O
with O
pre O
- O
trained O
GloVe B-MethodName
word O
embeddings O
of O
three O
different O
sizes O
: O
100 O
, O
200 O
and O
300 O
. O

We O
computed O
a O
signiﬁcance O
score B-MetricName
on O
the O
accuracy B-MetricName
of O
our O
best O
model O
BiLSTM3+CNN+GloVe200 B-MethodName
with O
and O
without O
multi O
- O
task O
learning O
. O

We O
evaluated O
our O
own O
implementation O
of O
the O
baseline O
BiLSTM B-MethodName
- O
only O
model O
and O
even O
with O
highway O
connections O
we O
only O
obtained O
89.25 O
% O
on O
the O
dev O
set O
compared O
to O
the O
built O
- O
in O
BiLSTM B-MethodName
implementation O
in O
Pytorch O
( O
without O
highway O
connections O
) O
which O
obtains O
89.94 O
% O
. O

All O
of O
those O
words O
areModel O
Multi O
- O
task O
Dev O
Test O
BiLSTM3+HW+CNN+POS+GloVe100 B-TaskName
( O
Kasai O
et O
al O
. O
, O
2018 O
) O
- O
90.45 O
90.81 O
BiLSTM2+GloVe100No B-MethodName
89.11 O
Yes O
89.67 O
BiLSTM2+CNN+GloVe100No B-MethodName
89.45 O
Yes O
90.12 O
BiLSTM3+GloVe100No B-MethodName
89.41 O
Yes O
90.02 O
BiLSTM3+CNN+GloVe100No B-MethodName
89.83 O
Yes O
90.41 O
BiLSTM3+CNN+GloVe200No B-MethodName
89.94 O
Yes O
90.55 O
91.37 O
BiLSTM3+CNN+GloVe300No B-MethodName
89.91 O
Yes O
90.45 O
Shared O
BiLSTM B-MethodName
layer O
( O
BiLSTM3+CNN+Glove200)No B-MethodName
90.11 O
90.83 O
Yes O
90.11 O
90.83 O
Table O
2 O
: O
Supertagging O
task O
results O
. O

The O
number O
after O
BiLSTM B-MethodName
represents O
the O
number O
of O
BiLSTM B-MethodName
layers O
; O
CNN O
refers O
to O
the O
word O
embedding O
model O
using O
character O
- O
level O
CNN O
; O
the O
number O
immediately O
after O
GloVe B-MethodName
represents O
the O
dimension O
of O
pre O
- O
trained O
GloVe B-MethodName
word O
vectors O
. O

The O
model O
used O
is O
BiLSTM+CNN+GloVe200 B-MethodName
. O

The O
base O
model O
is O
BiLSTM+CNN+GloVe200 B-MethodName
. O

Kasai O
et O
al O
. O
( O
2017 O
) O
extends O
the O
BiLSTM B-MethodName
model O
with O
predicted O
part O
- O
of O
- O
speech O
tags O
and O
sufﬁx O
embeddings O
as O
inputs O
, O
then O
Kasai O
et O
al O
. O
( O
2018 O
) O
further O
extends O
the O
BiLSTM B-MethodName
model O
with O
highway O
connection O
as O
well O
as O
character O
CNN O
as O
input O
, O
and O
jointly O
train O
the O
supertagging O
model O
with O
parsing O
model O
and O
this O
work O
had O
the O
state O
- O
of O
- O
the O
- O
art O
accuracy B-MetricName
before O
our O
paper O
on O
the O
Penn O
treebank O
dataset O
. O

Xu O
et O
al O
. O
( O
2015 O
) O
uses O
RNN O
for O
the O
CCG O
supertagging O
task O
, O
Lewis O
et O
al O
. O
( O
2016 O
) O
adopted O
the O
LSTM B-MethodName
structure O
into O
this O
task O
, O
while O
Vaswani O
et O
al O
. O
( O
2016 O
) O
also O
introduced O
another O
variation O
of O
Bi O
- O
LSTM B-MethodName
into O
this O
task O
. O

Xu O
( O
2016 O
) O
then O
proposed O
an O
attention O
- O
based O
Bi O
- O
LSTM B-MethodName
supertagging O
model O
. O

In O
this O
paper O
, O
we O
propose O
a O
model O
that O
uses O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
and O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNNs O
) O
in O
sequence O
to O
learn O
global O
and O
local O
context O
, O
respectively O
. O

The O
vector O
representation O
of O
these O
words O
is O
obtained O
using O
the O
GloVe B-MethodName
method O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

3.2 O
Recurrent O
layer O
RNN O
is O
a O
class O
of O
artiﬁcial O
neural B-MethodName
networks I-MethodName
which O
utilizes O
sequential O
information O
and O
maintains O
history O
through O
its O
intermediate O
layers O
( O
Graves O
et O
al O
. O
, O
2009 O
) O
. O

We O
use O
long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
based O
model O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
which O
uses O
memory O
and O
gated O
mechanism O
to O
compute O
the O
hidden O
state O
. O

In O
particular O
we O
use O
a O
bidirectional O
LSTM B-MethodName
model O
( O
Bi O
- O
LSTM B-MethodName
) O
similar O
to O
the O
ones O
used O
in O
( O
Graves O
, O
2013 O
; O
Huang O
et O
al O
. O
, O
2015 O
) O
. O

Leth(t O
) O
landh(t O
) O
rbe O
the O
outputs O
obtained O
from O
the O
forward O
and O
backward O
direction O
of O
the O
LSTM B-MethodName
at O
timet O
. O

4.2 O
Implementation O
details O
Pretrained O
100 O
- O
dimensional O
word O
vectors O
in O
the O
embedding O
layer O
are O
obtained O
using O
the O
GloVe B-MethodName
method O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
trained O
on O
a O
corpus O
of O
PubMed O
open O
source O
articles O
( O
Muneeb O
et O
al O
. O
, O
2015 O
) O
, O
and O
are O
updated O
during O
the O
training O
process O
. O

Feature O
- O
based O
methods O
We O
selected O
a O
feature O
- O
based O
SVM B-MethodName
classiﬁer O
( O
Rink O
et O
al O
. O
, O
2011 O
) O
that O
uses O
several O
handcrafted O
features O
such O
as O
distance O
of O
word O
from O
entities O
, O
POS B-TaskName
tags O
, O
chunk O
tags O
, O
etc O
. O
, O
to O
compare O
whether O
our O
models O
were O
able O
to O
outperform O
classiﬁers O
with O
rigorous O
feature O
engineering O
. O

It O
is O
to O
be O
noted O
that O
we O
use O
our O
own O
implementation O
of O
the O
SVM B-MethodName
classiﬁer O
( O
using O
the O
scikit O
- O
learn O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
library O
) O
, O
using O
features O
as O
described O
in O
( O
Sahu O
et O
al O
. O
, O
2016 O
) O
. O

Single O
- O
layer O
neural B-MethodName
networks I-MethodName
We O
selected O
a O
multiple-ﬁlter O
CNN O
with O
maxpooling O
( O
Sahu O
et O
al O
. O
, O
2016 O
) O
and O
an O
LSTM B-MethodName
model O
with O
max O
and O
attentive O
pooling O
( O
Sahu O
and O
Anand O
, O
2017 O
) O
. O

This O
is O
in O
contrast O
to O
earlier O
studies O
( O
Sahu O
and O
Anand O
, O
2017 O
; O
Collobert O
and O
Weston O
, O
2008 O
) O
where O
pretrained O
embeddingsModeli2b2 O
- O
2010 O
DDI O
extraction O
Precision B-MetricName
Recall B-MetricName
F1 B-MetricName
score B-MetricName
Precision B-MetricName
Recall B-MetricName
F1 B-MetricName
score B-MetricName
SVM B-MethodName
( O
Rink O
et O
al O
. O
, O
2011 B-MetricValue
) O
67.44 O
57.85 O
59.31 O
65.39 O
40.13 O
49.74 O
CNN O
- O
Max O
( O
Sahu O
et O
al O
. O
, O
2016 O
) O
55.73 O
50.08 O
49.42 O
68.15 O
46.58 O
54.05 O
LSTM B-MethodName
- O
Max O
( O
Sahu O
and O
Anand O
, O
2017 O
) O
57.54 O
55.40 O
55.60 O
73.98 O
59.96 O
65.41 O
LSTM B-MethodName
- O
Att O
( O
Sahu O
and O
Anand O
, O
2017 O
) O
65.23 O
56.77 O
60.04 O
53.43 O
64.86 O
58.27 O
RCNN O
( O
Wang O
et O
al O
. O
, O
2016b O
) O
50.07 O
45.34 O
46.47 O
– O
– O
– O
CRNN O
- O
Max O
67.91 O
61.98 O
64.38 O
72.91 O
60.88 O
65.89 O
CRNN O
- O
Att O
64.62 O
62.14 O
62.45 O
69.03 O
59.04 O
63.24 O
Table O
4 O
: O
Comparison O
of O
our O
proposed O
models O
CRNN O
- O
Max O
and O
CRNN O
- O
Att O
, O
with O
baselines O
, O
on O
the O
i2b2 O
- O
2010 O
and O
DDI O
extraction O
datasets O
. O

Class O
Size O
SVM B-MethodName
CNN O
LSTM B-MethodName
- O
Max O
RCNN O
CRNN O
- O
Max O
CRNN O
- O
Att O
TrCP O
108 O
34.90 O
34.01 O
35.48 O
18.30 O
43.18 O
47.66 O
TrAP O
532 O
63.48 O
46.69 O
58.74 O
45.15 O
67.39 O
63.94 O
TrWP O
26 O
7.41 O
10.26 O
0.00 O
0.00 O
16.67 O
9.52 O
TrIP O
41 O
9.09 O
21.74 O
0.00 O
0.00 O
25.71 O
34.48 O
TrNAP O
34 O
5.13 O
15.87 O
0.00 O
0.00 O
36.36 O
18.60 O
TeRP O
614 O
80.44 O
63.52 O
73.50 O
67.01 O
80.32 O
76.31 O
TeCP O
101 O
30.30 O
27.63 O
25.20 O
11.48 O
39.46 O
39.76 O
PIP O
443 O
49.44 O
49.30 O
51.54 O
45.05 O
58.04 O
55.53 O
Table O
6 O
: O
Classwise O
performance O
( O
in O
terms O
of O
F1 B-MetricName
score B-MetricName
) O
of O
various O
models O
on O
the O
i2b2 O
dataset O
. O

However O
, O
this O
result O
aligns O
with O
the O
observations O
made O
in O
( O
Johnson O
and O
Zhang O
, O
2015 O
) O
and O
supports O
the O
argument O
for O
one O
- O
hot O
LSTMs B-MethodName
. O

First O
, O
we O
note O
that O
in O
the O
formulas O
for O
LSTM B-MethodName
, O
e.g. O
,ut= O
tanh O
( O
W(u)xt+U(u)ht−1+b(u O
) O
) O
, O
ifxtis O
the O
one O
- O
hot O
representation O
of O
a O
word O
, O
the O
termW(u)xtserves O
as O
a O
word O
embedding O
. O

Thus O
, O
a O
one O
- O
hot O
LSTM B-MethodName
inherently O
includes O
a O
word O
embedding O
in O
its O
computation O
. O

Further O
, O
a O
word O
vector O
lookup O
is O
a O
linear O
operation O
, O
and O
hence O
it O
may O
be O
merged O
into O
the O
LSTM B-MethodName
layer O
itself O
by O
multiplying O
the O
LSTM B-MethodName
weights B-HyperparameterName
by O
the O
word O
embedding O
matrix O
. O

This O
means B-MetricName
that O
the O
expressive O
power O
of O
an O
LSTM B-MethodName
which O
uses O
pretrained O
vectors O
is O
the O
same O
as O
that O
of O
one O
which O
uses O
randomly O
initialized O
word O
embeddings O
. O

To O
conﬁrm O
our O
hypothesis O
, O
we O
determine O
the O
average B-MetricName
sentence O
lengths O
and O
entity O
separations O
for O
several O
sets O
of O
sentences O
belonging O
to O
classes O
where O
our O
models O
performed O
well O
, O
and O
for O
classes O
where O
either O
the O
CNN O
model O
or O
the O
LSTM B-MethodName
- O
Max O
model O
performed O
relatively O
well O
, O
for O
the O
i2b2 O
- O
2010 B-MetricValue
dataset O
. O

Similarly O
, O
our O
proposed O
models O
perform O
better O
on O
a O
larger O
range O
of O
sentence O
lengths O
than O
LSTMs B-MethodName
, O
which O
may O
be O
due O
to O
more O
effective O
modeling O
of O
local O
contexts O
. O

5.6 O
Effect O
of O
linguistic O
features O
The O
SVM B-MethodName
baseline O
model O
described O
earlier O
consists O
of O
the O
following O
features O
obtained O
for O
each O
word O
in O
the O
sentence O
: O
word O
embedding O
, O
part O
- O
ofspeech O
( O
POS B-TaskName
) O
tag O
, O
chunk O
tag O
, O
distance O
from O
ﬁrst O
entity O
, O
distance O
from O
second O
entity O
, O
and O
entity O
type O
. O

Of O
these O
, O
the O
entity O
type O
feature O
is O
already O
used O
in O
our O
CRNN O
model O
in O
the O
preprocessing O
stepClass O
Size O
SVM B-MethodName
CRNN O
- O
Max O
# O
1 O
# O
2 O
TrCP O
108 O
34.90 O
36.91 O
11 O
30 O
TrAP O
532 O
63.48 O
68.85 O
83 O
93 O
TrWP O
26 O
7.41 O
0.00 O
1 O
0 O
TrIP O
41 O
9.09 O
0.00 O
2 O
0 O
TrNAP O
34 O
5.13 O
0.00 O
1 O
0 O
TeRP O
614 O
80.44 O
81.29 O
69 O
83 O
TeCP O
101 O
30.30 O
36.90 O
5 O
14 O
PIP O
443 O
49.44 O
60.66 O
45 O
110 O
Total O
1899 O
59.31 O
63.78 O
217 O
330 O
Table O
7 O
: O
Classwise O
performance O
comparison O
between O
SVM B-MethodName
and O
CRNN O
- O
Max O
using O
linguistic O
features O
. O

# O
1 O
denotes O
number O
of O
sentences O
of O
a O
class O
classiﬁed O
correctly O
by O
SVM B-MethodName
but O
incorrectly O
by O
CRNN O
- O
Max O
; O
# O
2 O
denotes O
vice O
- O
versa O
. O

In O
this O
section O
, O
we O
add O
the O
four O
other O
linguistic O
features O
in O
our O
proposed O
model O
to O
observe O
its O
performance O
in O
comparison O
with O
the O
SVM B-MethodName
model O
. O

Although O
the O
F1 B-MetricName
scores B-MetricName
for O
the O
models O
are O
relatively O
close O
, O
the O
precision B-MetricName
( O
P O
) O
and O
recall B-MetricName
( O
R O
) O
vary O
signiﬁcantly O
: O
P O
is O
67.44 B-MetricValue
and O
61.00 O
, O
while O
R O
is O
57.85 O
and O
67.54 O
, O
for O
the O
SVM B-MethodName
and O
CRNN O
- O
Max O
models O
, O
respectively O
. O

Our O
CRNN O
- O
Max O
model O
, O
therefore O
, O
is O
more O
sensitive O
while O
the O
SVM B-MethodName
classiﬁer O
has O
a O
higher O
speciﬁcity O
. O

Furthermore O
, O
it O
is O
evident O
that O
SVM B-MethodName
outperforms O
our O
model O
only O
on O
classes O
with O
a O
disproportionately O
low O
instance O
count O
. O

SEQ2SEQ O
dialogue O
model O
This O
basic O
model O
maps O
an O
input O
message O
tto O
a O
vector O
representation O
using O
the O
encoder O
LSTM B-MethodName
layer O
and O
uses O
it O
as O
an O
initial O
state O
hd O
0for O
the O
decoder O
LSTM B-MethodName
layer O
. O

The O
decoder O
is O
an O
LSTM B-MethodName
layer O
with O
attention O
over O
the O
encoded O
memories O
. O

Implementation O
Details O
Similarly O
to O
( O
Zhang O
et O
al O
. O
, O
2018 O
) O
, O
we O
use O
a O
single O
layer O
LSTM B-MethodName
for O
both O
the O
encoder O
and O
the O
decoder O
with O
hidden B-HyperparameterName
size I-HyperparameterName
of O
1024 B-HyperparameterValue
for O
all O
models O
. O

The O
word O
embeddings O
are O
of O
size O
300 O
and O
are O
initialized O
with O
GloVe B-MethodName
word O
vectors O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

We O
use O
this O
technique O
to O
demonstrate O
that O
LSTM B-MethodName
LMs O
’ O
representations O
of O
different O
types O
of O
sentences O
with O
relative O
clauses O
are O
organized O
hierarchically O
in O
a O
linguistically O
interpretable O
manner O
, O
suggesting O
that O
the O
LMs O
track O
abstract O
properties O
of O
the O
sentence O
. O

2.3 O
LM O
adaptation O
as O
cumulative O
priming O
Van O
Schijndel O
and O
Linzen O
( O
2018 O
) O
modeled O
cumulative O
priming O
in O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
by O
adapting O
fully O
trained O
RNN O
LMs O
to O
new O
stimuli O
— O
i.e. O
taking O
a O
fully O
trained O
RNN O
LM O
and O
continuing O
to O
train O
it O
on O
a O
small O
set O
of O
sentences O
( O
cf O
. O

To O
minimize O
the O
likelihood O
that O
the O
adaptation O
effects O
are O
driven O
by O
irrelevant O
properties O
of O
the O
sentence O
, O
we O
introduced O
several O
sources O
of O
variability O
to O
our O
templates O
: O
nouns O
could O
either O
be O
singular O
or O
plural O
, O
noun O
phrases O
could O
be O
optionally O
modiﬁed O
by O
an O
adjective O
, O
adjectives O
were O
optionally O
modiﬁed O
with O
an O
intensiﬁer O
and O
verb O
phrases O
were O
optionally O
modiﬁed O
with O
adverbs O
which O
could O
occur O
either O
pre O
- O
verbally O
or O
postverbally O
( O
details O
in O
the O
Supplementary O
Materials).7 O
4.3 O
Models O
We O
used O
75 O
of O
the O
LSTM B-MethodName
language O
models O
trained O
by O
van O
Schijndel O
et O
al O
. O
( O
2019 O
) O
; O
these O
LMs O
varied O
in O
the O
number O
of O
hidden B-HyperparameterName
units I-HyperparameterName
per O
layer O
( O
100 B-HyperparameterValue
, O
200 O
, O
400 O
, O
800 O
, O
1600 O
) O
and O
the O
number O
of O
tokens O
they O
were O
trained O
on O
( O
2 O
million O
, O
10 O
million O
or O
20 O
million O
) O
. O

As O
a O
consequence O
, O
for O
three O
structuresX O
, O
YandZ O
, O
A(YjX)could O
be O
greater O
than O
A(ZjX)merely O
because O
Ywas O
a O
more O
surprising O
structure O
to O
begin O
with O
than O
Z. O
In O
order O
to O
remove O
this O
confound O
, O
we O
ﬁrst O
ﬁt O
a O
linear B-MethodName
regression I-MethodName
model O
predicting O
A(YjX)from O
the O
surprisal O
of O
Yprior O
to O
adaptation O
( O
Surp O
( O
Y O
) O
): O
A(YjX O
) O
= O
 B-MetricValue
0 O
+ O
 O
1Surp O
( O
Y O
) O
+ O
 O
We O
then O
regressed O
out O
the O
linear O
relationship O
betweenA(YjX)andSurp O
( O
Y)as O
follows O
: O
AE(YjX O
) O
= O
A(YjX)  O
 O
1Surp O
( O
Y O
) O
= O
 O
0+ O
Since O
Surp O
( O
Y)was O
centered O
around O
its O
mean B-MetricName
, O
 O
0reﬂects O
the O
mean B-MetricName
of O
A(YjX)when O
Surp O
( O
Y O
) O
is O
equal O
to O
the O
mean B-MetricName
surprisal O
of O
all O
sentences O
prior O
to O
adaptation O
. O

Using O
this O
dataset O
, O
they O
showed O
that O
LSTM B-MethodName
LMs O
could O
not O
accurately O
predict O
the O
number O
of O
the O
main O
verb O
if O
the O
main O
clause O
subject O
was O
modiﬁed O
by O
an O
object O
RCs O
( O
either O
reduced O
or O
unreduced O
) O
. O

In O
particular O
, O
we O
propose O
to O
use O
adversarial O
training O
of O
neural B-MethodName
networks I-MethodName
to O
learn O
high O
- O
level O
features O
that O
are O
discriminative O
for O
the O
main O
learning O
task O
, O
and O
at O
the O
same O
time O
are O
invariant O
across O
the O
input O
languages O
. O

Adversarial O
training O
of O
neural B-MethodName
networks I-MethodName
has O
shown O
a O
big O
impact O
recently O
, O
especially O
in O
areas O
such O
as O
computer O
vision O
, O
where O
generative O
unsupervised O
models O
have O
proved O
capable O
of O
synthesizing O
new O
images O
( O
Goodfellow O
et O
al O
. O
, O
2014 O
; O
Radford O
et O
al O
. O
, O
2016 O
; O
Makhzani O
et O
al O
. O
, O
2016 O
) O
. O

More O
relevant O
to O
our O
work O
is O
the O
work O
of O
Ganin O
et O
al O
. O
( O
2016 O
) O
, O
who O
proposed O
domain O
adversarial O
neural B-MethodName
networks I-MethodName
( O
DANN O
) O
to O
learn O
discriminative O
but O
at O
the O
same O
time O
domain O
- O
invariant O
representations O
, O
with O
domain O
adaptation O
as O
a O
target O
. O

Yet O
another O
emerging O
approach O
is O
to O
use O
neural B-MethodName
networks I-MethodName
, O
e.g. O
, O
dos O
Santos O
et O
al O
. O
( O
2015 O
) O
used O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNNs O
) O
, O
Romeo O
et O
al O
. O
( O
2016 O
) O
used O
long O
short O
- O
term O
memory O
( O
LSTMs B-MethodName
) O
networks O
with O
neural O
attention O
to O
select O
the O
important O
part O
when O
comparing O
two O
questions O
, O
and O
Lei O
et O
al O
. O
( O
2016 O
) O
used O
a O
combined O
recurrent O
– O
convolutional O
model O
to O
map O
questions O
to O
continuous O
semantic O
representations O
. O

The O
problem O
was O
studied O
in O
( O
Martino O
et O
al O
. O
, O
2017 O
) O
using O
cross O
- O
language O
kernels O
and O
deep O
neural B-MethodName
networks I-MethodName
; O
however O
, O
they O
used O
no O
adversarial O
training O
. O

We O
further O
plan O
to O
try O
LSTM B-MethodName
and O
CNN O
for O
generating O
the O
initial O
representation O
of O
the O
input O
text O
( O
instead O
of O
simple O
averaging O
of O
word O
embeddings O
) O
. O

Thecross O
- O
lingual O
semantic O
transfer O
by O
these O
models O
is O
captured O
from O
parallel O
corpora O
with O
sentential O
or O
document O
- O
level O
alignment O
, O
using O
techniques O
such O
as O
bilingual O
bag O
- O
of O
- O
words O
distances O
( O
BilBOWA O
) O
( O
Gouws O
et O
al O
. O
, O
2015 O
) O
, O
Skip O
- O
Gram O
( O
Coulmance O
et O
al O
. O
, O
2015 O
) O
and O
sparse O
tensor O
factorization O
( O
Vyas O
and O
Carpuat O
, O
2016 O
) O
. O

3.1.1 O
Attentive O
GRU O
Encoder O
The O
GRU O
encoder O
is O
a O
computationally O
efﬁcient O
alternative O
of O
the O
LSTM B-MethodName
( O
Cho O
et O
al O
. O
, O
2014 O
) O
. O

The O
Linear O
bag O
- O
of O
- O
words O
( O
BOW O
) O
encoder O
( O
Ji O
et O
al O
. O
, O
2017 O
; O
Hill O
et O
al O
. O
, O
2016 O
) O
is O
realized O
by O
the O
2Note O
that O
recent O
advances O
in O
monolingual O
contextualized O
embeddings O
like O
multilingual O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Che O
et O
al O
. O
, O
2018 O
) O
and O
M O
- O
BERT B-MethodName
( O
Pires O
et O
al O
. O
, O
2019 O
; O
Devlin O
et O
al O
. O
, O
2018 O
) O
can O
also O
be O
supported O
to O
represent O
sentences O
for O
our O
setting O
. O

LSG O
iandLSG O
jare O
the O
original O
Skip O
- O
Gram O
losses O
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
to O
separately O
obtain O
word O
embeddings O
on O
monolingual O
corpora O
of O
liandlj O
. O

Two O
monolingual O
threads O
select O
batches O
of O
monolingual O
contexts O
from O
the O
Wikipedia O
dump O
of O
two O
languages O
for O
Skip O
- O
Gram O
, O
one O
alignment O
thread O
randomly O
samples O
parallel O
sentences O
from O
Europarl O
, O
and O
one O
dictionary O
thread O
extracts O
samples O
of O
entries O
for O
a O
bilingual O
multi O
- O
task O
dictionary O
model O
. O

To O
generate O
negative O
examples O
, O
given O
a O
source O
word O
, O
we O
ﬁrst O
ﬁnd O
its O
15 O
nearest B-MethodName
neighbors I-MethodName
in O
the O
embedding O
space O
. O

Within O
the O
nearest B-MethodName
neighbors I-MethodName
, O
we O
use O
ConceptNet O
( O
Speer O
et O
al O
. O
, O
2017 O
) O
to O
ﬁlter O
out O
the O
synonyms O
of O
the O
source O
word O
, O
so O
as O
to O
prevent O
from O
generating O
false O
negative O
cases O
. O

These O
models O
include O
Siamese O
structures O
of O
CNN O
( O
BiCNN O
) O
( O
Yin O
and O
Sch O
¨utze O
, O
2015 O
) O
, O
RNN O
( O
BiLSTM B-MethodName
) O
( O
Mueller O
and O
Thyagarajan O
, O
2016 O
) O
, O
attentive O
CNN O
( O
ABCNN O
) O
( O
Yin O
et O
al O
. O
, O
2016 O
) O
, O
attentive O
GRU O
( O
BiATT O
) O
( O
Rockt O
¨aschel O
et O
al O
. O
, O
2016 O
) O
, O
and O
BOW O
( O
BiBOW O
) O
. O

F1 B-MetricName
BiBOW O
54.93 B-MetricValue
0.622 O
56.27 O
0.623 O
BiCNN O
54.33 O
0.625 O
53.80 O
0.611 O
ABCNN O
56.73 O
0.644 O
58.83 O
0.655 O
BiLSTM B-MethodName
59.60 O
0.662 O
57.60 O
0.637 O
BiATT O
61.47 O
0.699 O
61.27 O
0.689 O
BilDRL O
-GRU O
- O
MTL O
64.80 O
0.732 O
63.33 O
0.722 O
BilDRL O
-ATT O
- O
MTL O
65.27 O
0.735 O
66.07 O
0.735 O
BilDRL O
-ATT O
- O
joint O
68 O
: O
53 O
0 O
: O
78567 O
: O
13 O
0 O
: O
759 O
Table O
4 O
: O
Accuracy O
and O
F1 B-MetricName
- O
scores B-MetricName
of O
bilingual O
paraphrase O
identiﬁcation O
. O

To O
determine O
whether O
w O
should O
be O
added O
as O
a O
feature O
, O
they O
train O
a O
linear O
SVM B-MethodName
classiﬁer O
to O
separate O
the O
vector O
representations O
of O
the O
entities O
efor O
whichwis O
mentioned O
inDefrom O
the O
vector O
representations O
of O
the O
other O
entities O
. O

If O
this O
SVM B-MethodName
classiﬁer O
is O
sufﬁciently O
accurate2 O
, O
they O
assume O
that O
the O
word O
wcaptures O
a O
salient O
feature O
. O

The O
corresponding O
feature O
direction O
is O
then O
characterized O
by O
the O
normal O
vector O
df O
of O
the O
hyperplane O
that O
was O
learned O
by O
the O
SVM B-MethodName
classiﬁer O
. O

In O
our O
experiments O
, O
we O
used O
logistic B-MethodName
regression I-MethodName
classiﬁers O
instead O
of O
SVMs B-MethodName
, O
which O
we O
found O
to O
perform O
similarly O
but O
were O
faster O
to O
train O
. O

Note O
that O
( O
2 O
) O
essentially O
expresses O
that O
for O
eachf2Xi O
, O
we O
want O
to O
train O
a O
logistic B-MethodName
regression I-MethodName
classiﬁer O
with O
coefﬁcient O
vector O
cf O
. O

Country O
AL.DT O
- O
D1MDS O
0.34 O
0.26 O
0.26 O
0.26 O
0.38 O
0.43 O
0.67 O
0.24 O
0.47 O
0.47 O
IncAgg O
0.45 O
0.30 O
0.30 O
0.25 O
0.40 O
0.47 O
0.76 O
0.26 O
0.50 O
0.50 O
CosIncAgg O
0.45 O
0.26 O
0.30 O
0.24 O
0.38 O
0.43 O
0.75 O
0.23 O
0.43 O
0.42 O
IncHDB O
0.43 O
0.26 O
0.28 O
0.25 O
0.38 O
0.40 O
0.50 O
0.22 O
0.46 O
0.46 O
NonIncHDB O
0.30 O
0.20 O
0.27 O
0.23 O
0.34 O
0.40 O
0.50 O
0.20 O
0.46 O
0.47 O
NonIncAgg O
0.33 O
0.24 O
0.27 O
0.23 O
0.33 O
0.42 O
0.40 O
0.21 O
0.48 O
0.47DT O
- O
D3MDS O
0.52 O
0.27 O
0.32 O
0.27 O
0.43 O
0.47 O
0.70 O
0.27 O
0.47 O
0.46 O
IncAgg O
0.58 O
0.34 O
0.34 O
0.27 O
0.41 O
0.47 O
0.77 O
0.30 O
0.54 O
0.52 O
CosIncAgg O
0.54 O
0.28 O
0.34 O
0.25 O
0.40 O
0.45 O
0.78 O
0.26 O
0.47 O
0.45 O
IncHDB O
0.57 O
0.26 O
0.31 O
0.27 O
0.41 O
0.45 O
0.70 O
0.27 O
0.49 O
0.50 O
NonIncHDB O
0.43 O
0.24 O
0.27 O
0.26 O
0.38 O
0.44 O
0.60 O
0.21 O
0.48 O
0.49 O
NonIncAgg O
0.36 O
0.30 O
0.29 O
0.24 O
0.38 O
0.45 O
0.65 O
0.22 O
0.51 O
0.50SVMMDS B-MethodName
0.65 O
0.31 O
0.35 O
0.25 O
0.54 O
0.54 O
0.71 O
0.26 O
0.38 O
0.39 O
IncAgg O
0.73 O
0.33 O
0.37 O
0.26 O
0.54 O
0.55 O
0.76 O
0.26 O
0.52 O
0.51 O
CosIncAgg O
0.62 O
0.33 O
0.34 O
0.25 O
0.52 O
0.53 O
0.80 O
0.12 O
0.50 O
0.50 O
IncHDB O
0.65 O
0.30 O
0.36 O
0.23 O
0.50 O
0.51 O
0.70 O
0.20 O
0.51 O
0.51 O
NonIncHDB O
0.60 O
0.35 O
0.37 O
0.24 O
0.46 O
0.52 O
0.68 O
0.24 O
0.52 O
0.51 O
NonIncAgg O
0.58 O
0.35 O
0.35 O
0.24 O
0.48 O
0.51 O
0.72 O
0.26 O
0.50 O
0.51GaussianMDS O
0.81 O
0.45 O
0.46 O
0.26 O
0.58 O
0.48 O
0.74 O
0.27 O
0.53 O
0.51 O
IncAgg O
0.87 O
0.48 O
0.45 O
0.28 O
0.60 O
0.51 O
0.81 O
0.27 O
0.54 O
0.55 O
CosIncAgg O
0.81 O
0.45 O
0.46 O
0.28 O
0.60 O
0.51 O
0.81 O
0.28 O
0.53 O
0.53 O
IncHDB O
0.84 O
0.43 O
0.43 O
0.27 O
0.60 O
0.51 O
0.80 O
0.28 O
0.54 O
0.53 O
NonIncHDB O
0.75 O
0.41 O
0.40 O
0.23 O
0.51 O
0.47 O
0.75 O
0.27 O
0.59 O
0.53 O
NonIncAgg O
0.71 O
0.46 O
0.45 O
0.22 O
0.52 O
0.46 O
0.77 O
0.27 O
0.58 O
0.53 O
Table O
3 O
: O
Classiﬁcation O
tasks O
performance O
( O
in O
terms O
of O
F1 B-MetricName
score B-MetricName
) O
when O
using O
the O
MDS O
space O
and O
four O
variation O
of O
the O
facet O
- O
based O
representations O
. O

In O
all O
cases O
, O
we O
use O
50dimensional O
pre O
- O
trained O
GloVe B-MethodName
word O
vectors O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
for O
clustering O
the O
features O
. O

For O
example O
, O
Ager O
et O
al O
. O
( O
2018 O
) O
used O
low O
- O
depth O
decision B-MethodName
trees I-MethodName
to O
evaluate O
a O
method O
for O
learning O
feature O
directions O
in O
vector O
space O
embeddings O
. O

Given O
that O
a O
depth-1 O
decision B-MethodName
tree I-MethodName
can O
only O
use O
one O
of O
these O
features O
, O
the O
performance O
of O
such O
a O
decision B-MethodName
tree I-MethodName
essentially O
tells O
us O
to O
what O
extent O
the O
classes O
that O
are O
considered O
in O
the O
supervised O
classiﬁcation O
task O
have O
been O
discovered O
as O
features O
. O

In O
our O
experiments O
, O
we O
will O
report O
the O
result O
of O
depth-1 O
and O
depth-3 O
decision B-MethodName
trees I-MethodName
. O

The O
performance O
of O
the O
decision B-MethodName
trees I-MethodName
will O
allow O
us O
to O
evaluate O
whether O
we O
are O
able O
to O
learn O
higher O
- O
quality O
feature O
directions O
thanks O
to O
the O
decomposition O
of O
the O
vector O
space O
into O
facet O
subspaces O
. O

Speciﬁcally O
, O
we O
train O
a O
support O
vector O
machine O
( O
SVM B-MethodName
) O
for O
each O
of O
the O
facets O
, O
leading O
to O
the O
predictionsp1;:::p O
k. O
These O
predictions O
are O
then O
aggregated O
to O
a O
ﬁnal O
prediction O
using O
a O
logistic B-MethodName
regression I-MethodName
meta O
- O
classiﬁer O
. O

As O
baseline O
, O
we O
simply O
train O
a O
single O
SVM B-MethodName
classiﬁer O
in O
the O
full O
vector O
space O
. O

Looking O
more O
closely O
at O
the O
results O
of O
our O
main O
method O
IncAgg O
, O
it O
is O
interesting O
to O
note O
that O
large O
improvements O
are O
obtained O
for O
depth-1 O
decision B-MethodName
trees I-MethodName
, O
which O
shows O
that O
our O
facet O
subspaces O
make O
it O
easier O
to O
identify O
features O
that O
correspond O
to O
the O
categories O
from O
the O
corresponding O
classiﬁcation O
problems O
. O

However O
, O
large O
improvements O
can O
also O
be O
seen O
for O
SVMs B-MethodName
, O
which O
shows O
that O
the O
actual O
decomposition O
of O
the O
space O
is O
also O
helpful O
. O

While O
the O
neighbours O
in O
the O
full O
space O
are O
a O
mixture O
of O
different O
building O
types O
( O
hotels O
, O
commercial O
buildings O
, O
museum O
, O
and O
educational O
buildings O
) O
, O
in O
the O
facet O
subspace O
all O
nearest B-MethodName
neighbors I-MethodName
are O
universities O
. O

A O
logistic B-MethodName
regression I-MethodName
classiﬁer O
is O
used O
both O
for O
question O
classiﬁcation O
and O
candidate O
answer O
scoring O
. O

The O
typical O
architecture O
of O
such O
systems O
( O
Wang O
and O
Jiang O
, O
2016 O
; O
Xiong O
et O
al O
. O
, O
2016 O
; O
Seo O
et O
al O
. O
, O
2016 O
) O
can O
be O
summarized O
as O
follows O
: O
1.Embedding O
Layer O
: O
Question O
and O
context O
tokens O
are O
mapped O
to O
a O
high O
- O
dimensional O
vector O
space O
, O
for O
example O
via O
GloVe B-MethodName
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
( O
optionally O
) O
character O
embeddings O
( O
Seo O
et O
al O
. O
, O
2016).2.Encoding O
Layer O
: O
The O
token O
vectors O
are O
processed O
independently O
for O
question O
and O
context O
, O
usually O
by O
a O
recurrent O
neural O
network O
( O
RNN O
) O
. O

Examples O
are O
Match O
- O
LSTM B-MethodName
( O
Wang O
and O
Jiang O
, O
2016 O
) O
and O
Coattention O
( O
Xiong O
et O
al O
. O
, O
2016 O
) O
. O

The O
main O
drawback O
of O
this O
approach O
is O
catastrophic O
forgetting O
, O
which O
describes O
the O
phenomenon O
that O
neural B-MethodName
networks I-MethodName
tend O
to O
” O
forget O
” O
knowledge O
, O
i.e. O
, O
its O
performance O
in O
the O
source O
domain O
drops O
signiﬁcantly O
when O
they O
are O
trained O
on O
the O
new O
dataset O
. O

Progressive O
neural B-MethodName
networks I-MethodName
combat O
this O
issue O
by O
keeping O
the O
original O
parameters O
ﬁxed O
and O
adding O
new O
units O
that O
can O
access O
previously O
learned O
features O
( O
Rusu O
et O
al O
. O
, O
2016 O
) O
. O

We O
use O
three O
sources O
of O
embeddings O
, O
which O
are O
concatenated O
to O
form O
a O
single O
embedding O
vector O
: O
•GloVe B-MethodName
embeddings O
: O
300 O
- O
dimensional O
GloVe B-MethodName
vectors O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

•Biomedical O
Word2Vec B-MethodName
embeddings O
: O
200dimensional O
vectors O
trained O
using O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
on O
about O
10 O
million O
PubMed O
abstracts O
( O
Pavlopoulos O
et O
al O
. O
, O
2014 O
) O
. O

We O
start O
from O
the O
BiDAF O
model O
( O
Seo O
et O
al O
. O
, O
2016 O
) O
, O
whose O
input O
is O
two O
sequences O
of O
words O
: O
a O
sentence O
sand O
a O
question O
q. O
The O
model O
predicts O
the O
start O
and O
end O
positions O
ystart O
, O
yendof O
the O
answer O
span O
in O
s. O
BiDAF O
uses O
recurrent O
neural B-MethodName
networks I-MethodName
to O
encode O
contextual O
information O
within O
s O
andqalongside O
an O
attention O
mechanism O
to O
align O
parts O
of O
qwithsand O
vice O
- O
versa O
. O

Hyperparameters O
In O
our O
experiments O
, O
we O
initialized O
word O
embeddings O
with O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
and O
did O
not O
ﬁne O
- O
tune O
them O
. O

Unsupervised O
LMs O
include O
autoregressive O
and O
masked O
language O
models O
like O
GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
and O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
, O
among O
others O
. O

Other O
works O
probe O
the O
ability O
of O
neural B-MethodName
networks I-MethodName
to O
learn O
compositional O
and O
hierarchical O
semantics O
( O
Hupkes O
and O
Zuidema O
, O
2018 O
) O
and O
to O
represent O
symbolic O
semantics O
( O
Chrupała O
and O
Alishahi O
, O
2019 O
) O
on O
simple O
languages O
of O
nested O
arithmetic O
expressions O
. O

SPLAT O
Tokens O
per O
Game O
73.5 O
704.2 O
Number O
of O
Pre O
- O
training O
Tokens O
29.4 O
M O
10.6 O
M O
Number O
of O
Fine O
- O
tuning O
Questions O
300 O
K O
73 O
K O
V O
ocabulary O
Size O
4390 O
144 O
TLM O
Pre O
- O
training O
perplexity B-MetricName
6.8 B-MetricValue
2.0 O
LSTM B-MethodName
Pre O
- O
training O
Perplexity O
13.4 O
1.9 O
Table O
2 O
: O
Dataset O
Summary O
Statistics O
. O

A O
LSTM B-MethodName
language O
model O
( O
Zaremba O
et O
al O
. O
, O
2014 O
) O
is O
also O
evaluated O
. O

Although O
more O
sophisticated O
LSTM B-MethodName
architectures O
exist O
( O
Merity O
et O
al O
. O
, O
2018 O
; O
Yang O
et O
al O
. O
, O
2018 O
) O
, O
we O
elect O
to O
use O
a O
basic O
architecture O
since O
our O
goal O
is O
not O
to O
maximize O
performance O
but O
to O
identify O
which O
models O
exhibit O
competencies O
or O
deﬁciencies O
relative O
to O
a O
random O
baseline O
. O

Lastly O
, O
we O
evaluate O
using O
GPT-2 B-MethodName
, O
which O
is O
architecturally O
analogous O
to O
the O
TLM O
. O

Our O
primary O
motivation O
in O
using O
GPT-2 B-MethodName
is O
to O
evaluate O
the O
impact O
of O
pre O
- O
training O
on O
a O
large O
open O
- O
domain O
natural O
language O
corpus O
. O

The O
LSTM B-MethodName
language O
model O
also O
uses O
a O
512dimensional O
embedding O
space O
with O
tied O
embedding O
, O
two O
LSTM B-MethodName
layers O
, O
30 O
time O
steps O
, O
a O
dropout B-HyperparameterName
rate I-HyperparameterName
of O
0.5 B-HyperparameterValue
and O
gradients O
are O
clipped O
at O
a O
norm O
of O
2.0 O
. O

We O
use O
the O
pre O
- O
trained O
GPT2 B-MethodName
small O
conﬁguration O
. O

These O
question O
- O
answer O
pairs O
are O
inserted O
at O
random O
times O
t. O
The O
transformer O
- O
based O
and O
LSTM B-MethodName
language O
models O
are O
each O
pre O
- O
trained O
for O
100 O
epochs B-HyperparameterName
and O
ﬁne O
- O
tuned O
for O
25 B-HyperparameterValue
epochs B-HyperparameterName
. O

The O
GPT-2 B-MethodName
models O
are O
onlyChess O
Dataset O
Baseball O
Dataset O
Model O
Legal O
Assign- O
Who O
’s O
Goal- O
Model O
Legal O
Assign- O
Who O
’s O
GoalParams O
Verbs O
ment O
Win O
. O

state O
TLM O
Pretrained O
23.4 O
75.8 O
19.1 O
50.7 O
TLM O
Finetuned O
23.4 O
73.1 O
97.6 O
90.8 O
74.5 O
19.1 O
48.7 O
79.8 O
90.4 O
28.7 O
TLM O
Randomized O
23.4 O
66.1 O
97.2 O
79.4 O
68.2 O
19.1 O
54.5 O
82.9 O
90.4 O
20.8 O
LSTM B-MethodName
Finetuned O
8.7 O
55.2 O
62.9 O
40.3 O
14.6 O
4.4 O
53.2 O
68.3 O
45.6 O
6.6 O
LSTM B-MethodName
Randomized O
8.7 O
47.9 O
63.8 O
63.7 O
15.9 O
4.4 O
52.8 O
74.3 O
49.4 O
10.9 O
GPT-2 B-MethodName
Pretrained O
115.0 O
5.0 O
2.0 O
23.3 O
n O
/ O
a O
GPT-2 B-MethodName
Finetuned O
115.0 O
23.3 O
69.2 O
76.0 O
n O
/ O
a O
Random O
Baseline O
0.8 O
39.1 O
35.2 O
1.0 O
11.8 O
36.1 O
37.5 O
0.4 O
Table O
3 O
: O
Probing O
Question O
Results O
. O

TLMs O
generally O
outperform O
LSTM B-MethodName
language O
models O
, O
which O
may O
be O
related O
to O
the O
number O
of O
model O
parameters O
or O
differences O
in O
underlying O
architectures O
. O

Interestingly O
, O
TLMs O
consistently O
outperform O
GPT-2 B-MethodName
, O
despite O
almost O
identical O
architectures O
and O
fewer O
parameters O
. O

Language O
model O
performance O
on O
Legal O
Verbs O
is O
strong O
, O
with O
R O
- O
Precision B-MetricName
for O
chess O
and O
baseball O
exceeding O
50 B-MetricValue
% O
for O
TLMs O
and O
LSTM B-MethodName
. O

The O
transformer O
- O
based O
language O
models O
( O
including O
GPT-2 B-MethodName
) O
performed O
better O
on O
the O
Who O
’s O
Winning O
task O
than O
the O
LSTMs B-MethodName
. O

We O
evaluated O
on O
GPT-2 B-MethodName
to O
determine O
the O
extent O
to O
which O
the O
model O
already O
“ O
knows O
how O
to O
play O
chess O
” O
based O
upon O
pre O
- O
training O
on O
a O
web O
- O
scale O
natural O
language O
corpus O
. O

We O
did O
not O
evaluate O
GPT-2 B-MethodName
on O
the O
Goal O
State O
task O
for O
chess O
since O
it O
would O
require O
approximately O
1015 O
inference O
passes O
. O

We O
did O
not O
evaluate O
GPT-2 B-MethodName
on O
the O
baseball O
dataset O
since O
our O
SPLAT O
encoding O
is O
unique O
to O
this O
dataset O
. O

TLM O
TLM O
LSTM B-MethodName
GPT-2 B-MethodName
Pre O
Fine O
Fine O
Fine O
Legal O
( O
r O
- O
prec O
) O
75.8 O
73.1 O
55.2 O
23.3 O
In O
- O
Pattern O
( O
mass O
) O
97.8 O
93.7 O
57.5 O
30.0 O
Legal O
( O
mass O
) O
95.8 O
91.9 O
46.2 O
25.4 O
Blocked O
( O
mass O
) O
2.0 O
1.7 O
11.3 O
4.6 O
Table O
4 O
: O
Error O
Analysis O
of O
Legal O
Actions O
on O
the O
Chess O
Dataset O
. O

To O
create O
our O
datasets O
, O
we O
introduce O
a O
templating O
method O
, O
by O
which O
we O
generated O
hundreds O
of O
synthetic O
examples O
from O
a O
single O
original O
sentence O
, O
while O
controlling O
the O
variance O
between O
the O
datasets.1We O
employ O
a O
recent O
NLI O
model O
, O
based O
on O
the O
pretrained O
BERT B-MethodName
masked O
language O
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
ﬁne O
- O
tuned O
on O
the O
MultiNLI O
dataset O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
. O

Notable O
contextual O
representations O
are O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018b O
) O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
) O
and O
XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
, O
which O
are O
pre O
- O
trained O
as O
language O
models O
on O
large O
corpora O
. O

In O
these O
challenge O
datasets O
, O
a O
model O
is O
trained O
on O
the O
general O
NLI O
datasets O
, O
i.e. O
SNLI O
or O
the O
Multi O
- O
Genre B-MethodName
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
datasets O
( O
MultiNLI O
; O
Williams O
et O
al O
. O
, O
2018 O
) O
. O

4 O
Experiments O
We O
use O
a O
standard O
model O
based O
on O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
our O
NLI O
model O
. O

The O
6To O
ﬁne O
- O
tune O
BERT B-MethodName
, O
we O
use O
the O
Adam O
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2015 B-HyperparameterValue
) O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
710 7 O
, O
 B-HyperparameterValue
1= O
0:9 O
, O
 O
2= O
0:999 O
, O
andL2weight B-HyperparameterName
decay O
of O
0:01.good O
performance O
on O
this O
inference O
type O
indicates O
a O
blind O
spot O
in O
the O
MultiNLI O
dataset O
rather O
than O
a O
model O
weakness O
. O

We O
demonstrated O
our O
methodology O
on O
a O
standard O
model O
based O
on O
BERT B-MethodName
, O
focusing O
on O
dative O
alternation O
and O
numerical O
reasoning O
. O

2Recent O
contextualized O
word O
embeddings O
, O
such O
as O
BERT B-MethodName
Devlin O
et O
al O
. O
( O
2019 O
) O
, O
can O
not O
be O
reasonably O
used O
without O
ﬁne O
tuning O
. O

nel O
spectral O
clustering O
, O
kernel O
SVM B-MethodName
, O
graph O
kernel O
semi O
- O
supervised O
learning O
, O
etc O
. O
) O
. O

The O
next O
section O
investigates O
this O
claim O
in O
the O
speciﬁc O
case O
of O
SVMs B-MethodName
. O

Speciﬁcally O
, O
we O
consider O
here O
the O
standard O
leastsquare O
kernel O
support O
vector O
machine O
( O
LSSVM B-MethodName
) O
classiﬁer O
( O
used O
, O
e.g. O
, O
in O
Mitra O
et O
al O
. O
( O
2007 O
) O
for O
text O
classiﬁcation O
with O
some O
reﬁnement O
) O
, O
with O
kernel O
K O
= O
ff(1 O
pkxi xjk2)g1i;jn O
, O
for O
some O
function O
fto O
be O
speciﬁed O
. O

The O
LSSVM B-MethodName
classiﬁer O
allocates O
the O
class O
of O
a O
new O
datum O
xbased O
on O
its O
position O
with O
respect O
to O
a O
hyperplane O
in O
kernel O
space O
designed O
from O
the O
training O
set O
X. O
Although O
not O
directly O
a O
spectral O
method O
( O
as O
in O
the O
unsupervised O
spectral O
clustering O
algorithm O
( O
V O
on O
Luxburg O
, O
2007 O
) O
) O
, O
for O
largen;p O
, O
the O
LSSVM B-MethodName
classiﬁer O
inherently O
exploits O
the O
eigenspectrum O
of O
the O
kernel O
matrix O
K O
and O
its O
performance O
is O
proved O
in O
Liao O
and O
Couillet O
( O
2019 O
) O
to O
be O
asymptotically O
predictable O
( O
for O
large O
enoughp;n O
) O
and O
in O
closed O
form O
( O
which O
is O
thus O
simpler O
than O
the O
margin O
- O
based O
SVM B-MethodName
, O
whose O
asymptotic O
performances O
do O
not O
admit O
a O
closed O
form O
) O
. O

Figure O
5 O
reports O
the O
performances O
of O
LSSVM B-MethodName
as O
a O
function O
of O
the O
ratio O
f0(^ O
 O
p)=f00(^ O
 O
p O
) O
, O
where O
^ O
 O
p1 O
n(n 1)P O
1i6 O
= O
jn1 O
pkxi xjk2is O
a O
consistent O
( O
and O
fast O
converging O
) O
estimate O
for O
 O
p O
, O
here O
for O
two O
kernels O
: O
( O
a O
) O
the O
second O
order O
polynomial O
kernel O
such O
that O
f(^ O
 O
p O
) O
= O
4 O
, O
f00(^ O
 O
p O
) O
= O
1 O
andf0 O
( O
 O
p O
) O
varying O
from 2to1 O
, O
and O
( O
b O
) O
the O
RBF O
kernel O
with O
bandwidth2such O
that 22varies O
from 2to0 O
( O
of O
course 22cannot O
be O
positive O
) O
. O

The O
encoder O
consists O
of O
LSTM B-MethodName
cells O
that O
transform O
a O
sequence O
into O
a O
continuous O
vector O
. O

The O
ﬁnal O
time O
step O
’s O
hidden O
state O
and O
the O
cell O
state O
are O
used O
to O
initialize O
the O
decoder O
LSTM B-MethodName
network O
. O

The O
decoder O
LSTM B-MethodName
network O
predicts O
a O
character O
at O
each O
time O
step O
by O
passing O
the O
output O
of O
the O
decoder O
LSTM B-MethodName
through O
a O
softmax O
layer O
. O

The O
encoder O
LSTM B-MethodName
is O
trained O
to O
predict O
the O
next O
character O
in O
the O
source O
word O
at O
each O
time O
step O
. O

Both O
the O
encoder O
and O
decoder O
LSTM B-MethodName
units O
consisted O
of O
256 O
hidden B-HyperparameterName
units I-HyperparameterName
. O

We O
examine O
the O
extent O
to O
which O
deep O
learning O
methods O
support O
automatic O
detection O
and O
identiﬁcation O
of O
slang O
from O
natural O
sentences O
using O
a O
combination O
of O
bidirectional O
recurrent O
neural B-MethodName
networks I-MethodName
, O
conditional O
random O
ﬁeld O
, O
and O
multilayer O
perceptron O
. O

This O
work O
uses O
long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
models O
to O
generate O
slang O
words O
in O
terms O
of O
string O
components O
according O
to O
type O
- O
sensitive O
characteristics O
. O

We O
examine O
a O
comprehensive O
set O
of O
linguistic O
features O
that O
might O
be O
diagnostic O
of O
slang O
usage O
in O
natural O
settings O
, O
and O
we O
explore O
existing O
methods O
that O
leverage O
bidirectional O
LSTM B-MethodName
with O
multilayer O
perceptron O
( O
MLP O
) O
( O
Rauber O
and O
Berns O
, O
2011 O
) O
and O
conditional O
random O
ﬁeld O
( O
CRF O
) O
( O
Lafferty O
et O
al O
. O
, O
2001 O
) O
. O

3.2 O
Model O
architectures O
We O
present O
a O
BiLSTM B-MethodName
- O
MLP O
model O
that O
is O
capable O
of O
identifying O
slang O
words O
in O
a O
given O
sentence O
. O

Fully O
connected O
MLP O
layers O
are O
placed O
on O
top O
of O
both O
the O
forward O
and O
backward O
hidden O
states O
Hfand O
Hbof O
a O
biLSTM B-MethodName
network O
encoding O
the O
input O
sentence O
. O

The O
architecture O
for O
bidirectional O
LSTM B-MethodName
with O
Multilayer O
Perceptron O
( O
MLP O
) O
as O
the O
output O
layer O
. O

These O
tags O
apply O
’ O
BIO O
’ O
convention O
( O
Ramshaw O
and O
Marcus O
, O
1999 O
) O
that O
labels O
non O
- O
target O
token O
as O
‘ O
O O
’ O
, O
initial O
token O
of O
the O
interested O
region O
( O
e.g. O
, O
phrase O
) O
as O
‘ O
B- O
’ O
, O
and O
the O
subsequent O
intermediate O
tokens O
of O
interest O
as O
‘ O
I- O
’ O
, O
etc O
. O
The O
MLP O
block O
in O
BiLSTM B-MethodName
- O
MLP O
model O
can O
be O
swapped O
with O
an O
alternative O
conditional O
random O
ﬁeld O
( O
CRF O
) O
( O
Lafferty O
et O
al O
. O
, O
2001 O
) O
that O
better O
considers O
explicit O
sequential O
restrictions O
, O
e.g. O
, O
the O
tag O
‘ O
I O
- O
U O
’ O
has O
to O
be O
placed O
after O
a O
‘ O
B O
- O
U O
’ O
tag O
. O

This O
sequential O
restriction O
can O
be O
captured O
via O
combination O
of O
an O
LSTM B-MethodName
network O
and O
a O
CRF O
network O
. O

The O
source O
sentenceX(i)along O
with O
a O
sequence O
of O
tags O
Y(i)isevaluated O
via O
a O
CRF O
score B-MetricName
: O
S(X;Y O
) O
= O
TX O
t=1(AYt 1;Yt+fYt;Xt O
) O
( O
5 B-MetricValue
) O
The O
CRF O
layer O
uses O
output O
states O
from O
BiLSTM B-MethodName
layer O
to O
ﬁnd O
tags O
in O
sequence O
with O
optimal O
CRF O
score B-MetricName
to O
make O
prediction O
. O

This O
BiLSTM B-MethodName
- O
CRF O
model O
aims O
to O
identify O
the O
exact O
position O
of O
each O
slang O
word O
, O
in O
terms O
of O
sequential O
restrictions O
. O

To O
provide O
feature O
- O
level O
information O
for O
the O
vectors O
fed O
as O
inputs O
to O
the O
BiLSTM B-MethodName
layer O
, O
the O
feature O
vectors O
are O
weighted B-HyperparameterName
in O
terms O
of O
the O
computed O
 O
feat(x)for O
concatenation O
: O
Vx= O
[ O
f(x O
) O
1 O
 O
feat O
1(x):::f(x O
) O
k O
 O
feat O
k(x)](11 O
) O
The O
concatenation O
of O
weighted B-HyperparameterName
feature O
embeddings O
contains O
global O
feature O
- O
level O
information O
, O
allowing O
the O
inputs O
to O
selectively O
feed O
into O
the O
model O
in O
terms O
of O
the O
feature O
distribution O
. O

As O
an O
alternative O
, O
the O
last O
propagated O
hidden O
states O
from O
both O
forward O
and O
backward O
layers O
of O
the O
BiLSTM B-MethodName
can O
be O
concatenated O
with O
the O
raw O
features O
f(x O
) O
ito O
compute O
the O
local O
feature O
weights B-HyperparameterName
. O

Although O
all O
the O
out O
- O
of O
- O
vocabulary O
tokens O
are O
consistently O
labelled O
as O
‘ O
UKT O
’ O
such O
that O
they O
are O
truly O
unseen O
by O
the O
models O
, O
the O
sequential O
relations O
can O
be O
captured O
by O
the O
hidden B-HyperparameterName
layers I-HyperparameterName
of O
LSTMs B-MethodName
. O

The O
architecture O
for O
bidirectional O
LSTM B-MethodName
with O
Conditional O
Random O
Field O
as O
the O
output O
layer O
. O

The O
shared O
CRF O
layer O
takes O
as O
input O
the O
BiLSTM B-MethodName
’s O
outputs O
from O
hidden O
states O
, O
then O
ﬁnds O
the O
optimal O
path O
in O
terms O
of O
sum O
of O
emission O
scores B-MetricName
and O
transition O
scores B-MetricName
. O

Figure O
4 O
summarizes O
the O
degradation O
ofModel O
( O
features O
) O
Precision B-MetricName
Recall B-MetricName
F1 B-MetricName
- O
score B-MetricName
Random O
Guess O
( O
baseline O
) O
0.5000 B-MetricValue
0.5000 O
0.5000 O
BiLSTM B-MethodName
- O
MLP O
( O
POS+POSp B-TaskName
) O
0.9893 O
0.4806 O
0.6469 O
BiLSTM B-MethodName
- O
MLP O
( O
POS+POSp+POSt+PMI B-TaskName
) O
0.9777 O
0.5651 O
0.7162 O
BiLSTM B-MethodName
- O
MLP O
( O
POS+POSp+POSt+PMI B-TaskName
boosting O
) O
0.9771 O
0.6053 O
0.7475 O
BiLSTM B-MethodName
- O
MLP O
( O
full O
features O
) O
0.9433 O
0.6842 O
0.7931 O
BiLSTM B-MethodName
- O
CRF O
( O
POS+POSp B-TaskName
) O
0.9873 O
0.5969 O
0.7440 O
BiLSTM B-MethodName
- O
CRF O
( O
POS+POSp+POSt+PMI B-TaskName
) O
0.9749 O
0.6482 O
0.7787 O
BiLSTM B-MethodName
- O
CRF O
( O
POS+POSp+POSt+PMI B-TaskName
boosting O
) O
0.9749 O
0.6496 O
0.7797 O
BiLSTM B-MethodName
- O
CRF O
( O
full O
features O
) O
0.9518 O
0.6856 O
0.7971 O
Table O
1 O
: O
Model O
comparisons O
in O
the O
slang O
detection O
task O
. O

Model O
( O
features O
) O
Precision B-MetricName
Recall B-MetricName
F1 B-MetricName
- O
score B-MetricName
Random O
Guess O
( O
baseline O
) O
0.0263 B-MetricValue
0.4834 O
0.0498 O
BiLSTM B-MethodName
- O
MLP O
( O
POS+POSp B-TaskName
) O
0.6240 O
0.3172 O
0.4206 O
BiLSTM B-MethodName
- O
MLP O
( O
POS+POSp+POSt+PMI B-TaskName
) O
0.6172 O
0.3864 O
0.4753 O
BiLSTM B-MethodName
- O
MLP O
( O
POS+POSp+POSt+PMI B-TaskName
boosting O
) O
0.5967 O
0.3975 O
0.4771 O
BiLSTM B-MethodName
- O
MLP O
( O
full O
features O
) O
0.5423 O
0.4612 O
0.4985 O
BiLSTM B-MethodName
- O
CRF O
( O
POS+POSp B-TaskName
) O
0.5666 O
0.3712 O
0.4485 O
BiLSTM B-MethodName
- O
CRF O
( O
POS+POSp+POSt+PMI B-TaskName
) O
0.5763 O
0.4183 O
0.4847 O
BiLSTM B-MethodName
- O
CRF O
( O
POS+POSp+POSt+PMI B-TaskName
boosting O
) O
0.5954 O
0.4280 O
0.4980 O
BiLSTM B-MethodName
- O
CRF O
( O
full O
features O
) O
0.5499 O
0.4501 O
0.4950 O
Table O
2 O
: O
Model O
comparisons O
in O
the O
slang O
identiﬁcation O
task O
. O

Model O
( O
features O
) O
New O
Word O
New O
Sense O
BiLSTM B-MethodName
- O
MLP O
( O
POS+POSp B-TaskName
) O
194/523 O
35/199 O
BiLSTM B-MethodName
- O
MLP O
( O
POS+POSp+POSt+PMI B-TaskName
) O
228/523 O
53/199 O
BiLSTM B-MethodName
- O
MLP O
( O
POS+POSp+POSt+PMI B-TaskName
boosting O
) O
236/523 O
53/199 O
BiLSTM B-MethodName
- O
MLP O
( O
full O
features O
) O
267/523 O
66/199 O
BiLSTM B-MethodName
- O
CRF O
( O
POS+POSp B-TaskName
) O
227/523 O
41/199 O
BiLSTM B-MethodName
- O
CRF O
( O
POS+POSp+POSt+PMI B-TaskName
) O
251/523 O
50/199 O
BiLSTM B-MethodName
- O
CRF O
( O
POS+POSp+POSt+PMI B-TaskName
boosting O
) O
260/523 O
50/199 O
BiLSTM B-MethodName
- O
CRF O
( O
full O
features O
) O
242/523 O
83/199 O
Table O
3 O
: O
Model O
comparisons O
on O
identiﬁed O
slang O
by O
type O
( O
either O
as O
new O
word O
or O
existing O
word O
with O
new O
sense O
) O
. O

Model O
Identiﬁcation O
F1 B-MetricName
- O
score B-MetricName
Detection O
F1 B-MetricName
- O
score B-MetricName
BiLSTM B-MethodName
- O
MLP O
0.5101 B-MetricValue
0.8649 O
BiLSTM B-MethodName
- O
CRF O
0.5024 O
0.8679 O
BiLSTM B-MethodName
- O
MLP O
with O
Char O
- O
CNN O
0.5172 O
0.8693 O
BiLSTM B-MethodName
- O
CRF O
with O
Char O
- O
CNN O
0.5146 O
0.8679 O
Table O
4 O
: O
Comparisons O
of O
model O
performance O
with O
and O
without O
character O
- O
based O
embedding O
. O

We O
ﬁnd O
that O
the O
bidirectional O
LSTM B-MethodName
with O
feature O
- O
based O
inputs O
and O
character O
- O
based O
convolutional O
embeddings O
using O
multilayer O
perceptron O
yield O
the O
best O
performance O
in O
position O
identiﬁcation O
, O
and O
the O
model O
with O
similar O
mechanisms O
except O
with O
conditional O
random O
ﬁeld O
has O
better O
performance O
in O
detecting O
whether O
a O
source O
sentence O
contains O
a O
slang O
term O
. O

In O
this O
work O
, O
we O
propose O
a O
model O
integrating O
both O
perception- O
and O
production O
- O
based O
learning O
using O
artiﬁcial O
neural B-MethodName
networks I-MethodName
which O
we O
train O
on O
a O
large O
corpus O
of O
crowd O
- O
sourced O
images O
with O
corresponding O
descriptions O
. O

Conditioned O
on O
this O
image O
encoding O
, O
an O
autoregressive O
language O
model O
learns O
to O
produce O
utterances O
word O
by O
word O
: O
The O
words O
of O
a O
sentence O
are O
passed O
through O
a O
linear O
word O
embedding O
layer O
and O
then O
fed O
, O
together O
with O
the O
encoded O
image O
features6 O
, O
into O
a O
one O
- O
layer O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

6While O
Vinyals O
et O
al O
. O
( O
2015 O
) O
fed O
the O
image O
features O
only O
at O
the O
ﬁrst O
timestep O
into O
the O
LSTM B-MethodName
, O
here O
we O
feed O
it O
at O
every O
timestep O
as O
this O
showed O
to O
improve O
performance O
on O
our O
evaluation O
substantially O
. O

( O
Song O
et O
al O
. O
, O
2018 O
) O
( O
in O
an O
approach O
referred O
to O
as O
NQG O
LChereafter O
) O
encoded O
groundtruth O
answers O
and O
employed O
bi O
- O
directional O
LSTMs B-MethodName
in O
a O
Seq2Seq O
setting O
. O

Our O
encoder O
is O
a O
two O
- O
layer O
bidirectional O
LSTM B-MethodName
network O
, O
consisting O
of  O
! O
ht=     !LSTM B-MethodName
2(xt;  !ht 1)and O
 ht= O
     LSTM B-MethodName
2(xt O
; O
  ht 1 O
) O
, O
which O
generates O
a O
sequence O
of O
hidden O
states O
. O

Question O
Decoder O
: O
Our O
question O
decoder O
is O
a O
single O
- O
layer O
LSTM B-MethodName
network O
, O
initialized O
with O
the O
states= O
[ O
  O
! O
ht O
; O
 ht O
] O
, O
which O
is O
concatenation O
of O
hidden O
state O
from O
forward O
and O
backward O
passes O
. O

In O
our O
1https://github.com/huggingface/ O
pytorch O
- O
pretrained O
- O
BERTexperiments B-MethodName
we O
considered O
BLEU B-MetricName
for O
up O
to O
4grams O
. O

Deep O
neural B-MethodName
networks I-MethodName
have O
recently O
achieved O
remarkable O
results O
in O
many O
areas O
of O
machine O
learning O
. O

With O
a O
practical O
method O
for O
precomputing O
word O
embeddings O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
routine O
utilization O
of O
recurrent O
neural B-MethodName
networks I-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Cho O
et O
al O
. O
, O
2014 O
) O
, O
deep O
neural B-MethodName
networks I-MethodName
achieved O
state O
- O
of O
- O
the O
- O
art O
results O
in O
many O
NLP O
areas O
like O
POS B-TaskName
tagging O
( O
Ling O
et O
al O
. O
, O
2015 O
) O
, O
named O
entity O
recognition O
( O
Yang O
et O
al O
. O
, O
2016 O
) O
or O
machine O
translation O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

Many O
other O
parser O
models O
followed O
, O
employing O
various O
techniques O
like O
stack O
LSTM B-MethodName
( O
Dyer O
et O
al O
. O
, O
2015 O
) O
, O
global O
normalization O
( O
Andor O
et O
al O
. O
, O
2016 O
) O
, O
biafﬁne O
attention O
( O
Dozat O
and O
Manning O
, O
2016 O
) O
or O
recurrent O
neural O
network O
grammars O
( O
Kuncoro O
et O
al O
. O
, O
2016 O
) O
, O
improving O
LAS O
score B-MetricName
in O
English O
and O
Chinese O
dependency O
parsing O
by O
more O
than O
2 B-MetricValue
points O
in O
2016 O
. O

In O
our O
approach O
, O
we O
extract O
scene O
graphs O
from O
the O
images O
and O
then O
learn O
scene O
graph O
embeddingsusing O
graph O
neural B-MethodName
networks I-MethodName
( O
Marcheggiani O
and O
Perez O
- O
Beltrachini O
, O
2018 O
) O
for O
each O
image O
, O
which O
combine O
the O
visual O
features O
and O
the O
discrete O
semantic O
information O
from O
the O
scene O
graphs O
. O

We O
use O
a O
two O
- O
layer O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
to O
merge O
the O
node O
representations O
and O
generate O
the O
sequence O
of O
object O
labels O
. O

We O
tune O
three O
hyper O
- O
parameters O
on O
a O
validation O
set O
to O
minimise O
the O
loss O
, O
namely O
the O
number O
of O
hidden B-HyperparameterName
units I-HyperparameterName
in O
mRGCN O
encoder O
, O
the O
number O
of O
hidden B-HyperparameterName
units I-HyperparameterName
in O
LSTM B-MethodName
, O
and O
the O
number O
of O
GCN O
layers O
. O

We O
experimented O
with O
Lin O
’s O
similarity O
on O
WordNet B-DatasetName
synsets O
( O
Lin O
, O
1998 O
) O
and O
cosine O
similarity O
using O
GloVe B-MethodName
and O
BERT B-MethodName
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

We O
obtained O
the O
highest O
precision B-MetricName
using O
GloVe B-MethodName
embeddings O
, O
with O
a O
threshold B-MetricName
of O
0.85 B-MetricValue
( O
precision=0.82 B-MetricName
, O
recall=0.11 B-MetricName
) O
. O

1 O
Introduction O
Pretrained O
language O
models O
( O
PLMs O
) O
like O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
have O
emerged O
as O
universal O
tools O
that O
capture O
a O
diverse O
range O
of O
linguistic O
and O
– O
as O
more O
and O
more O
evidence O
suggests O
– O
factual O
knowledge O
( O
Petroni O
et O
al O
. O
, O
2019 O
; O
Radford O
et O
al O
. O
, O
2019 O
) O
. O

*equal O
contributionWe O
conduct O
our O
study O
by O
pretraining O
BERT B-MethodName
from O
scratch O
on O
synthetic O
corpora O
. O

To O
test O
whether O
BERT B-MethodName
has O
learned O
a O
fact O
, O
we O
mask O
the O
object O
, O
thereby O
generating O
a O
cloze O
- O
style O
query O
, O
and O
then O
evaluate O
predictions O
. O

We O
test O
BERT B-MethodName
’s O
ability O
to O
use O
the O
rule O
to O
infer O
unseen O
facts O
by O
holding O
out O
some O
facts O
in O
a O
test O
set O
. O

For O
example O
, O
for O
composition O
, O
BERT B-MethodName
should O
infer O
, O
after O
having O
seen O
that O
leopards O
are O
faster O
than O
sheep O
and O
sheep O
are O
faster O
than O
snails O
, O
that O
leopards O
are O
faster O
than O
snails O
. O

Talmor O
et O
al O
. O
( O
2019 O
) O
also O
investigate O
symbolic O
reasoning O
in O
BERT B-MethodName
using O
cloze O
- O
style O
queries O
. O

However O
, O
in O
their O
setup O
, O
there O
are O
two O
possible O
reasons O
for O
BERT B-MethodName
having O
answered O
a O
cloze O
- O
style O
query O
correctly O
: O
( O
i O
) O
the O
underlying O
fact O
was O
correctly O
inferred O
or O
( O
ii O
) O
it O
was O
seen O
during O
training O
. O

In O
contrast O
, O
since O
we O
pretrain O
BERT B-MethodName
from O
scratch O
, O
we O
have O
full O
control O
over O
the O
training O
setup O
and O
can O
distinguish O
cases O
( O
i O
) O
and O
( O
ii O
) O
. O

We O
ﬁnd O
that O
i O
) O
BERT B-MethodName
is O
capable O
of O
learning O
some O
one O
- O
hop O
rules O
( O
equivalence O
and O
implication O
) O
. O

iii O
) O
BERT B-MethodName
struggles O
with O
two O
- O
hop O
rules O
( O
composition O
) O
. O

Given O
that O
BERT B-MethodName
can O
in O
principle O
learn O
some O
reasoning O
rules O
, O
the O
question O
arises O
whether O
it O
does O
so O
for O
standard O
training O
corpora O
. O

We O
ﬁnd O
that O
BERTlarge B-MethodName
has O
only O
partially O
learned O
the O
types O
of O
rules O
we O
investigate O
here O
. O

For O
example O
, O
BERT B-MethodName
has O
some O
notion O
of O
“ O
X O
shares O
borders O
with O
Y O
” O
being O
symmetric O
, O
but O
it O
fails O
to O
understand O
rules O
like O
symmetry O
in O
other O
cases O
. O

During O
the O
course O
of O
pretraining O
, O
BERT B-MethodName
sees O
more O
data O
than O
any O
human O
could O
read O
in O
a O
lifetime O
, O
an O
amount O
of O
knowledge O
that O
surpasses O
its O
storage O
capacity O
. O

We O
simulate O
this O
with O
a O
scaled O
- O
down O
version O
of O
BERT B-MethodName
and O
a O
training O
set O
that O
ensures O
that O
BERT B-MethodName
can O
not O
memorize O
all O
facts O
in O
training O
. O

This O
experimental O
setup O
allows O
us O
to O
test O
to O
what O
extent O
BERT B-MethodName
learns O
the O
six O
rules O
, O
i.e. O
, O
to O
what O
extent O
the O
facts O
in O
the O
test O
set O
are O
correctly O
inferred O
from O
their O
premises O
in O
the O
training O
set O
. O

3 O
BERT B-MethodName
Model O
BERT B-MethodName
uses O
a O
deep O
bidirectional O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
encoder O
to O
perform O
masked O
language O
modeling O
. O

During O
pretraining O
, O
BERT B-MethodName
randomly O
masks O
positions O
and O
learns O
to O
predict O
ﬁllers O
. O

A O
reduction O
to O
125 O
attributes O
enables O
BERT B-MethodName
to O
successfully O
apply O
antonym O
negation O
to O
the O
test O
set O
. O

For O
symbolic O
rules O
, O
we O
start O
with O
BERT B-MethodName
- O
base O
and O
tune O
hyperparameters O
. O

Due O
to O
a O
limited O
compute O
infrastructure O
, O
we O
scale O
down O
BERT B-MethodName
to O
a O
single O
hidden B-HyperparameterName
layer I-HyperparameterName
with O
3 B-HyperparameterValue
attention O
heads O
, O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
192 B-HyperparameterValue
and O
an O
intermediate O
size O
of O
768 O
. O

BERT B-MethodName
has O
high O
test O
set O
precision B-MetricName
for O
EQUI O
, O
SYM O
, O
INV O
and O
IMP O
. O

Table O
2 O
seems O
to O
suggest O
that O
BERT B-MethodName
is O
able O
to O
learn O
one O
- O
hop O
rules O
and O
it O
can O
successively O
apply O
these O
rules O
in O
a O
natural O
setting O
in O
which O
the O
premise O
is O
not O
directly O
available O
. O

4.1.1 O
Analysis O
of O
SYM O
and O
INV O
Table O
2 O
seems O
to O
indicate O
that O
BERT B-MethodName
can O
learn O
that O
a O
relationris O
symmetric O
( O
SYM O
) O
and O
that O
sandtare O
inverses O
( O
INV O
) O
– O
the O
evidence O
is O
that O
it O
generates O
facts O
based O
on O
the O
successfully O
acquired O
symmetry O
and O
inversion O
properties O
of O
the O
relations O
r O
, O
sandt O
. O

We O
now O
show O
that O
while O
BERT B-MethodName
acquires O
SYM O
and O
INV O
partially O
, O
it O
also O
severely O
overgenerates O
. O

Our O
ﬁrst O
observation O
is O
that O
in O
the O
SYM O
experiment O
, O
BERT B-MethodName
understands O
all O
relations O
to O
be O
symmetric O
. O

If O
we O
take O
a O
fact O
with O
a O
random O
relationr O
, O
say O
( O
e;r;f O
) O
, O
and O
prompt O
BERT B-MethodName
with O
“ O
( O
f;r;[MASK O
] O
) O
” O
, O
theneis O
predicted O
in O
close O
to O
100 O
% O
of O
cases O
. O

So O
BERT B-MethodName
has O
simply O
learned O
that O
any O
relation O
is O
symmetric O
as O
opposed O
to O
distinguishing O
between O
symmetric O
and O
non O
- O
symmetric O
relations O
. O

This O
analysis O
brings O
to O
light O
that O
our O
setup O
is O
unfair O
to O
BERT B-MethodName
: O
it O
never O
sees O
evidence O
for O
nonsymmetry O
. O

ANTI O
training O
data O
indicate O
to O
BERT B-MethodName
thatr2Ris O
not O
symmetric O
since O
many O
instances O
of O
rfacts O
are O
seen O
, O
with O
speciﬁc O
entities O
( O
fin O
the O
example O
) O
occurring O
in O
both O
slots O
, O
but O
there O
is O
never O
a O
symmetric O
example O
. O

Table O
2 O
( O
ANTI O
) O
shows O
that O
BERT B-MethodName
memorizes O
ANTI O
facts O
seen O
during O
training O
but O
on O
test O
, O
BERT B-MethodName
only O
recognizes O
14.85 O
% O
of O
ANTI O
facts O
as O
nonsymmetric O
. O

So O
it O
is O
easy O
for O
BERT B-MethodName
to O
learn O
the O
concept O
of O
symmetry O
, O
but O
it O
is O
hard O
to O
teach O
it O
to O
distinguish O
between O
symmetric O
and O
non O
- O
symmetric O
relations O
. O

BERT B-MethodName
successfully O
predicts O
correct O
facts O
once O
it O
has O
learned O
thatsandtare O
inverses O
– O
but O
it O
overgeneralizes O
by O
also O
predicting O
many O
incorrect O
facts O
; O
e.g. O
, O
for O
( O
e;s;f O
) O
in O
train O
, O
it O
may O
predict O
( O
f;t;e O
) O
( O
correct O
) O
, O
but O
also O
( O
e;t;f O
) O
and(f;s;e O
) O
( O
incorrect O
) O
. O

In O
summary O
, O
we O
have O
found O
that O
SYM O
and O
INV O
are O
learned O
in O
the O
sense O
that O
BERT B-MethodName
generates O
correct O
facts O
for O
symmetric O
and O
inverse O
relations O
. O

BERT B-MethodName
must O
learn O
antonym O
negation O
from O
a O
large O
number O
of O
possible O
combinations O
. O

By O
reducing O
the O
number O
of O
possible O
combinations O
( O
decreasing O
the O
number O
of O
attributes O
from O
1000 O
to O
500 O
, O
250 O
and O
125 O
) O
BERT B-MethodName
’s O
test O
set O
precision B-MetricName
increases O
, O
see O
Figure O
3 B-MetricValue
( O
A O
) O
. O

We O
investigate O
BERT B-MethodName
’s O
behavior O
concerning O
negation O
further O
by O
adding O
an O
additional O
attribute O
setA O
, O
withA\A=;andjAj O
= O
jAjto O
the O
vocabulary O
. O
Adoes O
not O
follow O
an O
antonym O
schema O
. O

We O
see O
that O
BERT B-MethodName
is O
prone O
to O
predict O
both O
( O
e O
, O
r O
, O
b O
) O
and O
( O
e O
, O
notr O
, O
b O
) O
for O
b2A(for O
38 O
% O
) O
. O

This O
is O
in O
agreement O
with O
prior O
work O
( O
Ettinger O
, O
2020 O
; O
Kassner O
and O
Sch O
¨utze O
, O
2020 O
) O
showing O
that O
BERT B-MethodName
trained O
on O
natural O
language O
corpora O
is O
as O
likely O
to O
generate O
a O
true O
statement O
like O
“ O
birds O
can O
ﬂy O
” O
as O
a O
factually O
false O
negated O
statement O
like O
“ O
birds O
can O
not O
ﬂy O
” O
. O

4.1.3 O
Analysis O
of O
COMP O
Why O
does O
BERT B-MethodName
not O
learn O
COMP O
? O

Figure O
3 O
( O
B O
) O
shows O
that O
BERT B-MethodName
can O
learn O
COMP O
moderately O
well O
from O
this O
schema O
- O
enhanced O
corpus O
( O
blue O
curve O
): O
precision B-MetricName
is O
clearly O
above O
50 B-MetricValue
% O
and O
peaks O
at O
76 O
% O
. O

The O
takeaway O
from O
this O
experiment O
is O
that O
twohop O
rules O
pose O
a O
challenge O
to O
BERT B-MethodName
, O
but O
that O
they O
are O
learnable O
if O
entities O
and O
relations O
are O
embedded O
in O
a O
rich O
semantic O
structure O
. O

4.2 O
Natural O
Language O
Corpora O
In O
this O
section O
, O
we O
investigate O
to O
what O
extent O
the O
PLMs O
BERT B-MethodName
and O
RoBERTa B-MethodName
have O
learned O
SYM O
and O
INV O
from O
natural O
language O
corpora O
. O

For O
each O
of O
the O
ﬁve O
relations O
, O
we O
run O
both O
BERT B-MethodName
- O
large O
- O
cased O
and O
RoBERTa B-MethodName
- O
large O
and O
report O
the O
more O
consistent O
result.relation O
rule O
completions O
examples O
cons O
. O

shares O
borders O
with O
SYM O
152 O
152 O
2(ecuador O
, O
peru O
) O
( O
togo O
, O
ghana O
) O
, O
( O
ghana O
, O
nigeria O
) O
is O
the O
opposite O
of O
SYM O
179 O
170 O
71(demand O
, O
supply O
) O
( O
injustice O
, O
justice O
) O
, O
( O
justice O
, O
truth O
) O
is O
the O
capital O
of O
( O
C O
- O
of O
) O
’s O
capital O
is O
( O
s O
- O
C O
- O
is O
) O
INV O
59 O
59 O
1(indonesia O
, O
s O
- O
C O
- O
is O
, O
jakarta O
) O
( O
canada O
, O
s O
- O
C O
- O
is O
, O
ottawa O
) O
, O
( O
ottawa O
, O
C O
- O
of O
, O
ontario O
) O
is O
smaller O
/ O
larger O
than O
( O
countries O
) O
INV O
54 O
23 O
99(russia O
, O
larger O
, O
canada O
) O
, O
( O
canada O
, O
smaller O
, O
russia O
) O
( O
brazil O
, O
smaller O
, O
russia O
) O
, O
( O
russia O
, O
smaller O
, O
brazil O
) O
is O
smaller O
/ O
larger O
than O
( O
planets O
) O
INV O
9 O
9 O
36(jupiter O
, O
larger O
, O
mercury O
) O
, O
( O
mercury O
, O
smaller O
, O
jupiter O
) O
( O
sun O
, O
bigger O
, O
earth O
) O
, O
( O
earth O
, O
bigger O
, O
sun O
) O
Table O
3 O
: O
Can O
PLMs O
( O
BERT B-MethodName
and O
RoBERTa B-MethodName
) O
learn O
SYM O
and O
INV O
from O
natural O
language O
corpora O
? O

( O
B O
) O
shows O
that O
BERT B-MethodName
memorizes O
schema O
conformant O
facts O
perfectly O
( O
“ O
group O
attributes O
” O
) O
. O

Schema O
conformant O
facts O
and O
exceptions O
compete O
for O
memory O
if O
memory O
capacity O
is O
limited O
– O
depending O
on O
frequency O
one O
or O
the O
other O
is O
preferentially O
learned O
by O
BERT B-MethodName
. O

( O
B O
) O
BERT B-MethodName
memorizes O
schema O
conformant O
facts O
perfectly O
( O
“ O
group O
attributes O
” O
) O
. O

iii O
) O
Results O
are O
based O
on O
BERT B-MethodName
- O
base O
and O
scaleddown O
versions O
of O
BERT B-MethodName
- O
base O
only O
, O
just O
as O
training O
corpora O
are O
orders O
of O
magnitude O
smaller O
than O
natural O
training O
corpora O
. O

Clark O
et O
al O
. O
( O
2020 O
) O
test O
ﬁnetuned O
BERT B-MethodName
’s O
reasoning O
capabilities O
, O
but O
they O
always O
make O
premise O
and O
conclusion O
locally O
available O
to O
the O
model O
, O
during O
training O
and O
inference O
. O

They O
show O
that O
BERT B-MethodName
performs O
poorly O
on O
these O
new O
datasets O
, O
but O
can O
be O
quickly O
ﬁnetuned O
to O
good O
performance O
. O

Based O
on O
perceptrons O
and O
convolutional O
neural B-MethodName
networks I-MethodName
, O
Arpit O
et O
al O
. O
( O
2017 O
) O
; O
Zhang O
et O
al O
. O
( O
2017 O
) O
study O
the O
relation O
of O
generalizing O
from O
real O
structured O
data O
vs. O
memorizing O
random O
noise O
in O
the O
image O
domain O
, O
similar O
to O
our O
study O
of O
schemaconformant O
facts O
and O
outliers O
. O

They O
do O
not O
study O
transformer O
based O
models O
trained O
on O
natural O
language.7 O
Conclusion O
We O
studied O
BERT B-MethodName
’s O
ability O
to O
capture O
knowledge O
from O
its O
training O
corpus O
by O
investigating O
its O
reasoning O
and O
memorization O
capabilities O
. O

We O
saw O
that O
, O
to O
some O
extent O
, O
BERT B-MethodName
is O
able O
to O
infer O
facts O
not O
explicitly O
seen O
during O
training O
via O
symbolic O
rules O
. O

On O
the O
other O
hand O
, O
languagemodels O
based O
on O
recurrent O
neural B-MethodName
networks I-MethodName
( O
Elman O
, O
1990 O
) O
or O
Transformers O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
are O
well O
- O
suited O
for O
dealing O
with O
word O
sequences O
, O
but O
usually O
perform O
worse O
than O
static O
word O
embedding O
models O
on O
word O
- O
level O
tasks O
( O
Huebner O
and O
Willits O
, O
2018 O
) O
and O
are O
challenging O
to O
extract O
word O
embeddings O
from O
( O
Vuli O
´ O
c O
et O
al O
. O
, O
2020 O
) O
. O

In O
this O
paper O
, O
we O
diagnose O
why O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
learn O
sub O
- O
optimal O
representations O
of O
individual O
words O
. O

In O
contrast O
to O
the O
previous O
experiment O
, O
we O
also O
pre O
- O
trained O
the O
RNNs O
before O
2We O
replicated O
these O
results O
using O
an O
LSTM B-MethodName
as O
the O
algorithm O
underlying O
our O
language O
model O
, O
and O
observed O
similar O
results O
in O
all O
conditions O
, O
evidence O
that O
our O
results O
are O
not O
due O
to O
the O
idiosyncrasies O
of O
the O
vanilla O
RNN O
architecture O
, O
but O
a O
property O
of O
RNNs O
in O
general.exposing O
them O
to O
full O
- O
redundancy O
input.3 O
. O

In O
this O
paper O
, O
we O
develop O
a O
neural O
network O
based O
approach O
for O
classifying O
frames O
in O
news O
article O
headlines O
by O
ﬁne O
- O
tuning O
a O
state O
- O
of O
- O
the O
- O
art O
language O
representation O
model O
( O
BERT B-MethodName
: O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
) O
for O
the O
task O
of O
frame O
detection O
. O

Our O
contributions O
are O
twofold O
: O
Firstly O
, O
we O
have O
developed O
a O
state O
- O
of O
- O
theart O
news O
frame O
detection O
approach O
by O
ﬁne O
- O
tuning O
BERT B-MethodName
language O
model O
to O
perform O
the O
multiclass O
( O
frame O
) O
classiﬁcation O
on O
news O
article O
headlines O
. O

Naderi O
and O
Hirst O
( O
2017 O
) O
detect O
news O
frames O
at O
the O
sentence O
- O
level O
using O
deep O
recurrent O
neural B-MethodName
networks I-MethodName
, O
speciﬁcally O
LSTM B-MethodName
, O
BiLSTM B-MethodName
, O
and O
GRU O
. O

We O
also O
implement O
LSTM B-MethodName
- O
based O
neural B-MethodName
networks I-MethodName
for O
a O
more O
comprehensive O
evaluation O
. O

Long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
is O
a O
recurrent O
neural O
network O
( O
RNN O
) O
architecture O
that O
is O
widely O
used O
today O
in O
text O
classiﬁcation O
tasks O
. O

There O
are O
plenty O
of O
variants O
from O
this O
type O
of O
architecture O
: O
Gated O
Recurrent O
Unit O
( O
GRU O
) O
, O
Bi O
- O
directional O
LSTM B-MethodName
, O
and O
Bidirectional O
GRU O
. O

4.1 O
News O
Frame O
Detection O
with O
BERT B-MethodName
Bidirectional O
Encoder O
Representations O
from O
Transformers O
( O
BERT B-MethodName
) O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
take O
this O
idea O
of O
attention O
and O
bi O
- O
directionality O
furtherby O
building O
on O
the O
Transformer O
’s O
encoder O
model O
that O
solely O
relies O
on O
multi O
- O
layer O
self O
- O
attention O
to O
compute O
contextual O
representations O
of O
its O
input O
, O
dispensing O
with O
any O
kinds O
of O
recurrence O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

BERT B-MethodName
’s O
encoder O
implements O
the O
Transformer O
’s O
multi O
- O
layer O
self O
- O
attention O
mechanisms O
and O
fully O
utilizes O
its O
strength O
in O
storing O
the O
left O
and O
right O
context O
of O
each O
token O
by O
using O
a O
“ O
masked O
language O
model O
” O
( O
MLM O
) O
pre O
- O
training O
objective O
, O
inspired O
by O
the O
Cloze O
task O
( O
Taylor O
, O
1953 O
) O
. O

In O
its O
pre O
- O
training O
, O
BERT B-MethodName
randomly O
masks O
some O
of O
the O
tokens O
from O
its O
input O
, O
and O
predicts O
the O
original O
vocabulary O
i O
d O
of O
the O
masked O
word O
based O
only O
on O
its O
context O
. O

Unlike O
left O
- O
to O
- O
right O
language O
model O
pre O
- O
training O
, O
the O
MLM O
objective O
enables O
the O
representation O
to O
fuse O
the O
left O
and O
the O
right O
context O
, O
which O
allows O
BERT B-MethodName
to O
pre O
- O
train O
a O
deep O
bidirectional O
Transformer O
representations O
from O
unlabeled O
large O
text O
corpora O
. O

We O
ﬁne O
- O
tune O
the O
pre O
- O
trained O
BERT B-MethodName
- O
based O
uncased O
model O
on O
our O
multiclass O
frame O
classiﬁcation O
by O
adding O
a O
frame O
classiﬁcation O
layer O
on O
top O
of O
the O
model O
and O
ﬁne O
- O
tune O
all O
the O
parameters O
end O
- O
to O
- O
end O
. O

Given O
a O
headline O
, O
BERT B-MethodName
tokenizes O
the O
headline O
to O
tokens O
based O
on O
WordPiece O
tokens O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
and O
appends O
a O
special O
classiﬁcation O
tokenFrame O
Class O
# O
Headlines O
Baseline O
LSTM B-MethodName
Bi O
- O
LSTM B-MethodName
Bi O
- O
LSTM B-MethodName
Bi O
- O
GRU O
BERT B-MethodName
w/ O
Attention O
w/ O
Attention O
2nd O
Amendment O
38 O
44.74 O
23.68 O
21.05 O
44.74 O
26.32 O
65.79 O
Gun O
control O
/ O
regulation O
215 O
50.23 O
63.72 O
66.51 O
72.09 O
76.28 O
84.19 O
Politics O
373 O
40.48 O
78.28 O
77.75 O
84.18 O
85.79 O
89.54 O
Mental O
health O
65 O
35.38 O
50.77 O
40.00 O
58.46 O
60.00 O
78.46 O
School O
/ O
Public O
space O
safety O
137 O
39.42 O
48.91 O
50.36 O
54.74 O
58.39 O
78.10 O
Race O
/ O
Ethnicity O
114 O
67.54 O
75.44 O
71.93 O
84.21 O
81.28 O
92.11 O
Public O
opinion O
237 O
63.29 O
70.46 O
72.15 O
75.53 O
77.22 O
86.08 O
Society O
/ O
Culture O
41 O
43.90 O
24.39 O
19.51 O
36.59 O
21.95 O
58.54 O
Economic O
consequences O
80 O
38.75 O
45.00 O
51.25 O
61.25 O
60.00 O
80.00 O
Overall O
1300 O
48.38 O
64.37 O
64.48 O
72.15 O
72.76 O
84.23 O
Table O
1 O
: O
Class O
distribution O
of O
frame O
A O
annotations O
and O
micro O
- O
accuracies B-MetricName
for O
the O
baseline O
( O
Field O
et O
al O
. O
, O
2018 B-MetricValue
) O
, O
LSTM B-MethodName
, O
bidirectional O
LSTM B-MethodName
, O
bi O
- O
directional O
LSTM B-MethodName
and O
bi O
- O
directional O
GRU O
with O
attention O
, O
and O
our O
method O
based O
on O
ﬁne O
tuning O
BERT B-MethodName
. O

Our O
method O
based O
on O
BERT B-MethodName
signiﬁcantly O
outperforms O
not O
only O
the O
most O
recent O
news O
frame O
classiﬁcation O
baseline O
, O
but O
also O
some O
stateof O
- O
the O
- O
art O
deep O
classiﬁcation O
models O
, O
including O
bidirectional O
LSTM B-MethodName
/ O
GRU O
with O
attention O
on O
every O
frame O
of O
our O
GVFC O
dataset O
( O
Table O
1 O
) O
. O

AsMFC O
Issue O
# O
Head- O
Bi O
- O
GRU O
w/ O
BERT B-MethodName
lines O
Attention O
Immigration O
( O
I O
) O
7231 O
40.84 O
52.38 O
Tobacco O
( O
T O
) O
3959 O
57.20 O
67.94 O
Samesex O
( O
SS O
) O
3842 O
61.57 O
71.50 O
I O
( O
top-5 O
frames O
) O
4175 O
53.65 O
67.28 O
T O
( O
top-5 O
frames O
) O
2759 O
71.44 O
82.32 O
SS O
( O
top-5 O
frames O
) O
2937 O
74.94 O
83.07 O
Table O
2 O
: O
10 O
- O
fold O
cross O
- O
validation O
micro O
- O
accuracy B-MetricName
on O
the O
MFC O
dataset O
for O
our O
best O
baseline O
from O
previous O
evaluation O
, O
and O
our O
model O
based O
on O
BERT B-MethodName
. O

4.2 O
Discussion O
Our O
results O
show O
that O
ﬁne O
- O
tuning O
on O
BERT B-MethodName
performs O
well O
even O
on O
a O
small O
dataset O
like O
GVFC O
, O
which O
agrees O
with O
the O
ﬁndings O
of O
Devlin O
et O
al O
. O
( O
2018 O
) O
that O
ﬁne O
- O
tuning O
on O
BERT B-MethodName
’s O
pre O
- O
trained O
model O
can O
lead O
to O
large O
improvements O
even O
on O
very O
small O
scale O
tasks O
. O

Part O
of O
the O
reasons O
may O
be O
due O
to O
BERT B-MethodName
’s O
deep O
attention O
mechanism O
. O

Even O
adding O
standard O
attention O
improves O
the O
accuracy B-MetricName
of O
our O
LSTM B-MethodName
- O
based O
baselines O
signiﬁcantly O
( O
Table O
1 B-MetricValue
) O
. O

BERT B-MethodName
’s O
success O
can O
also O
be O
traced O
to O
its O
design O
of O
bidirectional O
Transformer O
that O
offers O
richer O
contextual O
information O
. O

Furthermore O
, O
BERT B-MethodName
was O
pre O
- O
trained O
on O
a O
large O
corpus O
to O
produce O
this O
representation O
. O

Since O
BERT B-MethodName
is O
pretrained O
to O
take O
context O
into O
consideration O
, O
words O
like O
“ O
gun O
” O
, O
which O
appears O
with O
all O
the O
frames O
, O
can O
have O
different O
contextual O
representation O
depend O
- O
ing O
on O
its O
context O
i.e. O
, O
“ O
gun O
lobby O
” O
vs. O
“ O
gun O
permit O
” O
. O

For O
example O
, O
the O
headline O
“ O
That O
’s O
it O
– O
no O
more O
guns O
” O
is O
classiﬁed O
correctly O
by O
BERT B-MethodName
as O
having O
“ O
Gun O
control O
/ O
regulation O
” O
frame O
by O
attending O
to O
the O
context O
“ O
no O
more O
” O
of O
“ O
guns O
” O
( O
Figure O
1(b O
) O
) O
. O

Also O
, O
despite O
not O
being O
trained O
to O
predict O
multiple O
frames O
, O
some O
of O
BERT B-MethodName
’s O
predictions O
of O
what O
it O
believes O
to O
be O
top O
frames O
align O
with O
that O
of O
human O
experts O
. O

However O
, O
we O
notice O
that O
out O
of O
the O
319 O
headlines O
that O
have O
two O
frames O
, O
164 O
of O
them O
have O
both O
frames O
predicted O
in O
the O
top-2 O
predictions O
of O
our O
model O
, O
showing O
the O
potential O
to O
ﬁne O
- O
tune O
BERT B-MethodName
for O
multilabel O
multi O
- O
class O
frame O
classiﬁcation O
, O
which O
we O
will O
explore O
in O
the O
future O
. O

gun O
violence O
by O
ﬁne O
- O
tuning O
BERT B-MethodName
- O
base O
uncased O
using O
the O
relevance O
annotations O
in O
GVFC O
. O

In O
their O
recent O
survey O
, O
Hládek O
et O
al O
. O
( O
2020 O
) O
noted O
the O
emerging O
popularity O
of O
encoder O
- O
decoder O
architectures O
and O
deep O
neural B-MethodName
networks I-MethodName
that O
treat O
the O
spelling O
correction O
problem O
as O
one O
of O
statistical O
machine O
translation O
. O

To O
do O
this O
we O
implement O
a O
shallow O
neural O
network O
performing O
logistic B-MethodName
regression I-MethodName
. O

A O
single O
- O
layer O
neural O
network O
is O
used O
to O
perform O
logistic B-MethodName
regression I-MethodName
in O
an O
effort O
to O
classify O
the O
training O
instances O
as O
either O
true O
misspelling O
: O
target O
pairs O
( O
0 O
label O
) O
or O
negative O
misspelling O
: O
non O
- O
target O
pairs O
( O
1 O
label O
) O
. O

The O
results O
of O
the O
logistic B-MethodName
regression I-MethodName
weight B-HyperparameterName
tuning O
were O
then O
passed O
to O
the O
S O
- O
capade O
spellchecker O
to O
be O
used O
as O
edit O
operation O
costs O
and O
the O
Holbrook O
ﬁne O
- O
tuned O
model O
was O
tested O
on O
the O
IAE O
- O
corpus O
of O
misspellings O
. O

Both O
are O
transition O
- O
based O
parsers O
using O
BERT B-MethodName
contextualized O
embeddings O
. O

2.2 O
Transition O
Classiﬁer O
To O
predict O
the O
next O
transition O
at O
each O
step O
, O
TUPA O
uses O
a O
BiLSTM B-MethodName
module O
followed O
by O
an O
MLP O
and O
a O
softmax O
layer O
for O
classiﬁcation O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
) O
. O

The O
BiLSTM B-MethodName
module O
is O
applied O
before O
the O
transition O
sequence O
starts O
, O
running O
over O
the O
input O
tokenized O
sequence O
. O

It O
consists O
of O
a O
pre O
- O
BiLSTM B-MethodName
MLP O
with O
feature O
embeddings O
( O
§ O
2.3 O
) O
and O
pre O
- O
trained O
contextualized O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
embeddings O
concatenated O
as O
inputs O
, O
followed O
by O
( O
multiple O
layers O
of O
) O
a O
bidirectional O
recurrent O
neural O
network O
( O
Schuster O
and O
Paliwal,1997 O
; O
Graves O
, O
2008 O
) O
with O
a O
long O
short O
- O
term O
memory O
cell O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

We O
use O
the O
weighted B-HyperparameterName
sum O
of O
last O
four O
hidden B-HyperparameterName
layers I-HyperparameterName
of O
a O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 B-HyperparameterValue
) O
pre O
- O
trained O
model4as O
extra O
input O
features O
, O
summing O
over O
wordpiece O
vectors O
to O
get O
word O
representations O
. O

2.8 O
Cross O
- O
lingual O
track O
For O
the O
cross O
- O
lingual O
track O
, O
as O
a O
generic O
contextualized O
encoder O
that O
supports O
many O
languages O
, O
we O
use O
multilingual O
BERT B-MethodName
( O
bertbase O
- O
multilingual O
- O
cased O
) O
and O
train O
the O
models O
exactly O
the O
same O
as O
in O
the O
cross O
- O
framework O
track O
( O
separate O
model O
for O
each O
framework O
’s O
respective O
monolingual O
dataset O
from O
the O
crosslingual O
track O
) O
, O
for O
Czech O
PTG O
and O
Chinese O
AMR O
. O

For O
German O
DRG O
, O
as O
the O
provided O
dataset O
contains O
a O
relatively O
small O
amount O
of O
examples O
, O
1575 O
as O
opposed O
to O
6606 O
in O
English O
DRG O
( O
from O
the O
cross O
- O
framework O
track O
) O
, O
we O
ﬁrst O
pre O
- O
train O
a O
model O
on O
the O
DRG O
data O
in O
English O
and O
then O
ﬁne O
- O
tune O
it O
on O
the O
DRG O
German O
dataset O
, O
in O
this O
case O
using O
mBERT B-MethodName
to O
facilitate O
cross O
- O
lingual O
transfer O
. O

3 O
HIT O
- O
SCIR O
Parser O
The O
HIT O
- O
SCIR O
parser O
( O
Che O
et O
al O
. O
, O
2019 O
) O
is O
a O
transition O
- O
based O
parser O
, O
which O
extended O
previous O
parsers O
by O
employing O
stack O
LSTM B-MethodName
( O
Dyer O
et O
al O
. O
, O
2015 O
) O
to O
allow O
computing O
homogeneous O
operation O
within O
a O
batch O
efﬁciently O
, O
and O
by O
adopting O
and O
ﬁne O
- O
tuning O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
embedding O
for O
effectively O
encoding O
contextual O
information O
. O

Transition O
classiﬁer O
takes O
S O
; O
L O
; O
B O
and O
also O
the O
action O
history O
as O
input O
, O
all O
are O
modeled O
with O
stack O
LSTM B-MethodName
, O
and O
outputs O
an O
action O
. O

The O
input O
to O
the O
parser O
is O
a O
sequence O
of O
BERT B-MethodName
embedding O
. O

BERT B-MethodName
embeddings O
are O
extracted O
for O
each O
token O
and O
are O
concatenated O
with O
framework O
- O
speciﬁc O
learned O
embedding O
. O

In O
the O
ﬁrst O
sharing O
architecture O
, O
both O
frameworks O
share O
a O
stacked O
selfattention O
encoder O
whose O
input O
is O
a O
BERT B-MethodName
embedding O
concatenated O
with O
a O
learned O
task O
embed6https://github.com/OfirArviv/ O
hit O
- O
scir O
- O
mrp2020 O
/ O
tree O
/ O
multitaskding O
of O
dimension O
20 O
. O

In O
the O
second O
architectures O
, O
both O
frameworks O
share O
a O
stacked O
selfattention O
encoder O
whose O
input O
is O
a O
BERT B-MethodName
embedding O
, O
and O
in O
addition O
each O
framework O
has O
another O
stacked O
self O
- O
attention O
encoder O
of O
it O
own O
, O
similar O
in O
concept O
to O
Peng O
et O
al O
. O
( O
2017 O
, O
2018 O
) O
’ O
sFREDA O
1 O
architecture O
( O
which O
, O
however O
, O
used O
BiLSTMs B-MethodName
) O
, O
also O
employed O
by O
Hershcovich O
et O
al O
. O
( O
2018a O
) O
; O
Lindemann O
et O
al O
. O
( O
2019 O
) O
. O

BERT B-MethodName
embeddings O
are O
extracted O
for O
each O
token O
. O

Acknowledgements O
Funded O
by O
the O
Government O
of O
India O
’s O
DST O
- O
SERB O
Early O
Career O
Research O
Grant O
( O
ECR/2017/001056 O
) O
to O
Sowmya O
Kamath O
S. O
LDA O
( O
140,792×100)MLP O
0.7950±0.0003 O
0.7168±0.0020 O
0.5735±0.0012 O
0.6685±0.0013 O
0.7848±0.0011 O
A O
- O
LSTM B-MethodName
0.7930±0.0007 O
0.7153±0.0034 O
0.5701±0.0022 O
0.6655±0.0013 O
0.7833±0.0020 O
NMF O
( O
140,792×100)MLP O
0.7829±0.0006 O
0.7029±0.0016 O
0.5498±0.0009 O
0.6530±0.0017 O
0.7744±0.0007 O
A O
- O
LSTM B-MethodName
0.7815±0.0008 O
0.6935±0.0052 O
0.5451±0.0024 O
0.6535±0.0014 O
0.7689±0.0031 O
Table O
2 O
: O
Experimental O
results O
for O
ICD-9 O
code O
group O
prediction O
using O
MLP O
and O
A O
- O
LSTM B-MethodName
. O

Our O
system O
uses O
relatively O
simple O
LSTM B-MethodName
networks O
to O
produce O
part O
of O
speech O
tags O
and O
labeled O
dependency O
parses O
from O
segmented O
and O
tokenized O
sequences O
of O
words O
. O

In O
order O
to O
address O
the O
rare O
word O
problem O
that O
abounds O
in O
languages O
with O
complex O
morphology O
, O
we O
include O
a O
character O
- O
based O
word O
representation O
that O
uses O
an O
LSTM B-MethodName
to O
produce O
embeddings O
from O
sequences O
of O
characters O
. O

Our O
system O
builds O
on O
the O
deep O
biafﬁne O
neural O
dependency O
parser O
presented O
by O
Dozat O
and O
Manning O
( O
2017 O
) O
, O
which O
uses O
a O
well O
- O
tuned O
LSTM B-MethodName
network O
to O
produce O
vector O
representations O
for O
each O
word O
, O
then O
uses O
those O
vector O
representations O
in O
novel O
biafﬁne O
classiﬁers O
to O
predict O
the O
head O
token O
of O
each O
dependent O
and O
the O
class O
of O
the O
resulting O
edge O
. O

In O
order O
to O
adapt O
it O
to O
the O
wide O
variety O
of O
different O
treebanks O
in O
Universal O
Dependencies O
, O
we O
make O
two O
noteworthy O
extensions O
to O
the O
system O
: O
ﬁrst O
, O
we O
incorporate O
a O
word O
representation O
built O
up O
from O
character O
sequences O
using O
an O
LSTM B-MethodName
, O
theorizing O
thatthis O
should O
improve O
the O
model O
’s O
ability O
to O
adapt O
to O
rare O
or O
unknown O
words O
in O
languages O
with O
rich O
morphology O
; O
second O
, O
we O
train O
our O
own O
taggers O
for O
the O
treebanks O
using O
nearly O
identical O
architecture O
to O
the O
one O
used O
for O
parsing O
, O
in O
order O
to O
capitalize O
on O
potential O
improvements O
in O
part O
of O
speech O
tag O
quality O
over O
baseline O
or O
off O
- O
the O
- O
shelf O
taggers O
. O

2 O
Architecture O
2.1 O
Deep O
biafﬁne O
parser O
The O
basic O
architecture O
of O
our O
approach O
follows O
that O
of O
Dozat O
and O
Manning O
( O
2017 O
) O
, O
which O
is O
closely O
related O
to O
Kiperwasser O
and O
Goldberg O
( O
2016 O
) O
, O
the O
ﬁrst O
neural O
graph O
- O
based O
( O
McDonald O
et O
al O
. O
, O
2005 O
) O
parser.1In O
Dozat O
and O
Manning O
’s O
2017 O
parser O
, O
the O
input O
to O
the O
model O
is O
a O
sequence O
of O
tokens O
and O
their O
part O
of O
speech O
tags O
, O
which O
is O
then O
put O
through O
a O
multilayer O
bidirectional O
LSTM B-MethodName
network O
. O

The O
output O
state O
of O
the O
ﬁnal O
LSTM B-MethodName
layer O
( O
which O
excludes O
the O
cell O
state O
) O
is O
then O
fed O
through O
four O
separate O
ReLU O
layers O
, O
producing O
four O
specialized O
vector O
representations O
: O
one O
for O
the O
word O
as O
a O
dependent O
seeking O
its O
head O
; O
one O
for O
the O
word O
as O
a O
head O
seeking O
all O
its O
dependents O
; O
another O
for O
the O
word O
as O
a O
dependent O
deciding O
on O
its O
label O
; O
and O
a O
fourth O
for O
the O
word O
as O
head O
deciding O
on O
the O
labels O
of O
its O
depen1For O
other O
neural O
graph O
- O
based O
parsers O
, O
cf O
. O

Put O
formally O
, O
given O
a O
sequence O
of O
nword O
embeddings O
( O
to O
be O
described O
in O
more O
detail O
in O
Section O
2.2)(v(word O
) O
1, O
... O
,v(word O
) O
n O
) O
andntag O
embeddings O
( O
v(tag O
) O
1, O
... O
,v(tag O
) O
n O
) O
, O
we O
concatenate O
each O
pair O
together O
and O
feed O
the O
result O
into O
a O
BiLSTM B-MethodName
with O
initial O
state O
r0:3 O
xi O
= O
v(word O
) O
i⊕v(tag O
) O
i O
( O
1 O
) O
ri O
= O
BiLSTM B-MethodName
/ O
parenleftbig O
r0,(x1, O
... O
,xn)/parenrightbig O
i(2 O
) O
hi O
, O
ci O
= O
split(ri O
) O
( O
3 O
) O
We O
then O
produce O
four O
distinct O
vectors O
from O
each O
recurrent O
hidden O
state O
hi(without O
the O
recurrent O
cell O
state O
ci O
) O
using O
ReLU O
perceptron O
layers O
: O
h(arc O
- O
dep O
) O
i O
= O
MLP(arc O
- O
dep)(hi O
) O
( O
4 O
) O
h(arc O
- O
head O
) O
i O
= O
MLP(arc O
- O
head O
) O
( O
hi O
) O
( O
5 O
) O
h(rel O
- O
dep O
) O
i O
= O
MLP(rel O
- O
dep)(hi O
) O
( O
6 O
) O
h(rel O
- O
head O
) O
i O
= O
MLP(rel O
- O
head O
) O
( O
hi O
) O
( O
7 O
) O
In O
order O
to O
produce O
a O
prediction O
y O
/ O
prime(arc O
) O
i O
for O
token O
i O
, O
we O
use O
a O
biafﬁne O
classiﬁer O
involving O
the O
( O
arc O
) O
2Interestingly O
, O
other O
researchers O
have O
found O
similar O
approaches O
to O
be O
beneﬁcial O
for O
other O
tasks O
; O
cf O
. O

Each O
character O
is O
given O
a O
trainable O
vector O
embedding O
, O
and O
each O
sequence O
of O
character O
embeddings O
is O
fed O
into O
a O
unidirectional O
LSTM B-MethodName
. O

However O
, O
the O
LSTM B-MethodName
produces O
a O
sequence O
of O
recurrent O
states O
( O
r1, O
... O
,rn O
) O
, O
which O
we O
need O
to O
convert O
into O
a O
single O
vector O
. O

Another O
approach O
, O
suggested O
by O
Cao O
and O
Rei O
( O
2016 O
) O
, O
is O
to O
use O
attention O
over O
the O
hidden O
states O
, O
and O
then O
4Although O
in O
the O
future O
we O
intend O
to O
implement O
than O
the O
Chu O
- O
Liu O
/ O
Edmonds O
algorithm O
for O
nonprojective O
MST O
parsing O
( O
Chu O
and O
Liu O
, O
1965 O
; O
Edmonds O
, O
1967 O
) O
5We O
use O
the O
provided O
CoNLL O
vectors O
trained O
on O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
; O
for O
Gothic O
, O
which O
had O
no O
provided O
vector O
embeddings O
, O
we O
used O
Facebook O
’s O
FastText O
vectors O
( O
Bojanowski O
et O
al O
. O
, O
2016)trasform O
the O
resulting O
context O
vector O
to O
the O
desired O
size O
; O
in O
theory O
, O
this O
should O
both O
allow O
the O
model O
to O
learn O
morpheme O
information O
more O
easily O
by O
attending O
more O
closely O
to O
the O
LSTM B-MethodName
output O
at O
morpheme O
boundaries O
. O

That O
is O
, O
given O
a O
sequence O
of O
ncharacter O
embeddings O
and O
an O
initial O
state O
r0for O
the O
LSTM B-MethodName
, O
we O
each O
embedding O
into O
an O
LSTM B-MethodName
as O
before O
, O
extracting O
hidden O
and O
cell O
states O
: O
ri O
= O
LSTM B-MethodName
/ O
parenleftbig O
r0,(v(char O
) O
1, O
... O
,v(char O
) O
n O
) O
/parenrightbig O
i O
( O
12 O
) O
hi O
, O
ci O
= O
split(ri O
) O
( O
13 O
) O
We O
then O
compute O
linear O
attention O
over O
the O
stack O
of O
hidden O
vectors O
Hand O
concatenate O
it O
to O
the O
ﬁnal O
cell O
state O
: O
a O
= O
softmax O
/ O
parenleftbig O
Hw(attn O
) O
/parenrightbig O
( O
14 O
) O
˜h O
= O
H O
/ O
latticetopa O
( O
15 O
) O
ˆ O
v O
= O
W O
/ O
parenleftbig˜h⊕cn O
/ O
parenrightbig O
( O
16 O
) O
In O
this O
way O
we O
use O
the O
hidden O
states O
for O
attention O
and O
the O
cell O
state O
as O
a O
ﬁnal O
summary O
vector O
. O

The O
resulting O
two O
vectors O
are O
used O
as O
input O
to O
the O
BiLSTM B-MethodName
parser O
in O
Section O
2.1.2.3 O
POS B-TaskName
tagger O
The O
ﬁnal O
piece O
of O
our O
system O
is O
a O
separatelytrained O
part O
of O
speech O
tagger O
. O

Ling O
et O
al O
. O
( O
2015 O
) O
; O
Plank O
et O
al O
. O
( O
2016))—it O
uses O
a O
BiLSTM B-MethodName
over O
word O
vectors O
( O
using O
the O
tripartite O
representation O
from O
Section O
2.2 O
) O
, O
then O
uses O
ReLU O
layers O
to O
produce O
one O
vector O
representation O
for O
each O
type O
of O
tag O
. O

Thus O
we O
use O
a O
BiLSTM B-MethodName
, O
as O
with O
the O
parser O
architecture O
: O
ri O
= O
BiLSTM B-MethodName
/ O
parenleftbig O
r0,(v(word O
) O
1, O
... O
,v(word O
) O
n O
) O
/parenrightbig O
i O
( O
17 O
) O
hi O
, O
ci O
= O
split(ri O
) O
( O
18 O
) O
And O
we O
use O
afﬁne O
classiﬁers O
for O
each O
type O
of O
tag O
, O
which O
we O
add O
together O
for O
the O
parser O
: O
h(pos O
) O
i O
= O
MLP(pos)(hi O
) O
( O
19 O
) O
s(pos O
) O
i O
= O
Wh(pos O
) O
i+b(pos)(20 O
) O
y O
/ O
prime(pos O
) O
i O
= O
arg O
max O
js(pos O
) O
ij O
( O
21 O
) O
The O
tag O
classiﬁers O
are O
trained O
jointly O
using O
crossentropy B-MetricName
losses O
that O
are O
summed O
together O
during O
optimization O
, O
but O
the O
tagger O
is O
trained O
independently O
from O
the O
parser O
. O

The O
parser O
uses O
three O
BiLSTM B-MethodName
layers O
with O
100 O
- O
dimensional O
word O
and O
tag O
embeddings O
and O
200 O
- O
dimensional O
recurrent O
states O
( O
in O
each O
direction O
) O
; O
the O
arc O
classiﬁer O
uses O
400 O
- O
dimensional O
head O
/ O
dependent O
vector O
states O
and O
the O
label O
classiﬁer O
uses O
100 O
- O
dimensional O
ones O
; O
we O
drop O
word O
and O
tag O
embeddings O
independently O
with O
33 O
% O
probability;6we O
use O
samemask O
dropout B-HyperparameterName
( O
Gal O
and O
Ghahramani O
, O
2015 B-HyperparameterValue
) O
in O
the O
LSTM B-MethodName
, O
ReLU O
layers O
, O
and O
classiﬁers O
, O
dropping O
input O
and O
recurrent O
connections O
with O
33 O
% O
probability O
; O
and O
we O
optimize O
with O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
, O
setting O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
2e−3and O
β1 O
= O
β2=.9 O
. O

We O
do O
n’t O
drop O
characters O
but O
do O
include O
33 O
% O
dropout B-HyperparameterName
in O
the O
LSTM B-MethodName
and O
attention O
connections O
. O

In O
the O
tagger O
we O
use O
nearly O
identical O
settings O
, O
with O
a O
few O
exceptions O
: O
the O
BiLSTM B-MethodName
is O
only O
two O
layers O
deep O
, O
we O
increase O
the O
dropout B-HyperparameterName
between O
recurrent O
connections O
to O
50 B-HyperparameterValue
% O
, O
and O
we O
use O
cased O
character O
embeddings O
. O

The O
system O
uses O
BiLSTM B-MethodName
networks O
for O
tagging O
and O
parsing O
, O
and O
includes O
character O
- O
level O
word O
representations O
in O
addition O
to O
token O
- O
level O
ones O
. O

3 O
Learning O
and O
Posterior O
Inference O
Due O
to O
the O
conditionals O
parameterized O
by O
neural B-MethodName
networks I-MethodName
we O
use O
amortized O
variational O
inference O
in O
a O
manner O
similar O
to O
Variational O
AutoEncoders O
( O
Kingma O
and O
Welling O
, O
2013 O
) O
, O
both O
to O
learn O
an O
approximate O
posterior O
q(S;ZjX)and O
to O
learn O
the O
generative O
model O
parameters O
by O
maximizing O
a O
lower O
bound O
on O
the O
data O
likelihood O
( O
ELBO O
) O
. O

To O
do O
this O
, O
we O
use O
recurrent O
neural B-MethodName
networks I-MethodName
to O
calculate O
the O
surprisal O
of O
stimuli O
from O
previously O
published O
neurolinguistic O
studies O
of O
the O
N400 O
. O

2.5 O
Other O
Models O
of O
N400 O
amplitude O
While O
a O
number O
of O
other O
researchers O
have O
used O
neural B-MethodName
networks I-MethodName
to O
model O
speciﬁc O
N400 O
ﬁndings O
this O
way O
( O
Laszlo O
and O
Plaut O
, O
2012 O
; O
Laszlo O
and O
Armstrong O
, O
2014 O
; O
Rabovsky O
and O
McRae O
, O
2014 O
; O
Cheyette O
and O
Plaut O
, O
2017 O
; O
Brouwer O
et O
al O
. O
, O
2017 O
; O
Rabovsky O
et O
al O
. O
, O
2018 O
; O
Venhuizen O
et O
al O
. O
, O
2018 O
; O
Fitz O
and O
Chang O
, O
2019 O
) O
, O
these O
studies O
differ O
in O
that O
these O
models O
all O
have O
semantic O
representations O
as O
part O
of O
their O
input O
or O
are O
trained O
to O
learn O
to O
output O
some O
form O
of O
semantic O
representation O
. O

We O
do O
this O
by O
running O
the O
( O
English O
language O
) O
stimuli O
from O
previously O
published O
N400 O
studies O
through O
two O
neural B-MethodName
networks I-MethodName
that O
have O
been O
used O
extensively O
to O
model O
human O
language O
processing O
( O
e.g. O
, O
in O
Wilcox O
et O
al O
. O
, O
2018 O
; O
Futrell O
et O
al O
. O
, O
2019 O
; O
Wilcox O
et O
al O
. O
, O
2019 O
; O
An O
et O
al O
. O
, O
2019 O
; O
Costa O
and O
Chaves O
, O
2020 O
) O
. O

The O
two O
models O
used O
are O
the O
the O
best O
English O
LSTM B-MethodName
from O
Gulordava O
et O
al O
. O
( O
2018 O
) O
and O
BIG O
LSTM+CNN B-MethodName
INPUTS O
from O
Jozefowicz O
et O
al O
. O
( O
2016 O
) O
, O
henceforth O
( O
following O
Futrell O
et O
al O
. O
, O
2019 O
) O
GRNN O
and O
JRNN O
, O
respectively O
. O

These O
models O
are O
both O
LSTM B-MethodName
- O
RNN O
- O
LMs O
, O
but O
differ O
most O
notably O
in O
size O
and O
training O
data O
: O
The O
JRNN O
has O
two O
hidden B-HyperparameterName
layers I-HyperparameterName
( O
8192 B-HyperparameterValue
and O
1024 O
units O
) O
, O
a O
793471word O
vocabulary O
, O
and O
was O
trained O
on O
1 O
billion O
tokens O
( O
Chelba O
et O
al O
. O
, O
2013 O
) O
; O
while O
the O
GRNN O
has O
two O
hidden B-HyperparameterName
layers I-HyperparameterName
( O
both O
650 B-HyperparameterValue
units O
) O
, O
a O
50000 O
- O
word O
vocabulary O
, O
and O
was O
trained O
on O
90 O
million O
tokens O
. O

Speciﬁcally O
, O
quantiﬁcation O
, O
aspects O
of O
event O
structure O
, O
and O
morphosyntactic O
anomalies O
seem O
to O
require O
some O
other O
learning O
architecture O
than O
the O
bottom O
- O
up O
statistical O
learning O
represented O
by O
standard O
recurrent O
neural B-MethodName
networks I-MethodName
. O

Firstly O
because O
similar O
techniques O
are O
often O
applicable O
, O
but O
more O
importantly O
because O
the O
knowledge O
of O
how O
the O
workings O
of O
artiﬁcial O
and O
biological O
neural B-MethodName
networks I-MethodName
are O
similar O
or O
different O
is O
valuable O
for O
the O
general O
enterprise O
of O
cognitive O
science O
. O

King O
and O
Taylor O
( O
2000 O
) O
train O
recurrent O
neural B-MethodName
networks I-MethodName
to O
extract O
phonological O
features O
from O
framewise O
cepstral O
representation O
of O
speech O
in O
the O
TIMIT B-DatasetName
speaker O
- O
independent O
database O
. O

A O
separate O
line O
of O
research O
has O
used O
neural B-MethodName
networks I-MethodName
for O
modeling O
phonology O
from O
a O
( O
neuro)cognitive O
perspective O
. O

For O
each O
representation O
we O
then O
train O
L2 O
- O
penalized O
logistic B-MethodName
regression I-MethodName
( O
with O
the O
ﬁxed O
penalty O
weight B-HyperparameterName
1.0 B-HyperparameterValue
) O
on O
the O
training O
data O
and O
measure O
classiﬁcation O
error O
rate O
on O
the O
heldout O
portion O
. O

For O
every O
type O
of O
input O
, O
we O
run O
10 O
- O
fold O
cross O
validation O
using O
Logistic B-MethodName
Regression I-MethodName
to O
predict O
which O
of O
the O
two O
words O
the O
utterance O
contains O
. O

6 O
Discussion O
Understanding O
distributed O
representations O
learned O
by O
neural B-MethodName
networks I-MethodName
is O
important O
but O
has O
the O
reputation O
of O
being O
hard O
or O
even O
impossible O
. O

Inspired O
by O
this O
, O
we O
explore O
several O
variants O
of O
a O
bidirectional O
LSTM B-MethodName
architecture O
that O
relies O
on O
different O
knowledge O
sources O
, O
such O
as O
Web O
data O
, O
search O
engine O
click O
logs O
, O
expert O
feedback O
from O
H2 O
M O
models O
, O
as O
well O
as O
previous O
utterances O
in O
the O
conversation O
. O

In O
this O
work O
, O
we O
introduce O
a O
modular O
architecture O
with O
a O
core O
bi O
- O
directional O
LSTM B-MethodName
network O
, O
and O
additional O
network O
components O
that O
utilize O
knowledge O
from O
multiple O
sources O
including O
: O
sentence O
embeddings O
to O
encode O
semantics O
and O
intents O
of O
noisy O
texts O
with O
web O
- O
data O
and O
click O
logs O
, O
H2 O
M O
based O
expert O
feedback O
, O
and O
contextual O
models O
relying O
on O
previous O
turns O
in O
the O
conversation O
. O

In O
summary O
, O
this O
paper O
makes O
the O
following O
contributions O
: O
A O
practical O
framework O
on O
slot O
tagging O
for O
task O
oriented O
SLU O
on O
H2H O
conversations O
using O
bidirectional O
LSTM B-MethodName
architecture O
. O

Extension O
of O
the O
LSTM B-MethodName
architecture O
utilizing O
knowledge O
from O
external O
sources O
( O
e.g. O
Web O
1https://en.wikipedia.org/wiki/CALO O
2https://www.cmu.edu/cmnews/extra/ O
030718_darpa.htmldata O
, O
click O
logs O
, O
H2 O
M O
expert O
feedback O
, O
and O
pervious O
sentences O
) O
with O
deep O
learning O
based O
ensemble O
methods O
Newly O
developed O
dataset O
for O
evaluating O
task O
oriented O
LU O
on O
H2H O
conversations O
We O
begin O
by O
describing O
our O
methods O
for O
H2H O
slot O
tagging O
in O
Section O
3 O
. O

Recently O
various O
deep O
neural O
network O
( O
DNN O
) O
models O
have O
been O
studied O
to O
solve O
each O
of O
these O
task O
, O
such O
as O
deep O
belief O
network O
( O
Sarikaya O
et O
al O
. O
, O
2011 O
) O
, O
deep O
convex O
network O
( O
Deng O
et O
al O
. O
, O
2012 O
) O
, O
RNN O
and O
LSTM B-MethodName
( O
Ravuri O
and O
Stolcke O
, O
2015 O
; O
Mesnil O
et O
al O
. O
, O
2015 O
) O
. O

Our O
modular O
architecture O
is O
a O
core O
LSTM B-MethodName
- O
based O
network O
and O
additional O
network O
components O
that O
encode O
knowledge O
from O
multiple O
sources O
. O

The O
ﬁrst O
characterlevel O
bidirectional O
LSTM B-MethodName
layer O
extracts O
the O
encoding O
from O
a O
sequence O
of O
characters O
from O
each O
word O
. O

The O
layer O
outputs O
fc O
= O
LSTM B-MethodName
forward O
( O
ec O
) O
( O
1 O
) O
bc O
= O
LSTM B-MethodName
backward O
( O
ec O
) O
( O
2 O
) O
for O
each O
character O
, O
where O
fc;bc2R25 O
. O

The O
second O
word O
- O
level O
bidirectional O
LSTM B-MethodName
layer O
extracts O
the O
encoding O
from O
a O
sequence O
of O
words O
for O
each O
sentence O
. O

We O
use O
pre O
- O
trained O
GloVe B-MethodName
with O
2B O
tweets3(Pennington O
et O
al O
. O
, O
2014 O
) O
for O
the O
word O
embedding O
. O

The O
forward O
and O
backward O
word O
- O
level O
LSTM B-MethodName
’s O
produce O
fw O
i O
= O
LSTM B-MethodName
forward O
( O
gi O
) O
( O
3 O
) O
bw O
i O
= O
LSTM B-MethodName
backward O
( O
gi O
) O
( O
4 O
) O
wherefw O
i;bw O
i2R100 O
. O

To O
obtain O
knowledge O
from O
a O
previous O
sentence O
in O
the O
conversation O
, O
we O
extract O
a O
contextual O
encoded O
vector O
using O
the O
memory O
network O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
, O
which O
uses O
the O
weighted B-HyperparameterName
sum O
of O
the O
output O
of O
word O
- O
level O
bidirectional O
LSTM B-MethodName
hin O
the O
core O
network O
( O
Section O
3.1 O
) O
from O
previous O
sentences O
. O

The O
output O
of O
word O
- O
level O
bidirectional O
LSTM B-MethodName
his O
then O
extracted O
as O
the O
encoded O
vector O
from O
H2 O
M O
expert O
model O
ve2R200 O
. O

Starting O
from O
a O
core O
network O
with O
bidirectional O
LSTM B-MethodName
, O
we O
proposed O
to O
use O
additional O
network O
components O
and O
ensemble O
them O
to O
augment O
useful O
knowledge O
from O
multiple O
sources O
( O
web O
data O
, O
search O
engine O
click O
logs O
, O
H2 O
M O
expert O
feedback O
, O
and O
previous O
utterances O
) O
. O

1 O
Introduction O
Deep O
neural B-MethodName
networks I-MethodName
have O
made O
a O
profound O
impact O
on O
natural O
language O
processing O
technology O
in O
general O
, O
and O
machine O
translation O
in O
particular O
( O
Blunsom O
, O
2013 O
; O
Sutskever O
et O
al O
. O
, O
2014 O
; O
Cho O
et O
al O
. O
, O
2014 O
; O
Jean O
et O
al O
. O
, O
2015 O
; O
LeCun O
et O
al O
. O
, O
2015 O
) O
. O

To O
address O
these O
limitations O
, O
we O
propose O
an O
alternative O
neural B-MethodName
MT I-MethodName
architecture O
, O
based O
on O
deep O
2D O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNNs O
) O
. O

Gehring O
et O
al O
. O
( O
2017b O
) O
outperformed O
deep O
LSTMs B-MethodName
for O
machine O
translation O
1D O
CNNs O
with O
gated O
linear O
units O
( O
Meng O
et O
al O
. O
, O
2015 O
; O
Oord O
et O
al O
. O
, O
2016c O
; O
Dauphin O
et O
al O
. O
, O
2017 O
) O
in O
both O
the O
encoder O
and O
decoder O
modules O
. O

Kalchbrenner O
et O
al O
. O
( O
2016a O
) O
proposed O
a O
2D O
LSTM B-MethodName
model O
similar O
to O
our O
2D O
CNN O
for O
machine O
translation O
. O

In O
a O
second O
LSTM B-MethodName
stream O
, O
each O
cell O
takes O
input O
from O
its O
left O
and O
top O
neighbor O
, O
as O
well O
as O
from O
the O
corresponding O
cell O
in O
the O
ﬁrst O
stream O
. O

For O
comparison O
with O
state O
- O
of O
- O
theart O
architectures O
, O
we O
implemented O
a O
bidirectional O
LSTM B-MethodName
encoder O
- O
decoder O
model O
with O
dotproduct O
attention O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
; O
Luong O
et O
al O
. O
, O
2015 O
) O
using O
PyTorch O
( O
Paszke O
et O
al O
. O
, O
2017 O
) O
, O
and O
used O
Facebook O
AI O
Research O
Sequence O
- O
toSequence O
Toolkit O
( O
Gehring O
et O
al O
. O
, O
2017b O
) O
to O
train O
the O
ConvS2S O
and O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
models O
on O
our O
data O
. O

For O
the O
Bi O
- O
LSTM B-MethodName
encoder O
- O
decoder O
, O
the O
encoder O
is O
a O
single O
layer O
bidirectional O
LSTM B-MethodName
with O
input O
embeddings O
of O
size O
128 O
and O
a O
hidden O
state O
of O
sizeModel O
BLEU B-MetricName
Flops O
105#params O
Average B-MetricName
31.57 B-MetricValue
0.11 O
3.63 O
7.18 O
M O
Max O
33.70 O
0.06 O
3.44 O
7.18 O
M O
Attn O
32.07 O
0.13 O
3.61 O
7.24 O
M O
Max O
, O
gated O
33.66 O
0.16 O
3.49 O
9.64 O
M O
[ O
Max O
, O
Attn O
] O
33.81 O
0.03 O
3.51 O
7.24 O
M O
Table O
1 O
: O
Our O
model O
( O
L=24;g=32;ds O
= O
dt=128 O
) O
with O
different O
pooling O
operators O
and O
using O
gated O
convolutional O
units O
. O

The O
decoder O
is O
a O
single O
layer O
LSTM B-MethodName
with O
similar O
input O
size O
and O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
256 B-HyperparameterValue
, O
the O
target O
input O
embeddings O
are O
also O
used O
in O
the O
pre O
- O
softmax O
projection O
. O

( O
8).Word O
- O
based O
De O
- O
EnFlops O
( O
105 O
) O
# O
prms O
En O
- O
De O
# O
prms O
Conv O
- O
LSTM B-MethodName
( O
MLE O
) O
( O
Bahdanau O
et O
al O
. O
, O
2017 O
) O
27.56 O
Bi O
- O
GRU O
( O
MLE+SLE O
) O
( O
Bahdanau O
et O
al O
. O
, O
2017 O
) O
28.53 O
Conv O
- O
LSTM B-MethodName
( O
deep+pos O
) O
( O
Gehring O
et O
al O
. O
, O
2017a O
) O
30.4 O
NPMT O
+ O
language O
model O
( O
Huang O
et O
al O
. O
, O
2018 O
) O
30.08 O
25.36 O
BPE O
- O
based O
RNNsearch O
* O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
31.02 O
1.79 O
6 O
M O
25.92 O
7 O
M O
Varational O
attention O
( O
Deng O
et O
al O
. O
, O
2018 O
) O
33.10 O
Transformer O
* O
* O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
32.83 O
3.53 O
59 O
M O
27.68 O
61 O
M O
ConvS2S O
* O
* O
( O
MLE O
) O
( O
Gehring O
et O
al O
. O
, O
2017b O
) O
32.31 O
1.35 O
21 O
M O
26.73 O
22 O
M O
ConvS2S O
( O
MLE+SLE O
) O
( O
Edunov O
et O
al O
. O
, O
2018 O
) O
32.84 O
Pervasive O
Attention O
( O
this O
paper O
) O
33.81 O
0.03 O
3.51 O
7 O
M O
27.770.1 O
7 O
M O
Table O
3 O
: O
Comparison O
to O
state O
- O
of O
- O
the O
art O
results O
on O
IWSLT O
German O
- O
English O
translation O
. O

In O
the O
future O
, O
we O
plan O
toexplore O
hybrid O
approaches O
in O
which O
the O
input O
to O
our O
joint O
encoding O
model O
is O
not O
provided O
by O
tokenembedding O
vectors O
, O
but O
the O
output O
of O
1D O
source O
and O
target O
embedding O
networks O
, O
e.g. O
( O
bi-)LSTM B-MethodName
or O
1D O
convolutional O
. O

M O
- O
BERT B-MethodName
: O
As O
the O
best O
performing O
models O
in O
CLEF O
2020 O
Check O
That O
! O

Lab O
( O
Barr O
´ O
on O
- O
Cedeno O
et O
al O
. O
, O
2020 O
) O
used O
variants O
of O
BERT B-MethodName
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
we O
ﬁne O
tune O
multiligual O
cased O
version O
of O
BERT B-MethodName
( O
i.e. O
, O
M O
- O
BERT B-MethodName
) O
using O
traning O
data O
. O

BERTurk B-MethodName
: O
Pires O
et O
al O
. O
( O
2019 O
) O
report O
that O
MBERT B-MethodName
’s O
performance O
might O
decrease O
for O
the O
languages O
with O
different O
word O
orders O
in O
sentences O
. O

Therefore O
, O
we O
ﬁne O
tune O
BERTurk B-MethodName
( O
Schweter O
, O
2020 O
) O
which O
is O
a O
monolingual O
BERT B-MethodName
model O
pre O
- O
trained O
using O
only O
Turkish O
texts O
. O

Logistic O
Regression O
with O
Bag O
- O
of O
- O
Words O
( O
LR O
- O
BOW O
): O
We O
train O
a O
logistic B-MethodName
regression I-MethodName
model O
with O
bag O
- O
of O
- O
words O
features O
. O

Support O
Vector O
Machines O
with O
Bag O
- O
ofWords O
( O
SVM B-MethodName
- O
BOW O
): O
We O
use O
the O
same O
bagof O
- O
words O
features O
and O
train O
an O
SVM B-MethodName
model O
with O
the O
training O
data O
. O

We O
use O
Scikit O
toolkit22for O
both O
SVM B-MethodName
- O
BOW O
and O
LR O
- O
BOW O
models O
with O
default O
parameters O
. O

Model O
AP O
P@1 O
P@5 O
P@10 O
P@30 O
R O
- O
P O
nDCG O
M O
- O
BERT B-MethodName
.3825 O
1.0000 O
.8000 O
.4000 O
.3000 O
.3510 O
.8708 O
BERTurk B-MethodName
.3687 O
0.0000 O
.6000 O
.4000 O
.3000 O
.3709 O
.8895 O
BOW O
- O
LR O
.3609 O
1.0000 O
.2000 O
.2000 O
.2667 O
.3245 O
.8815 O
BOW O
- O
SVM B-MethodName
.3716 O
1.0000 O
.2000 O
.2000 O
.3333 O
.3444 O
.8372 O
each O
tweet O
. O

Based O
on O
AP O
, O
P@1 O
, O
P@5 O
, O
and O
P@10 O
, O
M O
- O
BERT B-MethodName
yields O
the O
highest O
scores B-MetricName
. O

BERTurk B-MethodName
model O
outperforms O
others O
based O
on O
R O
- O
P O
and O
nDCG O
metrics O
. O

As O
expected O
, O
BERT B-MethodName
based O
models O
outperform O
LRBOW O
and O
SVM B-MethodName
- O
BOW O
models O
in O
most O
of O
the O
cases O
. O

However O
, O
their O
scores B-MetricName
are O
close O
in O
many O
cases O
, O
and O
SVM B-MethodName
- O
BOW O
and O
LR O
- O
BOW O
slightly O
outperform O
MBERT B-MethodName
and O
BERTurk B-MethodName
models O
in O
various O
cases O
. O

Future O
studies O
might O
explore O
weak O
supervision O
or O
cross O
lingual O
transfer O
learning O
to O
better O
ﬁne O
tune O
BERT B-MethodName
models O
. O

Qi O
et O
al O
. O
( O
2017 O
) O
and O
particularly O
Zhang O
et O
al O
. O
( O
2019b O
) O
proposed O
permutation O
- O
invariant O
neural B-MethodName
networks I-MethodName
for O
point O
clouds O
, O
which O
are O
of O
great O
relevance O
to O
our O
system O
. O

3.5.1 O
Contextual O
Embedding O
Extraction O
Different O
layers O
in O
BERT B-MethodName
- O
like O
models O
represent O
varying O
levels O
of O
syntactic O
and O
semantic O
knowledge O
( O
van O
Aken O
et O
al O
. O
, O
2019 O
) O
, O
raising O
a O
question O
of O
which O
layer O
( O
or O
layers O
) O
should O
be O
used O
to O
extract O
the O
embeddings O
from O
. O

Conﬁguration O
Tops O
Labels O
Properties O
Anchors O
Edges O
Average B-MetricName
´ O
UFAL O
PERIN O
* O
89.53 B-MetricValue
% O
93.45 O
% O
94.34 O
% O
93.40 O
% O
90.74 O
% O
92.73 O
% O
w/o O
MoS O
88.04 O
% O
93.39 O
% O
93.79 O
% O
93.48 O
% O
90.76 O
% O
92.65 O
% O
w/o O
focal O
loss O
89.08 O
% O
93.33 O
% O
93.59 O
% O
93.21 O
% O
90.46 O
% O
92.46 O
% O
BERT B-MethodName
encoder O
89.95 O
% O
92.97 O
% O
94.74 O
% O
92.92 O
% O
89.84 O
% O
92.27 O
% O
w/o O
balanced O
losses O
89.23 O
% O
92.28 O
% O
94.46 O
% O
92.12 O
% O
89.19 O
% O
91.60 O
% O
Table O
4 O
: O
Ablation O
study O
showing O
MRP O
scores B-MetricName
of O
different O
conﬁgurations O
on O
EDS O
. O

The O
top O
row O
contains O
the O
submitted O
conﬁguration O
without O
any O
changes O
; O
then O
we O
report O
the O
results O
for O
1 O
) O
label O
classiﬁer O
without O
the O
mixture O
of O
softmaxes O
( O
MoS O
) O
; O
2 O
) O
label O
loss O
not O
multiplied O
by O
the O
focal O
loss O
coefﬁcient O
; O
3 O
) O
encoder O
with O
ﬁnetuned O
BERT B-MethodName
- O
large O
( O
English O
) O
instead O
of O
multilingual O
XLM O
- O
R O
and O
4 O
) O
constant O
loss O
weights B-HyperparameterName
, O
equally O
set O
to O
1:0 O
. O

While O
convolutional O
neural B-MethodName
networks I-MethodName
( O
Sun O
et O
al O
. O
, O
2015 O
; O
Francis O
- O
Landau O
et O
al O
. O
, O
2016 O
) O
and O
probabilistic O
attention O
( O
Lazic O
et O
al O
. O
, O
2015 O
) O
have O
been O
applied O
to O
the O
task O
, O
this O
is O
the O
ﬁrst O
model O
to O
use O
RNNs O
and O
a O
neural O
attention O
model O
for O
NED O
. O

More O
recently O
, O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNNs O
) O
were O
employed O
for O
learning O
semantic O
similarity O
between O
context O
, O
mention O
, O
and O
candidate O
inputs O
( O
Sun O
et O
al O
. O
, O
2015 O
; O
Francis O
- O
Landau O
et O
al O
. O
, O
2016 O
) O
. O

To O
this O
end O
, O
we O
implemented O
an O
approach O
based O
on O
the O
Skip O
- O
Gram O
with O
Negative O
- O
Sampling O
( O
SGNS O
) O
algorithm O
by O
Mikolov O
et O
al O
. O
( O
2013 O
) O
that O
simultaneously O
trains O
both O
word O
and O
entity O
vectors O
. O

Additional O
Features O
We O
proceeded O
to O
use O
the O
model O
in O
a O
similar O
setting O
to O
Yamada O
et O
al O
. O
( O
2016 O
) O
where O
a O
Gradient B-MethodName
Boosting I-MethodName
Regression O
Tree O
( O
GBRT O
) O
( O
Friedman O
, O
2001 O
) O
model O
was O
trained O
with O
our O
model O
’s O
prediction O
as O
a O
feature O
along O
with O
a O
number O
of O
statistical O
and O
string O
based O
features O
deﬁned O
by O
Yamada O
. O

For O
the O
parsing O
step O
we O
applied O
an O
ensemble O
approach O
using O
three O
different O
parsers O
, O
sometimes O
using O
multiple O
instances O
of O
the O
same O
parser O
: O
one O
graph O
- O
based O
parser O
trained O
with O
the O
perceptron O
; O
one O
transition O
- O
based O
beam O
search O
parser O
also O
trained O
with O
the O
perceptron O
; O
and O
one O
greedy O
transition O
- O
based O
parser O
trained O
with O
neural B-MethodName
networks I-MethodName
. O

3.3 O
Transition O
- O
based O
greedy O
neural B-MethodName
parser I-MethodName
We O
use O
an O
in O
- O
house O
transition O
- O
based O
greedy O
parser O
with O
neural B-MethodName
networks I-MethodName
( O
Yu O
and O
Vu O
, O
2017 O
) O
, O
henceforth O
referred O
to O
as O
TN.7 O
The O
parser O
uses O
a O
CNN O
to O
compose O
word O
representations O
from O
characters O
, O
it O
also O
takes O
the O
embeddings O
of O
word O
forms O
, O
universal O
POS B-TaskName
tags O
and O
supertags O
and O
concatenates O
all O
of O
them O
as O
input O
features O
. O

In O
the O
era O
of O
Neural O
Machine O
Translation O
( O
NMT O
) O
, O
such O
biases O
are O
implicitly O
introduced O
by O
the O
sequential O
nature O
of O
the O
LSTM B-MethodName
architecture O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
, O
see O
§ O
2 O
) O
. O

The O
inﬂuential O
Transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
replaces O
the O
sequential O
LSTMs B-MethodName
with O
self O
- O
attention O
, O
which O
does O
not O
seem O
to O
possess O
this O
bias O
. O

2 O
Related O
Work O
2.1 O
Long O
- O
distance O
Dependencies O
in O
MT O
A O
common O
architecture O
for O
text O
- O
to O
- O
text O
generation O
tasks O
is O
the O
( O
Bi)LSTM B-MethodName
encoder O
- O
decoder O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O

This O
architecture O
consists O
of O
several O
LSTM B-MethodName
layers O
for O
the O
encoder O
and O
the O
decoder O
and O
a O
thin O
attention O
layer O
connecting O
them O
. O

LSTM B-MethodName
is O
a O
recurrent O
network O
with O
a O
state O
vector O
it O
updates O
. O

While O
theoretically O
information O
could O
be O
kept O
indeﬁnitely O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
practical O
evidence O
shows O
that O
LSTMs B-MethodName
performance O
decreases O
with O
the O
distance O
between O
the O
trigger O
and O
the O
prediction O
( O
Linzen O
et O
al O
. O
, O
2016 O
; O
Liu O
et O
al O
. O
, O
2018 O
) O
, O
and O
that O
they O
have O
difﬁculties O
generalizing O
over O
sequence O
lengths O
( O
Suzgun O
et O
al O
. O
, O
2018 O
) O
. O

Despite O
being O
affected O
by O
absolute O
distances O
between O
syntactically O
dependent O
tokens O
( O
Linzen O
et O
al O
. O
, O
2016 O
) O
, O
LSTMs B-MethodName
tend O
to O
learn O
to O
a O
certainextent O
structural O
information O
even O
without O
being O
instructed O
to O
do O
so O
explicitly O
( O
Gulordava O
et O
al O
. O
, O
2018 O
) O
. O

Futrell O
and O
Levy O
( O
2018 O
) O
discuss O
similar O
linguistic O
phenomena O
to O
what O
we O
discuss O
in O
§ O
4.2 O
, O
and O
show O
that O
LSTM B-MethodName
encoder O
- O
decoder O
systems O
handle O
them O
better O
than O
previous O
N O
- O
gram O
based O
systems O
, O
despite O
being O
profoundly O
affected O
by O
distance O
. O

Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
models O
are O
also O
encoder O
- O
decoder O
, O
but O
instead O
of O
LSTMs B-MethodName
, O
they O
use O
self O
- O
attention O
. O

3 O
Locality O
in O
SoTA O
NMT O
In O
this O
section O
we O
show O
that O
encoder O
- O
decoder O
models O
based O
on O
BiLSTM B-MethodName
with O
attention O
( O
see O
§ O
2 O
) O
, O
do O
exhibit O
a O
locality O
bias O
, O
but O
that O
the O
Transformer O
, O
whose O
encoder O
is O
based O
on O
self O
- O
attention O
, O
and O
in O
which O
token O
position O
is O
encoded O
only O
through O
learnedPEs O
, O
does O
not O
present O
any O
such O
bias O
. O

We O
train O
the O
BiLSTM B-MethodName
model O
using O
the O
Nematus O
implementation O
( O
Sennrich O
et O
al O
. O
, O
2017b O
) O
, O
and O
use O
their O
supplied O
scripts O
for O
preprocessing O
, O
training O
and O
testing O
, O
changing O
only O
the O
datasets O
used O
. O

In O
addition O
, O
we O
trained O
the O
BiLSTM B-MethodName
model O
and O
the O
Transformer O
with O
SinePEs O
both O
in O
the O
R O
EGULAR O
condition O
and O
in O
P O
ERMUTED O
, O
each O
was O
trained O
once O
. O

We O
ﬁnd O
that O
Nematus O
BiLSTM B-MethodName
suffers O
substantially O
from O
permuting O
the O
source O
- O
side O
tokens O
, O
but O
that O
the O
Transformer O
does O
not O
exhibit O
a O
locality O
bias O
. O

This O
is O
not O
the O
case O
with O
BiLSTMs B-MethodName
, O
which O
often O
require O
introducing O
the O
same O
information O
at O
each O
input O
token O
to O
allow O
them O
to O
be O
effectively O
used O
by O
the O
system O
( O
Yao O
et O
al O
. O
, O
2017 O
; O
Rennie O
et O
al O
. O
, O
2017 O
) O
. O

We O
note O
that O
our O
experiments O
were O
not O
designed O
to O
compare O
the O
performance O
of O
BiLSTM B-MethodName
and O
selfattention O
models O
. O

A O
potential O
confound O
is O
that O
performance O
might O
change O
with O
the O
length O
of O
the O
source O
in O
BiLSTMs B-MethodName
( O
Carpuat O
et O
al O
. O
, O
2013 O
; O
Murray O
and O
Chiang O
, O
2018 O
) O
, O
in O
Transformers O
it O
was O
reported O
to O
increase O
( O
Zhang O
et O
al O
. O
, O
2018 O
) O
. O

An O
accumulating O
body O
of O
research O
is O
devoted O
to O
the O
ability O
of O
modern O
neural O
architectures O
such O
as O
LSTMs B-MethodName
( O
Linzen O
et O
al O
. O
, O
2016 O
) O
and O
pretrained O
embeddings O
( O
Hewitt O
and O
Manning O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Jawahar O
et O
al O
. O
, O
2019 O
) O
to O
represent O
linguistic O
features O
. O

We O
train O
a O
logistic B-MethodName
regression I-MethodName
classiﬁer O
to O
categorize O
an O
ending O
, O
either O
as O
right O
vs.wrong O
or O
as O
original O
vs.new O
( O
right O
) O
. O

For O
the O
POS B-TaskName
features O
, O
we O
tag O
all O
endings O
with O
the O
Spacy O
POS B-TaskName
tagger.7 O
We O
use O
Python O
’s O
sklearn O
logistic B-MethodName
regression I-MethodName
imple699 O
% O
of O
all O
sentences O
end O
with O
a O
period O
or O
an O
exclamation O
mark O
, O
so O
we O
do O
not O
add O
a O
STOP O
symbol O
. O

Speciﬁcally O
, O
we O
experiment O
with O
an O
LSTM B-MethodName
- O
based O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
recurrent O
neural O
network O
language O
model O
( O
RNNLM O
; O
Mikolov O
et O
al O
. O
, O
2010 O
) O
. O

We O
train O
the O
RNNLM O
using O
a O
single O
- O
layer O
LSTM B-MethodName
of O
hidden O
dimension O
512 O
. O

Only O
during O
training O
, O
we O
apply O
a O
dropout B-HyperparameterName
rate I-HyperparameterName
of O
60 B-HyperparameterValue
% O
while O
running O
the O
LSTM B-MethodName
over O
all O
5 O
sentences O
of O
the O
stories O
. O

In O
order O
to O
answer O
this O
question O
, O
we O
consider O
the O
highest O
absolute O
positive O
and O
negative O
coefﬁcients O
in O
the O
logistic B-MethodName
regression I-MethodName
classiﬁer O
in O
Experiments O
1 O
and O
2 O
, O
an O
approach O
widely O
used O
as O
a O
method O
of O
extracting O
the O
most O
salient O
features O
( O
Nguyen O
et O
al O
. O
, O
2013 O
; O
Burke O
et O
al O
. O
, O
2013 O
; O
Brooks O
et O
al O
. O
, O
2013 O
) O
. O

It O
is O
worth O
noting O
that O
its O
reliability O
is O
not O
entirely O
clear O
, O
since O
linear O
models O
like O
logistic B-MethodName
regression I-MethodName
can O
assign O
large O
coefﬁcients O
to O
rare O
features O
( O
Yano O
et O
al O
. O
, O
2012 O
) O
. O

We O
train O
a O
linear O
SVM B-MethodName
classiﬁer O
( O
Joachims O
, O
1998 O
) O
, O
henceforth O
referred O
to O
as O
ET O
classiﬁer O
. O

We O
extended O
the O
basic O
transition O
- O
based O
parser O
with O
two O
improvements O
: O
a O
) O
Efﬁcient O
Trainingby O
realizing O
stack O
LSTM B-MethodName
parallel O
training O
; O
b)Effective O
Encoding O
via O
adopting O
deep O
contextualized O
word O
embeddings O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

Generally O
, O
we O
proposed O
a O
uniﬁed O
pipeline O
to O
meaning B-MetricName
representation O
parsing O
, O
including O
framework O
- O
speciﬁc O
transitionbased O
parsers O
, O
BERT B-MethodName
- O
enhanced O
word O
representation O
, O
and O
post O
- O
processing O
. O

pects O
: O
1 O
) O
Efﬁcient O
Training O
Aligning O
the O
homogeneous O
operation O
in O
stack O
LSTM B-MethodName
within O
a O
batch O
and O
then O
computing O
them O
simultaneously O
; O
2 O
) O
Effective O
Encoding O
Fine O
- O
tuning O
the O
parser O
with O
pretrained O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
embedding O
, O
which O
enrich O
the O
context O
information O
to O
make O
accurate O
local O
decisions O
, O
and O
global O
learning O
for O
exact O
search O
. O

We O
designed O
a O
simple O
but O
efﬁcient O
method O
to O
realize O
stack O
LSTM B-MethodName
parallel O
training O
. O

We O
showed O
that O
semantic B-TaskName
parsing I-TaskName
task O
beneﬁts O
a O
lot O
from O
adopting O
BERT B-MethodName
. O

Then O
, O
to O
speed O
up O
the O
training O
of O
stack O
LSTM B-MethodName
at O
transition O
- O
based O
parser O
, O
we O
propose O
a O
simple O
method O
to O
do O
batch O
- O
training O
in O
Section O
2.2 O
. O

And O
we O
adopt O
BERT B-MethodName
to O
extract O
the O
contextualized O
word O
representation O
in O
Section O
2.3 O
. O

We O
model O
the O
S O
, O
L O
, O
Band O
action O
history O
with O
stack O
LSTM B-MethodName
, O
which O
supports O
P O
USH O
and O
P O
OPoperation.2 O
Transition O
classiﬁer O
takes O
the O
parsing O
state O
from O
multiple O
stack O
LSTM B-MethodName
models O
as O
input O
at O
once O
, O
and O
outputs O
a O
action O
that O
maximizes O
the O
score B-MetricName
. O

The O
score B-MetricName
of O
a O
transition O
action O
aon O
state O
sis O
calculated O
as O
p(ajs O
) O
= O
expfgaSTACK O
LSTM B-MethodName
( O
s O
) O
+ O
bagP O
a0expfga0STACK O
LSTM B-MethodName
( O
s O
) O
+ O
ba0 O
g O
, O
where O
STACK O
LSTM B-MethodName
( O
s)encodes O
the O
state O
sinto O
a O
vector O
, O
gaandbaare O
embedding O
vector O
, O
bias O
vector O
of O
action O
arespectively O
. O

After O
that O
, O
new O
LSTM B-MethodName
states O
will O
be O
pushed O
to O
corresponding O
stacks O
. O

To O
solve O
this O
, O
we O
propose O
a O
method O
of O
maintaining O
stack O
LSTM B-MethodName
structure O
and O
using O
operation O
buffer O
. O

stack O
LSTM B-MethodName
The O
stack O
LSTM B-MethodName
augments O
the O
conventional O
LSTM B-MethodName
with O
a O
‘ O
stack O
pointer O
’ O
. O

Batch O
Data O
in O
Operation O
- O
Level O
Like O
conventional O
LSTM B-MethodName
ca O
n’t O
form O
a O
batch O
inside O
a O
sequence O
due O
to O
the O
characteristics O
of O
sequential O
processing O
, O
stack O
LSTM B-MethodName
ca O
n’t O
either O
. O

2.3 O
BERT B-MethodName
- O
Enhance O
Word O
Representation O
2.3.1 O
Deep O
Contextualized O
Word O
Representations O
Neural O
parsers O
often O
use O
pretrained O
word O
embeddings O
as O
their O
primary O
input O
, O
i.e. O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
which O
assign O
a O
single O
static O
representation O
to O
each O
word O
so O
that O
they O
can O
not O
capture O
context O
- O
dependent O
meaning B-MetricName
. O

By O
contrast O
, O
deep O
contextualized O
word O
representations O
, O
i.e. O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
encode O
words O
with O
respect O
to O
the O
context O
, O
which O
have O
been O
proven O
to O
be O
useful O
for O
many O
NLP O
tasks O
, O
achieving O
state O
- O
of O
- O
the O
- O
art O
performance O
in O
standard O
Natural O
Language O
Understanding O
( O
NLU O
) O
benchmarks O
, O
such O
as O
GLUE B-DatasetName
( O
Wang O
et O
al O
. O
, O
2018a O
) O
. O

Che O
et O
al O
. O
( O
2018 O
) O
adopted O
ELMo B-MethodName
in O
CoNLL O
2018 O
shared O
task O
( O
Zeman O
et O
al O
. O
, O
2018 O
) O
and O
achieved O
ﬁrst O
prize O
in O
terms O
of O
LAS O
metric O
. O

( O
Kondratyuk O
and O
Straka O
, O
2019 O
) O
exceeds O
the O
state O
- O
ofthe O
- O
art O
in O
UD O
with O
ﬁne O
- O
tuning O
model O
with O
BERT B-MethodName
. O

2.3.2 O
BERT B-MethodName
We O
adopt O
BERT B-MethodName
in O
our O
model O
, O
which O
uses O
the O
language O
- O
modeling O
objective O
and O
trained O
on O
unannotated O
text O
for O
getting O
deep O
contextualized O
embeddings O
. O

BERT B-MethodName
differs O
from O
ELMo B-MethodName
in O
that O
it O
employs O
a O
bidirectional O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
which O
beneﬁt O
from O
learning O
potential O
dependencies O
between O
words O
directly O
. O

For O
a O
token O
wkin O
sentence O
S O
, O
BERT B-MethodName
splits O
it O
to O
several O
pieces O
and O
use O
a O
sequence O
of O
WordPiece O
embedding O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
sk;1;sk;2 O
; O
: O
: O
: O
; O
sk;piece O
num O
kinstead O
ofa O
single O
token O
embedding O
. O

DM O
PSD O
UCCA O
EDS O
AMR O
Feature O
LF1 B-MetricName
MRP O
LF1 B-MetricName
MRP O
LF1 B-MetricName
MRP O
EDM O
MRP O
SMATCH O
MRP O
GloVe B-MethodName
87.1 O
87.3 O
74.1 O
73.7 O
56.3 O
87.5 O
82.5 O
88.2 O
64.8 O
65.3 O
BERT(base B-MethodName
) O
94.3 O
90.5 O
83.6 O
76.7 O
64.3 O
92.8 O
87.6 O
91.5 O
71.0 O
71.4 O
Table O
2 O
: O
HIT O
- O
SCIR O
parser O
results O
on O
MRP O
split O
dataset O
with O
GloVe B-MethodName
or O
BERT B-MethodName
as O
pretrained O
word O
representation O
. O

5 O
Experiments O
In O
this O
section O
, O
we O
will O
show O
the O
basic O
model O
setup O
including O
BERT B-MethodName
ﬁne O
- O
tuning O
, O
and O
results O
including O
overall O
evaluation O
, O
training O
speed O
. O

We O
split O
parameters O
into O
two O
groups O
, O
i.e. O
, O
BERT B-MethodName
parameters O
and O
the O
other O
parameters O
( O
base O
parameters O
) O
. O

Fine O
- O
Tuning O
BERT B-MethodName
with O
Parser O
Based O
on O
Devlin O
et O
al O
. O
( O
2019 O
) O
, O
ﬁne O
- O
tuning O
BERT B-MethodName
with O
supervised O
downstream B-TaskName
task O
will O
receive O
the O
most O
beneﬁt O
. O

So O
we O
choose O
to O
ﬁne O
- O
tune O
BERT B-MethodName
model O
together O
with O
the O
original O
parser O
. O

In O
our O
preliminary O
study O
, O
gradual O
unfreezing O
and O
slanted O
triangular O
learning B-HyperparameterName
rate I-HyperparameterName
scheduler O
is O
essential O
for O
BERT B-MethodName
ﬁne O
- O
tuning O
model O
. O

Training O
Speed O
To O
explore O
the O
effect O
of O
batchtraining O
methods O
which O
proposed O
in O
Section O
2.2 O
6Evaluation O
results O
of O
CoNLL O
2019 O
shared O
task O
are O
available O
at O
http://bit.ly/cfmrp19 O
.DM O
PAS O
PSD O
Parser O
Feature O
i O
d O
F O
ood O
F O
i O
d O
F O
ood O
F O
i O
d O
F O
ood O
F O
Wang O
et O
al O
. O
( O
2018b O
) O
T O
word2vec O
89.3 O
83.2 O
91.4 O
87.2 O
76.1 O
73.2 O
Dozat O
and O
Manning O
( O
2018 O
) O
G O
GloVe+char B-MethodName
92.7 O
87.8 O
94.0 O
90.6 O
80.5 O
78.6 O
HIT O
- O
SCIR O
T O
GloVe+char B-MethodName
86.1 O
79.2 O
89.8 O
85.2 O
72.8 O
68.5 O
AllenNLP O
G O
GloVe+char B-MethodName
91.6 O
86.1 O
93.1 O
89.6 O
77.4 O
73.0 O
HIT O
- O
SCIR O
T O
BERT B-MethodName
92.9 O
89.2 O
94.4 O
92.4 O
81.6 O
81.0 O
AllenNLP O
G O
BERT B-MethodName
94.1 O
90.8 O
94.8 O
92.9 O
80.7 O
79.5 O
Table O
3 O
: O
Semantic O
parsing O
accuracies B-MetricName
( O
i O
d O
= O
in O
domain O
test O
set O
; O
ood O
= O
out O
of O
domain O
test O
set O
) O
. O

We O
adopted O
BERT B-MethodName
( O
base+cased O
) O
model O
here O
. O

We O
use O
GloVe B-MethodName
pretrained O
embedding O
instead O
of O
BERT B-MethodName
to O
reduce O
memory O
cost O
and O
support O
a O
larger O
batch B-HyperparameterName
size I-HyperparameterName
in O
the O
speed O
test O
. O

Improvement O
through O
BERT B-MethodName
Our O
parser O
beneﬁts O
a O
lot O
from O
BERT B-MethodName
compared O
with O
GloVe B-MethodName
as O
shown O
in O
Table O
2 O
. O

The O
improvement O
is O
more O
obvious O
in O
the O
out O
- O
of O
- O
domain O
evaluations O
, O
illustrating O
BERT B-MethodName
’s O
ability O
to O
transfer O
across O
domains O
. O

However O
, O
when O
we O
concatenated O
those O
models O
with O
BERT B-MethodName
, O
we O
receivethe O
similar O
performance O
, O
which O
shows O
that O
powerful O
representation O
could O
eliminate O
the O
gap O
between O
structure O
or O
parsing O
strategy O
. O

7 O
Conclusion O
and O
Future O
Work O
Our O
system O
extends O
the O
basic O
transition O
- O
based O
parser O
with O
the O
following O
improvements O
: O
1 O
) O
adopting O
BERT B-MethodName
for O
better O
word O
representation O
; O
2 O
) O
realizing O
batch O
- O
training O
for O
stack O
LSTM B-MethodName
to O
speed O
up O
the O
training O
process O
. O

Both O
audio O
and O
image O
encoders O
are O
Inception O
- O
ResNet O
- O
v2 O
networks O
( O
Szegedy O
et O
al O
. O
, O
2017 O
) O
, O
allowing O
the O
model O
to O
reap O
the O
beneﬁts O
of O
relatively O
low O
computational O
cost O
, O
fast O
training O
and O
and O
strong O
performance O
when O
combining O
the O
Inception O
architecture O
with O
residual O
connections.3Related O
to O
our O
setting O
for O
audio O
processing O
, O
Li O
et O
al O
. O
( O
2019 O
) O
also O
uses O
residual O
convolutional O
neural B-MethodName
networks I-MethodName
for O
state O
of O
the O
art O
results O
on O
LibriSpeech B-DatasetName
dataset O
( O
Panayotov O
et O
al O
. O
, O
2015 O
) O
. O

Furthermore O
, O
very O
deep O
, O
residual O
convolutional O
neural B-MethodName
networks I-MethodName
over O
characters O
have O
been O
shown O
to O
perform O
well O
for O
text O
- O
based O
tasks O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
. O

Once O
a O
model O
is O
trained O
, O
any O
input O
( O
image O
or O
spoken O
caption O
) O
can O
be O
be O
used O
to O
query O
the O
corpus O
of O
images O
and O
spoken O
captions O
for O
nearest B-MethodName
neighbors I-MethodName
in O
the O
latent O
space O
. O

Figure O
4 O
shows O
some O
examples O
of O
retrieved O
nearest B-MethodName
neighbors I-MethodName
in O
FACC O
’s O
test O
set O
. O

For O
single O
feature O
models O
( O
SID O
or O
PID O
) O
we O
use O
a O
simple O
logistic B-MethodName
regression I-MethodName
classiﬁer O
. O

For O
models O
with O
multiple O
features O
we O
use O
the O
elastic O
net O
logistic B-MethodName
regression I-MethodName
with O
an O
elastic O
net O
hyperparameterα= O
0.5 O
. O

This O
difference O
is O
probably O
due O
to O
the O
differences O
in O
hyperparameters O
and O
experimental O
setup O
: O
we O
use O
an O
elastic O
- O
net O
regularised O
logistic B-MethodName
regression I-MethodName
classiﬁer O
while O
they O
used O
a O
random B-MethodName
forest I-MethodName
, O
we O
perform O
10 O
- O
fold O
cross O
- O
validation O
while O
they O
divided O
the O
DementiaBank O
into O
60 O
- O
20 O
- O
20 O
train O
- O
dev O
- O
test O
partitions O
. O

In O
order O
to O
check O
this O
, O
we O
extracted O
the O
normalised O
counts O
of O
nouns O
and O
verbs O
from O
all O
transcripts O
in O
both O
datasets O
and O
used O
it O
to O
train O
single O
feature O
logistic B-MethodName
regression I-MethodName
classiﬁers O
. O

We O
show O
that O
bagof O
- O
word O
- O
embeddings O
are O
better O
than O
LSTMs B-MethodName
for O
tasks O
with O
scarce O
training O
data O
, O
while O
the O
situation O
is O
reversed O
when O
having O
larger O
amounts O
. O

Transferring O
an O
LSTM B-MethodName
which O
is O
learned O
on O
all O
datasets O
is O
the O
most O
effective O
context O
representation O
option O
for O
the O
word O
experts O
in O
all O
frequency O
bands O
. O

On O
the O
left O
side O
, O
context O
models O
: O
( O
a O
) O
Sparse O
BoW O
, O
( O
b O
) O
Continuous O
BoW O
, O
( O
c O
) O
LSTM B-MethodName
. O

The O
transfer O
model O
ﬁrst O
learns O
an O
LSTM B-MethodName
on O
the O
single O
model O
, O
then O
reuses O
the O
LSTM B-MethodName
to O
learn O
each O
of O
the O
word O
expert O
models O
. O

Recurrent O
Neural O
Network O
( O
LSTM B-MethodName
): O
As O
a O
third O
alternative O
, O
we O
considered O
a O
recurrent O
neural O
network O
based O
on O
LSTMs B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
to O
exploit O
the O
dependencies O
among O
the O
word O
sequence O
that O
forms O
the O
input O
context O
( O
Figure O
1 O
, O
( O
c O
) O
) O
. O

We O
use O
a O
single O
LSTM B-MethodName
to O
encode O
the O
input O
contexts O
as O
follows O
. O

We O
ﬁrst O
replace O
the O
target O
mention O
with O
a O
special O
symbol O
which O
has O
a O
manually O
assigned O
constant O
embedding O
, O
and O
then O
feed O
the O
sequence O
into O
the O
LSTM B-MethodName
. O

The O
LSTMs B-MethodName
have O
512 O
hidden B-HyperparameterName
units I-HyperparameterName
and O
300 B-HyperparameterValue
dimensional O
word O
embeddings O
, O
which O
are O
initialized O
with O
the O
embeddings O
vectors O
used O
in O
the O
continuous O
BoW O
model O
described O
above O
. O

The O
LSTM B-MethodName
layers O
have O
a O
dropout B-HyperparameterName
layer O
, O
with O
0:2 O
dropout B-HyperparameterName
probability O
. O

We O
explored O
GRUs O
, O
stacking O
LSTMs B-MethodName
, O
temporal O
average B-MetricName
and O
max O
pooling O
among O
hidden O
states O
, O
but O
did O
not O
improve O
results O
on O
development O
. O

In O
this O
case O
, O
we O
also O
replace O
the O
target O
mention O
with O
a O
special O
symbol O
which O
has O
a O
manually O
assigned O
constant O
embedding O
vector O
, O
we O
feed O
it O
into O
the O
LSTM B-MethodName
, O
and O
use O
the O
last O
hidden O
vector O
has O
the O
context O
representation O
. O

In O
this O
case O
the O
LSTM B-MethodName
has O
2048 O
hidden B-HyperparameterName
units I-HyperparameterName
, O
producing O
a O
512 B-HyperparameterValue
- O
dimensional O
context O
representation O
and O
300dimensional O
word O
embeddings O
, O
which O
are O
again O
initialized O
with O
the O
pretrained O
embeddings O
in O
the O
previous O
section O
keeping O
them O
ﬁxed O
. O

That O
is O
, O
after O
training O
the O
single O
model O
with O
the O
whole O
Wikipedia O
, O
we O
use O
the O
learned O
model O
of O
the O
LSTM B-MethodName
as O
the O
text O
representation O
layer O
of O
the O
word O
expert O
models O
. O

This O
way O
, O
we O
reuse O
the O
LSTM B-MethodName
which O
was O
learned O
alongside O
the O
single O
model O
instead O
of O
learning O
a O
separate O
LSTM B-MethodName
layer O
for O
eachword O
expert O
( O
see O
Section O
2.2 O
) O
. O

When O
training O
the O
word O
experts O
, O
we O
keep O
the O
LSTM B-MethodName
layer O
ﬁxed O
. O

The O
LSTMs B-MethodName
learned O
separately O
do O
not O
improve O
over O
continuous O
BoW O
, O
while O
the O
LSTM B-MethodName
transferred O
from O
the O
single O
model O
obtains O
the O
best O
results O
. O

Regarding O
the O
performance O
of O
the O
word O
expert O
LSTMs B-MethodName
, O
our O
hypothesis O
was O
that O
, O
given O
the O
long O
tail O
distribution O
of O
the O
number O
of O
training O
instances O
, O
the O
per O
- O
mention O
LSTMs B-MethodName
of O
many O
mentions O
would O
not O
have O
enough O
training O
instances O
to O
learn O
effective O
representations O
. O

Continuous O
BoW O
overpeforms O
LSTMs B-MethodName
on O
mentions O
with O
a O
small O
number O
of O
6We O
set O
10 O
buckets O
with O
an O
equal O
number O
of O
mentions O
in O
each O
bucketSparse O
BoW O
CBoW O
LSTM B-MethodName
Transfer O
P(ejc)orig O
79.650.06 O
82.480.48 O
80.350.05 O
84.700.06 O
P(ejc)aug O
79.540.26 O
81.740.21 O
80.660.26 O
82.390.42 O
P(ejc O
) O
83.280.17 O
86.190.19 O
84.350.30 O
86.870.14 O
Table O
2 O
: O
Development O
results O
( O
Aidatesta O
) O
as O
inKB O
accuracy B-MetricName
and O
standard O
deviation O
for O
Sparse O
BoW O
, O
Continuous O
BoW O
, O
LSTM B-MethodName
and O
transferred O
LSTMs B-MethodName
. O

The O
graph O
also O
shows O
that O
the O
transferred O
LSTM B-MethodName
yields O
better O
results O
for O
all O
frequencies O
, O
and O
that O
the O
Sparse O
BoW O
model O
underperforms O
the O
rest O
of O
models O
consistently O
. O

As O
an O
aside O
, O
we O
observed O
that O
for O
the O
words O
which O
have O
more O
than O
200.000 O
training O
instances O
, O
both O
the O
per O
- O
mention O
LSTM B-MethodName
and O
the O
transferred O
LSTM B-MethodName
yield O
similar O
results O
. O

79.7 O
Sparse O
BoW O
86.720.23 O
Continuous O
BoW O
89.390.44 O
LSTM B-MethodName
88.440.26 O
Transfer O
LSTM B-MethodName
91.190.07 O
( O
Lazic O
et O
al O
. O
, O
2015 O
) O
ysemi O
- O
sup O
. O

— O
74.5 O
68.7 O
Sparse O
BoW O
85.82 O
80.25 O
63.12 O
Continuous O
BoW O
86.96 O
81.55 O
67.49 O
LSTM B-MethodName
86.73 O
81.44 O
67.32 O
Transfer O
LSTM B-MethodName
87.32 O
84.41 O
72.58 O
( O
Lazic O
et O
al O
. O
, O
2015 O
) O
ysemi O
- O
sup O
. O

More O
recently O
, O
Sil O
et O
al O
. O
( O
2018 O
) O
introduce O
a O
deep O
neural O
cross O
- O
lingual O
entity O
linking O
system O
using O
a O
combination O
of O
CNN O
, O
LSTM B-MethodName
and O
NTNs O
, O
with O
strong O
results O
. O

This O
paper O
compares O
NED O
and O
word O
sense O
disambiguation O
, O
and O
builds O
a O
bag O
of O
words O
logistic B-MethodName
regression I-MethodName
classiﬁer O
for O
each O
mention O
. O

In O
a O
setting O
similar O
to O
ours O
, O
( O
Yuan O
et O
al O
. O
, O
2016 O
; O
Peters O
et O
al O
. O
, O
2018 O
) O
propose O
to O
train O
a O
language O
model O
based O
on O
LSTMs B-MethodName
and O
then O
use O
it O
for O
word O
sense O
disambiguation O
. O

While O
bags O
of O
pre O
- O
trained O
word O
embeddings O
and O
LSTMs B-MethodName
are O
the O
most O
popular O
approaches O
for O
text O
representation O
, O
many O
alternatives O
exist O
. O

For O
instance O
, O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
obtains O
word O
embeddings O
that O
include O
contextual O
information O
, O
and O
then O
combine O
them O
using O
bag O
- O
of O
- O
words O
orother O
alternative O
. O

We O
think O
that O
, O
in O
supervised O
classiﬁcation O
tasks O
such O
as O
ours O
, O
the O
transferred O
LSTM B-MethodName
already O
captures O
well O
contextual O
information O
and O
that O
the O
performance O
bottleneck O
might O
lie O
on O
the O
classiﬁer O
. O

Given O
the O
scarce O
data O
available O
, O
learning O
a O
classiﬁer O
directly O
on O
a O
bag O
- O
of O
- O
words O
or O
LSTM B-MethodName
representation O
yields O
weak O
results O
. O

Bringing O
in O
pre O
- O
trained O
embeddings O
improves O
results O
, O
but O
the O
key O
to O
strong O
performance O
is O
to O
learn O
a O
single O
model O
for O
all O
entities O
using O
an O
LSTM B-MethodName
and O
then O
transfer O
the O
LSTM B-MethodName
to O
each O
of O
the O
word O
experts O
. O

Pretrained O
Language O
Model O
Our O
systems O
beneﬁt O
a O
lot O
from O
the O
pretrained O
language O
models O
, O
i.e. O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
ELECTRA B-MethodName
( O
Clark O
et O
al O
. O
, O
2020 O
) O
and O
XLM O
- O
RoBERTa B-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
. O

We O
model O
the O
S O
, O
L O
, O
Band O
action O
history O
with O
Stack O
- O
LSTM B-MethodName
, O
which O
supports O
P O
USH O
and O
P O
OPoperations.2 O
3.2 O
Transition O
Systems O
For O
brevity O
, O
we O
omit O
the O
descriptions O
of O
the O
transition O
system O
for O
UCCA O
and O
EDS O
( O
Che O
et O
al O
. O
, O
2019 O
) O
. O

We O
split O
parameters O
into O
two O
groups O
, O
i.e. O
, O
BERT B-MethodName
parameters O
and O
other O
parameters O
( O
base O
parameters O
) O
. O

6.2 O
Fine O
- O
Tuning O
BERT B-MethodName
with O
Parser O
Based O
on O
Devlin O
et O
al O
. O
( O
2019 O
) O
, O
ﬁne O
- O
tuning O
BERT B-MethodName
with O
supervised O
downstream B-TaskName
task O
will O
receive O
the O
most O
beneﬁt O
. O

So O
we O
choose O
to O
ﬁne O
- O
tune O
BERT B-MethodName
model O
together O
with O
the O
original O
parser O
. O

In O
our O
preliminary O
study O
, O
gradual O
unfreezing O
and O
slanted O
triangular O
learning B-HyperparameterName
rate I-HyperparameterName
scheduler O
is O
essential O
for O
BERT B-MethodName
ﬁne O
- O
tuning O
model O
. O

Gradual O
unfreezing O
is O
also O
used O
during O
training O
so O
in O
the O
ﬁrst O
few O
( O
1 O
5 O
) O
epochs B-HyperparameterName
BERT B-MethodName
parameters O
are O
frozen O
. O

We O
separate O
8Evaluation O
results O
of O
CoNLL O
2020 O
shared O
task O
are O
available O
at O
http://bit.ly/cfmrp20 O
.Track O
Cross O
- O
Framework O
Cross O
- O
Lingual O
UCCA O
BERT B-MethodName
- O
Base O
BERT B-MethodName
- O
Base O
German O
EDS O
BERT B-MethodName
- O
Base O
PTG O
BERT B-MethodName
- O
Base O
BERT B-MethodName
- O
Base O
Multilingual O
AMR O
ELECTRA B-MethodName
- O
Large O
ELECTRA B-MethodName
- O
Large O
Chinese O
DRG O
ELECTRA B-MethodName
- O
Large O
XLM O
- O
RoBERTa B-MethodName
- O
Large O
Table O
6 O
: O
The O
pretrained O
language O
model O
used O
in O
each O
track O
. O
. O
HYPERPARAMETER O
VALUE O
Hidden O
dimension O
200 O
Action O
dimension O
50 O
Optimizer O
Adam O
 O
1 O
; O
 O
2 O
0.9 O
, O
0.99 O
Dropout O
0.5 O
Layer O
dropout B-HyperparameterName
0.2 B-HyperparameterValue
Recurrent O
dropout B-HyperparameterName
0.2 O
Input O
dropout B-HyperparameterName
0.2 O
Batch O
size O
16 O
Epochs O
50 O
Base O
learning B-HyperparameterName
rate I-HyperparameterName
110 3 O
BERT B-MethodName
learning B-HyperparameterName
rate I-HyperparameterName
510 5 O
Gradient O
clipping O
5.0 B-HyperparameterValue
Gradient O
norm O
5.0 O
Learning O
rate O
scheduler O
slanted O
triangular O
Gradual O
Unfreezing O
True O
Cut O
Frac O
0.1 O
Ratio O
32 O
Table O
7 O
: O
Transition O
- O
based O
parser O
hyper O
- O
parameters O
settings O
. O

For O
all O
the O
tasks O
, O
we O
employ O
simple O
neural O
network O
architectures O
that O
rely O
on O
long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
networks O
for O
learning O
task O
- O
dependent O
features O
. O

Our O
segmentation O
models O
use O
a O
simple O
neural O
network O
classiﬁer O
that O
relies O
on O
character O
bidirectional O
LSTM B-MethodName
( O
BiLSTM B-MethodName
) O
representations O
of O
a O
focus O
character O
to O
produce O
a O
probabilistic O
distribution O
over O
two O
boundary O
markers O
: O
Begining O
of O
a O
word O
and O
Inside O
of O
a O
word O
. O

At O
training O
time O
, O
the O
transition O
actions O
are O
inferred O
from O
the O
gold O
parse O
trees O
and O
the O
mapping O
between O
the O
parser O
state O
and O
the O
transition O
action O
is O
learned O
using O
a O
simple O
LSTM B-MethodName
- O
based O
neural O
networking O
architecture O
presented O
in O
Goldberg O
( O
2016 O
) O
. O

Given O
that O
Bi O
- O
LSTMs B-MethodName
capture O
global O
sentential O
context O
at O
any O
given O
time O
step O
, O
we O
use O
minimal O
set O
of O
features O
in O
our O
parsing O
model O
. O

Since O
Swap O
action O
distorts O
the O
linear O
order O
of O
word O
sequence O
, O
it O
renders O
the O
LSTM B-MethodName
representations O
irrelevant O
in O
case O
of O
non O
- O
projective O
sentences O
. O

Tagger O
network O
: O
The O
input O
layer O
of O
the O
tagger O
encodes O
each O
input O
word O
in O
a O
sentence O
by O
concatenating O
a O
pre O
- O
trained O
word O
embedding O
with O
its O
character O
embedding O
given O
by O
a O
character O
Bi O
- O
LSTM B-MethodName
. O

In O
the O
feature O
layer O
, O
the O
concatenated O
word O
and O
character O
representations O
are O
passed O
through O
two O
stacked O
Bi O
- O
LSTMs B-MethodName
to O
generate O
a O
sequence O
of O
hidden O
representations O
which O
encode O
the O
contextual O
information O
spread O
across O
the O
sentence O
. O

The O
ﬁrst O
Bi O
- O
LSTM B-MethodName
is O
shared O
with O
the O
parser O
network O
while O
the O
other O
is O
speciﬁc O
to O
the O
tagger O
. O

Parser O
Network O
: O
Similar O
to O
the O
tagger O
network O
, O
the O
input O
layer O
encodes O
the O
input O
sentence O
using O
word O
and O
character O
embeddings O
which O
are O
then O
passed O
to O
the O
shared O
Bi O
- O
LSTM B-MethodName
. O

The O
hidden O
representations O
from O
the O
shared O
Bi O
- O
LSTM B-MethodName
are O
then O
concatenated O
with O
the O
dense O
representations O
from O
the O
feed O
- O
forward O
network O
of O
the O
tagger O
and O
passed O
through O
the O
Bi O
- O
LSTM B-MethodName
speciﬁc O
to O
the O
parser O
. O

From O
each O
parser O
conﬁguration O
, O
we O
extract O
the O
top O
node O
in O
the O
stack O
and O
the O
ﬁrst O
node O
in O
the O
buffer O
and O
use O
their O
hidden O
representations O
from O
the O
parser O
speciﬁc O
Bi O
- O
LSTM B-MethodName
for O
classiﬁcation O
. O

For O
transferring O
parsing O
knowledge O
, O
hidden B-HyperparameterName
representations I-HyperparameterName
from O
the O
parser O
speciﬁc O
Bi O
- O
LSTM B-MethodName
of O
the O
source O
parser O
are O
augmented O
with O
the O
input O
layer O
of O
the O
target O
parser O
which O
already O
includes O
the O
hidden B-HyperparameterName
layer I-HyperparameterName
of O
the O
target O
tagger O
, O
word O
and O
character O
embeddings O
. O

3.1 O
Hyperparameters O
Word O
Representations O
For O
the O
stack O
- O
prop O
and O
stacking O
models O
, O
we O
include O
lexical O
features O
in O
the O
input O
layer O
of O
the O
neural B-MethodName
networks I-MethodName
using O
64dimension O
pre O
- O
trained O
word O
embeddings O
concatenated O
with O
64 O
- O
dimension O
character O
- O
based O
embeddings O
obtained O
using O
a O
Bi O
- O
LSTM B-MethodName
over O
the O
characters O
of O
a O
word O
. O

For O
our O
backoff O
character O
model O
we O
only O
use O
64 O
- O
dimension O
character O
Bi O
- O
LSTM B-MethodName
embeddings O
in O
the O
input O
layer O
of O
the O
network O
. O

Hidden O
dimensions O
The O
word O
- O
level O
Bi O
- O
LSTMs B-MethodName
have O
128 O
cells O
while O
the O
character O
- O
level O
Bi O
- O
LSTMs B-MethodName
have O
64 O
cells O
. O

The O
LSTM B-MethodName
weights B-HyperparameterName
are O
initialized O
with O
random O
orthonormal O
matrices O
as O
described O
in O
( O
Saxe O
et O
al O
. O
, O
2013 O
) O
. O

The O
corpus O
therefore O
allows O
training O
deep O
neural B-MethodName
networks I-MethodName
for O
automated O
fact O
- O
checking O
, O
which O
reach O
higher O
performance O
than O
shallow O
machine O
learning O
techniques O
. O

3 O
) O
For O
evidence O
extraction O
, O
stance O
detection O
, O
and O
claim O
validation O
we O
evaluate O
the O
performance O
of O
high O
- O
scoring O
systems O
from O
the O
FEVER O
shared O
task O
( O
Thorne O
et O
al O
. O
, O
2018b O
) O
4and O
the O
Fake O
News O
Challenge O
( O
Pomerleau O
and O
Rao O
, O
2017 O
) O
5as O
well O
as O
the O
Bidirectional O
Transformer O
model O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
on O
our O
data O
. O

Similar O
to O
the O
outcome O
of O
theFake O
News O
Challenge O
, O
feature O
- O
based O
models O
outperform O
neural B-MethodName
networks I-MethodName
based O
on O
word O
embeddings O
( O
Hanselowski O
et O
al O
. O
, O
2018a O
) O
. O

We O
also O
evaluate O
the O
performance O
of O
DecompAttent O
and O
a O
simpleBiLSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
architecture O
. O

The O
results O
in O
Table O
7illustrate O
that O
, O
in O
terms O
of O
recall B-MetricName
, O
the O
neural B-MethodName
networks I-MethodName
with O
a O
small O
number O
of O
parameters O
, O
BiLSTM B-MethodName
andDecompAttent O
, O
perform O
best O
. O

model O
precision B-MetricName
@5 O
recall B-MetricName
@5 O
random O
baseline O
0.296 B-MetricValue
0.529 O
BiLSTM B-MethodName
0.451 O
0.637 O
DecompAttent O
0.420 O
0.627 O
Tf O
- O
Idf O
0.627 O
0.601 O
rankingESIM O
0.288 O
0.507 O
Table O
7 O
: O
Evidence O
extraction O
: O
ranking O
setting O
5.2.2 O
Error O
analysis O
We O
performed O
an O
error O
analysis O
for O
the O
BiLSTM B-MethodName
and O
theTf O
- O
Idf O
model O
, O
as O
they O
reach O
the O
highest O
recall B-MetricName
and O
precision B-MetricName
, O
respectively O
. O

The O
BiLSTM B-MethodName
isbetter O
able O
to O
capture O
the O
semantics O
of O
the O
sentences O
. O

FEVER O
Snopes O
refuted O
: O
false O
, O
mostly O
false O
supported O
: O
true O
, O
mostly O
true O
NEI O
: O
mixture O
, O
unproven O
, O
undetermined O
Table O
8 O
: O
Compression O
of O
Snopes O
verdicts O
5.3.1 O
Experiments O
For O
the O
claim O
validation O
, O
we O
consider O
models O
of O
different O
complexity O
: O
BertEmb O
is O
an O
MLP O
classiﬁer O
which O
is O
based O
on O
BERT B-MethodName
pre O
- O
trained O
embeddings O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
; O
DecompAttent O
was O
used O
in O
the O
FEVER O
shared O
task O
as O
baseline O
; O
extendedESIM O
is O
an O
extended O
version O
of O
the O
ESIM O
model O
( O
Hanselowski O
et O
al O
. O
, O
2018b O
) O
reaching O
the O
third O
rank O
in O
the O
FEVER O
shared O
task O
; O
BiLSTM B-MethodName
is O
a O
simple O
BiLSTM B-MethodName
architecture O
; O
USE+MLP O
is O
the O
Universal O
Sentence O
Encoder O
combined O
with O
a O
MLP;SVM B-MethodName
is O
an O
SVM B-MethodName
classiﬁer O
based O
on O
bag O
- O
ofwords O
, O
unigrams O
, O
and O
topic O
models O
. O

The O
results O
illustrated O
in O
Table O
9show O
thatBertEmb O
, O
USE+MLP O
, O
BiLSTM B-MethodName
, O
and O
extendedESIM O
reach O
similar O
performance O
, O
withBertEmb O
being O
the O
best O
. O

m O
F1 B-MetricName
m O
random O
baseline O
0.333 B-MetricValue
0.333 O
0.333 O
majority O
vote O
0.198 O
0.170 O
0.249 O
BertEmb O
0.477 O
0.493 O
0.485 O
USE+MLP O
0.483 O
0.468 O
0.475 O
BiLSTM B-MethodName
0.456 O
0.473 O
0.464 O
extendedESIM O
0.561 O
0.503 O
0.454 O
featureSVM B-MethodName
0.384 O
0.396 O
0.390 O
DecompAttent O
0.336 O
0.312 O
0.324 O
Table O
9 O
: O
Claim O
validation O
results O
( O
m O
= O
macro O
) O
where O
systems O
reach O
up O
to O
0.7 O
F1 B-MetricName
macro O
, O
the O
scores B-MetricName
are O
relatively O
low O
. O

It O
encodes O
the O
parser O
’s O
conﬁguration O
( O
buffer O
, O
stack O
and O
intermediate O
graph O
structure O
) O
using O
BiLSTMs B-MethodName
, O
and O
predicts O
the O
next O
transition O
using O
an O
MLP O
, O
stacked O
on O
top O
of O
them O
. O

Token O
- O
based O
features O
, O
including O
POS B-TaskName
tags O
, O
dependency O
parses O
, O
as O
well O
as O
NER B-TaskName
and O
orthographic O
features O
, O
are O
embedded O
in O
the O
BiLSTM B-MethodName
. O

4.2 O
Pipeline O
We O
extend O
TUPA O
by O
providing O
the O
SNACS O
label O
as O
a O
feature O
on O
the O
adposition O
token.9This O
is O
added O
in O
preprocessing O
in O
the O
same O
way O
as O
the O
syntactic O
features O
listed O
above O
( O
including O
the O
BiLSTM B-MethodName
encoding O
) O
. O

At O
testing O
time O
, O
we O
obtain O
SNACS O
labels O
for O
automatically O
identiﬁed O
targets O
from O
the O
SVM B-MethodName
model O
of O
Schneider O
et O
al O
. O
( O
2018 O
) O
. O

This O
is O
the O
multitask O
learning O
( O
MTL O
) O
setup O
from O
Hershcovich O
et O
al O
. O
( O
2018 O
) O
, O
where O
separate O
transition O
classiﬁers O
are O
trained O
on O
different O
tasks O
simultaneously O
, O
sharing O
and O
mutually O
updating O
the O
BiLSTM B-MethodName
encoding.10We O
consider O
as O
auxiliary O
tasks O
( O
a O
) O
SNACS O
scene O
role O
classiﬁcation O
and O
( O
b O
) O
the O
decision O
of O
which O
UCCA O
unit O
is O
reﬁned O
by O
a O
SNACS O
- O
annotated O
token O
. O

Its O
input O
features O
are O
the O
same O
as O
for O
the O
transition O
classiﬁer O
, O
including O
the O
BiLSTM B-MethodName
encoding O
. O

Since O
the O
two O
classiﬁers O
alternate O
in O
making O
forward O
passes O
and O
updating O
the O
shared O
BiLSTM B-MethodName
, O
they O
indirectly O
contribute O
to O
each O
other O
’s O
input O
. O

For O
classifying O
the O
next O
transition O
, O
TUPA O
uses O
a O
multi O
- O
layer O
perceptron O
( O
MLP O
) O
with O
2 B-HyperparameterValue
hidden B-HyperparameterName
layers I-HyperparameterName
and O
a O
softmax O
output O
layer O
, O
on O
top O
of O
a O
2 O
- O
layer O
BiLSTM B-MethodName
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
) O
. O

The O
SNACS O
baseline O
system O
is O
the O
SVM B-MethodName
classiﬁer O
of O
Schneider O
et O
al O
. O
( O
2018 O
) O
. O

State O
- O
of O
- O
the O
- O
art O
results O
on O
UCCA O
parsing O
and O
SNACS O
disambiguation O
are O
described O
in O
contemporaneous O
work O
by O
Jiang O
et O
al O
. O
( O
2019 O
) O
; O
Liu O
et O
al O
. O
( O
2019 O
) O
, O
who O
achieve O
substantial O
gains O
using O
the O
ELMo B-MethodName
and O
BERT B-MethodName
contextualized O
word O
embeddings O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

Following O
Conneau O
et O
al O
. O
( O
2018 O
) O
, O
we O
report O
results O
with O
a O
set O
of O
CBOW B-MethodName
embeddings O
trained O
on O
the O
WaCky O
corpus O
( O
Barone O
, O
2016 O
) O
, O
and O
with O
Wikipedia O
embeddings O
. O

For O
interpretability O
, O
we O
query O
the O
system O
with O
words O
in O
Bosnian O
and O
seek O
their O
nearest B-MethodName
neighbors I-MethodName
in O
the O
English O
embedding O
space O
. O

The O
attention O
- O
based O
NMT O
system O
is O
implemented O
as O
an O
encoder O
- O
decoder O
framework O
with O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNN O
) O
, O
which O
can O
be O
Gated O
Recurrent O
Unit O
( O
GRU O
) O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
or O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
networks O
in O
practice O
. O

Conceptually O
, O
GAN O
consists O
of O
two O
“ O
adversarial O
” O
models O
: O
a O
generator O
Gthat O
captures O
the O
data O
distribution O
, O
and O
a O
discriminator O
Dthat O
estimates O
the O
probability O
that O
a O
sample O
is O
sampled O
from O
the O
training O
data O
rather O
than O
from O
G. O
When O
GAN O
is O
used O
for O
NMT O
, O
NMT O
model O
is O
employed O
as O
G O
, O
and O
CNN O
- O
based O
or O
RNN O
- O
based O
neural B-MethodName
networks I-MethodName
serve O
as O
D O
( O
Yang O
et O
al O
. O
, O
2017 O
; O
Wu O
et O
al O
. O
, O
2017 O
) O
. O

Unlike O
Sorokin O
( O
2016 O
) O
and O
Ahlberg O
et O
al O
. O
( O
2014 O
) O
, O
who O
applied O
a O
ﬁnite O
state O
machine O
to O
extract O
the O
LCS O
and O
Ahlberg O
et O
al O
. O
( O
2015 O
) O
who O
used O
an O
SVM B-MethodName
classiﬁer O
, O
we O
made O
use O
of O
a O
sequence O
alignment O
function O
provided O
by O
Biopython1 O
. O

For O
90out O
of O
103 O
languages O
a O
neural O
network O
yielded O
the O
best O
results O
on O
the O
development O
- O
set O
followed O
by O
the O
Decision O
Tree O
( O
8languages O
) O
, O
Support O
Vector O
Machines O
( O
3languages O
) O
, O
Random B-MethodName
Forest I-MethodName
( O
1language O
) O
, O
and O
Logistic B-MethodName
Regression I-MethodName
( O
1language O
) O
algorithm O
. O

Gulordava O
et O
al O
. O
: O
A O
successful O
variant O
of O
RNNs O
, O
the O
long O
short O
- O
term O
memory O
model O
( O
LSTM B-MethodName
, O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
used O
for O
syntactic O
error O
detection O
in O
Gulordava O
et O
al O
. O
( O
2018 O
) O
. O

BERT B-MethodName
: O
A O
recent O
bidirectional O
encoder O
representations O
from O
transformers O
( O
BERT B-MethodName
) O
LM O
released O
by O
Google O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

BERT B-MethodName
performs O
best O
overall O
, O
and O
both O
it O
and O
Google O
1B O
exceed O
the O
baseline O
for O
learners O
, O
but O
BERT B-MethodName
performs O
only O
at O
baseline O
for O
advanced O
L2s O
, O
conﬁrming O
the O
extreme O
difﬁculty O
of O
this O
task O
. O

To O
address O
this O
question O
, O
we O
test O
BERT B-MethodName
for O
infelicitous O
choices O
compared O
to O
annotators O
’ O
decisions O
: O
That O
is O
, O
for O
each O
sentence O
, O
we O
compare O
the O
pronoun O
preferred O
by O
the O
model O
to O
the O
gold O
annotation O
. O

Table O
8 O
presents O
statistics O
across O
usage O
classes O
, O
for O
learners O
and O
advanced O
L2s O
( O
taken O
from O
Table O
5 O
) O
, O
as O
well O
as O
for O
BERT B-MethodName
. O

The O
model O
also O
hasLearners O
Infelicitous O
class O
Correct O
class O
model O
P O
R O
F1 B-MetricName
P O
R O
F1 B-MetricName
acc0:8Gulordava O
et O
al O
. O
( O
trained O
on O
Reddit O
) O
0.437 B-MetricValue
0.573 O
0.496 O
0.920 O
0.870 O
0.894 O
0.825 O
Google O
1B O
( O
pre O
- O
trained O
) O
0.500 O
0.686 O
0.578 O
0.946 O
0.889 O
0.917 O
0.861 O
BERT B-MethodName
( O
pre O
- O
trained O
) O
0.602 O
0.736 O
0.673 O
0.956 O
0.911 O
0.933 O
0.889= O
1Gulordava O
et O
al O
. O
( O
trained O
on O
Reddit O
) O
0.499 O
0.652 O
0.565 O
0.954 O
0.916 O
0.935 O
0.887 O
Google O
1B O
( O
pre O
- O
trained O
) O
0.523 O
0.720 O
0.606 O
0.970 O
0.932 O
0.950 O
0.912 O
BERT B-MethodName
( O
pre O
- O
trained O
) O
0.681 O
0.859 O
0.759 O
0.981 O
0.949 O
0.965 O
0.939 O
Table O
6 O
: O
Automatic O
detection O
of O
infelicities O
in O
learner O
data O
( O
sentences O
where O
annotation O
disagrees O
with O
author O
usage O
of O
IP O
) O
, O
with O
conﬁdence O
level O
0:8(top O
) O
, O
and O
with O
conﬁdence O
level O
= O
1(bottom O
) O
. O

Advanced O
L2s O
Infelicitous O
class O
Correct O
class O
model O
P O
R O
F1 B-MetricName
P O
R O
F1 B-MetricName
acc0:8Gulordava O
et O
al O
. O
( O
trained O
on O
Reddit O
) O
0.274 B-MetricValue
0.583 O
0.373 O
0.959 O
0.863 O
0.908 O
0.840 O
Google O
1B O
( O
pre O
- O
trained O
) O
0.380 O
0.704 O
0.494 O
0.976 O
0.912 O
0.943 O
0.898 O
BERT B-MethodName
( O
pre O
- O
trained O
) O
0.506 O
0.701 O
0.585 O
0.972 O
0.938 O
0.955 O
0.919= O
1Gulordava O
et O
al O
. O
( O
trained O
on O
Reddit O
) O
0.219 O
0.690 O
0.332 O
0.984 O
0.886 O
0.932 O
0.877 O
Google O
1B O
( O
pre O
- O
trained O
) O
0.380 O
0.760 O
0.507 O
0.988 O
0.942 O
0.964 O
0.934 O
BERT B-MethodName
( O
pre O
- O
trained O
) O
0.503 O
0.790 O
0.614 O
0.990 O
0.964 O
0.977 O
0.956 O
Table O
7 O
: O
Automatic O
detection O
of O
infelicities O
in O
advanced O
L2 O
data O
( O
sentences O
where O
annotation O
disagrees O
with O
author O
usage O
of O
IP O
) O
, O
with O
conﬁdence O
level O
0:8(top O
) O
, O
and O
with O
conﬁdence O
level O
= O
1 O
( O
bottom O
) O
. O

DN O
QU O
CD O
CP O
MIXED O
learners O
8.1 O
24.0 O
12.4 O
8.9 O
8.7 O
BERT B-MethodName
0.8 O
6.1 O
3.6 O
4.0 O
2.2 O
advanced O
L2s O
5.3 O
7.1 O
9.1 O
5.1 O
5.7 O
BERT B-MethodName
1.3 O
2.5 O
2.7 O
1.6 O
1.5 O
Table O
8 O
: O
Distribution O
of O
% O
of O
infelicities O
( O
difference O
from O
gold O
annotation O
) O
across O
classes O
for O
humans O
and O
for O
BERT B-MethodName
on O
the O
corresponding O
data O
. O

The O
recent O
BERT B-MethodName
model O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
has O
been O
shown O
to O
be O
highly O
effective O
for O
detection O
of O
syntactic O
anomalies O
stemming B-TaskName
from O
subject O
- O
verb O
disagreement O
( O
Goldberg O
, O
2019 O
) O
. O

They O
performed O
deep O
multi O
- O
modal O
fusion O
using O
a O
deep O
belief O
network O
that O
learnt O
non O
- O
linear O
relations O
across O
modalities O
and O
then O
used O
a O
linear O
SVM B-MethodName
to O
classify O
emotions O
. O

Similarly O
, O
Vielzeuf O
et O
al O
. O
( O
2017 O
) O
explored O
VGG O
- O
LSTM B-MethodName
and O
3DCNN O
- O
LSTM B-MethodName
architectures O
and O
introduced O
a O
weighted B-HyperparameterName
score B-MetricName
to O
prioritise O
the O
most O
relevant O
windows O
during O
learning O
. O

The O
analysis O
window O
of O
the O
global O
memory O
is O
limited O
to O
the O
current O
and O
previous O
cell O
memories O
of O
each O
LSTM B-MethodName
encoder O
, O
whereas O
our O
approach O
abstracts O
the O
shared O
memory O
update O
dynamics O
away O
from O
the O
ties O
of O
the O
encoding O
dynamics O
. O

In O
early O
fusion O
, O
encodersModel O
Modality O
Accuracy O
Human O
performance O
Audio O
40:9 O
COV O
AREP O
Features O
+ O
LSTM B-MethodName
Decoder O
Audio O
41:5 O
OpenFace O
Features O
+ O
LSTM B-MethodName
Decoder O
Vision O
52:5 O
Human O
performance O
Vision O
58:2 O
Human O
performance O
Vision+Audio O
63:6 O
( O
OpenFace O
features O
+ O
LSTM B-MethodName
) O
+ O
( O
COV O
AREP O
Features O
+ O
LSTM B-MethodName
) O
+ O
Dual O
Attention O
Vision+Audio O
65:0 O
Table O
1 O
: O
Results O
on O
the O
CREMA B-MetricName
- O
D O
dataset O
across O
8 O
emotions O
Modality O
Feature O
Encoder O
Attention O
Accuracy O
Audio O
COV O
AREP O
LSTM B-MethodName
Nil O
41:25 O
Vision O
OpenFace O
LSTM B-MethodName
Nil O
52:08 O
Audio O
+ O
Vision O
COV O
AREP O
, O
OpenFace O
LSTM B-MethodName
GCA O
58:33 O
Table O
2 O
: O
Results O
on O
the O
RA O
VDESS O
dataset O
across O
8 O
emotions O
for O
normal O
speech O
mode O
1 O
2 O
3 O
4audio O
vision O
Time O
( O
seconds)I O
thinkﬁnances O
forsomereasonshasbeenaverydelicate O
issue O
betweencouples O
. O

It O
can O
be O
seen O
that O
the O
GCA O
emotion O
recognitionModality O
Feature O
Encoder O
Attention O
/ O
Fusion O
Corruption O
MSE O
MAE B-MetricName
WA O
Text O
( O
T O
) O
Word O
- O
vec O
LSTM B-MethodName
Nil O
Nil O
0:6326 O
0 B-MetricValue
: O
9830 O
0 O
: O
5485 O
Audio O
( O
A O
) O
COV O
AREP O
LSTM B-MethodName
Nil O
Nil O
0:6049 O
1 O
: O
0562 O
0 O
: O
5249 O
Vision O
( O
V O
) O
FACET O
LSTM B-MethodName
Nil O
Nil O
0:5026 O
0 O
: O
9909 O
0 O
: O
5476 O
T+A+V O
COV O
AREP O
, O
FACET O
, O
Word O
- O
vec O
LSTM B-MethodName
Early O
fusion O
Nil O
0:5319 O
0 O
: O
7694 O
0 O
: O
5188 O
T+A+V O
COV O
AREP O
, O
FACET O
, O
Word O
- O
vec O
LSTM B-MethodName
Late O
fusion O
Nil O
0:5047 O
0 O
: O
9825 O
0 O
: O
5889 O
T+A+V O
COV O
AREP O
, O
FACET O
, O
Word O
- O
vec O
LSTM B-MethodName
GCA O
Nil O
0:4696 O
0 O
: O
9412 O
0 O
: O
6163 O
T+A+V O
COV O
AREP O
, O
FACET O
, O
Word O
- O
vec O
LSTM B-MethodName
GCA O
Vision O
0:5034 O
0 O
: O
9920 O
0 O
: O
6068 O
T+A+V O
COV O
AREP O
, O
FACET O
, O
Word O
- O
vec O
LSTM B-MethodName
GCA O
+ O
Gating O
Nil O
0:4691 O
0 O
: O
8705 O
0 O
: O
5765 O
T+A+V O
COV O
AREP O
, O
FACET O
, O
Wrod O
- O
vec O
LSTM B-MethodName
GCA O
+ O
Gating O
Vision O
0:4742 O
0 O
: O
8857 O
0 O
: O
5688 O
Table O
3 O
: O
Results O
on O
CMU O
- O
MOSEI O
dataset O
system O
was O
trained O
to O
attend O
dynamically O
to O
features O
of O
varying O
importance O
across O
the O
time O
, O
unlike O
systems O
performing O
early O
or O
late O
fusion O
. O

Zeng O
et O
al O
. O
( O
2014 O
, O
2015 O
) O
used O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNN O
) O
with O
max O
- O
pooling O
to O
ﬁnd O
the O
relation O
between O
two O
given O
entities O
. O

We O
use O
a O
bi O
- O
directional O
long O
short O
- O
term O
memory O
( O
Bi O
- O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
layer O
to O
capture O
the O
interaction O
among O
words O
in O
a O
sentenceS O
= O
fw1;w2;:::::;w O
ng O
, O
wherenis O
the O
sentence O
length O
. O

The O
input O
to O
this O
layer O
is O
the O
concatenated O
vector O
x2Rdw+dzof O
word O
embedding O
vector O
wand O
entity O
token O
indicator O
embedding O
vector O
z. O
xt O
= O
wtjjzt O
  O
! O
ht=    !LSTM B-MethodName
( O
xt;ht 1 O
) O
 O
 ht= O
    LSTM B-MethodName
( O
xt;ht+1 O
) O
ht=  O
! O
htjj O
 ht  O
! O
ht2Rdw+dzand O
 ht2Rdw+dzare O
the O
output O
at O
thetth O
step O
of O
the O
forward O
LSTM B-MethodName
and O
backward O
LSTM B-MethodName
respectively O
. O

We O
concatenate O
them O
to O
obtain O
thetth O
Bi O
- O
LSTM B-MethodName
output O
ht2R2(dw+dz O
) O
. O

We O
concatenate O
the O
positional O
embeddings O
u1andu2of O
words O
with O
the O
hidden O
representation O
of O
the O
Bi O
- O
LSTM B-MethodName
layer O
and O
use O
the O
convolution O
operation O
with O
max O
- O
pooling O
on O
concatenated O
vectors O
to O
extract O
the O
global O
feature O
vector O
. O

We O
adopt O
a O
simple O
linear O
function O
as O
follows O
to O
measure O
the O
semantic O
similarity O
of O
words O
with O
the O
given O
entities O
: O
f1 O
score(hi;v1 B-MetricName
e O
) O
= O
hT O
iW1 O
av1 O
e O
f2 O
score(hi;v2 B-MetricName
e O
) O
= O
hT O
iW2 O
av2 O
e O
hiis O
the O
Bi O
- O
LSTM B-MethodName
hidden O
representation O
of O
the O
ith O
word O
. O

The O
hidden B-HyperparameterName
layer I-HyperparameterName
dimension O
of O
the O
forward O
and O
backward O
LSTM B-MethodName
is O
60 B-HyperparameterValue
, O
which O
is O
the O
same O
as O
the O
dimension O
of O
input O
word O
representation O
vector O
x. O
The O
dimension O
of O
Bi O
- O
LSTM B-MethodName
output O
is O
120 O
. O

F1 B-MetricName
CNN O
( O
Zeng O
et O
al O
. O
, O
2014 B-MetricValue
) O
0.413 O
0.591 O
0.486 O
0.444 O
0.625 O
0.519 O
PCNN O
( O
Zeng O
et O
al O
. O
, O
2015 O
) O
0.380 O
0.642 O
0.477 O
0.446 O
0.679 O
0.538y O
EA O
( O
Shen O
and O
Huang O
, O
2016 O
) O
0.443 O
0.638 O
0.523y0.419 O
0.677 O
0.517 O
BGWA O
( O
Jat O
et O
al O
. O
, O
2017 O
) O
0.364 O
0.632 O
0.462 O
0.417 O
0.692 O
0.521 O
BiLSTM B-MethodName
- O
CNN O
0.490 O
0.507 O
0.498 O
0.473 O
0.606 O
0.531 O
Our O
model O
0.541 O
0.595 O
0.566 O
* O
0.507 O
0.652 O
0.571 O
* O
Table O
2 O
: O
Performance O
comparison O
of O
different O
models O
on O
the O
two O
datasets O
. O

( O
5 O
) O
BiLSTM B-MethodName
- O
CNN O
: O
This O
is O
our O
own O
baseline O
. O

They O
are O
passed O
to O
a O
bidirectional O
LSTM B-MethodName
. O

Hidden O
representations O
of O
the O
LSTMs B-MethodName
are O
concatenated O
with O
two O
positional O
embeddings O
. O

When O
we O
add O
multi O
- O
factor O
attention O
to O
the O
baseline O
BiLSTM B-MethodName
- O
CNN O
model O
without O
the O
dependency O
distance O
- O
based O
weight B-HyperparameterName
factor O
in O
the O
attention O
mechanism O
, O
we O
get O
0:8%F1 B-MetricName
score B-MetricName
improvement O
( O
A2 A1 O
) O
. O

These O
max O
- O
pooled O
attention O
scores B-MetricName
are O
used O
to O
obtain O
the O
weighted B-HyperparameterName
average B-MetricName
vector O
of O
Bi O
- O
LSTM B-MethodName
hidden O
vectors O
. O

F1 B-MetricName
( O
A1 O
) O
BiLSTM B-MethodName
- O
CNN O
0.473 B-MetricValue
0.606 O
0.531 O
( O
A2 O
) O
Standard O
attention O
0.466 O
0.638 O
0.539 O
( O
A3 O
) O
Window O
size O
( O
ws O
) O
= O
5 O
0.507 O
0.652 O
0.571 O
( O
A4 O
) O
Window O
size O
( O
ws O
) O
= O
10 O
0.510 O
0.640 O
0.568 O
( O
A5 O
) O
Softmax O
0.490 O
0.658 O
0.562 O
( O
A6 O
) O
Max O
- O
pool O
0.492 O
0.600 O
0.541 O
Table O
4 O
: O
Effectiveness O
of O
model O
components O
( O
m= O
4 O
) O
on O
the O
NYT11 O
dataset O
. O

Xu O
et O
al O
. O
( O
2015 O
) O
and O
Miwa O
and O
Bansal O
( O
2016 O
) O
used O
an O
LSTM B-MethodName
network O
and O
the O
shortest O
dependency O
path O
between O
two O
entities O
to O
ﬁnd O
the O
relation O
between O
them O
. O

Attention O
- O
based O
neural B-MethodName
networks I-MethodName
are O
quite O
successful O
for O
many O
other O
NLP O
tasks O
. O

2 O
Convolutional O
Seq2Seq O
Learning O
Usually O
encoder O
and O
decoder O
in O
Seq2Seq O
architecture O
are O
recurrent O
neural O
network(RNN O
) O
or O
its O
variants O
like O
Long O
Short O
Term O
Memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
and O
Gated O
Recurrent O
Unit O
( O
GRU O
) O
( O
Chung O
et O
al O
. O
, O
2014 O
) O
network O
. O

Recently O
, O
a O
Seq2Seq O
architecture O
based O
entirely O
on O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNN O
Seq2Seq O
) O
( O
Gehring O
et O
al O
. O
, O
2017 O
) O
was O
proposed O
. O

To O
gain O
more O
insight O
into O
what O
information O
LSTMs B-MethodName
base O
their O
decisions O
on O
, O
we O
propose O
a O
generalisation O
ofContextual O
Decomposition O
( O
GCD O
) O
. O

1 O
Introduction O
Modern O
language O
models O
that O
use O
deep O
learning O
architectures O
such O
as O
LSTMs B-MethodName
, O
bi O
- O
LSTMs B-MethodName
and O
Transformers O
, O
have O
shown O
enormous O
gains O
in O
performance O
in O
the O
last O
few O
years O
and O
are O
ﬁnding O
applications O
in O
novel O
domains O
, O
ranging O
from O
speech O
recognition O
and O
writing O
assistance O
to O
autonomous O
generation O
of O
fake O
news O
. O

An O
inﬂuential O
paper O
using O
this O
approach O
was O
presented O
by O
Linzen O
et O
al O
. O
( O
2016 O
) O
, O
who O
investigated O
the O
performance O
of O
an O
LSTM B-MethodName
- O
based O
language O
model O
on O
number O
agreement O
. O

We O
derive O
equations O
for O
GCD O
for O
the O
case O
of O
a O
unidirectional O
( O
one O
or O
multi O
- O
layer O
) O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
and O
use O
the O
method O
to O
analyse O
how O
a O
language O
model O
processes O
two O
different O
phenomena O
: O
number O
agreement O
and O
gendered O
pronoun O
resolution O
. O

Furthermore O
, O
GCD O
enables O
us O
to O
investigate O
pronoun O
resolution O
in O
a O
way O
that O
has O
not O
been O
done O
before O
: O
by O
delving O
into O
the O
model O
reasoning O
we O
are O
able O
to O
accurately O
pinpoint O
where O
and O
how O
this O
resolution O
takes O
place.1 O
2 O
Network O
analysis O
methods O
Recently O
, O
methods O
to O
open O
the O
blackbox O
of O
deep O
neural B-MethodName
networks I-MethodName
have O
become O
an O
important O
research O
area O
( O
see O
Poerner O
et O
al O
. O
, O
2018 O
; O
Belinkov O
and O
Glass O
, O
2019 O
, O
for O
recent O
reviews O
of O
proposed O
methods O
in O
NLP O
) O
. O

An O
example O
of O
this O
approach O
is O
Giulianelli O
et O
al O
. O
( O
2018 O
) O
, O
who O
trained O
linear O
diagnostic O
classiﬁers O
on O
all O
layers O
and O
gate O
activations B-HyperparameterName
of O
an O
LSTM B-MethodName
to O
predict O
the O
number O
of O
the O
subject O
that O
the O
verb O
, O
occurring O
later O
in O
the O
sentence O
, O
needs O
to O
agree O
with O
( O
i.e. O
the O
number O
- O
agreement O
task O
) O
. O

An O
example O
of O
a O
gradient O
- O
based O
method O
is O
Arras O
et O
al O
. O
( O
2017 O
) O
, O
who O
adapt O
Layer O
- O
wise O
Relevance O
Propagation O
( O
LRP O
, O
Bach O
et O
al O
. O
, O
2015 O
) O
to O
the O
case O
of O
LSTMs B-MethodName
. O

The O
key O
idea O
is O
to O
run O
the O
LSTM B-MethodName
on O
each O
1We O
have O
integrated O
all O
our O
code O
in O
diagnnose O
( O
Jumelet O
and O
Hupkes O
, O
2019 O
) O
, O
a O
well O
- O
documented O
analysis O
library O
which O
facilitate O
the O
diagnosis O
of O
neural O
network O
activations B-HyperparameterName
: O
github.com/i-machine-think/diagnnose O
.input O
of O
interest O
( O
the O
forward O
pass O
) O
, O
then O
deﬁne O
a O
relevance O
vector O
at O
the O
output O
layer O
and O
propagate O
that O
relevance O
backwards O
through O
the O
network O
. O

An O
alternative O
data O
driven O
method O
, O
and O
the O
one O
that O
we O
will O
expand O
on O
in O
this O
paper O
, O
is O
Contextual O
Decomposition O
for O
LSTMs B-MethodName
( O
CD O
, O
Murdoch O
et O
al O
. O
, O
2018 O
) O
. O

denotes O
an O
individual O
interaction O
; O
green O
interactions O
are O
added O
the O
 O
part O
and O
red O
interactions O
to O
 O
.V O
, O
W O
, O
andDrepresent O
the O
linear O
projections O
of O
the O
LSTM B-MethodName
itself O
. O

3 O
Generalised O
Contextual O
Decomposition O
In O
this O
particular O
study O
, O
we O
consider O
the O
LSTM B-MethodName
language O
model O
that O
was O
made O
available O
by O
Gulordava O
et O
al O
. O
( O
2018 O
) O
. O

This O
language O
model O
( O
LM O
) O
is O
a O
2 O
- O
layer O
LSTM B-MethodName
with O
650 O
hidden B-HyperparameterName
units I-HyperparameterName
in O
both O
layers O
, O
trained O
on O
a O
corpus O
with O
Wikipedia O
data O
. O

Given O
the O
relevance O
of O
the O
speciﬁc O
LSTMdynamics B-MethodName
for O
the O
understanding O
of O
the O
main O
method O
of O
our O
paper O
, O
we O
repeat O
the O
equations O
that O
describe O
it O
below O
. O

CD O
To O
compute O
the O
contributions O
of O
one O
or O
multiple O
input O
tokens O
( O
said O
to O
be O
in O
focus O
) O
to O
the O
output O
of O
an O
LSTM B-MethodName
cell O
, O
Murdoch O
et O
al O
. O
( O
2018 O
) O
divide O
each O
cell O
and O
hidden O
state O
into O
a O
sum O
of O
two O
parts O
: O
a O
 O
part O
, O
which O
contains O
the O
part O
of O
this O
particu O
- O
lar O
state O
that O
stems O
from O
inside O
this O
phrase O
, O
and O
a O
 O
part O
, O
which O
contains O
information O
coming O
from O
words O
outside O
this O
phrase O
. O

In O
a O
multi O
- O
layer O
LSTM B-MethodName
, O
 O
and O
 O
parts O
are O
not O
only O
propagated O
forward O
, O
but O
also O
upward O
, O
where O
they O
are O
added O
to O
their O
respective O
parts O
in O
the O
layer O
above O
them O
. O

For O
initialisation O
 O
is O
set O
to O
a O
zero O
vector O
, O
and O
 O
is O
set O
to O
the O
initial O
LSTM B-MethodName
states.4 O
Generalising O
CD O
While O
Murdoch O
et O
al O
. O
( O
2018 O
) O
consider O
only O
one O
way O
of O
partitioning O
interactions O
between O
inside O
and O
outside O
components O
, O
their O
setup O
can O
be O
quite O
easily O
generalised O
to O
also O
allow O
other O
interactions O
to O
be O
included O
in O
the O
inside O
terms O
 O
. O

4 O
Experimental O
setup O
We O
use O
GCD O
to O
study O
how O
our O
LSTM B-MethodName
model O
handles O
two O
different O
linguistic O
phenomena O
: O
subjectverb O
agreement O
and O
anaphora O
resolution O
in O
relation O
to O
gender O
. O

This O
higher O
frequency O
is O
in O
fact O
represented O
in O
the O
decoder O
intercept O
, O
which O
is O
higher O
on O
average B-MetricName
for O
plural O
verbs O
, O
but O
it O
is O
surprising O
that O
the O
LSTM B-MethodName
weights B-HyperparameterName
encode O
a O
default O
for O
the O
minority O
class O
. O

7 O
Conclusion O
We O
propose O
a O
generalised O
version O
of O
Contextual O
Decomposition O
( O
Murdoch O
et O
al O
. O
, O
2018 O
) O
– O
GCD O
– O
that O
allows O
to O
study O
speciﬁcally O
selected O
interactions O
of O
components O
in O
an O
LSTM B-MethodName
language O
model O
. O

The O
axiomatic O
approach O
of O
Montavon O
( O
2019 O
) O
could O
provide O
further O
insight O
into O
how O
GCD O
relates O
to O
other O
explanation O
methods O
, O
and O
we O
are O
conﬁdent O
that O
combining O
the O
strengths O
of O
GCD O
with O
that O
of O
other O
frameworks O
will O
ultimately O
lead O
to O
a O
more O
robust O
andfaithful O
insight O
into O
deep O
neural B-MethodName
networks I-MethodName
. O

Concretely O
, O
at O
timestep O
t O
, O
the O
probability O
distribution O
over O
edits O
atis O
computed O
with O
a O
softmax O
classiﬁer O
: O
P(at|a O
< O
t O
, O
x O
) O
= O
softmax O
/ O
parenleftbig O
W·st+b O
/ O
parenrightbig O
, O
( O
1 O
) O
where O
Wandbare O
classiﬁer O
weights B-HyperparameterName
and O
stis O
the O
output O
of O
the O
long O
short O
- O
term O
memory O
cell O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
, O
LSTM B-MethodName
): O
st O
= O
LSTM B-MethodName
( O
ct−1,/bracketleftbig O
E(at−1);hi;f O
/ O
bracketrightbig O
) O
. O

( O
2 O
) O
Vector O
ct−1denotes O
the O
previous O
hidden O
state O
, O
E(at−1)is O
the O
embedding O
of O
the O
previous O
edit O
, O
f O
is O
the O
embedding O
of O
the O
morpho O
- O
syntactic O
description O
( O
MSD O
) O
, O
and O
hiis O
the O
encoding O
of O
an O
inputcharacterxiwith O
a O
bidirectional O
LSTM B-MethodName
( O
Graves O
and O
Schmidhuber O
, O
2005 O
) O
. O

For O
track O
2 O
of O
Task O
II O
, O
the O
context O
vector O
is O
a O
concatenation O
of O
the O
character O
LSTM B-MethodName
encodings O
of O
the O
words O
to O
the O
immediate O
right O
and O
left O
. O

Following O
Bayes O
Rule O
, O
the O
adaptive O
agent O
’s O
beliefs O
abouti O
, O
conditioning O
on O
observations O
Difrom O
the O
shared O
history O
of O
interactions O
in O
that O
context O
, O
are O
: O
P(ijDi;)/P(Diji)P(ij O
) O
( O
1 O
) O
1Traditionally O
, O
this O
semantic O
function O
is O
truth O
- O
conditional O
, O
mapping O
utterance O
- O
state O
pairs O
to O
Boolean O
values O
, O
but O
recent O
approaches O
have O
shifted O
to O
more O
graded O
, O
real O
- O
valued O
functions O
such O
as O
those O
implemented O
by O
neural B-MethodName
networks I-MethodName
. O
This O
formulation O
decomposes O
the O
inference O
into O
two O
terms O
, O
a O
prior O
term O
P(ij)and O
a O
likelihood O
termP(Diji).2The O
prior O
captures O
the O
idea O
that O
different O
partners O
share O
some O
general O
features O
of O
the O
semantics O
, O
represented O
by O
 O
, O
since O
they O
speak O
the O
same O
language O
; O
in O
the O
absence O
of O
partner O
- O
speciﬁc O
information O
, O
the O
agent O
ought O
to O
be O
regularized O
toward O
this O
background O
knowledge O
. O

Concretely O
, O
we O
consider O
an O
architecture O
that O
combines O
a O
convolutional O
visual O
encoder O
( O
ResNet-152 O
) O
with O
an O
LSTM B-MethodName
decoder O
( O
Vinyals O
et O
al O
. O
, O
2015 O
) O
. O

The O
LSTM B-MethodName
takes O
a O
300 O
- O
dimensional O
embedding O
as O
input O
for O
each O
word O
in O
an O
utterance O
and O
its O
output O
is O
linearly O
projected O
back O
to O
a O
softmax O
distribution O
over O
the O
vocabulary O
size O
. O

The O
CNN O
- O
LSTM B-MethodName
architecture O
allows O
an O
agent O
to O
select O
utterances O
, O
by O
using O
beam O
search O
over O
captions O
given O
a O
target O
image O
as O
input O
, O
and O
also O
to O
select O
objects O
from O
the O
context O
, O
by O
evaluating O
the O
likelihood O
of O
the O
caption O
for O
each O
image O
in O
context O
and O
taking O
the O
most O
likely O
one O
. O

Using O
the O
pre O
- O
trained O
model O
as O
our O
initialization B-HyperparameterName
, O
we O
can O
ﬁne O
- O
tune O
the O
decoder O
weights B-HyperparameterName
( O
i.e. O
word O
embeddings O
, O
LSTM B-MethodName
, O
and O
linear O
output O
layer O
) O
within O
a O
particular O
communicative O
interaction O
. O

Fine O
- O
tuning O
an O
LSTM B-MethodName
architecture O
will O
increase O
the O
likelihood O
of O
sub O
- O
strings O
to O
some O
extent O
after O
a O
successful O
selection O
, O
but O
this O
is O
insufﬁcient O
for O
two O
reasons O
. O

First O
, O
not O
all O
sub O
- O
strings O
are O
syntactically O
well O
- O
formed O
referring O
expressions O
( O
e.g. O
“ O
two O
men O
are O
” O
) O
, O
and O
the O
LSTM B-MethodName
lacks O
a O
syntactic O
representation O
to O
represent O
such O
coherence O
. O

We O
then O
used O
these O
features O
to O
partition O
the O
images O
into O
100 O
groups O
using O
ak O
- O
means B-MetricName
algorithm O
, O
sampled O
one O
image O
from O
each O
cluster O
, O
and O
took O
its O
3 O
nearest B-MethodName
neighbors I-MethodName
in O
feature O
space O
, O
yielding O
100 O
unique O
contexts O
of O
4 O
images O
each O
. O

In O
a O
mixed O
- O
effects O
logistic B-MethodName
regression I-MethodName
predicting O
trial O
- O
level O
accuracy B-MetricName
, O
including O
pair- O
and O
image O
- O
level O
random O
effects O
, O
we O
found O
a O
signiﬁcant O
increase O
in O
the O
probability O
of O
a O
correct O
response O
with O
successive O
repetitions O
, O
z= O
12:6 O
; O
p O
< O
0:001 O
, O
from O
37 B-MetricValue
% O
correct O
( O
slightly O
above O
chance O
levels O
of O
25 O
% O
) O
to O
93 O
% O
at O
the O
end O
. O

Third O
, O
scaling O
the O
principles O
of O
computationallevel O
Bayesian O
cognitive O
models O
to O
neural B-MethodName
networks I-MethodName
capable O
of O
adapting O
to O
natural O
language O
in O
practice O
required O
several O
algorithmic O
- O
level O
innovations O
which O
are O
not O
yet O
plausible O
proposals O
for O
human O
cognition O
( O
Marr O
, O
2010 O
) O
. O

Our O
data O
augmentation O
mechanism O
was O
introduced O
speciﬁcally O
to O
compensate O
for O
the O
inability O
of O
the O
LSTM B-MethodName
architecture O
to O
propagate O
the O
use O
of O
a O
referring O
expression O
to O
its O
entailments O
, O
but O
we O
expect O
that O
human O
language O
processing O
mechanisms O
achieve O
this O
effect O
by O
different O
means B-MetricName
. O

Since O
most O
of O
the O
state O
- O
of O
- O
the O
- O
art O
research O
focuses O
on O
NER B-TaskName
for O
modern O
available O
datasets O
, O
the O
performance O
of O
the O
NER B-TaskName
systems O
grew O
at O
a O
fast O
pace O
, O
enabled O
by O
the O
representational O
capacity O
of O
neural B-MethodName
networks I-MethodName
and O
off O
- O
the O
- O
shelf O
pre O
- O
trained O
word O
embeddings O
( O
Ma O
and O
Hovy O
, O
2016 O
; O
Lample O
et O
al O
. O
, O
2016 O
; O
Yadav O
and O
Bethard O
, O
2018 O
) O
. O

More O
recently O
, O
NER B-TaskName
models O
based O
on O
contextual O
word O
and O
subword O
representations O
provided O
by O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
Flair O
( O
Akbik O
et O
al O
. O
, O
2018 O
) O
, O
or O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
achieved O
impressive O
improvements O
. O

The O
Transformer O
- O
based O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
architectures O
for O
NER B-TaskName
became O
popular O
since O
the O
release O
of O
the O
BERT B-MethodName
( O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
) O
model O
. O

To O
address O
these O
challenges O
of O
NER B-TaskName
on O
historical O
documents O
, O
we O
propose O
a O
robust O
NER B-TaskName
model O
based O
on O
a O
stack O
of O
Transformers O
that O
includes O
ﬁne O
- O
tuned O
BERT B-MethodName
encoders O
. O

2 O
Related O
Work O
NER B-TaskName
for O
modern O
documents O
The O
ﬁrst O
end O
- O
toend O
systems O
for O
sequence O
labeling O
tasks O
are O
based O
on O
pre O
- O
trained O
word O
and O
character O
embeddings O
encoded O
either O
by O
a O
bidirectional O
Long O
Short O
Term O
Memory O
( O
BiLSTM B-MethodName
) O
network O
or O
a O
Convolutional O
Neural O
Network O
( O
CNN O
) O
( O
Collobert O
et O
al O
. O
, O
2011 O
; O
Lample O
et O
al O
. O
, O
2016 O
; O
Ma O
and O
Hovy O
, O
2016 O
; O
Aguilar O
et O
al O
. O
, O
2017 O
; O
Chiu O
and O
Nichols O
, O
2016 O
) O
, O
along O
with O
a O
Conditional O
Random O
Fields O
( O
CRF O
) O
decoder O
. O

These O
recent O
large O
- O
scale O
language O
models O
methods O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
further O
enhanced O
the O
performance O
of O
NER B-TaskName
, O
yielding O
state O
- O
of O
- O
the O
- O
art O
performances O
( O
Peters O
et O
al O
. O
, O
2017 O
, O
2018 O
; O
Baevski O
et O
al O
. O
, O
2019 O
) O
. O

The O
BiLSTM B-MethodName
- O
based O
model O
proposed O
by O
Hubkov O
´ O
a O
( O
2019 O
) O
applied O
a O
characterbased O
CNN O
to O
encode O
the O
different O
spellings O
of O
words O
. O

Differently O
, O
we O
introduce O
the O
NER B-TaskName
for O
historical O
documents O
to O
the O
language O
model O
methods O
based O
on O
the O
Transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
methods O
, O
that O
, O
to O
our O
knowledge O
, O
have O
not O
been O
approached O
in O
previous O
research O
, O
with O
regard O
to O
processing O
historical O
documents O
. O

4 O
Model O
We O
based O
our O
NER B-TaskName
model O
on O
the O
pre O
- O
trained O
model O
BERT B-MethodName
proposed O
by O
Devlin O
et O
al O
. O
( O
2019 O
) O
. O

Although O
original O
recommendations O
suggest O
that O
unsupervised O
pre O
- O
training O
of O
BERT B-MethodName
encoders O
are O
expected O
to O
be O
sufﬁciently O
powerful O
on O
modern O
datasets O
, O
we O
consider O
that O
adding O
extra O
Transformer O
layers O
could O
contribute O
to O
the O
alleviation O
of O
word O
errors O
or O
misspellings O
. O

First O
, O
we O
use O
a O
pre O
- O
trained O
BERT B-MethodName
model O
, O
and O
second O
, O
we O
stack O
nTransformer O
blocks O
on O
top O
, O
ﬁnalized O
with O
a O
CRF O
prediction O
layer O
. O

We O
refer O
to O
this O
model O
as O
BERT B-MethodName
+ O
nTransf O
where O
nis O
a O
hyper2Quaero O
guidelines O
3https://www.bnf.fr O
4https://www.onb.ac.at/ O
5The O
main O
difference O
is O
that O
several O
named O
entities O
subtypes O
were O
ignored O
. O

The O
reasons O
for O
using O
BERT B-MethodName
models O
are O
that O
they O
can O
easily O
be O
ﬁne O
- O
tuned O
for O
a O
wide O
range O
of O
tasks O
, O
but O
also O
that O
they O
produce O
high O
- O
performing O
systems O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Conneau O
and O
Lample O
, O
2019 O
; O
Radford O
et O
al O
. O
, O
2018 O
) O
. O

Nonetheless O
, O
despite O
the O
major O
impact O
of O
BERT B-MethodName
in O
the O
NLP O
community O
, O
re7Note O
that O
they O
can O
vary O
as O
multiple O
BERT B-MethodName
- O
based O
models O
are O
available O
for O
different O
languages.searchers O
question O
the O
ability O
of O
this O
model O
to O
deal O
with O
noisy O
text O
( O
Sun O
et O
al O
. O
, O
2020 O
) O
unless O
complementary O
techniques O
are O
used O
( O
Muller O
et O
al O
. O
, O
2019 O
; O
Pruthi O
et O
al O
. O
, O
2019 O
) O
. O

More O
speciﬁcally O
, O
the O
built O
- O
in O
tokenizer O
of O
BERT B-MethodName
ﬁrst O
performs O
simple O
white O
- O
space O
tokenization O
, O
then O
applies O
a O
Byte O
Pair O
Encoding O
( O
BPE O
) O
based O
tokenization O
, O
WordPiece O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
. O

Between O
the O
types O
of O
OCR O
errors O
that O
can O
be O
encountered O
in O
historical O
documents O
, O
the O
character O
insertion O
modiﬁcation O
has O
the O
minimum O
inﬂuence O
( O
Sun O
et O
al O
. O
, O
2020 O
) O
, O
because O
the O
tokenization O
at O
the O
sub O
- O
word O
level O
of O
BERT B-MethodName
would O
not O
change O
much O
in O
some O
cases O
, O
such O
as O
‘ O
practically O
’ O
! O
‘ O
practicaally O
’ O
. O

BERT B-MethodName
has O
been O
demonstrated O
to O
have O
a O
sensitivity O
to O
its O
sub O
- O
word O
segmentation O
when O
it O
comes O
to O
such O
words O
, O
as O
the O
meaning B-MetricName
of O
the O
sub O
- O
words O
can O
diminish O
the O
initial O
meaning B-MetricName
of O
the O
correctly O
spelled O
word O
( O
Sun O
et O
al O
. O
, O
2020 O
) O
. O

Thus O
, O
these O
new O
noisy O
tokens O
could O
inﬂuence O
the O
performance O
of O
BERT B-MethodName
- O
based O
models8 O
. O

On O
top O
of O
BERT B-MethodName
, O
we O
add O
a O
stack O
of O
Transformer O
blocks O
( O
encoders O
) O
. O

Vaswani O
et O
al O
. O
8To O
increase O
the O
chances O
for O
misspelled O
, O
non O
- O
canonical O
, O
or O
new O
words O
to O
be O
recognized O
, O
we O
enrich O
the O
vocabulary O
of O
the O
tokenizer O
with O
these O
tokens O
, O
while O
allowing O
not O
only O
the O
BERT B-MethodName
encoder O
but O
also O
the O
added O
Transformer O
layers O
to O
learn O
them O
from O
scratch O
. O

We O
assume O
that O
the O
additional O
Transformer O
layers O
can O
alleviate O
the O
sensitivity O
of O
the O
built O
- O
in O
tokenizer O
of O
BERT B-MethodName
towards O
OOV O
, O
OCR O
errors O
, O
or O
misspellings O
, O
and O
contribute O
to O
the O
learning O
or O
ﬁnding O
the O
proper O
informative O
words O
around O
entities O
. O

5 O
Experiments O
5.1 O
Baseline O
We O
chose O
as O
a O
baseline O
the O
model O
proposed O
by O
Ma O
and O
Hovy O
( O
2016 O
) O
, O
an O
end O
- O
to O
- O
end O
model O
combining O
a O
BiLSTM B-MethodName
and O
a O
CNN O
character O
encoding O
, O
in O
order O
to O
take O
advantage O
of O
the O
word O
and O
character O
features O
. O

Additionally O
, O
we O
analyze O
the O
aid O
that O
can O
be O
brought O
by O
an O
available O
larger O
dataset O
by O
training O
the O
baseline O
model O
in O
two O
stages O
in O
a O
transfer O
learning O
setting O
, O
similar O
to O
the O
setting O
in O
which O
the O
BERT B-MethodName
encoder O
is O
used O
in O
our O
model O
: O
1.pre O
- O
training O
, O
where O
the O
network O
is O
trained O
on O
a O
larger O
- O
scale O
available O
contemporary O
dataset O
2.ﬁne O
- O
tuning O
, O
where O
the O
pre O
- O
trained O
network O
is O
further O
trained O
on O
the O
historical O
datasets O
The O
modern O
datasets O
are O
the O
following O
: O
For O
French O
, O
we O
use O
the O
fr O
- O
WikiNER12dataset B-TaskName
that O
is O
extracted O
from O
Wikipedia O
articles O
. O

Since O
BERT B-MethodName
is O
able O
to O
consume O
only O
a O
limited O
context O
of O
tokens O
as O
their O
input O
( O
512 O
) O
, O
we O
segment O
the O
articles O
at O
sentence O
- O
level O
. O

Moreover O
, O
for O
the O
BERT B-MethodName
+ O
nTransf O
, O
we O
feed O
the O
model O
with O
batches O
of O
same O
sized O
inputs O
. O

This O
BERT B-MethodName
model O
has O
been O
used O
in O
other O
NER B-TaskName
tasks O
for O
processing O
contemporary O
and O
historical O
German O
documents O
( O
Schweter O
and O
Baiter O
, O
2019 O
; O
Riedl O
and O
Pad O
´ O
o O
, O
2018 O
) O
. O

It O
was O
trained O
using O
a O
large O
collection O
of O
newspapers O
provided O
by O
the O
Europeana O
Library.14 O
For O
the O
French O
NER B-TaskName
, O
we O
rely O
on O
the O
large O
version O
of O
the O
pre O
- O
trained O
CamemBERT B-MethodName
( O
Martin O
et O
al O
. O
, O
2020 O
) O
model O
, O
i.e. O
( O
camembert O
- O
large O
) O
. O

CamemBERT B-MethodName
proposes O
some O
differences O
with O
respect O
to O
other O
BERT B-MethodName
models O
. O

For O
instance O
, O
it O
uses O
whole O
- O
word O
masking O
and O
SentencePiece O
tokenization O
( O
Kudo O
and O
Richardson O
, O
2018 O
) O
instead O
of O
WordPiece O
tokenization O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
as O
the O
original O
BERT B-MethodName
. O

The O
BERT B-MethodName
- O
based O
encoders O
are O
ﬁne O
- O
tuned O
on O
the O
task O
during O
training O
. O

In O
Table O
3 O
, O
the O
results O
for O
the O
baseline O
model O
without O
any O
transfer O
learning O
( O
as O
it O
was O
unnecessary O
) O
are O
presented O
, O
along O
with O
the O
same O
ablation O
study O
for O
the O
BERT B-MethodName
+ O
nTransf O
. O

From O
the O
results O
in O
the O
Table O
2 O
, O
we O
can O
see O
the O
evidence O
that O
the O
BERT B-MethodName
- O
based O
models O
with O
nTransf O
achieve O
, O
for O
both O
datasets O
and O
languages O
, O
higher O
micro O
- O
fuzzy O
andmicro O
- O
strict O
performance O
values O
than O
the O
BERT B-MethodName
model O
stand O
- O
alone O
and O
the O
baselineHIPE O
N O
EWSEYE O
DE O
FR O
DE O
FR O
P O
R O
F1 B-MetricName
P O
R O
F1 B-MetricName
P O
R O
F1 B-MetricName
P O
R O
F1 B-MetricName
BiLSTM B-MethodName
- O
CNN O
fuzzy O
83.3 B-MetricValue
70.1 O
76.1 O
89.9 O
83.9 O
86.8 O
81.2 O
42.4 O
55.7 O
82.2 O
77.2 O
79.6 O
strict O
69.4 O
58.4 O
63.4 O
77.7 O
72.5 O
75.0 O
54.8 O
28.6 O
37.6 O
65.5 O
61.4 O
63.4 O
BiLSTM B-MethodName
- O
CNN O
( O
transfer O
learning)y O
fuzzy O
81.1 O
75.0 O
77.9 O
* O
* O
87.8 O
88.8 O
88.3 O
76.4 O
49.4 O
60.0 O
* O
* O
83.6 O
77.8 O
80.6 O
* O
strict O
67.4 O
62.2 O
64.7 O
* O
* O
77.3 O
78.2 O
77.7 O
48.6 O
31.4 O
38.1 O
* O
* O
66.9 O
62.3 O
64.5 O
* O
BERT B-MethodName
fuzzy O
83.4 O
88.3 O
85.8 O
* O
* O
89.5 O
91.9 O
90.7 O
* O
60.1 O
67.0 O
63.4 O
* O
* O
86.1 O
81.8 O
83.9 O
* O
* O
strict O
74.1 O
78.5 O
76.2 O
* O
* O
81.1 O
83.3 O
82.1 O
* O
46.8 O
52.2 O
49.4 O
* O
* O
70.1 O
66.6 O
68.3 O
* O
* O
BERT B-MethodName
+1Transf O
fuzzy O
85.8 O
87.3 O
86.5 O
* O
* O
91.3 O
92.9 O
92.1 O
* O
* O
82.3 O
66.4 O
73.5 O
* O
* O
88.7 O
82.1 O
85.3 O
* O
* O
strict O
77.2 O
78.6 O
77.9 O
* O
* O
83.5 O
84.9 O
84.2 O
* O
* O
62.7 O
50.6 O
56.0 O
* O
* O
74.4 O
68.9 O
71.5 O
* O
* O
BERT B-MethodName
+2Transf O
fuzzy O
87.0 O
87.2 O
87.1 O
* O
* O
91.5 O
92.4 O
91.9 O
* O
* O
83.3 O
64.4 O
72.6 O
* O
* O
89.7 O
80.1 O
84.7 O
* O
* O
strict O
78.6 O
78.7 O
78.7 O
* O
* O
83.4 O
84.2 O
83.8 O
* O
* O
64.9 O
50.2 O
56.6 O
* O
* O
75.0 O
67.0 O
70.8 O
* O
* O
Table O
2 O
: O
NER B-TaskName
test O
results O
for O
the O
HIPE O
and O
N O
EWSEYEdatasets O
in O
French O
and O
German O
. O

* O
denotes O
a O
signiﬁcant O
improvement O
over O
the O
BiLSTM B-MethodName
model O
at O
p O
0.05 O
, O
* O
* O
denotes O
p0.01 O
. O

Comparing O
with O
the O
baseline O
models O
, O
the O
BERT B-MethodName
+ O
nTransf O
only O
achieves O
a O
20percentage O
points O
difference O
between O
precision B-MetricName
and O
recall B-MetricName
, O
while O
the O
baseline O
suffers O
from O
40points O
difference O
. O

CoNLL-03 O
EN O
P O
R O
F1 B-MetricName
BiLSTM B-MethodName
- O
CNN O
micro O
- O
fuzzy O
91.0 B-MetricValue
89.7 O
90.4 O
micro O
- O
strict O
89.2 O
87.9 O
88.5 O
P O
R O
F1 B-MetricName
P O
R O
F1 B-MetricName
bert O
- O
base O
- O
cased O
bert O
- O
large O
- O
cased O
BERT B-MethodName
micro O
- O
fuzzy O
91.7 B-MetricValue
93.0 O
92.3 O
92.4 O
93.5 O
92.9 O
micro O
- O
strict O
90.3 O
91.6 O
90.9 O
91.1 O
92.2 O
91.6 O
BERT B-MethodName
+1Transf O
micro O
- O
fuzzy O
92.5 O
93.2 O
92.8 O
92.7 O
93.4 O
93.1 O
micro O
- O
strict O
91.1 O
91.8 O
91.4 O
91.4 O
92.1 O
91.8 O
BERT B-MethodName
+2Transf O
micro O
- O
fuzzy O
92.0 O
93.2 O
92.6 O
92.9 O
93.4 O
93.1 O
micro O
- O
strict O
90.6 O
91.8 O
91.2 O
91.6 O
92.1 O
91.8 O
Table O
3 O
: O
NER B-TaskName
test O
results O
for O
the O
CoNLL O
2003 O
dataset O
. O

BERT+ B-MethodName
 O
transfBERT B-MethodName
LOCLOC O
PERS O
PERSPERS O
LOC O
LOCLOCLOC O
Gold O
Standard O
BERT+ B-MethodName
 O
transfBERTAmiens B-MethodName
werde O
zwar O
i O
m O
Augenblick O
noch O
gehalten O
, O
aber O
der O
Entwichlungsangrif O
f O
von O
Lille O
aus O
, O
also O
vo^i O
dem O
toten O
Punkte O
, O
 O
der O
leichter O
und O
rafcher O
die O
Zusammenziehung O
der O
Referven O
gestatte O
, O
sei O
vorauszusehen O
. O

In O
the O
context O
of O
modern O
data O
, O
in O
the O
Table O
3 O
, O
the O
F1 B-MetricName
values O
of O
the O
stand O
- O
alone O
BERT B-MethodName
model O
applied O
on O
the O
CoNLL O
2003 B-MetricValue
dataset O
fairly O
correspond O
to O
the O
ones O
reported O
in O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
( O
the O
authors O
report O
a O
F1 B-MetricName
of O
92:4 O
% O
for O
bert O
- O
base O
- O
cased O
and92:8 O
% O
for O
bert O
- O
large O
- O
cased O
) O
. O

While O
the O
F1 B-MetricName
value O
has O
a O
very O
small O
margin O
difference O
from O
the O
( O
Devlin O
et O
al O
. O
, O
2019 B-MetricValue
) O
, O
the O
performance O
results O
for O
the O
BERT B-MethodName
+ O
nTransf O
slightly O
increased O
for O
both O
proposed O
models O
. O

While O
this O
improvement O
is O
more O
visible O
for O
the O
BERT B-MethodName
bert O
- O
base O
- O
cased O
+1Transf O
( O
a O
difference O
of O
a O
half O
of O
percentage O
point O
) O
, O
and O
0:3percentage O
points O
for O
bert O
- O
base O
- O
cased O
+2Transf O
, O
for O
the O
bert O
- O
large O
- O
cased O
BERT B-MethodName
+ O
nTransf O
, O
the O
values O
remain O
unchanged O
( O
with O
a O
difference O
of O
0:2percentage O
points O
from O
BERT B-MethodName
) O
. O

6.1 O
Discussion O
For O
more O
qualitative O
analysis O
, O
we O
examine O
the O
number O
of O
unrecognized O
words O
by O
the O
pre O
- O
trained O
BERT B-MethodName
- O
based O
models O
that O
were O
added O
to O
the O
speciﬁc O
tokenizers O
( O
WordPiece O
for O
BERT B-MethodName
and O
SentencePiece O
for O
CamemBERT B-MethodName
) O
. O

Following O
this O
observation O
, O
we O
notice O
that O
there O
is O
a O
large O
F1 B-MetricName
margin O
between O
BERT+CRF B-MethodName
and O
BERT B-MethodName
+ O
nTransf O
( O
63:4 O
% O
in O
comparison O
with O
73:5 O
% O
and O
72:6 O
% O
, O
respectively O
) O
, O
a O
fact O
that O
could O
be O
motivated O
by O
the O
large O
percentage O
of O
unknown O
words O
. O
Moreover O
, O
for O
German O
, O
even O
though O
the O
BERT B-MethodName
encoder O
was O
pre O
- O
trained O
on O
a O
digitized O
historical O
dataset O
( O
bert O
- O
base O
- O
german O
- O
europeana O
) O
, O
the O
proposed O
model O
contributed O
greatly O
to O
the O
coverage O
of O
the O
misspelled O
or O
abnormal O
words O
present O
in O
the O
NEWSEYE O
. O

For O
French O
, O
the O
results O
vary O
of O
around O
1 2percentage O
F1 B-MetricName
points O
between O
the O
stand O
- O
alone O
BERT B-MethodName
and O
the O
BERT B-MethodName
+ O
nTransf O
models O
. O

In O
Figure O
3 O
, O
we O
compare O
BERT B-MethodName
and O
BERT B-MethodName
+ O
nTransf O
by O
analyzing O
the O
number O
of O
correct O
predicted O
entities O
with O
respect O
to O
the O
Levenshtein O
distance O
. O

A O
French O
example O
of O
a O
misspelled O
entity O
that O
is O
recognized O
by O
both O
BERT B-MethodName
+ O
nTransf O
but O
not O
by O
BERT B-MethodName
is O
presented O
in O
Figure O
2 O
, O
in O
the O
upper O
part O
. O

We O
noticed O
that O
BERT B-MethodName
+ O
nTransf O
is O
better O
than O
BERT B-MethodName
at O
predicting O
entities O
composed O
of O
multiple O
tokens O
( O
large O
entities O
) O
. O

German O
HIPE O
has O
less O
entities O
longer O
than O
ﬁve O
tokens16 O
, O
more O
exactly O
97 O
, O
and O
while O
the O
stand O
- O
alone O
BERT B-MethodName
detected O
50:51 O
% O
of O
them O
, O
the O
BERT B-MethodName
+ O
nTransf O
models O
correctly O
detected O
and O
classiﬁed O
55:67 O
% O
for O
n= O
1 O
and54:63 O
% O
for O
n= O
2 O
. O

In O
the O
following O
examples O
from O
Table O
4 O
, O
our O
method O
correctly O
predicted O
the O
full O
entity O
frequently O
while O
the O
stand O
- O
alone O
BERT B-MethodName
only O
predicted O
a O
part O
of O
it O
. O

Analyzing O
the O
French O
predictions O
for O
BERT B-MethodName
and O
BERT B-MethodName
+ O
nTransf O
, O
we O
observed O
that O
BERT B-MethodName
detects O
on O
average B-MetricName
75:04 O
% O
of O
the O
entities O
of O
size O
1to10 O
, O
with O
other O
models O
performing O
slightly O
better O
. O

However O
, O
for O
entities O
with O
more O
than O
10tokens O
, O
there O
is O
clear O
a O
difference O
, O
since O
BERT B-MethodName
detects O
55:54 O
% O
of O
the O
entities O
, O
while O
BERT B-MethodName
+1Transf O
detects O
57:13 O
% O
, O
and O
BERT B-MethodName
+2Transf O
reaches O
82.52 O
% O
. O

Gold O
standard O
Predicted O
by O
BERT B-MethodName
BERT B-MethodName
+ O
nTransf O
sign´eKocH O
, O
avocat O
, O
avocat O
sign O
´ O
eKocH O
, O
avocat O
district O
de O
GumbinnenGumbinnen O
district O
de O
Gumbinnen O
Armel O
Guerne O
. O

Javits O
, O
s´enateur O
de O
New O
York O
juif O
et O
pro- O
isra O
` O
elien O
Table O
4 O
: O
Examples O
of O
long O
entities O
predicted O
by O
all O
models O
( O
the O
entity O
parts O
detected O
by O
BERT B-MethodName
alone O
are O
highlighted O
in O
bold O
font O
under O
BERT B-MethodName
+ O
nTransf O
) O
. O

In O
the O
lower O
part O
of O
Figure O
2 O
, O
we O
present O
a O
German O
example O
where O
BERT B-MethodName
becomes O
confused O
and O
16The O
length O
of O
German O
HIPE O
entities O
ranges O
from O
one O
to O
16tokens.predicts O
multiple O
partial O
spurious O
entities O
in O
a O
sentence O
. O

In O
this O
case O
, O
there O
is O
an O
overprediction O
of O
these O
types O
, O
which O
leads O
us O
to O
the O
interpretation O
that O
BERT B-MethodName
is O
sensitive O
to O
misspellings O
and O
might O
overﬁt O
on O
OCR O
- O
related O
patterns O
. O

This O
observation O
proves O
that O
BERT B-MethodName
has O
unbalanced O
attention O
to O
misspelled O
or O
corrupted O
words O
when O
the O
most O
informative O
words O
contain O
such O
errors O
( O
Sun O
et O
al O
. O
, O
2020 O
) O
. O

Moreover O
, O
we O
can O
see O
that O
BERT B-MethodName
achieved O
its O
result O
by O
creating O
more O
spurious O
cases O
in O
comparison O
to O
BERT B-MethodName
+ O
nTransf O
. O

This O
could O
mean B-MetricName
that O
BERT B-MethodName
learned O
that O
overpredicting O
was O
a O
straightforward O
solution O
to O
achieve O
better O
results O
. O

In O
the O
case O
of O
BERT B-MethodName
+ O
nTransf O
, O
we O
can O
see O
that O
the O
Transformer O
layers O
made O
the O
models O
to O
be O
more O
conservative O
and O
at O
the O
same O
time O
more O
accurate O
in O
their O
predictions O
. O

7 O
Conclusions O
and O
Future O
Work O
We O
presented O
a O
deep O
learning O
architecture O
for O
NER B-TaskName
based O
on O
stacked O
Transformer O
layers O
that O
includes O
a O
ﬁne O
- O
tuned O
BERT B-MethodName
encoder O
and O
several O
Transformer O
blocks O
. O

More O
concretely O
, O
the O
shared O
encoder O
( O
Section O
3.2 O
) O
fuses O
together O
rich O
input O
features O
for O
each O
token O
including O
features O
extracted O
from O
pretrained O
language O
models O
, O
which O
are O
then O
fed O
to O
bidirectional O
long O
short O
- O
term O
memories O
( O
biLSTMs B-MethodName
; O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Schuster O
and O
Paliwal O
, O
1997 O
) O
to O
obtain O
task O
- O
independent O
contextualized O
token O
representations O
. O

Speciﬁcally O
, O
the O
proposed O
system O
combines O
surface O
, O
lemma O
, O
partof O
- O
speech O
( O
POS B-TaskName
) O
tags O
, O
named O
entity O
label O
, O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
embedding O
, O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
embedding O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
embedding O
as O
input O
features O
. O

GloVe B-MethodName
We O
use O
300 O
- O
dimensional O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
pretrained O
on O
Common O
Crawl2 O
which O
are O
kept O
ﬁxed O
during O
the O
training O
. O

Surfaces O
that O
do O
not O
appear O
in O
the O
pretrained O
GloVe B-MethodName
are O
mapped O
to O
a O
special O
< O
UNK O
> O
token O
which O
is O
set O
to O
a O
vector O
whose O
values O
are O
randomly O
drawn O
from O
normal O
distribution O
with O
standard O
deviation O
of O
1 O
= O
p O
dimension O
of O
a O
GloVe B-MethodName
vector O
. O

ELMo B-MethodName
We O
use O
the O
pretrained O
“ O
original O
” O
ELMo3 B-MethodName
. O

Following O
Peters O
et O
al O
. O
( O
2018 O
) O
, O
we O
“ O
mix O
” O
different O
layers O
of O
ELMo B-MethodName
for O
each O
token O
; O
~sj= O
softmax O
j(sj O
) O
= O
exp(sj)P O
kexp(sk O
) O
; O
hELMo B-MethodName
= O
NELMo 1X B-MethodName
j=0 O
~ O
sjhELMo B-MethodName
j O
; O
where O
hELMo B-MethodName
j O
( O
0j O
< O
NELMo B-MethodName
) O
is O
the O
hidden O
state O
of O
the O
j O
- O
th O
layer O
of O
ELMo B-MethodName
, O
hELMo B-MethodName
0 O
is O
the O
features O
from O
character O
- O
level O
CNN O
of O
ELMo B-MethodName
, O
andsjare O
trainable O
parameters O
. O

hELMo B-MethodName
j O
are O
ﬁxed O
in O
the O
training O
by O
truncating O
backpropagation O
to O
hELMo B-MethodName
j O
. O
BERT B-MethodName
We O
use O
the O
pretrained O
BERT B-MethodName
- O
Large O
, O
Uncased O
( O
Original)4 O
. O

Since O
BERT B-MethodName
takes O
subword O
units O
as O
input O
, O
a O
BERT B-MethodName
embedding O
for O
a O
token O
is O
generated O
as O
the O
average B-MetricName
of O
its O
subword O
BERT B-MethodName
embeddings O
as O
in O
Zhang O
et O
al O
. O
( O
2019 B-MetricValue
) O
. O

4https://s3.amazonaws.com/ O
models.huggingface.co/bert/ O
bert-large-uncased-pytorch_model.bin O
, O
which O
is O
converted O
from O
the O
whitelisted O
BERT B-MethodName
model O
in O
https://github.com/google-research/bertis O
prepended O
to O
each O
input O
sequence O
. O

For O
GloVe B-MethodName
, O
ELMo B-MethodName
and O
BERT B-MethodName
, O
the O
< O
ROOT O
> O
is O
also O
embedded O
in O
the O
similar O
manner O
as O
other O
tokens O
with O
< O
ROOT O
> O
as O
the O
surface O
for O
the O
token O
. O

A O
multilayered O
perceptron O
( O
MLP O
) O
is O
applied O
to O
each O
of O
GloVe B-MethodName
, O
ELMo B-MethodName
and O
BERT B-MethodName
embeddings O
. O

3.2 O
Obtaining O
Contextualized O
Token O
Representation O
The O
input O
token O
representations O
h0 O
iare O
fed O
to O
the O
multi O
- O
layered O
biLSTM B-MethodName
with O
Nlayers O
to O
obtain O
the O
contextualized O
token O
representations O
. O

  O
! O
hl O
i=  O
    O
! O
LSTM B-MethodName
( O
hl 1 O
i;  O
! O
hl O
i 1;  O
! O
cl O
i 1 O
) O
; O
 O
 hl O
i= O
    O
 LSTM B-MethodName
( O
hl 1 O
i O
; O
 hl O
i+1 O
; O
 cl O
i+1 O
) O
; O
hl O
i O
= O
h  O
! O
hl O
i O
; O
 hl O
ii O
; O
where O
hl O
iandcl O
i(0 O
< O
lN O
) O
are O
the O
hidden O
states O
and O
the O
cell O
states O
of O
the O
l O
- O
th O
layer O
LSTM B-MethodName
for O
i O
- O
th O
token O
. O

Presence O
of O
an O
abstract O
node O
on O
a O
node O
or O
an O
edge O
is O
detected O
with O
rules O
( O
e.g. O
andc O
implies O
presence O
of O
qnode O
) O
or O
with O
binary O
logistic B-MethodName
regression I-MethodName
( O
e.g. O
udef O
qon O
chicken O
n1 O
) O
. O

The O
system O
predicts O
labels O
of O
the O
nodes O
generated O
in O
2 O
using O
multi O
- O
class O
logistic B-MethodName
regression I-MethodName
. O

The O
system O
predicts O
labels O
of O
edges O
from O
/ O
to O
the O
generated O
nodes O
using O
multi O
- O
class O
logistic B-MethodName
regression I-MethodName
. O

POS B-TaskName
tags O
, O
predicted O
DM O
frames O
and O
edge O
labels O
of O
adjacent O
nodes O
are O
used O
as O
features O
for O
the O
logistic B-MethodName
regression I-MethodName
. O

Then O
, O
we O
embedxeds O
i;jto O
obtain O
trainable O
vector O
eeds O
i;jand O
feed O
them O
to O
a O
biLSTM B-MethodName
to O
obtain O
a O
contextualized O
representation O
heds O
i;j O
. O
Finally O
, O
we O
predict O
a O
span O
in O
input O
tokens O
[ O
argmaxjyedsfrom O
i;j;argmaxjyedsto O
i;j]for O
thei O
- O
th O
abstract O
node O
, O
yedsfrom O
i;j O
= O
softmax O
j O
( O
heds O
i;j)>MLPedsfrom(hN O
j) O
; O
yedsto O
i;j O
= O
softmax O
j O
( O
heds O
i;j)>MLPedsto(hN O
j) O
: O
The O
loss O
for O
the O
anchor O
prediction O
` O
edsis O
the O
sum O
of O
cross O
entropy B-MetricName
between O
the O
predicted O
span O
( O
yedsfrom O
i;j;yedsto O
i;j)and O
the O
corresponding O
ground O
truth O
span O
. O

Our O
proposed O
UCCA O
parser O
( O
Figure O
3 O
) O
consists O
of O
( O
i O
) O
a O
pointer O
network O
( O
Vinyals O
et O
al O
. O
, O
2015 O
) O
which O
generates O
non O
- O
terminal O
nodes O
from O
the O
contextualized O
token O
representations O
of O
the O
encoder O
, O
( O
ii O
) O
an O
additional O
biLSTM B-MethodName
that O
encodes O
context O
of O
both O
the O
terminal O
and O
generated O
non O
- O
terminal O
nodes O
, O
and O
( O
iii O
) O
a O
biafﬁne O
network O
which O
predicts O
edges.6.1 O
Preprocessing O
We O
treat O
the O
generation O
of O
non O
- O
terminal O
nodes O
as O
a O
“ O
pointing O
” O
problem O
. O

Given O
hidden O
states O
of O
the O
encoder O
hN O
j O
, O
hidden O
states O
of O
the O
decoder O
are O
initialized O
by O
the O
last O
states O
of O
the O
shared O
encoder O
: O
hucca O
dec O
 1 O
= O
h  O
! O
hN K O
: O
N O
Lin O
; O
 hN K O
: O
N O
0i O
; O
cucca O
dec O
 1 O
= O
h  O
! O
cN K O
: O
N O
Lin O
; O
 cN K O
: O
N O
0i O
; O
whereKis O
the O
stacking O
number O
of O
the O
biLSTMs B-MethodName
in O
the O
shared O
encoder O
. O

We O
then O
obtain O
the O
hiddenstates O
of O
the O
decoder O
hucca O
dec O
i O
as O
: O
hucca O
dec O
i O
= O
LSTM B-MethodName
dec(xucca O
dec O
i;hucca O
dec O
i 1;cucca O
dec O
i 1 O
): O
Attention O
distribution O
~ai;jover O
the O
input O
tokens O
is O
calculated O
as O
: O
ai;j O
= O
v O
> O
tanh  O
Wucca O
dec[hucca O
dec O
i O
; O
hN O
j] O
; O
~ai;j= O
softmax O
j(ai;j O
) O
; O
where O
Wucca O
decandvare O
parameters O
of O
the O
pointer O
network O
. O

To O
remedy O
this O
problem O
, O
a O
positional O
encoding O
Vaswani O
et O
al O
. O
( O
2017 O
) O
is O
concatenated O
to O
each O
of O
hucca O
ptr O
i O
to O
obtain O
position O
- O
aware O
hucca O
ptr O
’ O
i O
. O
Furthermore O
, O
we O
feed O
hucca O
ptr O
’ O
i O
to O
an O
additional O
biLSTM B-MethodName
and O
obtain O
hucca O
iin O
order O
to O
further O
encode O
the O
order O
information O
. O

Given O
contextualized O
token O
representation O
of O
the O
encoder O
Henc O
l= O
fhl O
0;:::;hl O
Lin 1 O
g O
, O
we O
obtain O
hidden O
states O
of O
the O
decoder O
hamr O
iandpias O
: O
hamr O
i;pi O
= O
Decoder O
amr(henc O
’ O
i;hamr O
i 1;pi 1;Henc O
N O
) O
; O
henc O
’ O
i O
= O
Encoder O
amr(pi;hamr O
0:::hamr O
i 1;Henc O
0 O
) O
; O
henc O
’ O
0;hamr O
 1= O
MLP O
amrh  O
! O
hN O
Lin O
; O
 hN O
0;  O
! O
cN O
Lin O
; O
 cN O
0i O
: O
Encoder O
amrtreats O
a O
node O
as O
if O
it O
is O
a O
token O
, O
and O
utilizes O
the O
encoder O
( O
Section O
3 O
) O
with O
shared O
model O
parameters O
to O
obtain O
representation O
of O
( O
i 1)th O
generated O
nodes O
henc O
’ O
i. O
Concretely O
, O
Encoder O
amr O
combines O
lemma O
( O
corresponds O
to O
the O
node O
label O
) O
, O
POS B-TaskName
tags O
( O
only O
when O
copied O
from O
a O
token O
) O
and O
GloVe B-MethodName
( O
from O
the O
node O
label O
) O
of O
a O
node O
, O
embeds O
each O
of O
them O
to O
a O
feature O
vector O
using O
the O
encoder O
and O
concatenates O
feature O
vectors O
to O
obtain O
henc O
’ O
i. O

BERT B-MethodName
was O
excluded O
for O
the O
formal O
run O
since O
it O
did O
not O
make O
it O
to O
the O
task O
deadline O
. O

We O
experimented O
with O
enhanced O
models O
with O
BERT B-MethodName
after O
the O
formal O
run O
. O

Variant O
Average B-MetricName
DM O
PSD O
EDS O
UCCA O
AMR O
SFL O
0.7575/5 O
0.9071/9 O
0.9064/3 O
0.8339/7 O
0.7014/6 O
0.4386/8 O
SFL(ensemble)y0.7604/5 O
0.9102/9 O
0.9121/2 O
0.8374/7 O
0.7036/6 O
0.4386/8 O
BERT+SFL(NT B-MethodName
) O
0.7450/6 O
0.9038/9 O
0.9069/3 O
0.8301/7 O
0.6945/6 O
0.3896/8 O
BERT+MTL(NT B-MethodName
) O
0.7144/6 O
0.8726/9 O
0.8791/7 O
0.7987/7 O
0.6422/6 O
0.3794/9 O
BERT+MTL+FT(NT B-MethodName
) O
0.7507/5 O
0.9045/9 O
0.9054/4 O
0.8304/7 O
0.7126/6 O
0.4008/8 O
SFL O
: O
single O
- O
framework O
learning O
, O
MTL O
: O
multi O
- O
task O
learning O
, O
FT O
: O
ﬁne O
- O
tuning O
, O
ensemble O
: O
with O
ensembles O
, O
NT O
: O
random O
seed O
is O
not O
tuned O
, O
yformal O
run O
of O
using O
the O
biafﬁne O
network O
as O
a O
shared O
architecture O
for O
MRP O
. O

The O
singleframework O
learning O
variant O
( O
SFL O
) O
without O
BERT B-MethodName
( O
SFL O
) O
performed O
better O
than O
SFL O
with O
BERT B-MethodName
( O
BERT+SFL(NT B-MethodName
) O
) O
, O
which O
suggests O
that O
impact O
of O
hyperparameter O
tuning O
was O
larger O
than O
that O
of O
incorporating O
BERT B-MethodName
. O

The O
multi O
- O
task O
learning O
variant O
( O
MTL O
) O
with O
ﬁne O
- O
tuning O
( O
BERT+MTL+FT(NT B-MethodName
) O
) O
outperformed O
the O
SFL O
in O
the O
comparable O
condition O
( O
BERT+SFL(NT B-MethodName
) O
) O
. O

2.1.4 O
Stress O
Test O
( O
ST O
) O
We O
also O
include O
the O
“ O
word O
overlap O
” O
( O
ST O
- O
WO O
) O
, O
“ O
negation O
” O
( O
ST O
- O
NE O
) O
, O
“ O
length O
mismatch O
” O
( O
ST O
- O
LM O
) O
and O
“ O
spelling O
errors O
” O
( O
ST O
- O
SE O
) O
in O
Naik O
et O
al O
. O
( O
2018 O
) O
, O
in O
which O
ST O
- O
WO O
aims O
at O
detecting O
lexical O
overlap O
heuristics O
described O
in O
McCoy O
et O
al O
. O
( O
2019 O
) O
( O
IS O
- O
SD O
in O
Sec O
2.1.2 O
) O
; O
ST O
- O
NE O
aims O
at O
detecting O
strong O
negative O
lexical O
cues O
in O
partial O
- O
input O
sentences O
like O
PI O
- O
SP O
in O
Sec O
2.1.2.SNLI O
MNLI B-MethodName
DNLI O
ANLI O
Train O
549362 O
392702 O
249947 O
162765 O
Valid O
9842 O
9832 O
31696 O
2200 O
Test O
9824 O
9815 O
31232 O
2200 O
Table O
2 O
: O
Statistics O
for O
datasets O
used O
in O
Sec O
5 O
. O

For O
MNLI B-MethodName
, O
we O
utlize O
the O
matched O
dev O
and O
mismatched O
dev O
sets O
as O
valid O
and O
test O
sets O
respectively O
. O

MNLI B-MethodName
InferSent O
52.1 O
55.3 O
53.9 O
33.5 O
43.6 O
70.5 O
53.3 O
51.7 O
61.8 O
10.6 O
25.4 O
24.7 O
30.6 O
70.5 O
+ O
ELMO O
48.6 O
59.8 O
55.2 O
42.1 O
38.5 O
72.4 O
52.7 O
52.8 O
62.5 O
9.8 O
24.6 O
18.5 O
28.9 O
72.5 O
DAM O
55.0 O
54.4 O
50.2 O
35.7 O
62.7 O
74.3 O
53.0 O
55.0 O
62.7 O
10.3 O
27.0 O
30.0 O
32.5 O
70.3 O
ESIM O
55.1 O
66.3 O
49.8 O
52.7 O
63.2 O
79.6 O
53.8 O
60.1 O
66.2 O
11.3 O
25.1 O
27.5 O
32.5 O
77.3 O
BERT B-MethodName
B O
72.2 O
73.9 O
63.8 O
65.4 O
85.6 O
82.6 O
63.5 O
72.4 O
75.4 O
36.2 O
54.2 O
66.1 O
58.0 O
83.5 O
BERT B-MethodName
L O
74.7 O
75.5 O
70.4 O
70.6 O
87.9 O
83.8 O
67.3 O
75.7 O
77.6 O
39.4 O
55.5 O
68.3 O
60.2 O
85.7 O
XLNet B-MethodName
B O
73.1 O
77.9 O
71.2 O
70.4 O
85.5 O
84.8 O
68.5 O
75.9 O
78.0 O
39.2 O
55.8 O
66.7 O
59.9 O
86.6 O
XLNet B-MethodName
L O
78.8 O
81.7 O
76.7 O
77.3 O
93.4 O
88.5 O
72.4 O
81.3 O
83.4 O
45.9 O
57.6 O
73.0 O
65.0 O
89.3 O
RoBERTa B-MethodName
B O
76.6 O
80.9 O
72.0 O
74.1 O
89.6 O
85.3 O
66.4 O
77.8 O
80.9 O
42.1 O
55.9 O
69.0 O
62.0 O
87.4 O
RoBERTa B-MethodName
L O
80.0 O
79.2 O
80.0 O
77.0 O
92.4 O
88.6 O
73.4 O
81.5 O
84.4 O
50.5 O
57.3 O
72.2 O
66.1 O
89.9 O
Table O
3 O
: O
The O
performance O
of O
models O
on O
adversarial O
and O
generalization O
power O
tests O
( O
Sec O
2 O
) O
trained O
on O
MultiNLI O
. O

The O
general O
trend O
is O
that O
more O
powerful O
model O
which O
has O
higher O
performance O
on O
the O
original O
( O
in O
- O
domain O
) O
test O
sets O
( O
RoBERTa B-MethodName
( O
large O
) O
) O
outperforms O
most O
models O
in O
both O
adversarial O
and O
general O
purpose O
settings O
. O

We O
use O
their O
trainedBaseline O
Word O
Overlap O
Partial O
Input O
Sentence O
Length O
Debiasing O
Combination O
( O
BERT B-MethodName
base)ReW O
BiasProd O
ReW O
BiasProd O
ReW O
BiasProd O
MixW O
AddProd O
BestEn O
PI O
- O
CD O
72.2 O
70.9 O
71.4 O
72.6 O
71.8 O
72.6 O
72.3 O
71.9 O
71.3 O
72.6 O
PI O
- O
SP O
73.9 O
70.6 O
70.1 O
74.7 O
73.0 O
75.2 O
73.3 O
71.7 O
70.4 O
73.9 O
IS O
- O
SD O
63.8 O
69.2 O
71.0 O
65.7 O
63.8 O
56.9 O
59.5 O
54.6 O
61.5 O
72.5 O
IS O
- O
CS O
65.4 O
64.8 O
64.2 O
67.1 O
68.9 O
64.9 O
66.9 O
65.4 O
68.9 O
64.9 O
LI O
- O
LI O
85.6 O
87.0 O
87.8 O
86.0 O
85.0 O
85.7 O
85.5 O
86.8 O
88.4 O
87.7 O
LI O
- O
TS O
82.6 O
81.8 O
81.7 O
82.0 O
82.3 O
81.3 O
83.7 O
82.3 O
81.9 O
84.5 O
ST O
- O
LM O
82.2 O
82.3 O
81.7 O
81.6 O
81.1 O
82.6 O
82.7 O
82.6 O
79.9 O
83.1 O
Gen. O

58.0 O
56.8 O
56.6 O
57.5 O
56.7 O
57.9 O
57.5 O
57.1 O
55.9 O
58.1 O
MNLI B-MethodName
83.5 O
84.2 O
82.8 O
84.3 O
83.3 O
80.3 O
80.9 O
84.0 O
81.2 O
84.5 O
Table O
4 O
: O
The O
performance O
of O
debiasing O
methods O
( O
Sec O
3 O
) O
based O
on O
BERT B-MethodName
base O
model O
( O
baseline O
) O
trained O
on O
MultiNLI O
. O

Partial O
input O
heuristics O
: O
To O
combat O
the O
hypothesis O
- O
only O
bias O
in O
NLI O
( O
PI O
- O
CD O
and O
PI O
- O
SP O
in O
Sec O
2.1.1 O
) O
, O
we O
use O
RoBERTa B-MethodName
( O
base O
) O
model O
to O
train O
a O
bias O
- O
only O
model O
by O
taking O
only O
hypothesis O
sentences O
as O
inputs O
. O

MNLI B-MethodName
Baseline O
72.2 O
73.9 O
63.8 O
65.4 O
85.6 O
82.6 O
63.5 O
72.4 O
75.4 O
36.2 O
54.2 O
66.1 O
58.0 O
83.5 O
Text O
Swap O
71.7 O
72.8 O
63.5 O
67.4 O
86.3 O
86.8 O
66.5 O
73.6 O
73.3 O
35.3 O
54.7 O
66.8 O
57.6 O
83.7 O
Sub O
( O
synonym O
) O
69.8 O
72.0 O
62.4 O
65.8 O
85.2 O
82.8 O
64.3 O
71.8 O
74.4 O
34.2 O
55.1 O
65.8 O
57.4 O
83.5 O
Sub O
( O
MLM O
) O
71.0 O
72.8 O
64.4 O
65.9 O
85.6 O
83.3 O
64.9 O
72.6 O
74.8 O
34.7 O
55.4 O
65.7 O
57.7 O
83.6 O
Paraphrase O
72.1 O
74.6 O
66.5 O
66.4 O
85.7 O
83.1 O
64.8 O
73.3 O
75.8 O
35.1 O
55.0 O
65.0 O
57.7 O
83.7 O
Table O
5 O
: O
The O
performance O
of O
BERT B-MethodName
base O
model O
under O
different O
data O
augmentation O
strategies O
( O
Sec O
4 O
) O
. O

For O
the O
entailment O
andneutral O
instances O
, O
we O
using O
the O
ensembled O
RoBERTa B-MethodName
large O
model O
trained O
on O
‘ O
all4 O
’ O
training O
set O
( O
Table O
6 O
) O
to O
label O
the O
swapped O
sentence O
pairs O
. O

2)Masked O
LM O
: O
we O
randomly O
select O
30 O
% O
content O
words O
and O
then O
load O
the O
pretrained O
BERT B-MethodName
large O
model O
to O
perform O
masked O
LM O
task O
. O

For O
automatic O
evaluation O
, O
we O
use O
the O
best O
NLI O
model O
( O
RoBERTa(large B-MethodName
) O
model O
with O
‘ O
All4+SinEN O
’ O
in O
Table O
6 O
) O
in O
this O
paper O
to O
judge O
if O
the O
labels O
of O
augmented O
data O
are O
consistent O
with O
the O
predictions O
of O
our O
best O
NLI O
model O
. O

4.3 O
Discussions O
for O
Data O
Augmentations O
ForData O
Augmentation O
, O
we O
show O
the O
performance O
of O
a O
BERT B-MethodName
base O
model O
using O
different O
data O
augmentation O
methods O
in O
Table O
5 O
. O

DNLI O
ANLI O
SNLI O
MNLI B-MethodName
RoBERTa B-MethodName
( O
base O
) O
Model O
D(only O
) O
38.5 O
48.2 O
55.6 O
40.9 O
12.6 O
72.9 O
40.9 O
44.2 O
54.9 O
9.1 O
40.9 O
39.4 O
36.1 O
92.9 O
32.6 O
42.1 O
47.0 O
A(only O
) O
64.6 O
60.6 O
57.9 O
66.9 O
92.6 O
80.8 O
68.1 O
70.2 O
80.6 O
33.8 O
51.2 O
63.7 O
57.3 O
58.9 O
49.1 O
73.6 O
78.5 O
S(only O
) O
82.2 O
64.4 O
67.4 O
62.2 O
93.2 O
80.7 O
64.6 O
73.5 O
72.5 O
36.0 O
57.8 O
49.6 O
54.0 O
58.8 O
31.3 O
91.3 O
79.9 O
M(only O
) O
76.6 O
80.9 O
72.0 O
74.1 O
89.6 O
85.3 O
66.4 O
77.8 O
80.9 O
42.1 O
55.9 O
69.0 O
62.0 O
59.3 O
29.4 O
84.2 O
87.4 O
M+S O
82.8 O
80.1 O
73.3 O
74.4 O
91.8 O
85.6 O
67.8 O
79.4 O
81.2 O
40.7 O
57.5 O
67.4 O
61.7 O
60.5 O
28.3 O
91.7 O
87.4 O
M+S+D O
82.7 O
79.8 O
75.1 O
72.9 O
92.1 O
84.7 O
68.1 O
79.3 O
80.4 O
40.9 O
57.1 O
68.3 O
61.8 O
92.8 O
30.3 O
91.7 O
87.7 O
All4 O
82.6 O
81.7 O
77.0 O
74.7 O
94.7 O
85.3 O
69.1 O
80.7 O
83.7 O
41.9 O
57.3 O
70.5 O
63.4 O
93.0 O
49.2 O
91.9 O
87.7 O
All4+SR O
82.6 O
82.5 O
74.7 O
73.8 O
95.2 O
86.0 O
69.0 O
80.5 O
83.9 O
41.3 O
57.3 O
69.6 O
63.0 O
92.8 O
49.1 O
91.7 O
87.8 O
All4+PR O
83.4 O
79.5 O
75.5 O
73.8 O
94.6 O
85.5 O
69.1 O
80.2 O
83.8 O
44.0 O
57.5 O
70.5 O
64.0 O
92.9 O
51.2 O
91.9 O
87.6 O
RoBERTa B-MethodName
( O
large O
) O
Model O
All4 O
84.6 O
83.8 O
79.6 O
79.3 O
94.9 O
88.6 O
71.6 O
83.2 O
87.6 O
50.2 O
57.9 O
73.1 O
67.2 O
93.2 O
55.5 O
92.7 O
90.4 O
All4+ME O
85.0 O
81.4 O
80.1 O
77.7 O
95.7 O
88.7 O
72.2 O
83.0 O
87.2 O
47.4 O
58.0 O
73.7 O
66.6 O
93.3 O
54.8 O
93.0 O
90.2 O
All4+SE O
85.0 O
81.9 O
77.5 O
77.9 O
95.4 O
89.2 O
72.5 O
82.8 O
88.5 O
49.3 O
57.9 O
73.9 O
67.4 O
93.3 O
55.7 O
93.0 O
90.6 O
Table O
6 O
: O
Performance O
of O
RoBERTa B-MethodName
model O
trained O
on O
different O
datasets O
using O
multiple O
reweighting B-HyperparameterName
and O
ensemble O
strategries O
( O
Sec O
5 O
) O
. O

‘ O
D O
’ O
, O
‘ O
A O
’ O
, O
‘S O
’ O
, O
‘ O
M O
’ O
, O
‘ O
All4 O
’ O
denotes O
DNLI O
, O
ANLI O
, O
SNLI O
, O
MNLI B-MethodName
and O
the O
merge O
of O
all O
4 O
datasets O
respectively O
. O

‘ O
M+S O
’ O
is O
created O
by O
merging O
MNLI B-MethodName
and O
SNLI O
datasets O
, O
same O
principle O
in O
other O
settings O
. O

‘ O
ME O
’ O
and O
‘ O
SE O
’ O
denote O
the O
ensemble O
strategies O
in O
Sec O
5.2 O
: O
the O
ensemble O
of O
3 O
distinct O
models O
: O
BERT(large B-MethodName
) O
, O
XLNet(large B-MethodName
) O
and O
RoBERTa(large B-MethodName
) O
and O
the O
ensemble O
of O
3 O
RoBERTa(large B-MethodName
) O
models O
. O

5.1 O
Merging O
Heterogeneous O
Datasets O
To O
set O
up O
more O
diverse O
and O
stronger O
baselines O
for O
the O
proposed O
benchmark O
datasets O
, O
we O
use O
4 O
large O
- O
scale O
training O
datasets O
: O
SNLI O
, O
MNLI B-MethodName
, O
DNLI O
and O
ANLI O
for O
the O
following O
experiments O
. O

Speciﬁcally O
, O
SNLI O
and O
MNLI B-MethodName
are O
created O
in O
a O
human O
elicited O
way O
( O
Poliak O
et O
al O
. O
, O
2018b O
): O
the O
human O
annotators O
are O
asked O
to O
write O
a O
hypothesis O
sentence O
according O
to O
the O
given O
premise O
and O
label O
. O

while O
in O
the O
single O
mode O
, O
we O
ensemble O
three O
same O
models O
( O
RoBERTa*3 B-MethodName
) O
. O

2 O
) O
In O
RoBERTa B-MethodName
base O
model O
, O
the O
‘ O
All4+PR O
’ O
model O
get O
higher O
scores B-MetricName
on O
diagnostic O
and O
ANLI O
test O
sets O
than O
‘ O
All4 O
’ O
baseline O
, O
which O
shows O
that O
increasing O
the O
weight B-HyperparameterName
of O
higher O
quality O
dataset O
may O
help O
to O
increase O
accuracy B-MetricName
on O
certain O
test O
sets O
. O

Notably O
, O
performance O
based O
reweighting B-HyperparameterName
helps O
the O
model O
gain O
2 O
points O
( O
49.2 O
vs O
51.2 O
) O
on O
ANLI O
compared O
with O
baseline O
model O
while O
keeping O
the O
inference O
ability O
on O
DNLI O
, O
SNLI O
and O
MNLI B-MethodName
test O
sets O
. O

3 O
) O
In O
RoBERTa B-MethodName
large O
model O
, O
we O
see O
that O
on O
some O
datasets O
, O
like O
IS O
- O
SD O
, O
the O
mixed O
ensemble O
model O
may O
even O
outperform O
the O
single O
ensemble O
model O
even O
if O
its O
two O
components O
( O
XLNet B-MethodName
and O
BERT B-MethodName
) O
are O
less O
powerful O
than O
those O
( O
RoBERTa B-MethodName
) O
in O
single O
ensemble O
mode O
. O
Labels O
Transformation O
Datasets O
(: O
E O
, O
E O
) O
C O
) O
: O
E O
, O
N O
) O
: O
EIS O
- O
SD O
, O
RTE B-MetricName
, O
DNLI O
(: O
C O
, O
C O
) O
E O
) O
: O
C O
, O
N O
) O
: O
C O
LI O
- O
TS O
( O
E O
, O
C O
) O
- O
LI O
- O
LI O
( O
N O
, O
E O
) O
- O
SciTail O
Table O
7 B-MetricValue
: O
How O
we O
evaluate O
the O
test O
sets O
with O
only O
two O
labels O
in O
3 O
- O
way O
NLI O
classiﬁcation O
. O

RTE B-MetricName
SICK O
SciTail O
DNLI O
ANLI O
SNLI O
MNLI B-MethodName
Origin O
75.4 B-MetricValue
54.2 O
66.1 O
54.2 O
27.7 O
80.0 O
83.5 O
Mixed O
75.5 O
54.3 O
67.3 O
54.8 O
27.4 O
79.9 O
83.4 O
Oracle O
75.5 O
55.2 O
67.3 O
56.7 O
28.0 O
80.3 O
83.5 O
Table O
8 O
: O
The O
performance O
of O
BERT B-MethodName
base O
model O
under O
different O
model O
selection O
strategies O
. O

We O
rerun O
their O
public O
available O
codebases O
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
, O
including O
InferSent O
( O
Conneau O
et O
al O
. O
, O
2017)6(w/ O
and O
w/o O
Elmo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
) O
, O
DAM O
( O
Parikh O
et O
al O
. O
, O
2016)7 O
, O
ESIM O
( O
Chen O
et O
al O
. O
, O
2017)8 O
, O
BERT B-MethodName
( O
uncased O
) O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
XLNet B-MethodName
( O
cased O
) O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019),9 O
. O
we O
map O
the O
vector O
at O
the O
position O
of O
the O
‘ O
[ O
CLS O
] O
’ O
token O
in O
the O
pretrained O
models O
to O
three O
- O
way O
NLI O
classiﬁcation O
via O
linear O
transformation O
. O

We O
show O
the O
per O
- O
layer O
analyses O
for O
RoBERTa B-MethodName
model O
in O
Table O
2 O
. O

We O
show O
the O
performance O
of O
a O
BERT B-MethodName
base O
model O
trained O
on O
MultiNLI O
utilizing O
the O
above O
mentioned O
model O
selection O
strategies O
in O
Table O
8 O
. O

With O
the O
advent O
of O
deep O
learning O
, O
also O
several O
approaches O
have O
been O
proposedrecently O
which O
utilize O
comprehensive O
neural B-MethodName
networks I-MethodName
. O

From O
a O
methodical O
view O
, O
a O
wide O
range O
of O
machine O
learning O
techniques O
is O
in O
use O
, O
including O
Bayesian O
models O
, O
logistic B-MethodName
regression I-MethodName
, O
support O
vector O
machines O
( O
SVM B-MethodName
) O
or O
decision B-MethodName
trees I-MethodName
. O

Thereby O
, O
various O
approaches O
have O
been O
proposed O
which O
use O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNN O
, O
Rhodes O
, O
2015 O
; O
Shrestha O
et O
al O
. O
, O
2017 O
) O
. O

In O
addition O
, O
studies O
have O
reported O
thatembeddings O
are O
also O
highly O
efﬁcient O
when O
fed O
into O
common O
machine O
learning O
techniques O
like O
SVMs B-MethodName
or O
logistic B-MethodName
regression I-MethodName
( O
Agun O
and O
Yilmazel O
, O
2017 O
; O
Posadas O
- O
Dur O
´ O
an O
et O
al O
. O
, O
2017 O
) O
. O

In O
general O
, O
it O
has O
been O
shown O
that O
especially O
character O
n O
- O
grams O
and O
variants O
thereof O
are O
among O
the O
most O
discriminating O
features O
, O
which O
perform O
very O
well O
with O
common O
machine O
learning O
techniques O
such O
as O
out O
- O
of O
- O
the O
- O
box O
SVMs B-MethodName
( O
e.g. O
, O
Stamatatos O
, O
2013 O
; O
Kestemont O
et O
al O
. O
, O
2018 O
) O
, O
ensembles O
( O
e.g. O
, O
Cust O
´ O
odio O
and O
Paraboni O
, O
2018 O
) O
as O
well O
as O
with O
recent O
deep O
learning O
methods O
( O
e.g. O
, O
Shrestha O
et O
al O
. O
, O
2017 O
; O
Rhodes O
, O
2015 O
) O
. O

It O
computes O
character O
3 O
- O
grams O
and O
makes O
classiﬁcations O
using O
a O
standard O
SVM B-MethodName
, O
yet O
achieving O
competitive O
results O
by O
applying O
grid O
search O
( O
Murauer O
et O
al O
. O
, O
2018 O
) O
. O

The O
other O
approaches O
generally O
perform O
similarly O
, O
except O
for O
the O
largest O
datasets O
where O
the O
SVM B-MethodName
achieved O
the O
best O
results O
( O
0.21 O
for O
the O
datasets O
with O
more O
than O
10,000 O
authors O
) O
. O

sf O
- O
VS O
andr O
- O
VS O
represent O
space O
- O
free O
and O
regular O
n O
- O
grams O
, O
respectively O
, O
represented O
in O
a O
vector O
space O
as O
proposed O
by O
Koppel O
et O
al O
. O
( O
2011 O
) O
, O
and O
SVM B-MethodName
refers O
to O
the O
reference O
implementation O
provided O
by O
PAN-2018 O
. O

Considering O
the O
best O
results O
of O
direct O
attribution O
as O
presented O
previously O
, O
we O
evaluated O
the O
regular O
3 O
- O
gram O
vector O
space O
approach O
and O
the O
SVM B-MethodName
implementation O
of O
PAN11 O
. O

It O
can O
be O
seen O
that O
in O
general O
the O
accuracy B-MetricName
– O
especially O
that O
of O
the O
SVM B-MethodName
– O
can O
be O
improved O
, O
nevertheless O
, O
the O
best O
ﬁrst O
- O
step O
reduction O
rate O
depends O
on O
the O
problem O
size O
: O
For O
datasets O
up O
to O
10,000 O
candidates O
, O
the O
best O
option O
is O
to O
reduce O
the O
number O
of O
authors O
by O
99 B-MetricValue
% O
before O
performing O
attribution O
. O

Considering O
the O
superiority O
of O
the O
SVM B-MethodName
with O
character O
3 O
- O
grams O
in O
the O
previous O
experiment O
, O
we O
compared O
the O
performance O
of O
the O
SVM B-MethodName
on O
all O
datasets O
of O
the O
SE-179 O
collection O
with O
more O
than O
5,000 O
authors O
with O
the O
performance O
on O
the O
recreated O
blog O
dataset O
. O

work O
that O
suggests O
document O
embeddings O
, O
nevertheless O
other O
embedding O
techniques O
like O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
fastText O
( O
Joulin O
et O
al O
. O
, O
2016 O
) O
orGloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
could O
be O
evaluated O
. O

In O
the O
case O
of O
authorship O
attribution O
, O
we O
similarly O
utilized O
a O
common O
, O
established O
technique O
( O
SVM B-MethodName
) O
. O

Tree O
kernels O
( O
TKs O
) O
and O
neural B-MethodName
networks I-MethodName
are O
two O
effective O
approaches O
for O
automatic O
feature O
engineering O
. O

We O
carry O
out O
an O
extensive O
comparison O
between O
different O
methods O
, O
i.e. O
, O
word2vec O
, O
using O
CBOW B-MethodName
and O
SkipGram B-MethodName
, O
andGlove O
, O
in O
terms O
of O
their O
impact O
on O
convolution O
semantic O
TKs O
for O
question O
classiﬁcation O
( O
QC O
) O
. O

In O
Tai O
et O
al O
. O
( O
2015 O
) O
, O
several O
LSTM B-MethodName
architectures O
that O
follow O
an O
order O
determined O
bysyntax O
are O
presented O
. O

In O
Mueller O
and O
Thyagarajan O
( O
2016 O
) O
, O
an O
LSTM B-MethodName
is O
used O
to O
encode O
similar O
sentences O
, O
and O
their O
Manhattan O
distance O
is O
minimized O
. O

In O
Neculoiu O
et O
al O
. O
( O
2016 O
) O
, O
a O
character O
level O
bidirectional O
LSTM B-MethodName
is O
used O
to O
determine O
the O
similarity O
between O
job O
titles O
. O

In O
Tan O
et O
al O
. O
( O
2016 O
) O
, O
the O
problem O
of O
question O
/ O
answer O
matching O
is O
treated O
as O
a O
similarity O
task O
, O
and O
convolutions O
and O
pooling O
on O
top O
of O
LSTM B-MethodName
states O
are O
used O
to O
extract O
the O
sentence O
representations O
. O

They O
can O
effec O
- O
Model O
Features O
Accuracy O
SVM B-MethodName
Unigram O
, O
syntactic O
information O
, O
parser O
output O
, O
WordNet B-DatasetName
features O
, O
hand O
- O
coded O
features95.0 O
DCNN O
Unsupervised O
vectors O
93.0 O
CNN O
ns O
CBOW B-MethodName
ﬁne O
- O
tuned O
vectors O
93.6 O
DepCNN O
Depencency O
guided O
ﬁlters O
95.6 O
SPTK O
SPTK O
and O
LSA O
word O
vectors O
94.8 O
Table O
1 O
: O
QC O
accuracy B-MetricName
( O
% O
) O
and O
description O
of O
SVM B-MethodName
( O
Silva O
et O
al O
. O
, O
2010 B-MetricValue
) O
, O
DCNN O
( O
Kalchbrenner O
et O
al O
. O
, O
2014 O
) O
, O
CNN O
ns O
( O
Kim O
, O
2014 O
) O
, O
DepCNN O
, O
( O
Ma O
et O
al O
. O
, O
2015 O
) O
and O
SPTK O
( O
Croce O
et O
al O
. O
, O
2011 O
) O
models O
. O

4 O
Context O
Word O
Embeddings O
for O
SPTK O
We O
propose O
to O
compute O
the O
similarity O
function O
σ O
in O
SPTK O
as O
the O
cosine O
similarity O
of O
word O
embeddings O
obtained O
with O
neural B-MethodName
networks I-MethodName
. O

We O
experimented O
with O
the O
popular O
Continuous O
BagOf O
- O
Words O
( O
CBOW B-MethodName
) O
, O
SkipGram B-MethodName
models O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
and O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

These O
networks O
suffer O
from O
the O
vanishing O
gradient O
problem O
( O
Bengio O
et O
al O
. O
, O
1994 O
) O
, O
which O
is O
mitigated O
by O
a O
popular O
RNN O
variant O
, O
the O
Long O
Short O
Term O
Memory O
( O
LSTM B-MethodName
) O
network O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

An O
LSTM B-MethodName
can O
control O
the O
amount O
of O
information O
from O
the O
input O
that O
affects O
its O
internal O
state O
, O
the O
amount O
of O
information O
in O
the O
internal O
state O
that O
can O
be O
forgotten O
, O
and O
how O
the O
internal O
state O
affects O
the O
output O
of O
the O
network O
. O

The O
Gated O
Recurrent O
Unit O
( O
GRU O
) O
( O
Chung O
et O
al O
. O
, O
2014 O
) O
is O
an O
LSTM B-MethodName
variant O
with O
similar O
performance O
and O
less O
parameters O
, O
thus O
faster O
to O
train O
. O

An O
alternative O
approach O
is O
to O
train O
the O
context O
embedding O
using O
neural B-MethodName
networks I-MethodName
on O
a O
sense O
annotated O
corpus O
, O
which O
can O
remap O
the O
word O
embeddings O
in O
a O
supervised O
fashion O
. O

our O
experiments O
as O
they O
are O
more O
efﬁcient O
and O
accurate O
than O
LSTMs B-MethodName
for O
our O
tasks O
. O

The O
QC O
dataset O
( O
Li O
and O
Roth O
, O
2006 O
) O
contains O
a O
set O
of O
questions O
labelled O
according O
to O
a O
twolayered O
taxonomy O
, O
which O
describes O
their O
expectedCBOW B-MethodName
SkipGram B-MethodName
GloVe B-MethodName
hs O
ns O
hs O
ns O
dim O
50 O
89.8 O
89.8 O
91.0 O
91.6 O
89.8 O
100 O
93.0 O
93.6 O
94.2 O
92.8 O
91.6 O
150 O
94.2 O
94.0 O
94.2 O
93.8 O
92.4 O
200 O
94.6 O
93.6 O
93.2 O
94.2 O
93.2 O
250 O
94.4 O
94.4 O
94.2 O
94.2 O
93.6 O
300 O
94.2 O
94.0 O
94.4 O
94.0 O
93.8 O
500 O
95.2 O
95.0 O
94.8 O
93.8 O
94.4 O
750 O
94.8 O
94.6 O
95.0 O
94.4 O
94.2 O
1000 O
93.4 O
95.2 O
95.2 O
94.6 O
94.0 O
Table O
2 O
: O
QC O
test O
set O
accuracies B-MetricName
( O
% O
) O
of O
NSPTK O
, O
given O
embeddings O
with O
window O
size O
equal O
to O
5 B-MetricValue
, O
and O
dimensionality O
ranging O
from O
50 O
to O
1,000 O
. O
answer O
type O
. O

We O
use O
GloVe B-MethodName
word O
embeddings O
( O
300 O
dimensions O
) O
, O
and O
we O
ﬁx O
them O
during O
training O
. O

Embeddings O
for O
words O
that O
are O
not O
present O
in O
2http://www.cis.uni-muenchen.de/ O
schmid O
/ O
tools O
/ O
TreeTagger/ O
3https://radimrehurek.com/gensim/CBOW B-MethodName
SkipGram B-MethodName
GloVe B-MethodName
LSA O
hs O
hs O
- O
dim O
100 O
84.47 O
84.63 O
82.92 O
250 O
85.75 O
85.85 O
85.04 O
85.39 O
500 O
86.48 O
86.32 O
85.73 O
Table O
3 O
: O
QC O
cross O
- O
validation O
accuracies B-MetricName
( O
% O
) O
of O
NSPTK O
given O
embeddings O
with O
the O
selected O
dimensionalities O
. O

word O
context O
QC O
test O
accuracy B-MetricName
QC O
CV O
accuracy B-MetricName
w2v O
- O
95.2 B-MetricValue
86.48 O
w2v O
w2v O
95.4 O
86.08† O
w2v O
p2v O
95.0 O
86.46 O
p2v O
- O
92.8 O
82.65† O
p2v O
p2v O
93.6 O
83.47† O
Table O
4 O
: O
QC O
accuracies B-MetricName
for O
the O
word O
embeddings O
( O
CBOW B-MethodName
vectors O
with O
500 B-MetricValue
dimensions O
, O
trained O
using O
hierarchical O
softmax O
) O
and O
paragraph2vec O
. O

Word O
vectors O
are O
trained O
with O
CBOW B-MethodName
( O
hs O
) O
and O
have O
500 O
dimensions O
. O

models O
using O
SVM B-MethodName
- O
Light O
- O
TK O
( O
Moschitti O
, O
2004 O
) O
, O
an O
SVM B-MethodName
- O
Light O
extension O
( O
Joachims O
, O
1999 O
) O
with O
tree O
kernel O
support O
. O

7.2 O
Context O
Embedding O
Results O
Table O
2 O
shows O
the O
QC O
accuracy B-MetricName
of O
NSPTK O
with O
CBOW B-MethodName
, O
SkipGram B-MethodName
and O
GloVe B-MethodName
. O

The O
performance O
for O
the O
CBOW B-MethodName
hierarchical O
softmax O
( O
hs O
) O
and O
negative O
sampling O
( O
ns O
) O
, O
and O
for O
the O
SkipGram B-MethodName
hssettings O
are O
similar O
. O

For O
the O
SkipGramnssettings B-MethodName
, O
the O
accuracy B-MetricName
is O
slightly O
lower O
for O
smaller O
dimension O
sizes O
. O

GloVe B-MethodName
embeddings O
yield O
a O
lower O
accuracy B-MetricName
, O
which O
steadily O
increases O
with O
the O
size O
of O
the O
embeddings O
. O

The O
English O
- O
Italian O
dataset O
provided O
by O
Dinu O
et O
al O
. O
( O
2015 O
) O
contains O
300 O
- O
dimensional O
CBOW B-MethodName
monolingual O
word O
embeddings O
for O
a O
total O
of O
200 O
K O
words O
trained O
on O
the O
WacKy O
crawling O
corpora.5 O
The O
English O
word O
embeddings O
use O
2.8 O
billion O
tokens O
( O
ukWAC O
+ O
Wikipedia O
+ O
BNC O
) O
and O
the O
Italian O
word O
embeddings O
use O
1.6 O
billion O
tokens O
( O
itWAC O
) O
. O

So O
we O
have O
three O
cross O
- O
lingual O
models O
: O
two O
versions O
of O
V O
ECMAP B-MetricName
, O
one O
trained O
on O
CBOW B-MethodName
( O
wordlevel O
) O
and O
the O
other O
on O
FastText O
( O
character O
- O
level O
) O
, O
and O
the O
concept O
- O
based O
adversarial O
model O
M2VEC O
, O
which O
also O
uses O
character O
- O
based O
FastText O
representations O
. O

Having O
two O
versions O
of O
V O
ECMAP B-MetricName
, O
one O
that O
use O
Skip O
- O
gram O
and O
one O
that O
uses O
CBOW B-MethodName
is O
based O
on O
their O
different O
performance O
on O
rare O
words O
. O

The O
CBOW B-MethodName
model O
predicts O
a O
word O
from O
its O
context O
and O
is O
better O
in O
accuracy B-MetricName
for O
frequent O
words O
, O
but O
encounters O
problems O
with O
rare O
words O
, O
while O
the O
Skip O
- O
gram O
model O
predicts O
the O
context O
from O
a O
target O
word O
, O
and O
so O
has O
good O
representation O
of O
rare O
words O
or O
phrases O
( O
Mikolov O
et O
al O
. O
, O
2013a O
, O
c O
) O
. O

To O
serve O
as O
a O
benchmark O
for O
future O
work O
, O
we O
provide O
two O
baselines O
, O
lemma O
- O
match O
, O
and O
a O
BERT B-MethodName
- O
based O
cross O
- O
encoder O
. O

Cross O
- O
Encoder O
: O
As O
a O
second O
baseline O
, O
we O
implement O
BERT B-MethodName
- O
based O
cross O
- O
encoder O
model O
. O

To O
serve O
as O
a O
benchmark O
for O
future O
coreference B-TaskName
resolution O
systems O
, O
we O
provide O
results O
on O
two O
baseline O
models O
, O
lemma O
- O
match O
and O
BERTbased B-MethodName
cross O
- O
encoder O
. O

Several O
scoring O
functions O
have O
been O
proposed O
in O
the O
literature O
, O
such O
as O
the O
conditional O
Bidirectional O
LSTM B-MethodName
( O
cBiLSTM B-MethodName
) O
( O
Rocktäschel O
et O
al O
. O
, O
2016 O
) O
, O
the O
Decomposable O
Attention O
Model O
( O
DAM O
) O
( O
Parikh O
et O
al O
. O
, O
2016 O
) O
, O
and O
the O
Enhanced O
LSTM B-MethodName
model O
( O
ESIM O
) O
( O
Chen O
et O
al O
. O
, O
2017 O
) O
. O

In O
this O
work O
, O
we O
use O
a O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
neural O
language O
model O
pL(w1;:::;w O
t)for O
generating O
low O
- O
perplexity B-MetricName
adversarial O
examples O
. O

TestMultiNLIcBiLSTM B-MethodName
61.52 O
63.95 O
66.98 O
66.68 O
DAM O
72.78 O
73.28 O
73.57 O
73.51 O
ESIM O
73.66 O
75.22 O
75.72 O
75.80SNLIcBiLSTM B-MethodName
81.41 O
80.99 O
82.27 O
81.12 O
DAM O
86.96 O
86.29 O
87.08 O
86.43 O
ESIM O
87.83 O
87.25 O
87.98 O
87.55 O
Table O
3 O
: O
Accuracy O
on O
the O
SNLI O
and O
MultiNLI O
datasets O
with O
different O
neural O
NLI O
models O
before O
( O
left O
) O
and O
after O
( O
right O
) O
adversarial O
regularisation O
. O

Model O
Rule O
jBj O
j O
B^:HjViolations O
( O
% O
) O
cBiLSTMR1 B-MethodName
1,098,734 O
261,064 O
23.76 O
% O
R2 O
174,902 O
80,748 O
46.17 O
% O
R3 O
197,697 O
24,294 O
12.29 O
% O
R4 O
176,768 O
33,435 O
18.91 O
% O
DAMR1 O
1,098,734 O
956 O
00.09 O
% O
R2 O
171,728 O
28,680 O
16.70 O
% O
R3 O
196,042 O
11,599 O
05.92 O
% O
R4 O
181,597 O
29,635 O
16.32 O
% O
ESIMR1 O
1,098,734 O
10,985 O
01.00 O
% O
R2 O
177,950 O
17,518 O
09.84 O
% O
R3 O
200,852 O
6,482 O
03.23 O
% O
R4 O
170,565 O
17,190 O
10.08 O
% O
Table O
4 O
: O
Violations O
( O
% O
) O
of O
rules O
R1;R2;R3;R4 O
from O
Table O
1 O
on O
the O
SNLI O
training O
set O
, O
yield O
by O
cBiLSTM B-MethodName
, O
DAM O
, O
and O
ESIM O
. O

8 O
Experiments O
We O
trained O
DAM O
, O
ESIM O
and O
cBiLSTM B-MethodName
on O
the O
SNLI O
corpus O
using O
the O
hyperparameters O
provided O
in O
the O
respective O
papers O
. O

In O
Table O
4 O
we O
report O
the O
number O
sentence O
pairs O
in O
the O
SNLI O
training O
set O
where O
DAM O
, O
ESIM O
and O
cBiLSTM B-MethodName
violate O
R1;R2;R3;R4 O
. O

In O
thejBj O
column O
we O
report O
the O
number O
of O
times O
the O
bodyModelDatasetA100 O
DAMA500 O
DAMA1000 O
DAMA100 O
ESIMA500 O
ESIMA1000 O
ESIMA100 O
cBiLSTMA500 B-MethodName
cBiLSTMA1000 B-MethodName
cBiLSTM B-MethodName
DAMAR83.33 O
79.15 O
79.37 O
71.35 O
72.19 O
70.05 O
93.00 O
88.99 O
86.00 O
DAM O
47.40 O
47.93 O
51.66 O
55.73 O
60.94 O
60.88 O
81.50 O
77.37 O
75.28 O
ESIMAR89.06 O
86.00 O
85.08 O
78.12 O
76.04 O
73.32 O
96.50 O
91.92 O
88.52 O
ESIM O
72.40 O
74.59 O
76.92 O
52.08 O
58.65 O
60.78 O
87.00 O
84.34 O
82.05 O
cBiLSTMAR85.42 B-MethodName
80.39 O
78.74 O
73.96 O
70.52 O
65.39 O
92.50 O
88.38 O
83.62 O
cBiLSTM B-MethodName
56.25 O
59.96 O
61.75 O
47.92 O
53.23 O
53.73 O
51.50 O
52.83 O
53.24 O
Table O
5 O
: O
Accuracy O
of O
unregularised O
and O
regularised O
neural O
NLI O
models O
DAM O
, O
cBiLSTM B-MethodName
, O
and O
ESIM O
, O
and O
their O
adversarially O
regularised O
versions O
DAMAR O
, O
cBiLSTMAR B-MethodName
, O
and O
ESIMAR O
, O
on O
adversarial O
datasets O
Ak O
m. O
of O
the O
rule O
holds O
, O
according O
to O
the O
model O
. O

However O
, O
in O
the O
case O
of O
cBiLSTM B-MethodName
, O
we O
can O
see O
that O
, O
each O
sentence O
s2S O
in O
the O
SNLI O
training O
set O
, O
with O
a O
23.76 O
% O
chance O
, O
sdoes O
not O
entail O
itself O
– O
which O
violates O
our O
background O
knowledge O
. O

This O
phenomenon O
happens O
16.70 O
% O
of O
times O
with O
DAM O
, O
9.84 O
% O
of O
times O
with O
ESIM O
, O
and O
46.17 O
% O
with O
cBiLSTM B-MethodName
: O
this O
indicates O
that O
all O
considered O
models O
are O
prone O
to O
violating O
R2 O
in O
their O
predictions O
, O
with O
ESIM O
being O
the O
more O
robust O
. O

We O
started O
with O
pre O
- O
trained O
DAM O
, O
ESIM O
, O
and O
cBiLSTM B-MethodName
models O
, O
trained O
using O
the O
hyperparameters O
published O
in O
their O
respective O
papers O
. O

By O
increasing O
the O
regularisation O
parameter O
2f10 4;10 3;10 2;10 1 O
g O
, O
we O
noticed O
sensible O
accuracy B-MetricName
increases O
, O
yielding O
relative O
accuracy B-MetricName
improvements O
up O
to O
75:8%in O
the O
case O
of O
DAM O
, O
and79:6%in O
the O
case O
of O
cBiLSTM B-MethodName
. O

For O
instance O
we O
can O
see O
that O
, O
while O
cBiLSTM B-MethodName
is O
vulnerable O
also O
to O
adversarial O
examples O
generated O
using O
DAM O
and O
ESIM O
, O
its O
adversari O
- O
ally O
regularised O
version O
cBiLSTMARis B-MethodName
generally O
more O
robust O
to O
any O
sort O
of O
adversarial O
examples O
. O

We O
introduce O
TAXINLI O
, O
a O
new O
dataset O
, O
that O
has O
10k O
examples O
from O
the O
MNLI B-MethodName
dataset O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
with O
these O
taxonomic O
labels O
. O

These O
corpora O
, O
in O
turn O
, O
have O
been O
successfully O
used O
to O
train O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
ydenotes O
equal O
contribution.Work O
was O
done O
while O
Authors O
were O
at O
Microsoft O
Research O
India.2019 O
) O
to O
achieve O
state O
- O
of O
- O
the O
- O
art O
( O
SOTA O
) O
performance O
in O
these O
tasks O
. O

Next O
, O
we O
annotate O
10,071 O
P O
- O
H O
pairs O
from O
the O
MNLI B-MethodName
dataset O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
with O
the O
lowest O
level O
taxonomic O
categories O
, O
18 O
in O
total O
( O
Sec O
3 O
) O
. O

Then O
we O
conduct O
various O
experiments O
and O
careful O
error O
analysis O
of O
the O
SOTA O
models O
— O
BERT B-MethodName
and O
RoBERTa B-MethodName
, O
as O
well O
as O
other O
baselines O
, O
such O
as O
Bag O
- O
of O
- O
words O
Na O
¨ıve O
Bayes O
and O
ESIM O
, O
on O
their O
performance O
across O
these O
categories(Sec O
4 O
) O
. O

Using O
the O
probing O
task O
methodology O
, O
researchers O
( O
Jawahar O
et O
al O
. O
, O
2019 O
; O
Goldberg O
, O
2019 O
) O
observed O
that O
BERT B-MethodName
captures O
syntactic O
structure O
, O
along O
with O
some O
semantics O
such O
as O
NER B-TaskName
, O
and O
semantic O
role O
labels O
( O
Tenney O
et O
al O
. O
, O
2019b O
) O
. O

However O
, O
BERT B-MethodName
’s O
ability O
to O
reason O
is O
questioned O
by O
the O
observed O
performance O
degradation O
in O
MNLI B-MethodName
( O
McCoy O
et O
al O
. O
, O
2019 O
) O
. O

Among O
recent O
error O
analysis O
efforts O
, O
the O
GLUE B-DatasetName
diagnostic O
dataset O
( O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
inference O
types O
for O
Adversarial O
NLI O
( O
Nie O
et O
al O
. O
, O
2019 O
) O
, O
the O
new O
CheckList B-MethodName
( O
Ribeiro O
et O
al O
. O
, O
2020 O
) O
system O
and O
the O
Stress O
Tests O
( O
Naik O
et O
al O
. O
, O
2018 O
) O
are O
mentionworthy O
. O

However O
, O
we O
observed O
exceedingly O
low O
number O
of O
examples O
in O
this O
category O
group O
from O
our O
pilot O
study O
on O
SNLI O
and O
MNLI B-MethodName
, O
and O
hence O
we O
remove O
these O
from O
our O
ﬁnal O
annotations O
. O

Table O
1 O
: O
For O
each O
category O
, O
we O
provide O
an O
example O
from O
the O
MNLI B-MethodName
dataset O
. O

For O
the O
MNLI B-MethodName
datapoints O
with O
‘ O
neutral O
’ O
gold O
labels O
, O
we O
realized O
, O
through O
observation O
and O
annotator O
feedback O
, O
that O
annotating O
the O
categories O
were O
difﬁcult O
, O
as O
sometimes O
the O
hypotheses O
could O
not O
be O
connected O
well O
back O
to O
their O
premise O
. O

For O
the O
entailment O
/ O
contradiction O
examples O
, O
We O
collected O
binary O
annotations O
for O
each O
of O
the O
15 O
categories O
in O
our O
NLI O
taxonomy O
, O
for O
datapoints O
in O
MNLI B-MethodName
which O
had O
‘ O
entailment O
’ O
or O
‘ O
contradiction O
’ O
as O
gold O
labels O
. O

We O
observe O
the O
average B-MetricName
inference O
accuracy B-MetricName
( O
86.7 B-MetricValue
% O
) O
is O
high O
despite O
known O
issues O
in O
MNLI B-MethodName
example O
ambiguity O
. O

Total O
datapoints O
10,071 O
Datapoints O
overlapping O
with O
MNLI2343 B-MethodName
( O
train O
) O
7728 O
( O
dev O
) O
Avg O
. O

We O
see O
that O
a O
large O
number O
of O
P O
- O
H O
pairs O
in O
MNLI B-MethodName
require O
lexical O
andsyntactic O
knowledge O
to O
make O
an O
inference O
; O
whereas O
the O
challenges O
of O
relational O
, O
spatial O
, O
and O
taxonomic O
for O
inference O
are O
not O
adequately O
represented O
. O

We O
observe O
that O
most O
categories O
show O
weak O
correlation B-MetricName
in O
the O
MNLI B-MethodName
dataset O
, O
hinting O
at O
a O
possible O
independence O
of O
categories O
with O
respect O
to O
each O
other O
. O

We O
specifically O
looked O
at O
the O
genre O
- O
wise O
split O
of O
datapoints O
containing O
boolean O
-quantifier O
and O
saw O
thatnearly O
25 O
% O
of O
them O
came O
from O
the O
‘ O
telephone O
’ O
genre O
of O
MNLI B-MethodName
. O

As O
baselines O
, O
we O
choose O
BERT B-MethodName
- O
base O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
and O
RoBERTa B-MethodName
- O
large O
( O
Liu O
et O
al O
. O
, O
2019b O
) O
as O
two O
state O
- O
of O
- O
the O
- O
art O
NLI O
systems O
. O

For O
our O
experiments O
, O
we O
use O
the O
pre O
- O
trained O
BERTbase B-MethodName
and O
RoBERTa B-MethodName
models O
from O
HuggingFace O
’s O
Transformers O
implementation O
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
. O

As O
pre O
- O
Transformer O
baselines O
, O
we O
use O
the O
bidirectional O
LSTM B-MethodName
- O
based O
Enhanced O
Sequential O
Inference O
model O
( O
ESIM O
) O
( O
Chen O
et O
al O
. O
, O
2017 O
) O
. O

We O
also O
train O
a O
Naive B-MethodName
Bayes I-MethodName
( O
NB O
) O
model O
using O
bag O
- O
of O
- O
words O
features O
for O
the O
P O
- O
H O
pairs O
after O
removing O
stop O
words4 O
. O

4.1 O
T O
AXINLI O
Error O
Analysis O
We O
report O
the O
NLI O
task O
accuracy B-MetricName
of O
the O
baseline O
systems O
on O
the O
MNLI B-MethodName
validations O
sets O
in O
Table O
3 B-MetricValue
. O

4Using O
NLTK O
’s O
RTEFeatureExtractorThe B-MetricName
systems O
are O
ﬁne O
- O
tuned O
on O
the O
MNLI B-MethodName
training O
set O
using O
the O
procedures O
followed O
in O
Devlin O
et O
al O
. O
( O
2019 O
) O
; O
Liu O
et O
al O
. O
( O
2019b O
) O
; O
Chen O
et O
al O
. O
( O
2017 O
) O
. O

MNLI B-MethodName
- O
dev O
NB O
ESIM O
BERT B-MethodName
BASE O
RoBERTa B-MethodName
LARGE O
Matched O
51.46 O
72.3 O
84.7 O
92.3 O
Mismatched O
52.31 O
72.1 O
84.8 O
90.0 O
Table O
3 O
: O
MNLI B-MethodName
- O
validation O
set O
accuracy B-MetricName
. O

We O
evaluate O
the O
systems O
on O
a O
total O
of O
7.7k O
examples O
, O
which O
are O
in O
the O
intersection O
of O
TAXINLI O
and O
the O
validation O
sets O
of O
MNLI B-MethodName
. O

We O
observe O
that O
compared O
to O
NB O
, O
the O
improvements O
in O
BERT B-MethodName
have O
been O
higher O
in O
lexical O
, O
syntactic O
categories O
compared O
to O
others O
. O

4.2 O
Factor O
Analysis O
In O
order O
to O
quantify O
the O
precise O
inﬂuence O
of O
the O
category O
labels O
on O
the O
prediction O
of O
the O
NLI O
models O
, O
we O
probe O
into O
indicators O
and O
confounding O
factors O
using O
two O
methods O
: O
linear O
discriminant O
analysis O
( O
LDA O
) O
and O
logistic B-MethodName
regression I-MethodName
( O
LR O
) O
. O

The O
coefﬁcients O
of O
these O
analyses O
on O
BERT B-MethodName
are O
shown O
in O
Fig O
6 O
. O

The O
values O
for O
RoBERTa B-MethodName
follow O
a O
similar O
trend O
, O
and O
are O
presented O
in O
the O
appendix O
. O

The O
earliest O
separation O
is O
observed O
for O
the O
lexical O
category O
, O
at O
layer O
3 O
in O
BERT B-MethodName
( O
and O
layer O
1 O
in O
RoBERTa B-MethodName
) O
, O
much O
before O
any O
other O
categories O
are O
realized O
. O

At O
layers O
6 O
in O
BERT B-MethodName
, O
and O
19 O
in O
RoBERTa B-MethodName
, O
about O
the O
same O
time O
as O
clustering O
by O
NLI O
label O
is O
seen O
, O
the O
connectives O
cluster O
is O
revealed O
. O

2 O
) O
, O
and O
syntactic O
categories O
are O
revealed O
in O
later O
layers O
( O
layer O
11 O
and O
12 O
in O
BERT B-MethodName
and O
layer O
21 O
in O
RoBERTa B-MethodName
) O
. O

The O
knowledge O
cate O
- O
gory O
is O
revealed O
more O
prominently O
in O
RoBERTa B-MethodName
at O
layer O
21 O
, O
while O
BERT B-MethodName
does O
not O
seem O
to O
show O
such O
a O
cluster O
. O

Recasting O
: O
The O
under O
- O
representation O
of O
certain O
categories O
in O
the O
MNLI B-MethodName
dataset O
raises O
a O
need O
for O
more O
balanced O
data O
collection O
. O

We O
present O
a O
systematic O
study O
of O
the O
linear O
geometry O
of O
contextualized O
word O
representations O
in O
ELMO O
and O
BERT B-MethodName
. O

Finally O
, O
we O
demonstrate O
that O
these O
linear O
subspaces O
are O
causally O
related O
to O
model O
behavior O
, O
and O
can O
be O
used O
to O
perform O
ﬁne O
- O
grained O
manipulation O
of O
BERT B-MethodName
’s O
output O
distribution O
. O

Additional O
ﬁndings O
include O
that O
linguistic O
features O
tend O
to O
be O
encoded O
in O
lower O
- O
dimensional O
subspaces O
in O
early O
layers O
of O
both O
ELMO O
and O
BERT B-MethodName
, O
and O
that O
relational O
features O
( O
like O
dependency O
relations O
between O
pairs O
of O
words O
) O
are O
encoded O
less O
compactly O
than O
categorical O
features O
like O
part O
of O
speech O
. O

As O
an O
example O
of O
how O
this O
kind O
of O
information O
can O
inform O
ongoing O
NLP O
research O
, O
we O
conclude O
with O
a O
demonstration O
that O
discovered O
subspaces O
can O
be O
used O
to O
exert O
ﬁne O
- O
grained O
control O
over O
masked O
language O
models O
: O
speciﬁc O
linear O
transformations O
of O
the O
last O
layer O
of O
BERT B-MethodName
narrowly O
ablate O
its O
ability O
to O
model O
agreement O
in O
nouns O
and O
verbs O
( O
Section O
6 O
) O
. O

ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
is O
a O
2 O
- O
layer O
bidirectional O
LSTM B-MethodName
trained O
for O
language O
modeling O
. O

BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
is O
a O
transformer O
trained O
on O
masked O
language O
modeling O
. O

POS B-TaskName
DLP O
DEP O
Real O
Control O
Real O
Control O
Real O
Control O
ELMo0 B-MethodName
.92 O
/ O
6 O
.95 O
/ O
256 O
.86 O
/ O
5 O
.55 O
/ O
512 O
.37 O
/ O
11 O
.76 O
/ O
13 O
1 B-MetricValue
.93 O
/ O
5 O
.88 O
/ O
256 O
.93 O
/ O
3 O
.40 O
/ O
512 O
.84 O
/ O
13 O
.82 O
/ O
17 O
2 O
.93 O
/ O
6 O
.82 O
/ O
256 O
.92 O
/ O
4 O
.33 O
/ O
256 O
.79 O
/ O
21 O
.78 O
/ O
23 O
BERT0 B-MethodName
.81 O
/ O
4 O
.77 O
/ O
26 O
.83 O
/ O
6 O
.47 O
/ O
256 O
.60 O
/ O
13 O
.80 O
/ O
7 O
1 O
.84 O
/ O
5 O
.80 O
/ O
128 O
.88 O
/ O
6 O
.43 O
/ O
256 O
.71 O
/ O
17 O
.80 O
/ O
9 O
4 O
.88 O
/ O
6 O
.80 O
/ O
128 O
.90 O
/ O
5 O
.38 O
/ O
256 O
.77 O
/ O
13 O
.79 O
/ O
10 O
8 O
.88 O
/ O
7 O
.75 O
/ O
256 O
.90 O
/ O
5 O
.25 O
/ O
64 O
.81 O
/ O
13 O
.71 O
/ O
12 O
12 O
.85 O
/ O
10 O
.68 O
/ O
256 O
.88 O
/ O
6 O
.20 O
/ O
24 O
.76 O
/ O
20 O
.65 O
/ O
11 O
Table O
1 O
: O
Each O
table O
entry O
reports O
two O
numbers O
: O
the O
accuracy B-MetricName
/dimensionality O
an O
approximately O
optimal O
ddimenional O
probe O
( O
Equation O
1 O
, O
with O
 O
= O
0:05 O
) O
. O

In O
general O
, O
real O
tasks O
are O
encoded O
in O
lower O
- O
dimensional O
subspaces O
than O
control O
tasks O
; O
in O
BERT B-MethodName
, O
representations O
become O
more O
diffuse O
( O
distributed O
across O
more O
dimensions O
) O
in O
deeper O
layers O
. O

4.4 O
Results O
Table O
1 O
shows O
the O
highest O
accuracy B-MetricName
of O
any O
probe O
on O
each O
task O
for O
each O
representation O
model O
and O
layer O
, O
and O
Figure O
2 B-MetricValue
plots O
accuracy B-MetricName
against O
projection O
rank O
for O
all O
BERT B-MethodName
layers O
on O
POS B-TaskName
and O
DLP O
. O

We O
see O
that O
layer O
1 O
of O
ELMo B-MethodName
and O
early O
layers O
of O
BERT B-MethodName
solve O
real O
tasks O
with O
a O
lower O
- O
dimensional O
subspace O
than O
the O
other O
layers O
. O

These O
results O
agree O
with O
V O
oita O
and O
Titov O
( O
2020 O
) O
, O
who O
ﬁnd O
that O
the O
middle O
layers O
of O
ELMo B-MethodName
and O
BERT B-MethodName
can O
be O
used O
to O
train O
probes O
of O
shorter O
description O
length.4 O
Contextual O
representations O
encode O
linguistic O
variables O
better O
, O
but O
not O
more O
compactly O
, O
than O
non O
- O
contextual O
word O
embeddings O
. O

For O
example O
, O
a O
probe O
trained O
on O
BERT-0 B-MethodName
achieves O
81 O
% O
accuracy B-MetricName
on O
the O
POS B-TaskName
task O
with O
only O
a O
4 B-MetricValue
- O
dimensional O
subspace O
, O
while O
BERT-8 B-MethodName
can O
achieve O
88 O
% O
accuracy B-MetricName
at O
the O
task O
, O
but O
requires O
almost O
twice O
as O
many O
dimensions O
. O

Probes O
generally O
struggle O
to O
learn O
non O
- O
linguistic O
control O
tasks O
from O
the O
projected O
representations O
, O
with O
the O
worst O
probes O
being O
trained O
on O
BERT-12 B-MethodName
and O
achieving O
68 O
% O
accuracy B-MetricName
on O
the O
POS B-TaskName
control O
and O
20 B-MetricValue
% O
accuracy B-MetricName
on O
the O
DLP O
control O
. O

BERT B-MethodName
layers O
0 O
, O
1 O
, O
and O
4 O
in O
particular O
do O
not O
appear O
to O
contain O
low O
- O
dimensional O
subspaces O
of O
their O
POS B-TaskName
subspace O
that O
solve O
the O
noun O
task O
. O

Bau O
et O
al O
. O
( O
2017 O
) O
propose O
a O
general O
framework O
for O
probing O
units O
in O
convolutional O
neural B-MethodName
networks I-MethodName
, O
and O
Dalvi O
et O
al O
. O
( O
2019 O
) O
similarly O
analyze O
the O
neurons O
of O
NLM O
and O
NMT O
from O
Section O
4 O
that O
the O
optimal O
dwith O
 O
= O
: O
05across O
any O
representation O
is O
10 O
, O
which O
is O
captured O
by O
our O
choice O
of O
d0.networks O
. O

Figure O
4 O
plots O
test O
accuracy B-MetricName
against O
axis O
- O
aligned O
projection O
size O
for O
BERT B-MethodName
. O

However O
, O
aside O
from O
BERT-8 B-MethodName
and O
BERT-12 B-MethodName
, O
all O
representations O
maintain O
over O
70 O
% O
accuracy B-MetricName
on O
POS B-TaskName
and O
DLP O
until O
the O
last O
third O
of O
representation O
components O
are O
zeroed O
. O

In O
general O
, O
later O
BERT B-MethodName
layers O
like O
BERT-8 B-MethodName
and O
BERT-12 B-MethodName
appear O
less O
axis O
aligned O
because O
probe O
accuracy B-MetricName
decreases O
fasterthan O
it O
does O
other O
layers O
. O

We O
project O
representations O
from O
the O
ﬁnal O
layer O
of O
BERT B-MethodName
out O
of O
the O
subspace O
that O
encodes O
part O
of O
speech O
information O
. O

6.1 O
Task O
We O
use O
three O
groups O
of O
sentences O
from O
the O
dataset O
of O
Marvin O
and O
Linzen O
( O
2018 O
) O
designed O
to O
test O
BERT B-MethodName
’s O
predictions O
at O
subject O
– O
verb O
agreement O
. O

If O
BERT B-MethodName
relies O
on O
the O
lowdimensional O
POS B-TaskName
information O
encoded O
in O
its O
representation O
, O
then O
ablating O
the O
POS B-TaskName
subspaces O
should O
impair O
its O
performance O
on O
the O
task O
. O

6.2 O
Ablation O
Method O
Our O
goal O
is O
to O
remove O
the O
low O
- O
dimensional O
linear O
part O
of O
speech O
information O
from O
BERT B-MethodName
representations O
. O

Nulling O
out O
nounspace O
( O
verbspace O
) O
should O
damage O
BERT B-MethodName
’s O
ability O
to O
distinguish O
nouns O
( O
verbs O
) O
from O
each O
other O
. O

Letting O
be O
the O
learned O
linear O
transformation O
and O
Uits O
left O
singular O
vectors O
, O
the O
projection O
onto O
the O
nullspace O
of O
is O
given O
by O
: O
N O
= O
I UU>(2 O
) O
Note O
that O
our O
method O
is O
similar O
to O
, O
but O
distinct O
from O
, O
the O
INLP O
method O
of O
Ravfogel O
et O
al O
. O
( O
2020 O
) O
, O
which O
has O
previously O
been O
used O
to O
study O
the O
causal O
relationships O
between O
linear O
encodings O
of O
linguistic O
variables O
and O
BERT B-MethodName
’s O
predictions O
( O
Elazar O
et O
al O
. O
, O
2021 O
) O
. O

By O
contrast O
, O
our O
method O
ablates O
a O
single O
subspace O
in O
which O
an O
arbitrary O
probe O
can O
learn O
to O
predict O
a O
set O
of O
labels O
, O
allowing O
us O
to O
evaluate O
how O
BERT B-MethodName
uses O
these O
subspaces O
to O
make O
predictions.6 O
6For O
comparison O
, O
we O
repeat O
our O
experiments O
using O
INLP O
in O
Appendix O
C O
and O
observe O
a O
less O
controlled O
effect O
. O
Marginal O
ProbabilityAblated O
Nothing O
Verbspace O
Nounspace O
Subject O
is O
noun O
.85 O
.85 O
.82 O
Matrix O
is O
verb O
.54 O
.50 O
.52 O
Table O
3 O
: O
Probability O
mass O
assigned O
by O
BERT B-MethodName
to O
nouns O
in O
the O
masked O
subject O
slot O
and O
verbs O
in O
the O
masked O
verb O
slot O
before O
and O
after O
ablation O
. O

We O
then O
feedsmasked O
to O
the O
BERT B-MethodName
transformer O
, O
recording O
only O
the O
output O
of O
the O
ﬁnal O
layer O
. O

We O
also O
measure O
the O
total O
probability O
mass O
that O
BERT B-MethodName
assigns O
to O
any O
noun O
in O
subject O
slot O
and O
any O
verb O
in O
the O
verb O
slot O
. O

6.4 O
Results O
Figure O
5 O
highlights O
that O
both O
ablations O
substantially O
reduce O
BERT B-MethodName
’s O
performance O
on O
subject O
– O
verb O
agreement O
. O

This O
effect O
is O
highly O
controlled O
: O
ablating O
nounspace O
does O
not O
appear O
to O
change O
BERT B-MethodName
’s O
ability O
to O
choose O
a O
verb O
that O
agrees O
with O
the O
subject O
, O
nor O
does O
ablating O
verbspace O
impair O
BERT B-MethodName
in O
choosing O
a O
subject O
that O
agrees O
with O
the O
verb O
. O

Neither O
ablation O
substantially O
alters O
BERT B-MethodName
’s O
ability O
to O
distinguish O
nouns O
and O
verbs O
from O
other O
parts O
of O
speech O
. O

Table O
3 O
shows O
that O
the O
verbspace O
ablation O
only O
slightly O
decreases O
the O
probability O
of O
BERT B-MethodName
predicting O
a O
verb O
in O
the O
matrix O
verb O
slot O
, O
and O
does O
not O
at O
all O
decrease O
the O
probability O
of O
predicting O
a O
noun O
in O
the O
subject O
slot O
. O

Ablating O
nounspace O
increases O
BERT B-MethodName
’s O
confusion O
about O
the O
subject O
( O
points O
below O
the O
line O
) O
, O
but O
has O
limited O
effect O
on O
its O
verb O
predictions O
( O
points O
on O
the O
line O
) O
. O

clude O
that O
our O
ablations O
are O
selective O
: O
they O
only O
impair O
BERT B-MethodName
’s O
ability O
to O
distinguish O
between O
subcategories O
of O
nouns O
and O
verbs O
, O
not O
its O
ability O
to O
reason O
about O
coarse O
parts O
of O
speech O
. O

It O
is O
surprising O
that O
our O
ablations O
produce O
such O
ﬁne O
- O
grained O
changes O
to O
BERT B-MethodName
’s O
outputs O
despite O
removing O
so O
little O
information O
from O
the O
representations O
. O

Ablation O
experiments O
reveal O
that O
BERT B-MethodName
relies O
on O
subspaces O
with O
as O
few O
as O
3 O
dimensions O
tomake O
ﬁne O
- O
grained O
part O
of O
speech O
distinctions O
when O
enforcing O
subject O
– O
verb O
agreement O
. O

1 O
Introduction O
End O
- O
to O
- O
end O
learning O
with O
neural B-MethodName
networks I-MethodName
has O
proven O
to O
be O
effective O
in O
parsing O
natural O
language O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
) O
. O

2 O
Related O
work O
Kiperwasser O
and O
Goldberg O
( O
2016 O
) O
use O
trainable O
BiLSTMs B-MethodName
to O
represent O
features O
of O
each O
word O
, O
instead O
of O
deﬁning O
the O
features O
manually O
. O

Instead O
, O
we O
trained O
a O
three O
- O
layer O
BiLSTM B-MethodName
from O
scratch O
to O
encode O
contextual O
features.3 O
Model O
In O
this O
section O
, O
we O
depict O
important O
aspects O
of O
our O
architecture O
which O
is O
shown O
in O
Figure O
1 O
. O

This O
model O
represents O
each O
word O
using O
a O
character O
- O
level O
LSTM B-MethodName
, O
which O
is O
a O
suitable O
setting O
for O
morphologically O
rich O
languages O
, O
as O
shown O
in O
Dozat O
et O
al O
. O
( O
2017 O
) O
. O

Sentence O
Model O
We O
used O
a O
three O
- O
layer O
bidirectional O
LSTM B-MethodName
to O
represent O
a O
sentence O
. O

We O
used O
the O
hidden B-HyperparameterName
size I-HyperparameterName
of O
200 B-HyperparameterValue
for O
both O
forward O
and O
backward O
LSTMs B-MethodName
. O

Dropout O
( O
Srivastava O
et O
al O
. O
, O
2014 O
) O
is O
performed O
at O
the O
input O
of O
each O
LSTM B-MethodName
layer O
, O
including O
the O
ﬁrst O
layer O
. O

Our O
LSTM B-MethodName
simply O
use O
nth O
hidden O
state O
for O
nth O
word O
, O
different O
from O
the O
language O
model O
in O
Kırnap O
et O
al O
. O
( O
2017 O
) O
. O

Formally O
( O
1 O
) O
h(arc dep O
) O
i O
= O
MLP(arc dep)(hi)(2 O
) O
h(arc head O
) O
i O
= O
MLP(arc head O
) O
( O
hi O
) O
( O
3 O
) O
h(rel dep O
) O
i O
= O
MLP(rel dep)(hi O
) O
( O
4 O
) O
h(rel head O
) O
i O
= O
MLP(rel head O
) O
( O
hi O
) O
where O
hirepresents O
ith O
hidden O
state O
of O
the O
biLSTM B-MethodName
embedding O
. O

Lazaridou O
et O
al O
. O
( O
2015 O
) O
extend O
the O
architecture O
of O
the O
skip O
- O
gram O
model O
associated O
with O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
to O
incorporate O
a O
measure O
of O
visual O
semantic O
information O
by O
forcing O
the O
network O
to O
learn O
linguistic O
and O
visualbased O
features O
. O

Instead O
of O
performing O
a O
contextbased O
prediction O
task O
, O
Ngiam O
et O
al O
. O
( O
2011 O
) O
combine O
multimodal O
information O
from O
both O
audio O
and O
image O
- O
based O
information O
using O
a O
stacked O
autoencoder O
to O
reconstruct O
both O
modalities O
with O
a O
shared O
representation O
layer O
in O
the O
middle O
of O
the O
network O
. O
Modality O
Source O
Embeddings O
# O
D O
# O
S O
Text O
GloVe B-MethodName
1000 O
200 O
Text O
Word2Vec B-MethodName
1000 O
200 O
Image O
CNN O
- O
Mean O
6144 O
1000 O
Image O
CNN O
- O
Max O
6144 O
1000 O
Both O
CNN O
- O
Mean O
+ O
GloVe B-MethodName
7144 O
200 O
Both O
CNN O
- O
Max O
+ O
GloVe B-MethodName
7144 O
200 O
Both O
CNN O
- O
Mean O
+ O
Word2Vec B-MethodName
7144 O
200 O
Both O
CNN O
- O
Max O
+ O
Word2Vec B-MethodName
7144 O
200 O
Table O
1 O
: O
List O
of O
all O
dense O
( O
D O
) O
and O
sparse O
( O
S O
) O
models O
used O
in O
this O
paper O
, O
and O
the O
number O
of O
dimensions O
( O
# O
) O
in O
each O
model O
. O

3.1 O
Text O
- O
based O
models O
We O
implemented O
two O
state O
- O
of O
- O
the O
- O
art O
text O
- O
based O
embedding O
models O
, O
Word2Vec B-MethodName
and O
GloVe B-MethodName
, O
to O
act O
as O
initialisers O
for O
our O
sparse O
models O
, O
following O
a O
similar O
approach O
to O
Faruqui O
et O
al O
. O
( O
2015 O
) O
. O

GloVe B-MethodName
. O

It O
achieves O
this O
by O
constructing O
real O
vector O
embeddings O
using O
bilinear O
logistic B-MethodName
regression I-MethodName
with O
nonzero O
word O
co O
- O
occurrences O
in O
the O
training O
corpus O
within O
a O
speciﬁc O
context O
. O

Word2Vec B-MethodName
. O

Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
uses O
shallow O
neural B-MethodName
networks I-MethodName
with O
negative O
sampling O
techniques O
, O
which O
are O
trained O
to O
predict O
either O
the O
word O
from O
the O
context O
or O
the O
context O
from O
the O
word O
using O
a O
ﬁxed O
window O
of O
words O
as O
the O
context O
. O

In O
particular O
, O
we O
choose O
the O
CBOW B-MethodName
version O
( O
predict O
the O
word O
using O
the O
context O
) O
of O
this O
model O
which O
was O
trained O
using O
the O
gensim O
package O
with O
the O
minimum O
word O
count O
threshold B-MetricName
set O
to O
0(i.e O
. O
, O
a O
vector O
representation O
was O
created O
for O
allwords O
in O
the O
corpus O
) O
. O

JNNSE O
gives O
a O
new O
joint O
sparse O
feature O
matrixA2Rwpby O
minimising O
the O
objective O
function O
: O
arg O
min O
D(x);D(y);AwX O
i=1jjXi;: Ai;:D(x)jj2 O
+ O
wX O
i=1jjYi;: Ai;:D(y)jj2+jjAi;:jj1(3 O
) O
where O
D(x O
) O
i;:D(x)T O
i;:1;81ip O
D(y O
) O
i;:D(y)T O
i;:1;81ip O
Ai;j0;81iw;81jp O
For O
the O
NNSE O
factorization O
of O
each O
of O
the O
four O
initial O
dense O
unimodal O
text O
and O
image O
models O
( O
GloVe B-MethodName
, O
Word2Vec B-MethodName
, O
CNN O
- O
Mean O
and O
CNNMax O
) O
, O
the O
sparsity O
parameter O
was O
set O
to O
0.05 O
and O
each O
model O
’s O
dimensionality O
( O
p O
) O
was O
reduced O
down O
from O
its O
original O
size O
by O
a O
factor O
of O
approximately O
5 O
; O
the O
text O
embedding O
size O
was O
reduced O
to O
200and O
both O
image O
model O
embedding O
sizes O
were O
reduced O
to O
1000 O
( O
see O
Table O
1 O
) O
. O

Not O
all O
words O
used O
in O
the O
similarity O
benchmarks O
appear O
in O
our O
word O
embedding O
mod O
- O
Model O
EncyclopedicFunctional O
Taxonomic O
Visual O
Other O
Perceptual O
Overall O
CNN O
- O
Mean O
23.479 O
28.309 O
45.756 O
31.256 O
26.467 O
29.244 O
CNN O
- O
Max O
22.878 O
28.765 O
50.140 O
32.843 O
27.508 O
30.202 O
GloVe B-MethodName
30.870 O
37.176 O
61.517 O
35.909 O
38.385 O
36.984 O
Word2Vec B-MethodName
27.494 O
30.372 O
55.455 O
32.298 O
32.800 O
32.363 O
GloVe B-MethodName
NNSE O
31.171 O
34.645 O
59.497 O
35.066 O
36.738 O
35.880 O
Word2Vec B-MethodName
NNSE O
29.662 O
34.320 O
55.073 O
35.302 O
33.261 O
34.956 O
CNN O
- O
Max O
NNSE O
15.320 O
17.138 O
26.263 O
19.646 O
17.453 O
18.279 O
CNN O
- O
Mean O
NNSE O
15.996 O
18.297 O
27.330 O
20.954 O
18.376 O
19.339 O
CNN O
- O
Max O
+ O
GloVe B-MethodName
30.669 O
37.404 O
63.887 O
35.790 O
36.077 O
36.760 O
CNN O
- O
Mean O
+ O
GloVe B-MethodName
31.560 O
38.441 O
64.459 O
36.675 O
36.625 O
37.637 O
CNN O
- O
Max O
+ O
Word2Vec B-MethodName
22.114 O
24.653 O
51.471 O
27.566 O
27.332 O
27.088 O
CNN O
- O
Mean O
+ O
Word2Vec B-MethodName
22.057 O
24.780 O
51.926 O
27.527 O
27.407 O
27.124 O
CNN O
- O
Max O
+ O
GloVe B-MethodName
JNNSE O
32.481 O
38.787 O
63.669 O
39.848 O
36.245 O
39.080 O
CNN O
- O
Mean O
+ O
GloVe B-MethodName
JNNSE O
31.104 O
38.009 O
64.866 O
40.267 O
35.998 O
38.784 O
CNN O
- O
Max O
+ O
Word2Vec B-MethodName
JNNSE O
32.718 O
38.601 O
61.493 O
39.663 O
36.496 O
38.901 O
CNN O
- O
Mean O
+ O
Word2Vec B-MethodName
JNNSE O
31.084 O
36.939 O
57.659 O
38.145 O
33.436 O
37.057 O
Table O
2 O
: O
Average B-MetricName
cross O
- O
validation O
F1 B-MetricName
100scores B-MetricName
for O
each O
model O
. O

For O
each O
embedding O
model O
, O
we O
train O
an O
L2regularised O
logistic B-MethodName
regression I-MethodName
classiﬁer O
for O
each O
property O
that O
predicts O
whether O
the O
property O
is O
true O
for O
a O
given O
concept O
. O

For O
the O
logistic B-MethodName
regression I-MethodName
model O
trained O
for O
each O
semantic O
property O
, O
we O
therefore O
balance O
positive O
and O
negative O
training O
items O
by O
weighting B-HyperparameterName
coefﬁcients O
inversely O
proportional O
to O
the O
frequency O
of O
the O
two O
classes O
. O

To O
evaluate O
the O
logistic B-MethodName
regression I-MethodName
models O
’ O
ability O
to O
predict O
human O
property O
knowledge O
for O
held O
- O
out O
concepts O
, O
we O
used O
5 O
- O
fold O
cross O
- O
validation O
with O
stratiﬁed O
sampling O
to O
ensure O
that O
at O
least O
one O
positive O
case O
occurred O
in O
each O
test O
set O
. O

Using O
the O
embedding O
dimensions O
as O
training O
data O
, O
we O
train O
on O
the O
4folds O
and O
test O
on O
the O
ﬁnal O
fold O
, O
and O
evaluate O
the O
logistic B-MethodName
regression I-MethodName
classiﬁer O
by O
taking O
the O
average B-MetricName
F1 B-MetricName
score B-MetricName
over O
all O
the O
test O
folds O
. O

To O
address O
this O
question O
, O
we O
evaluated O
how O
the O
dense O
and O
sparse O
embeddings O
differ O
in O
their O
degree O
of O
correspondence O
to O
the O
property O
norms O
by O
analysing O
the O
ﬁtted O
parameters O
of O
our O
property O
prediction O
logistic B-MethodName
regression I-MethodName
classiﬁers O
. O

For O
each O
embedding O
model O
and O
semantic O
property O
, O
we O
average B-MetricName
the O
ﬁtted O
parameters O
in O
the O
logistic B-MethodName
regression I-MethodName
models O
across O
cross O
- O
validation O
iterations O
and O
extract O
the O
20parameters O
with O
the O
highest O
average B-MetricName
magnitude O
. O

Figure O
2 O
shows O
the O
magnitudes O
of O
these O
20 O
averaged B-MetricName
parameters O
for O
the O
dense O
and O
sparse O
multimodal O
GloVe+CNN B-MethodName
- O
Mean O
models4 O
. O

For O
a O
given O
semantic O
property O
, O
we O
can O
test O
which O
of O
two O
embedding O
models O
best O
encode O
that O
semantic O
property O
in O
a O
single O
dimension O
– O
an O
embedding O
model O
that O
more O
directly O
matches O
the O
property O
norm O
data O
will O
tend O
to O
have O
a O
dimension O
that O
correlates O
more O
strongly O
with O
that O
4The O
results O
are O
similar O
for O
all O
other O
pairs O
of O
sparse O
and O
dense O
models O
. O
GloVe B-MethodName
Word2Vec B-MethodName
CNN O
- O
Max O
CNN O
- O
Mean O
CNN O
- O
Max O
+ O
GloVeCNN B-MethodName
- O
Mean O
+ O
GloVeCNN B-MethodName
- O
Max O
+ O
Word2VecCNN B-MethodName
- O
Mean O
+ O
Word2Vec B-MethodName
fMRI O
( O
S O
) O
0.654 O
0.652 O
0.641 O
0.647 O
0.662 O
0.686 O
0.649 O
0.671 O
fMRI O
( O
D O
) O
0.670 O
0.676 O
0.654 O
0.651 O
0.673 O
0.677 O
0.676 O
0.676 O
MEG O
( O
S O
) O
0.664 O
0.669 O
0.651 O
0.641 O
0.671 O
0.668 O
0.675 O
0.665 O
MEG O
( O
D O
) O
0.680 O
0.664 O
0.654 O
0.643 O
0.684 O
0.684 O
0.664 O
0.664 O
Table O
3 O
: O
Results O
of O
all O
sparse O
( O
S O
) O
and O
dense O
( O
D O
) O
models O
on O
2vs.2tests O
against O
the O
fMRI O
and O
MEG O
neuroimaging O
data O
, O
averaged B-MetricName
over O
participants O
. O

Figure O
2 O
: O
The O
ranking O
of O
the O
top O
20model O
coefﬁcients O
for O
the O
logistic B-MethodName
regression I-MethodName
classiﬁers O
trained O
on O
each O
feature O
, O
for O
the O
dense O
GloVe B-MethodName
+ O
CNN O
- O
Mean O
model O
( O
blue O
bars O
) O
, O
and O
the O
joint O
sparse O
GloVe B-MethodName
+ O
CNN O
- O
Mean O
model O
( O
red O
bars O
) O
. O

For O
each O
distributional O
model O
, O
we O
calculated O
the O
pairwise O
correlation B-MetricName
between O
concepts O
to O
produce O
the O
6060GloVe B-MethodName
Word2Vec B-MethodName
CNN O
- O
Max O
CNN O
- O
Mean O
CNN O
- O
Max O
+ O
GloVeCNN B-MethodName
- O
Mean O
+ O
GloVeCNN B-MethodName
- O
Max O
+ O
Word2VecCNN B-MethodName
- O
Mean O
+ O
Word2Vec B-MethodName
fMRI O
( O
D O
) O
0.162 B-MetricValue
0.164 O
0.145 O
0.151 O
0.150 O
0.152 O
0.152 O
0.155 O
fMRI O
( O
S O
) O
0.138 O
0.136 O
0.140 O
0.144 O
0.139 O
0.140 O
0.154 O
0.168 O
MEG O
( O
D O
) O
0.163 O
0.161 O
0.163 O
0.158 O
0.162 O
0.158 O
0.168 O
0.162 O
MEG O
( O
S O
) O
0.168 O
0.152 O
0.149 O
0.149 O
0.152 O
0.157 O
0.145 O
0.147 O
Table O
4 O
: O
Average B-MetricName
RSA O
results O
( O
Spearman O
’s O
 O
) O
for O
all O
sparse O
( O
S O
) O
and O
dense O
( O
D O
) O
models O
. O

For O
the O
fMRI O
data O
, O
the O
model O
with O
the O
highest O
average B-MetricName
2vs.2 O
test O
score B-MetricName
is O
the O
sparse O
multimodal O
GloVe+CNNMax B-MethodName
embedding O
, O
whilst O
on O
the O
MEG O
data O
the O
highest O
scoring O
model O
is O
a O
tie O
between O
the O
dense O
multimodal O
GloVe+CNN B-MethodName
- O
Max O
embedding O
and O
the O
dense O
multimodal O
GloVe+CNN B-MethodName
- O
Mean O
embedding O
. O

The O
results O
show O
that O
sparse O
models O
give O
the O
closest O
representation O
to O
both O
fMRI O
and O
MEG O
data O
, O
with O
the O
multimodal O
sparse O
word2vec+CNN O
- O
Mean O
model O
best O
ﬁtting O
the O
fMRI O
data O
, O
and O
the O
sparse O
GloVe B-MethodName
model O
best O
ﬁtting O
the O
MEG O
data O
. O

This O
approach O
is O
inspired O
by O
shift O
- O
reduce O
parsing O
withstack O
LSTMs B-MethodName
( O
Dyer O
et O
al O
. O
, O
2015 O
) O
and O
transitionbased O
named O
entity O
recognition O
( O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

Architecture O
The O
hard O
monotonic O
attention O
model O
uses O
a O
single O
- O
layer O
bidirectional O
LSTM B-MethodName
encoder O
( O
Graves O
and O
Schmidhuber O
, O
2005 O
) O
to O
encode O
input O
lemma O
x1 O
: O
nas O
a O
sequence O
of O
vectors O
h1 O
: O
n O
, O
hi∈R2H O
, O
whereHis O
the O
hidden O
dimension O
of O
the O
LSTM B-MethodName
layer O
. O

The O
sequence O
of O
states O
is O
modeled O
2We O
refer O
the O
reader O
to O
Aharoni O
and O
Goldberg O
( O
2017 O
) O
for O
the O
description O
of O
the O
algorithm.with O
a O
single O
- O
layer O
LSTM B-MethodName
that O
receives O
, O
at O
time O
t O
, O
a O
concatenated O
input O
of O
: O
1 O
. O
the O
currently O
attended O
vector O
hi∈R2H O
, O
whereiis O
the O
attention O
pointer O
, O
2 O
. O
the O
concatenated O
vector O
of O
feature O
embeddings O
f∈RF·|Φ| O
, O
whereFis O
the O
dimension O
of O
the O
feature O
embedding O
layer O
, O
3 O
. O
the O
embedding O
of O
the O
previous O
output O
action O
E(at−1)∈RE O
, O
whereEis O
the O
dimension O
of O
the O
action O
embedding O
layer O
. O

st O
= O
LSTM B-MethodName
/ O
parenleftbig O
[ O
E(at−1);hi;f]/parenrightbig O
( O
2 O
) O
LetΣtrain⊆Σbe O
the O
set O
of O
characters O
in O
training O
data O
. O

The O
HAEM B-MetricName
system O
, O
however O
, O
increments O
the O
attention O
index O
on O
every O
COPY O
action.4.3 O
Architecture O
Similarly O
to O
HACM O
, O
the O
input O
lemma O
is O
encoded O
as O
a O
sequence O
of O
vectors O
h1 O
: O
n O
, O
hi∈R2Hwith O
a O
single O
- O
layer O
bidirectional O
LSTM B-MethodName
. O

Additionally O
, O
we O
use O
a O
single O
- O
layer O
LSTM B-MethodName
to O
represent O
the O
predicted O
inﬂected O
form O
y1 O
: O
m O
, O
to O
which O
we O
refer O
as O
LSTMy B-MethodName
. O

In O
case O
the O
model O
outputs O
a O
character O
with O
WRITE O
σor O
COPY O
, O
LSTMygets B-MethodName
updated O
with O
the O
embedding O
of O
this O
character O
. O

At O
time O
t O
, O
a O
concatenation O
of O
: O
1 O
. O
the O
currently O
attended O
vector O
hi∈R2H O
, O
2 O
. O
the O
set O
- O
of O
- O
features O
vector O
f∈R|Φ| O
, O
3 O
. O
the O
output O
of O
the O
latest O
state O
y∈RHof O
the O
inﬂected O
form O
representation O
LSTMy B-MethodName
, O
passes O
through O
a O
rectiﬁer O
linear O
unit O
( O
ReLU O
) O
layer O
( O
Glorot O
et O
al O
. O
, O
2011 O
) O
to O
ﬁnally O
produce O
the O
state O
vector O
st O
. O

In O
our O
experiments O
, O
we O
extend O
it O
to O
include O
two O
more O
representations O
: O
an O
LSTM B-MethodName
that O
represents O
the O
action O
history O
, O
LSTMa B-MethodName
, O
and O
another O
LSTM B-MethodName
that O
encodes O
a O
sequence O
of O
deleted O
lemma O
characters O
, O
LSTMd B-MethodName
. O

The O
deletion O
LSTMd B-MethodName
gets O
emptied O
once O
a O
WRITE O
σaction O
is O
generated O
. O

In O
the O
extended O
system O
, O
the O
state O
stis O
thus O
derived O
from O
an O
input O
vector O
[ O
y;hi;f;a;d O
] O
, O
where O
a∈RHis O
the O
output O
of O
the O
latest O
state O
of O
the O
action O
history O
LSTMaandd∈RHthe B-MethodName
output O
of O
the O
latest O
state O
of O
the O
deletion O
LSTMd B-MethodName
. O

The O
improved O
models O
will O
be O
available O
soon O
in O
UDPipe.2 O
2 O
Related O
Work O
Deep O
neural B-MethodName
networks I-MethodName
have O
recently O
achieved O
remarkable O
results O
in O
many O
areas O
of O
machine O
learning O
. O

With O
a O
practical O
method O
for O
pretraining O
word O
embeddings O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
routine O
utilization O
of O
recurrent O
neural B-MethodName
networks I-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Cho O
et O
al O
. O
, O
2014 O
) O
, O
deep O
neural B-MethodName
networks I-MethodName
achieved O
state O
- O
of O
- O
the O
- O
art O
results O
in O
many O
NLP O
areas O
like O
POS B-TaskName
tagging O
( O
Ling O
et O
al O
. O
, O
2015 O
) O
, O
named O
entity O
recognition O
( O
Yang O
et O
al O
. O
, O
2016 O
) O
or O
machine O
translation O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

Many O
other O
parser O
models O
followed O
, O
employing O
various O
techniques O
like O
stack O
LSTM B-MethodName
( O
Dyer O
et O
al O
. O
, O
2015 O
) O
, O
global O
normalization O
( O
Andor O
et O
al O
. O
, O
2016 O
) O
, O
biafﬁne O
attention O
( O
Dozat O
and O
Manning O
, O
2016 O
) O
or O
recurrent O
neural O
network O
grammars O
( O
Kuncoro O
et O
al O
. O
, O
2016 O
) O
, O
improving O
LAS O
score B-MetricName
in O
English O
and O
Chinese O
dependency O
parsing O
by O
more O
than O
2 B-MetricValue
points O
2http://ufal.mff.cuni.cz/udpipein O
2016 O
. O

4.3 O
POS B-TaskName
Tagging O
We O
process O
the O
embedded O
words O
through O
a O
multi O
- O
layer O
bidirectional O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
to O
obtain O
contextualized O
embeddings O
. O

Notably O
, O
the O
shared O
contextualized O
embeddings O
are O
computed O
using O
2 O
layers O
of O
bidirectional O
LSTM B-MethodName
. O

The O
loosely O
joint O
model O
shares O
only O
the O
word O
embeddings O
between O
the O
tagger O
and O
the O
parser O
, O
which O
both O
compute O
contextualized O
embeddings O
using O
2 O
layers O
of O
bidirectional O
LSTM B-MethodName
, O
resulting O
again O
in O
4 O
RNN O
layers O
. O

The O
trained O
word O
embeddings O
and O
the O
sentence O
- O
level O
LSTMs B-MethodName
have O
a O
dimension O
of O
512 O
. O

Besides O
, O
the O
multilingual O
variant O
of O
BERT B-MethodName
( O
mBERT B-MethodName
) O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
has O
shown O
to O
perform O
well O
in O
cross O
- O
lingual O
tasks O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
and O
outperform O
the O
models O
trained O
on O
multilingual O
word O
embeddings O
by O
a O
large O
margin O
. O

Therefore O
, O
we O
consider O
conducting O
experiments O
with O
both O
multilingual O
word O
embeddings O
and O
mBERT B-MethodName
. O

We O
use O
aligned O
multilingual O
word O
embeddings O
( O
Smith O
4https://github.com/uclanlp/CrossLingualDepParser O
5We O
adopt O
the O
same O
hyper O
- O
parameters O
, O
experiment O
settings O
and O
evaluation O
metrics O
as O
those O
in O
( O
Ahmad O
et O
al O
. O
, O
2019).et O
al O
. O
, O
2017 O
; O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
with O
300dimensionss O
or O
contextualized O
word O
representations O
provided O
by O
multilingual O
BERT6(Devlin B-MethodName
et O
al O
. O
, O
2019 O
) O
with O
768dimensions O
as O
the O
word O
representations O
. O

We O
train O
the O
parsing O
models O
for400and500epochs B-HyperparameterName
with O
multilingual O
BERT B-MethodName
and O
multilingual O
word O
embeddings O
respectively O
. O

9With O
the O
source O
( O
English O
) O
and O
six O
auxiliary O
languages O
. O
LangMultilingual O
Word O
Embeddings O
Multilingual O
BERT B-MethodName
( O
en O
) O
( O
en O
- O
fr O
) O
( O
en O
- O
ru O
) O
( O
en O
) O
( O
en O
- O
fr O
) O
( O
en O
- O
ru O
) O
en O
90.23 O
/88.23 O
90.01/88.08 O
89.93/87.93 O
93.19 O
/91.21 O
92.81/90.97 O
92.77/90.86 O
no O
80.82/72.94 O
80.60/72.83 O
80.98 O
/73.10 O
85.81 O
/79.03 O
85.50/78.64 O
85.43/78.76 O
sv O
80.33/72.54 O
79.90/72.16 O
80.43 O
/72.68 O
85.61/78.34 O
85.64 O
/78.58 O
85.44/78.33 O
fr O
77.71/72.35 O
78.49y/73.30y78.31/73.29 O
85.22/80.78 O
84.76/80.26 O
85.91y/81.63y O
pt O
76.41/67.35 O
76.88y/67.74 O
77.09y/67.81 O
82.93/73.33 O
82.71/73.13 O
83.43y/73.88y O
da O
76.58 O
/68.11 O
75.99/67.64 O
76.25/68.03 O
82.36/73.53 O
82.40 O
/73.68 O
82.36/ O
73.86y O
es O
73.76/65.46 O
74.14 O
/65.78 O
74.08/ O
65.84 O
80.81/72.66 O
81.11/72.80 O
81.38y/73.29y O
it O
80.89/75.61 O
81.33y/76.14y80.70/75.57 O
87.07/82.38 O
86.90/82.22 O
87.41 O
/82.67 O
hr O
62.21/52.67 O
63.38y/53.83y63.11y/53.62y72.96/62.65 O
73.39y/62.20 O
74.20y/63.55y O
ca O
73.18/64.53 O
73.46y/64.71 O
73.40/ O
64.90y80.40/71.42 O
80.30/71.42 O
80.75 O
/71.78 O
pl O
74.65/62.72 O
75.65y/63.31y75.93 O
/63.60 O
81.51/69.25 O
82.33y/69.91y82.48y/70.54y O
uk O
59.25/51.92 O
60.58y/52.72y60.81y/52.66y69.98/61.52 O
70.24/61.61 O
71.21y/62.84y O
sl O
67.51/56.42 O
68.14/56.52 O
68.40 O
/56.87 O
75.15/63.12 O
74.60/62.52 O
75.50 O
/63.65y O
nl O
68.54/59.99 O
68.80/60.23 O
69.23y/60.51y76.76/68.35 O
76.94 O
/68.28 O
76.89/ O
68.76y O
bg O
79.09/67.61 O
80.01y/68.42 O
79.72/68.39 O
86.82/75.47 O
87.08/75.40 O
87.61y/75.94y O
ru O
60.91/52.03 O
61.42y/52.27y61.67y/52.41y71.92/62.09 O
72.31/62.15 O
72.88y/62.94y O
de O
71.41/61.97 O
70.70/61.41 O
71.05/61.84 O
78.66/69.81 O
78.04/69.23 O
79.08y/70.26y O
he O
55.70/48.08 O
57.33y/49.37y57.15y/49.36y64.46/ O
55.82 O
64.97y/55.63 O
65.30y/55.76 O
cs O
63.30/54.14 O
63.94y/54.63y64.37y/55.08y73.78/63.52 O
74.57y/63.86 O
74.56y/64.17y O
ro O
65.13/53.98 O
65.86 O
/54.76 O
65.57/54.42 O
75.10/62.99 O
75.85y/63.92y76.06y/63.78y O
sk O
66.79/58.23 O
67.46y/58.77 O
67.42y/58.70 O
76.30/67.38 O
77.08y/67.57 O
77.86y/68.28y O
i O
d O
49.85/44.09 O
52.05y/45.76y51.57/45.31 O
56.80/50.24 O
57.45y/50.27 O
57.30y/50.70y O
lv O
70.45/49.47 O
70.03/49.38 O
70.67y/49.61y75.63 O
/53.93 O
75.27/53.78 O
75.62/ O
54.29 O
ﬁ O
66.11/48.73 O
65.84/48.61 O
66.28 O
/48.82 O
71.59/ O
53.81 O
71.35/53.63 O
71.74 O
/53.79 O
et O
65.01/44.78 O
65.31y/45.12y65.38y/45.32y71.55/50.98 O
71.73 O
/51.27 O
71.25/51.16 O
ar O
37.63/27.48 O
38.72y/28.00y38.98y/27.89y49.27/37.62 O
50.37y/39.37y50.95y/39.57y O
la O
47.74/34.90 O
48.80y/35.64y49.17y/35.73y51.83/38.20 O
51.48/38.00 O
52.20 O
/38.28 O
ko O
34.44 O
/16.18 O
33.98/15.93 O
34.23/16.08 O
38.10/20.62 O
38.03/20.59 O
38.98y/21.54y O
hi O
36.34/27.43 O
36.72/27.40 O
37.37y/28.01y45.40/35.03 O
47.74y/35.90y46.10y/34.74 O
Average B-MetricName
65.92/55.86 O
66.40y/56.22y66.53y/56.32y73.34/62.93 O
73.55/62.99 O
73.88y/63.43y O
Table O
2 B-MetricValue
: O
Cross O
- O
lingual O
transfer O
performances O
( O
UAS%/LAS% O
, O
excluding O
punctuation O
) O
of O
the O
SelfAtt O
- O
Graph O
parser O
( O
Ahmad O
et O
al O
. O
, O
2019 O
) O
on O
the O
test O
sets O
. O

When O
richer O
multilingual O
representation O
technique O
like O
mBERT B-MethodName
is O
employed O
, O
adversarial O
training O
can O
still O
improve O
cross O
- O
lingual O
transfer O
performances O
( O
0.21 O
and O
0.54 O
UAS O
over O
the O
29 O
languages O
by O
using O
French O
and O
Russian O
, O
respectively O
) O
. O

BERT B-MethodName
pt O
0.144 O
66.40/56.22 O
73.47/63.11 O
ru O
0.146 O
66.53/56.32 O
73.88/63.43 O
de O
0.151 O
66.41/56.13 O
73.92/63.56 O
es O
0.151 O
66.38/56.24 O
71.71/62.49 O
fr O
0.160 O
66.40/56.22 O
73.55/62.99 O
la O
0.242 O
66.45/56.20 O
73.69/63.29 O
Table O
6 O
: O
Average B-MetricName
cross O
- O
lingual O
transfer O
performances O
( O
UAS%/LAS% O
, O
w/o O
punctuation O
) O
on O
the O
test O
sets O
using O
SelfAtt O
- O
Graph O
parser O
when O
different O
languages O
play O
the O
role O
of O
the O
auxiliary O
language O
in O
adversarial O
training O
. O

We O
use O
multilingual O
BERT B-MethodName
in O
this O
set O
of O
experiments O
. O

This O
idea O
is O
further O
extended O
to O
learn O
multilingual O
contextualized O
word O
representations O
, O
for O
example O
, O
multilingual O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
have O
been O
shown O
very O
effective O
for O
many O
crosslingual O
transfer O
tasks O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
. O

In O
this O
work O
, O
we O
show O
that O
further O
improvements O
can O
be O
achieved O
by O
adaptating O
the O
contextual O
encoders O
via O
unlabeled O
auxiliary O
languages O
even O
when O
the O
encoders O
are O
trained O
on O
top O
of O
multilingual O
BERT B-MethodName
. O

2.1 O
Input O
Encoding O
Query O
encoder O
: O
We O
represent O
each O
textual O
query O
with O
an O
attention O
- O
based O
Bi O
- O
LSTM B-MethodName
language O
model(Conneau O
et O
al O
. O
, O
2017 O
) O
with O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
distributed O
word O
embeddings O
trained O
on O
the O
Wikipedia O
and O
the O
Gigaword O
corpus O
with O
a O
total O
of O
6B O
tokens O
. O

4.2 O
Results O
Parameters O
: O
We O
tune O
the O
parameters O
of O
each O
model O
with O
the O
following O
search O
space O
( O
bold O
indicate O
the O
choice O
for O
our O
ﬁnal O
model O
): O
graph O
embeddings O
size O
: O
f64,128 O
, O
256 O
, O
512 O
g O
, O
Bi O
- O
LSTM B-MethodName
hidden O
states O
for O
the O
language O
model O
: O
f64,128 O
, O
256 O
, O
512 O
g O
, O
MGN O
hidden O
states O
: O
f64,128 O
, O
256 O
, O
512 O
g O
, O
word O
embeddings O
size O
: O
f100 O
, O
200 O
, O
300 O
g O
, O
and O
max O
memory O
slots O
: O
f1 O
, O
5 O
, O
10 O
, O
20 O
, O
40 O
, O
80 O
g. O

1 O
Introduction O
Recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
have O
seen O
rapid O
adoption O
in O
natural O
language O
processing O
applications O
. O

The O
effectiveness O
of O
RNNs O
in O
language O
modeling O
, O
in O
particular O
LSTMs B-MethodName
, O
has O
been O
demonstrated O
in O
numerous O
studies O
( O
Mikolov O
et O
al O
. O
, O
2010 O
; O
Sundermeyer O
et O
al O
. O
, O
2012 O
; O
Jozefowicz O
et O
al O
. O
, O
2016 O
) O
. O

2.4 O
Multitask O
Learning O
The O
beneﬁts O
of O
multi O
- O
task O
learning O
in O
neural B-MethodName
networks I-MethodName
are O
straightforward O
. O

3.2 O
Model O
The O
model O
in O
all O
of O
our O
experiments O
was O
a O
standard O
single O
- O
layer O
LSTM.3The B-MethodName
ﬁrst O
layer O
was O
a O
vector O
embedding O
of O
word O
tokens O
into O
D O
- O
dimensional O
space O
. O

The O
second O
was O
a O
D O
- O
dimensional O
LSTM B-MethodName
. O

Both O
output O
layers O
received O
the O
shared O
LSTM B-MethodName
representations O
as O
their O
input O
. O

Pre O
- O
training O
: O
In O
this O
setup O
, O
we O
ﬁrst O
trained O
the O
network O
on O
one O
of O
the O
tasks O
; O
we O
then O
used O
the O
weights B-HyperparameterName
learned O
by O
the O
network O
for O
the O
embedding O
layer O
and O
the O
LSTM B-MethodName
layer O
as O
the O
initial O
weights B-HyperparameterName
of O
a O
new O
network O
which O
we O
then O
trained O
on O
the O
second O
task O
. O

3.3 O
Training O
All O
neural B-MethodName
networks I-MethodName
were O
implemented O
in O
Keras O
( O
Chollet O
, O
2015 O
) O
and O
Theano O
( O
Theano O
Development O
Team O
, O
2016 O
) O
. O

Recent O
work O
indicated O
that O
pretrained O
language O
models O
( O
PLMs O
) O
such O
as O
BERT B-MethodName
and O
RoBERTa B-MethodName
can O
be O
transformed O
into O
effective O
sentence O
and O
word O
encoders O
even O
via O
simple O
self O
- O
supervised O
techniques O
. O

1 O
Introduction O
Pretrained O
Language O
Models O
( O
PLMs O
) O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
provide O
dynamic O
contextual O
representations O
; O
they O
induce O
token O
- O
level O
lexical O
representations O
that O
capture O
the O
impact O
of O
the O
word O
’s O
context O
on O
its O
embedding O
. O

Similar O
to O
the O
supervised O
approaches O
such O
as O
Sentence O
- O
BERT B-MethodName
( O
Reimers O
and O
Gurevych O
, O
2019b O
) O
or O
SapBERT B-MethodName
( O
Liu O
et O
al O
. O
, O
2021a O
) O
, O
the O
idea O
is O
to O
transform O
an O
input O
PLM O
into O
an O
effective O
sentence O
encoder O
via O
additional O
ﬁne O
- O
tuning O
. O

MIRROR B-MethodName
BERT B-MethodName
( O
Liu O
et O
al O
. O
, O
2021b O
) O
is O
a O
general O
self O
- O
supervised O
contrastive O
ﬁne O
- O
tuning O
framework O
that O
transforms O
off O
- O
the O
- O
shelf O
PLMs O
into O
effective O
word O
and O
sentence O
encoders O
. O

Our O
proposed O
MIRROR B-MethodName
WICmethod I-MethodName
can O
be O
seen O
as O
an O
extension O
of O
MIRROR B-MethodName
BERT B-MethodName
, O
now O
focused O
on O
eliciting O
improved O
word O
- O
in O
- O
context O
representations O
and O
context O
- O
sensitive O
lexical O
tasks O
. O

Inspired O
by O
MIRROR B-MethodName
BERT B-MethodName
( O
Liu O
et O
al O
. O
, O
2021b O
) O
, O
we O
apply O
a O
self O
- O
supervised O
contrastive O
learning O
scheme O
to O
elicit O
better O
word O
- O
incontext O
representations O
. O

We O
then O
follow O
MIRROR B-MethodName
BERT B-MethodName
and O
generate O
a O
labelled O
dataset O
by O
duplicating O
each O
instance O
in O
the O
set O
and O
assigning O
identical O
labels O
to O
identical O
instances O
and O
different O
labels O
to O
different O
word O
- O
in O
- O
contexts O
( O
Table O
2 O
, O
upper O
half O
): O
D= O
f(x1;y1);(x1;y1 O
) O
; O
: O
: O
: O
; O
( O
xN;yN);(xN;yN)g O
, O
where8i= O
1;:::N O
, O
it O
holdsxi O
= O
xi;yi O
= O
yi O
. O

We O
further O
follow O
MIRROR B-MethodName
BERT B-MethodName
to O
create O
a O
slightly O
altered O
( O
or O
augmented O
) O
‘ O
view O
’ O
of O
the O
same O
text O
sequence O
: O
we O
randomlyreplace O
a O
span O
of O
text O
with O
‘ O
[ O
MASK]’1 O
in O
all O
duplicated O
examples O
. O

There O
is O
a O
fundamental O
difference O
toMIRROR O
BERTwhere B-MethodName
such O
‘ O
random O
span O
masking O
’ O
technique O
is O
applied O
on O
sentences O
; O
for O
word O
- O
incontext O
, O
we O
keep O
the O
target O
word O
intact O
( O
otherwise O
the O
semantics O
changes O
drastically O
) O
and O
randomly O
replace O
a O
span O
of O
length O
Konboth O
sides O
of O
the O
target O
word O
; O
see O
Table O
2 O
( O
lower O
half O
) O
. O

Naturally O
, O
1Or O
‘ O
< O
MASK O
> O
’ O
for O
input O
to O
RoBERTa B-MethodName
. O

2Note O
that O
random O
span O
masking O
is O
applied O
on O
only O
one O
instance O
of O
each O
duplicated O
pair O
, O
while O
the O
dropouts B-HyperparameterName
are O
applied O
to O
all O
instances.model O
representations O
ﬁne O
- O
tuned O
representations O
extracted O
off O
- O
the O
- O
shelf O
PLMs O
( O
[ O
CLS O
] O
+ O
) O
language O
modelling O
head O
word O
token O
average B-MetricName
( O
top O
four O
layers O
) O
MIRROR B-MethodName
BERT B-MethodName
[ O
CLS O
] O
/mean B-MetricName
- O
pooling O
[ O
CLS O
] O
/mean B-MetricName
- O
pooling O
MIRROR O
WIC O
word O
token O
average B-MetricName
( O
top O
four O
layers O
) O
word O
token O
average B-MetricName
( O
top O
four O
layers O
) O
Table O
1 B-MetricValue
: O
M O
IRROR O
WIC O
beneﬁts O
from O
the O
consistency O
of O
representations O
at O
( O
i O
) O
ﬁne O
- O
tuning O
and O
( O
ii O
) O
feature O
extraction O
and O
inference O
: O
both O
are O
focused O
on O
word O
- O
in O
- O
context O
( O
WiC O
) O
representations O
. O

We O
experiment O
with O
several O
standard O
input O
PLMs O
for O
English O
, O
but O
we O
remind O
the O
reader O
that O
the O
MIRROR O
WICframework O
is O
applicable O
with O
a O
wide O
range O
of O
PLMs O
: O
1)BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
a O
standard O
choice O
for O
WiC O
representation O
learning O
and O
evaluation O
( O
Raganato O
et O
al O
. O
, O
2020 O
) O
; O
2)RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
as O
an O
optimised O
and O
improved O
PLM O
; O
and O
3)DeBERTa B-MethodName
( O
He O
et O
al O
. O
, O
2020 O
) O
as O
a O
more O
recent O
PLM O
that O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
in O
a O
range O
of O
natural O
language O
understanding O
tasks O
( O
Wang O
et O
al O
. O
, O
2019).6 O
For O
all O
non O
- O
English O
experiments O
, O
unless O
noted O
otherwise O
, O
we O
rely O
on O
multilingual O
BERT B-MethodName
( O
mBERT B-MethodName
) O
as O
the O
underlying O
PLM O
( O
see O
App O
. O

We O
largely O
follow O
the O
MIRRORBERTﬁne B-MethodName
- O
tuning O
setup O
( O
Liu O
et O
al O
. O
, O
2021b O
) O
, O
using O
10k O
sentences O
randomly O
drawn O
from O
Wikipedia O
as O
the O
MIRROR O
WICﬁne O
- O
tuning O
corpus O
. O

6DeBERTa B-MethodName
extends O
the O
standard O
BERT B-MethodName
architecture O
by O
incorporating O
two O
novel O
techniques O
: O
disentangled O
attention O
that O
encodes O
a O
word O
’s O
content O
and O
position O
separately O
, O
and O
an O
enhanced O
masked O
decoder O
that O
incorporates O
absolute O
position O
for O
predicting O
masked O
tokens O
during O
masked O
language O
modelling.cross O
- O
lingual O
models O
, O
we O
sample O
5k O
sentences O
from O
English O
Wikipedia O
and O
5k O
from O
Wikipedia O
of O
each O
target O
language O
. O

We O
setK(random O
span O
masking O
rate O
) O
to O
10,0and O
1for O
BERT B-MethodName
, O
RoBERTa B-MethodName
and O
DeBERTa B-MethodName
respectively O
. O

The O
respective O
dropout B-HyperparameterName
rates I-HyperparameterName
are O
0:4,0:3and O
0:3 O
for O
BERT B-MethodName
, O
RoBERTa B-MethodName
and O
DeBERTa B-MethodName
. O

MIRRORWICis O
effective O
with O
BERT B-MethodName
, O
RoBERTa B-MethodName
and O
DeBERTa B-MethodName
. O

DeBERTa+ B-MethodName
MIRROR O
WICyields O
larger O
gains O
, O
and O
even O
results O
in O
the O
highest O
absolute O
scores B-MetricName
on O
average B-MetricName
. O

In O
other O
words O
, O
a O
seemingly O
‘ O
weaker O
’ O
off O
- O
the O
- O
shelf O
PLM O
under O
the O
naive O
feature O
extraction O
baseline O
( O
DeBERTa B-MethodName
) O
is O
transformed O
into O
the O
best O
- O
performing O
WiC O
encoder O
after O
the O
MIRRORWICprocedure O
. O

In O
particular O
, O
we O
evaluate O
the O
standard O
’ O
go O
- O
to O
’ O
sentence O
encoder O
Sentence O
- O
BERT B-MethodName
( O
Reimers O
and O
Gurevych O
, O
2019b O
) O
, O
and O
the O
original O
MIRROR B-MethodName
BERT(Liu B-MethodName
et O
al O
. O
, O
2021b O
) O
. O

The O
ﬁnding O
validates O
our O
hypothesis O
that O
naively O
applying O
sentence O
encodersmodel O
# O
, O
dataset!Usim O
( O
 O
) O
WiC O
( O
acc O
) O
TSV-1 O
( O
acc O
) O
TSV-2 O
( O
acc O
) O
TSV-3 O
( O
acc O
) O
CoSimLex O
( O
 O
) O
One O
- O
shot O
WSD O
( O
acc O
) O
Sentence O
- O
BERT B-MethodName
23.57 O
61.91 O
62.46 O
59.64 O
62.72 O
- O
42.63 O
MIRROR B-MethodName
BERT B-MethodName
23.21 O
64.10 O
66.32 O
64.78 O
66.32 O
- O
44.93 O
BERT B-MethodName
54.52 O
68.49 O
61.69 O
60.66 O
61.95 O
76.2 O
52.90 O
+ O
M O
IRROR O
WIC O
61.82 O
71.94 O
69.15 O
66.06 O
68.38 O
77.41 O
57.10 O
RoBERTa B-MethodName
50.25 O
66.77 O
55.52 O
56.55 O
57.58 O
75.64 O
51.38 O
+ O
M O
IRROR O
WIC O
57.95 O
71.15 O
69.92 O
67.60 O
71.70 O
77.27 O
56.51 O
DeBERTa B-MethodName
54.77 O
66.14 O
59.38 O
59.89 O
60.41 O
72.06 O
53.99 O
+ O
M O
IRROR O
WIC O
62.79 O
71.78 O
70.95 O
67.86 O
71.20 O
77.70 O
59.02 O
Table O
3 O
: O
Results O
across O
a O
collection O
of O
context O
- O
aware O
lexical O
semantic O
tasks O
in O
English O
. O

The O
scores B-MetricName
reveal O
that O
the O
unsupervised O
BERT B-MethodName
+ O
MIRROR O
WICvariant O
can O
even O
outperform O
the O
supervised O
model O
( O
ﬁne O
- O
tuned O
with O
labelled O
in O
- O
model O
# O
, O
dataset O
! O

WiC O
TSV-1 O
TSV-2 O
TSV-3 O
BERT B-MethodName
65.85 O
65.08 O
62.09 O
63.16 O
+ O
M O
IRROR O
WIC O
69.64 O
73.66 O
69.83 O
73.73 O
task O
- O
supervised O
BERT B-MethodName
69.00 O
75.30 O
71.40 O
76.60 O
Table O
4 O
: O
BERT+M B-MethodName
IRROR O
WIC O
versus O
supervised O
BERT B-MethodName
- O
based O
methods O
on O
the O
test O
sets O
of O
English O
WiCstyle O
tasks O
. O

XL O
- O
WiC O
ZH O
* O
KO O
* O
HR O
ET O
BERT B-MethodName
73.74 O
68.41 O
61.10 O
57.06 O
+ O
M O
IRROR O
WIC O
75.70 O
72.26 O
67.32 O
61.43 O
AM2iCo O
ZH O
KA O
JA O
AR O
BERT B-MethodName
63.80 O
59.90 O
64.10 O
60.60 O
+ O
M O
IRROR O
WIC O
64.60 O
61.00 O
64.70 O
63.90 O
Table O
5 O
: O
Results O
( O
test O
set O
accuracy B-MetricName
) O
on O
multilingual O
and O
cross O
- O
lingual O
word O
- O
in O
- O
context O
tasks O
. O

We O
use O
mBERT B-MethodName
as O
the O
underlying O
PLM O
for O
all O
the O
languages O
except O
for O
ZH O
* O
and O
KO O
* O
( O
in O
XL O
- O
WiC O
) O
where O
their O
monolingual O
BERT B-MethodName
models O
were O
used O
. O

The O
results O
on O
TSV O
indicate O
that O
the O
gap O
between O
the O
unsupervised O
BERTbased B-MethodName
approach O
to O
the O
supervised O
performance O
is O
much O
reduced O
: O
from O
the O
10 O
% O
gap O
to O
only2 O
% O
in O
all O
three O
TSV O
tasks O
when O
MIRROR O
WICis O
applied O
. O

We O
observe O
consistent O
improvements O
with O
the O
underlying O
PLMs O
monolingually O
pretrained O
in O
other O
languages O
, O
as O
well O
as O
with O
the O
multilingually O
pretrained O
mBERT B-MethodName
. O

2d O
, O
both O
BERT B-MethodName
and O
DeBERTa B-MethodName
create O
more O
isotropic O
embedding O
spaces O
in O
general O
in O
the O
last O
four O
layers O
after O
MIRROR O
WIC O
training O
. O

Note O
that O
DeBERTa B-MethodName
’s O
space O
isotropy O
is O
able O
to O
beneﬁt O
more O
from O
MIRROR O
WIC O
, O
which O
also O
explains O
its O
large O
gains O
in O
the O
end O
tasks O
. O

We O
repeat O
the O
process O
for O
ﬁve O
times O
to O
reduce O
the O
randomness O
introduced O
in O
sampling O
. O
Word O
- O
in O
- O
context O
1 O
Word O
- O
in O
- O
context O
2 O
BERT B-MethodName
+ O
M O
IRROR O
WICGold O
Spend O
money O
. O

For O
both O
BERT B-MethodName
and O
DeBERTa B-MethodName
, O
we O
can O
see O
that O
the O
last O
four O
layers O
become O
more O
contextualised O
after O
applying O
MIRROR O
WIC O
: O
they O
encode O
more O
information O
about O
the O
context O
as O
the O
contextual O
word O
representations O
become O
much O
more O
similar O
to O
its O
context O
in O
the O
top O
Transformer O
layers O
than O
in O
the O
base O
PLM O
. O

Conducting O
an O
error O
analysis O
of O
BERT B-MethodName
before O
and O
after O
MIRROR O
WIC O
on O
the O
WiC O
dev O
set O
, O
we O
observe O
that O
94 O
instances O
changed O
their O
labels O
, O
among O
which O
58 O
are O
MIRRORWICcorrecting O
the O
original O
predictions O
. O

Contextualised O
embeddings O
for O
an O
ambiguous O
word O
( O
spring O
) O
with O
off O
- O
the O
- O
shelf O
BERT B-MethodName
, O
MIRROR B-MethodName
BERT B-MethodName
andMIRROR O
WICare O
visualised O
in O
Fig O
. O

While O
MIRROR B-MethodName
WICmaintains I-MethodName
the O
sense O
clusters O
from O
BERT B-MethodName
and O
teases O
apart O
the O
different O
senses O
even O
more O
, O
MIRROR B-MethodName
BERTexhibits B-MethodName
no O
clear O
sense O
distinctions O
. O

This O
shows O
a O
fundamental O
difference O
between O
MIRROR B-MethodName
WICand I-MethodName
MIRROR B-MethodName
BERT B-MethodName
: O
MIRROR B-MethodName
BERTis B-MethodName
insensitive O
to O
the O
target O
word O
, O
and O
directly O
applying O
it O
to O
contextsensitive O
lexical O
tasks O
yields O
subpar O
performance O
. O

The O
MIRROR B-MethodName
WICperformance I-MethodName
is O
most O
sensitive O
to O
the O
dropout B-HyperparameterName
rate I-HyperparameterName
; O
it O
requires O
larger O
dropout B-HyperparameterName
rates I-HyperparameterName
( O
0.3 B-HyperparameterValue
for O
DeBERTa B-MethodName
and O
0.4 O
for O
BERT B-MethodName
) O
than O
MIRROR B-MethodName
BERT(0.1 B-MethodName
dropout B-HyperparameterName
) O
. O

Sentence O
meanings B-MetricName
can O
largely O
change O
with O
even O
slight O
differences O
in O
context O
: O
therefore O
, O
positive O
sentence O
pairs O
forMIRROR O
BERTare B-MethodName
required O
to O
be O
very O
similar.dropout B-HyperparameterName
rate O
! O

0 O
0.1 O
0.2 O
0.3 O
0.4 O
0.5 O
0.6 O
BERT B-MethodName
+ O
M O
IRROR O
WIC O
68.02 O
68.65 O
70.21 O
71.31 O
71.94 O
68.80 O
68.49 O
DeBERTa B-MethodName
+ O
M O
IRROR O
WIC O
65.67 O
69.12 O
70.53 O
71.78 O
67.08 O
65.98 O
66.30 O
Table O
7 O
: O
Impact O
of O
dropout B-HyperparameterName
rate I-HyperparameterName
in O
M O
IRROR O
WIC O
. O

off O
on O
BERT B-MethodName
+ O
M O
IRROR O
WIC O
71.31 O
71.47 O
" O
0:16 O
DeBERTa B-MethodName
+ O
M O
IRROR O
WIC O
71.78 O
71.94 O
" O
0:16 O
Table O
8 O
: O
Impact O
of O
random O
span O
masking O
. O

1 O
2 O
3 O
4 O
5 O
6 O
12 O
BERT B-MethodName
+ O
M O
IRROR O
WIC O
68.96 O
68.80 O
70.06 O
71.94 O
70.68 O
70.84 O
67.71 O
DeBERTa B-MethodName
+ O
M O
IRROR O
WIC O
71.47 O
73.04 O
72.41 O
71.78 O
71.15 O
70.53 O
69.74 O
Table O
9 O
: O
Impact O
of O
layer O
averaging O
strategies O
. O

Averaging O
across O
all O
layers O
of O
the O
PLM O
is O
suboptimal O
for O
WiC O
representations O
, O
and O
the O
strategy O
of O
averaging O
only O
over O
the O
last O
four O
layers O
is O
indeed O
the O
optimal O
one O
for O
BERT B-MethodName
. O

However O
, O
DeBERTa B-MethodName
reaches O
its O
peak O
when O
averaging O
over O
the O
last O
2 O
layers O
. O

This O
is O
in O
line O
with O
ﬁndings O
in O
MIRROR B-MethodName
BERT B-MethodName
, O
and O
also O
shows O
that O
the O
model O
does O
not O
require O
plenty O
of O
ﬁne O
- O
tuning O
data O
to O
transform O
into O
a O
WiC O
encoder O
. O

BERT B-MethodName
’s O
token O
level O
knowledge O
also O
allows O
the O
testing O
of O
a O
type O
- O
level O
hypothesis O
about O
lexical O
abstractness O
, O
demonstrating O
the O
relationship O
between O
token O
- O
level O
behavior O
and O
type O
- O
level O
concreteness O
ratings O
. O

Our O
ﬁndings O
provide O
important O
insight O
into O
the O
interpretability O
of O
BERT B-MethodName
: O
layer O
7 O
approximates O
semantic O
similarity O
, O
while O
the O
ﬁnal O
layer O
( O
11 O
) O
approximates O
relatedness O
. O

1 O
Introduction O
The O
rampant O
success O
enjoyed O
by O
contextualized O
language O
models O
( O
CLMs O
) O
like O
CoVe O
( O
McCann O
et O
al O
. O
, O
2017 O
) O
, O
ElMo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019b O
) O
has O
precipitated O
a O
deluge O
of O
research O
into O
analyzing O
and O
interpreting O
their O
functionality O
. O

This O
paper O
seeks O
to O
answer O
two O
questions O
: O
1 O
) O
Is O
it O
possible O
to O
generate O
useful O
static O
word O
- O
type O
embeddings O
from O
BERT B-MethodName
activations B-HyperparameterName
for O
word O
tokens O
? O

2 O
) O
What O
sort O
of O
semantic O
relations O
are O
represented O
in O
embeddings O
generated O
from O
BERT B-MethodName
? O

BERT B-MethodName
’s O
contextual O
knowledge O
is O
useful O
for O
dealing O
with O
the O
effects O
of O
this O
variation O
on O
similarity O
and O
relatedness O
judgments O
. O

In O
contrast O
, O
BERT B-MethodName
enables O
the O
representation O
of O
a O
single O
word O
as O
a O
constellation O
of O
points O
, O
with O
each O
point O
corresponding O
to O
a O
different O
usage O
or O
usage O
type O
. O

In O
BERT B-MethodName
, O
heterogeneity O
translates O
to O
the O
dispersion O
of O
tokens O
or O
prototypes O
in O
space O
. O

However O
, O
these O
models O
do O
not O
leverage O
the O
contextual O
knowledge O
resident O
in O
BERTs B-MethodName
later O
layers O
. O

A O
set O
of O
BERT B-MethodName
token O
vectors O
for O
a O
single O
word O
naturally O
tends O
to O
separate O
spatially O
into O
groups O
of O
similar O
usages O
, O
or O
‘ O
usage O
types O
’ O
( O
Giulianelli O
et O
al O
. O
, O
2020 O
) O
. O

To O
test O
whether O
these O
usage O
types O
retain O
enough O
contextual O
information O
to O
aid O
in O
context O
- O
sensitive O
lexical O
tasks O
, O
we O
use O
K O
- O
means B-MetricName
clustering O
of O
BERT B-MethodName
token O
representations O
to O
derive O
multi O
- O
prototype O
lexical O
embeddings O
. O

Clustered O
BERT B-MethodName
- O
based O
representations O
provide O
high O
- O
quality O
predictions O
of O
human O
judgments O
for O
both O
tasks O
. O

The O
contributions O
of O
this O
paper O
are O
as O
follows O
: O
1.Application O
of O
BERT B-MethodName
for O
type O
- O
level O
lexical O
modeling O
, O
and O
the O
testing O
of O
type O
- O
level O
lexicalsemantic O
hypotheses O
. O

3.Insight O
into O
the O
semantic O
interpretability O
of O
BERT B-MethodName
, O
most O
notably O
that O
middle O
layers O
best O
approximate O
similarity O
while O
the O
ﬁnal O
layer O
approximates O
relatedness O
. O

2 O
Related O
work O
The O
pre O
- O
trained O
BERT B-MethodName
language O
model O
is O
a O
bidirectional O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
encoder O
. O

The O
preponderance O
of O
research O
surrounding O
analysis O
and O
interpretability O
of O
BERT B-MethodName
has O
been O
dubbed O
BERTology B-MethodName
, O
after O
the O
poster O
- O
child O
of O
the O
contextual O
revolution O
( O
cf O
. O

Probing O
Tasks O
Most O
studies O
of O
word O
meaning B-MetricName
in O
BERT B-MethodName
follow O
an O
agenda O
of O
extrinsic O
evaluation O
( O
Artetxe O
et O
al O
. O
, O
2018 O
) O
. O

Tenney O
et O
al O
. O
( O
2019b O
) O
introduced O
edge O
- O
probing O
tasks O
to O
analyze O
the O
layer O
- O
wise O
structure O
of O
BERT B-MethodName
, O
and O
found O
that O
early O
layers O
perform O
syntactic O
tasks O
like O
part O
- O
of O
- O
speech O
and O
dependency O
tagging O
, O
while O
later O
layers O
encode O
information O
pertinent O
to O
semantic O
tasks O
like O
coreference B-TaskName
resolution O
, O
relation O
labeling O
, O
and O
semantic O
proto O
- O
role O
labeling O
. O

We O
aim O
to O
advance O
this O
kind O
of O
structural O
analysis O
of O
BERT B-MethodName
through O
the O
intrinsic O
evaluation O
of O
representations O
at O
different O
layers O
. O
Contextual O
Word O
Embeddings O
Existing O
applications O
of O
CLMs O
to O
lexical O
tasks O
use O
exemplar O
models O
or O
single O
- O
prototype O
models O
. O

Wiedemann O
et O
al O
. O
( O
2019 O
) O
successfully O
employed O
a O
K O
- O
nearestneighbor O
approach O
to O
BERT B-MethodName
exemplar O
models O
for O
word O
sense O
disambiguation O
( O
WSD O
) O
. O

Coenen O
et O
al O
. O
( O
2019 O
) O
created O
a O
visualization O
tool O
that O
generates O
a O
‘ O
word O
cloud O
’ O
from O
BERT B-MethodName
tokens O
, O
browsable O
by O
layer.2They O
also O
achieved O
a O
state O
- O
of O
- O
the O
- O
art O
F1 B-MetricName
score B-MetricName
on O
a O
WSD O
task O
with O
the O
simple O
scheme O
involving O
sense O
- O
annotated O
training O
data O
from O
Peters O
et O
al O
. O
( O
2018 B-MetricValue
) O
. O

Ethayarajh O
( O
2019 O
) O
generated O
static O
embeddings O
from O
CLM O
models O
, O
using O
the O
ﬁrst O
PCA B-MethodName
component O
sets O
of O
token O
representations O
. O

However O
, O
averaging O
evaporates O
much O
of O
BERT B-MethodName
’s O
contextual O
variation O
, O
cutting O
off O
its O
potential O
to O
aid O
in O
similarity O
estimation O
. O

It O
’s O
no O
surprise O
that O
averages B-MetricName
derived O
from O
earlier O
layers O
of O
BERT B-MethodName
performed O
best O
at O
similarity O
estimation O
. O

Given O
the O
complaints O
lodged O
against O
similarity O
, O
and O
the O
existence O
of O
contextual O
similarity O
datasets O
( O
Erk O
et O
al O
. O
, O
2013 O
; O
Pilehvar O
and O
Camacho O
- O
Collados O
, O
2019 O
; O
Stanovsky O
and O
Hopkins O
, O
2018 O
) O
, O
why O
bother O
applying O
BERT B-MethodName
to O
non O
- O
contextual O
datasets O
at O
all O
? O

The O
next O
section O
describes O
a O
BERT B-MethodName
- O
based O
approach O
to O
inject O
a O
degree O
of O
context O
- O
sensitivity O
into O
similarity O
and O
relatedness O
estimation O
. O

3 O
Multi O
- O
prototype O
BERT B-MethodName
embeddings O
As O
stated O
, O
BERT B-MethodName
token O
representations O
demonstrate O
greater O
contextual O
variation O
at O
each O
progressive O
layer O
( O
Ethayarajh O
, O
2019 O
) O
. O

We O
use O
clustering O
to O
approximate O
BERT B-MethodName
’s O
usage O
- O
types O
and O
the O
affordances O
they O
capture O
, O
and O
demonstrate O
their O
usefulness O
for O
lexical O
tasks O
. O

3.1 O
Materials O
& O
Methods O
The O
transformation O
of O
token O
- O
vectors O
into O
multiprototype O
vectors O
requires O
several O
steps O
, O
described O
here O
for O
a O
single O
word O
w. O
Data O
Collection O
First O
, O
a O
setSof O
up O
to O
100 O
sentences O
containing O
a O
token O
tofwwas O
sampled O
at O
random O
from O
the O
British O
National O
Corpus O
( O
BNC O
, O
Burnard O
, O
2000).3As O
the O
human O
judgments O
in O
our O
evaluation O
datasets O
were O
collected O
agnostic O
to O
part O
of O
speech O
, O
and O
are O
particular O
to O
word O
forms O
, O
no O
3Indices O
of O
the O
sampled O
sentences O
are O
available O
at O
https O
: O
//github.com O
/ O
gchronis O
/ O
MProBERTlemmatization B-MethodName
or O
tagging O
was O
used O
. O

Any O
sentences O
too O
large O
to O
input O
to O
BERT B-MethodName
were O
discarded O
. O

If O
BERT B-MethodName
split O
the O
token O
tinto O
subword O
WordPieces4 O
, O
we O
followed O
the O
now O
- O
standard O
practice O
of O
averaging O
t O
’s O
subword O
vectors O
to O
obtain O
a O
single O
token O
vector O
fortat O
each O
layer O
. O

The O
other O
datasets O
show O
a O
similar O
pattern O
, O
with O
peak O
performance O
at O
layer O
11 O
between O
7 O
and O
9 O
clusters O
: O
K=7 O
for O
WordSim353 O
rel(= O
0:665),K=9 O
for O
YP-130 O
( O
= O
0:715 O
) O
, O
andK=7 O
for O
WordSim353 O
( O
= O
0:747).Similarity O
Relatedness O
SL-999 O
SV-3500 O
WS-353 O
sim O
WS-353 O
WS-353 O
rel O
MEN O
YP-130 O
Distributional0.563 O
0.364 O
0.795 O
0.738 O
0.681 O
0.801 O
0.535 O
SP-15 O
CBOW B-MethodName
GloVe B-MethodName
FastText O
FastText O
GloVe B-MethodName
GloVe B-MethodName
CLM O
- O
based0.550 O
0.455 O
- O
0.730 O
- O
0.200 O
XLNet-24 B-MethodName
( O
4 O
) O
XLNet-24 B-MethodName
( O
3 O
) O
- O
BERT-24 B-MethodName
( O
6 O
) O
- O
BERT B-MethodName
- O
pca-1 O
( O
1 O
) O
MProBERT0.605 B-MethodName
0.528 O
0.807 O
0.741 O
0.653 O
0.781 O
0.711 O
( O
layer=7 O
, O
K9 O
) O
( O
layer=11 O
, O
K O
9 O
) O
Table O
1 O
: O
Performance O
of O
best O
unioned O
multi O
- O
prototype O
BERT B-MethodName
embeddings O
( O
M O
- O
ProBert O
) O
for O
both O
similarity O
and O
relatedness O
estimation O
tasks O
( O
Spearman O
’s O
 O
) O
compared O
to O
other O
CLM O
- O
based O
word O
embeddings O
and O
to O
state O
- O
of O
- O
theart O
corpus O
- O
based O
distributional O
models O
. O

For O
distributional O
models O
we O
compared O
to O
Symmetric O
Pattern O
embeddings O
( O
SP-15 O
, O
[ O
Schwartz O
et O
al O
. O
, O
2015 O
] O
) O
, O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
CBOW B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
and O
FastText O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
. O

For O
other O
CLMbased O
embeddings O
, O
we O
compared O
to O
Bommasani O
et O
al O
. O
( O
2020 O
) O
, O
who O
tested O
layer O
- O
wise O
token O
aggregations O
for O
numerous O
architectures O
: O
BERT B-MethodName
, O
RoBERTa B-MethodName
, O
GPT2 B-MethodName
, O
XLNet B-MethodName
, O
and O
DistilBert O
. O

We O
also O
compared O
to O
Ethayarajh O
( O
2019 O
) O
, O
who O
examined O
the O
ﬁrst O
PCA B-MethodName
component O
of O
individual O
layers O
of O
several O
CLMs O
. O

The O
ﬁnal O
layer O
of O
BERT B-MethodName
approximate O
relatedness O
, O
while O
layer O
7 O
is O
optimal O
for O
estimating O
similarity O
. O

This O
ﬁnding O
bears O
an O
interesting O
connection O
to O
recent O
insights O
in O
BERTology B-MethodName
. O

Multi O
- O
prototype O
BERT B-MethodName
meets O
this O
demand O
, O
at O
least O
for O
modelingvariability O
, O
which O
can O
be O
viewed O
a O
proxy O
for O
complexity O
. O

Ethayarajh O
( O
2019 O
) O
noted O
that O
inter O
- O
token O
variance O
increases O
throughout O
the O
layers O
of O
BERT B-MethodName
. O

We O
ﬁrstaddressed O
the O
common O
tasks O
of O
predicting O
word O
similarity O
and O
relatedness O
, O
and O
demonstrated O
that O
BERT B-MethodName
produces O
high O
- O
quality O
multi O
- O
prototype O
word O
embeddings O
. O

The O
Rational O
Speech O
Act O
formalization O
of O
metaphor O
interpretation O
( O
Kao O
et O
al O
. O
, O
2014 O
) O
utilizes O
hand O
- O
crafted O
feature O
vectors O
; O
it O
might O
be O
extended O
by O
inducing O
metaphorical O
sense O
representations O
from O
BERT B-MethodName
- O
based O
prototypes O
. O

CMU O
ELMO O
FLAIR O
Batch O
size O
32 O
32 O
32 O
Max O
epochs B-HyperparameterName
150 B-HyperparameterValue
150 O
150 O
LSTM B-MethodName
hidden B-HyperparameterName
size I-HyperparameterName
256 B-HyperparameterValue
256 O
256 O
LSTM B-MethodName
layers O
1 O
2 O
1 O
Learning O
rate O
0.1 O
0.1 O
0.1 O
Patience O
3 O
4 O
3 O
Table O
1 O
: O
The O
most O
important O
parameters O
for O
the O
reproduced O
models O
( O
the O
other O
parameters O
remained O
as O
default O
library O
options O
) O
. O

In O
contrast O
to O
the O
other O
models O
, O
the O
BERT B-MethodName
model O
works O
in O
a O
ﬁne O
tune O
mode O
in O
which O
we O
used O
a O
pretrained O
language O
model O
and O
added O
a O
simple O
classiﬁcation O
layer O
on O
all O
heads O
of O
tokens O
( O
the O
ﬁrst O
BPE O
sub O
- O
token O
for O
each O
original O
word O
) O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

In O
order O
to O
ﬁne O
- O
tune O
the O
BERT B-MethodName
- O
base O
model O
, O
we O
used O
the O
’ O
huggingface O
’ O
library O
, O
version O
0.6.1 O
( O
pretrained O
BERT B-MethodName
, O
2019 O
) O
, O
with O
the O
following O
parameters O
: O
max O
sequence O
length O
– O
256 O
; O
batch B-HyperparameterName
size I-HyperparameterName
– O
16 B-HyperparameterValue
; O
learning B-HyperparameterName
rate I-HyperparameterName
– O
1e-5 O
; O
warmup O
proportion O
– O
0.4 B-HyperparameterValue
; O
number O
of O
train O
epoch O
– O
100 O
. O

The O
Stanford O
and O
BERT B-MethodName
models O
used O
the O
BIO O
tag O
encoding O
type O
, O
the O
other O
models O
were O
trained O
with O
the O
IOBES O
encoding O
type O
. O

Contrarily O
to O
recurrent O
networks O
such O
as O
LSTMs B-MethodName
, O
attention O
networks O
do O
not O
have O
a O
builtin O
notion O
of O
position O
so O
it O
must O
be O
provided O
externally O
. O

The O
probability O
of O
a O
derived O
tree O
decomposes O
as O
independent O
head O
predictions O
computed O
by O
a O
logistic B-MethodName
regression I-MethodName
over O
head O
scores B-MetricName
. O

Filler O
- O
gaps O
that O
neural B-MethodName
networks I-MethodName
fail O
to O
generalize O
. O

It O
can O
be O
difﬁcult O
to O
separate O
abstract O
linguistic O
knowledge O
in O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
from O
surface O
heuristics O
. O

5 O
Models O
We O
focused O
in O
this O
work O
on O
recurrent O
neural O
language O
models O
with O
long O
short O
- O
term O
memory O
units O
( O
LSTMs B-MethodName
; O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

We O
analyzed O
ﬁve O
of O
the O
highest O
performing O
models O
released O
by O
van O
Schijndel O
et O
al O
. O
( O
2019),4who O
showed O
that O
these O
models O
perform O
comparably O
to O
state O
- O
ofthe O
- O
art O
transformers O
GPT B-MethodName
and O
BERT B-MethodName
on O
many O
simple O
syntactic O
agreement O
tasks O
. O

The O
models O
are O
2layer O
LSTMs B-MethodName
with O
400 O
hidden B-HyperparameterName
units I-HyperparameterName
per O
layer O
, O
each O
with O
a O
unique O
random O
initialization B-HyperparameterName
, O
trained O
on O
80 B-HyperparameterValue
million O
training O
tokens O
of O
English O
Wikipedia O
data O
. O

Analyzing O
multiple O
similar O
models O
with O
different O
random B-HyperparameterName
seeds I-HyperparameterName
helps O
ensure O
that O
our O
results O
are O
more O
representative O
of O
a O
class O
of O
models O
rather O
than O
simply O
revealing O
how O
a O
single O
exceptional O
model O
behaves O
( O
e.g. O
, O
BERT B-MethodName
; O
Devlin O
et O
al O
. O
, O
2019 B-HyperparameterValue
) O
. O

Therefore O
, O
we O
used O
linear B-MethodName
regression I-MethodName
to O
predict O
the O
size O
of O
the O
priming O
effect O
using O
the O
original O
model O
’s O
performance O
on O
the O
test O
set O
: O
A(STjSP;M) O
 O
0 O
+ O
 O
1perf(M;S O
T O
) O
+ O
(3 O
) O
To O
obtain O
a O
more O
appropriate O
adaptation O
effect O
( O
AE O
) O
for O
analysis O
, O
we O
subtracted O
out O
the O
predicted O
linear O
relation O
between O
the O
original O
model O
’s O
test O
performance O
and O
the O
size O
of O
the O
ﬁnal O
priming O
effect O
: O
AE(STjSP;M O
) O
= O
A(STjSP;M)  O
 O
1perf(M;S O
T)(4 O
) O
This O
measure O
of O
priming O
more O
directly O
reﬂects O
the O
interaction O
of O
the O
original O
model O
with O
the O
priming O
set O
, O
normalizing O
the O
adaptation O
effect O
by O
each O
model O
and O
prime O
construction O
, O
and O
producing O
a O
comparable O
measure O
to O
that O
studied O
by O
Prasad O
et O
al O
. O
( O
2019 O
) O
. O

In O
this O
paper O
, O
we O
propose O
a O
new O
word O
embedding O
model O
which O
is O
based O
on O
SVM B-MethodName
regression O
. O

1 O
Introduction O
Word O
embedding O
models O
such O
as O
Skip O
- O
gram O
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
and O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
represent O
words O
as O
vectors O
of O
typically O
around O
300 O
dimensions O
. O

First O
, O
we O
empirically O
show O
that O
effective O
word O
embeddings O
can O
be O
learned O
from O
purely O
ordinal O
information O
, O
which O
stands O
in O
contrast O
to O
the O
probabilistic O
view O
taken O
by O
e.g. O
Skip O
- O
gram O
and O
GloVe B-MethodName
. O

2 O
Background O
and O
Related O
Work O
2.1 O
Word O
Embedding O
Various O
methods O
have O
already O
been O
proposed O
for O
learning O
vector O
space O
representations O
of O
words O
, O
e.g. O
based O
on O
matrix O
factorization O
( O
Turney O
and O
Pantel O
, O
2010 O
) O
or O
neural B-MethodName
networks I-MethodName
. O

Here O
we O
brieﬂy O
review O
Skip O
- O
gram O
and O
GloVe B-MethodName
, O
two O
popular O
models O
which O
share O
some O
similarities O
with O
our O
model O
. O

GloVe B-MethodName
is O
another O
popular O
model O
for O
word O
embedding O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

They O
show O
that O
capturing O
this O
uncertainty O
leads O
to O
vectors O
that O
outperform O
those O
of O
the O
GloVe B-MethodName
model O
, O
on O
which O
their O
model O
is O
based O
. O

For O
the O
baseline O
models O
, O
we O
have O
used O
the O
default O
settings O
, O
apart O
from O
the O
D O
- O
GloVe B-MethodName
model O
for O
which O
no O
default O
values O
were O
provided O
by O
the O
authors O
. O

For O
D O
- O
GloVe B-MethodName
, O
we O
have O
therefore O
tuned O
the O
parameters O
using O
the O
ranges O
discussed O
in O
( O
Jameel O
and O
Schockaert O
, O
2016 O
) O
. O

As O
baselines O
we O
have O
used O
the O
following O
standard O
word O
embedding O
models O
: O
the O
Skip O
- O
gram O
( O
SG O
) O
and O
Continuous O
Bag O
- O
of O
- O
Words O
( O
CBOW B-MethodName
) O
models3 O
, O
proposed O
in O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
, O
the O
GloVe B-MethodName
model4 O
, O
proposed O
in O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
and O
the O
D O
- O
GloVe B-MethodName
model5proposed O
in O
( O
Jameel O
and O
Schockaert O
, O
2016 O
) O
. O

Gsem O
Gsyn O
MSR O
SG O
71.5 O
64.2 O
68.6 O
CBOW B-MethodName
74.2 O
62.3 O
66.2 O
GloVe B-MethodName
80.2 O
58.0 O
50.3 O
D O
- O
GloVe B-MethodName
81.4 O
59.1 O
59.6 O
Gauss O
- O
D O
- O
cos O
61.5 O
53.6 O
50.7 O
Gauss O
- O
D O
- O
eucl O
61.5 O
53.6 O
50.7 O
Gauss O
- O
S O
- O
cos O
61.2 O
53.2 O
49.8 O
Gauss O
- O
S O
- O
eucl O
61.4 O
53.3 O
49.8 O
Reg O
- O
li O
- O
cos O
77.8 O
62.4 O
62.6 O
Reg O
- O
li O
- O
eucl O
77.9 O
62.6 O
62.6 O
Reg O
- O
qu O
- O
cos O
78.6 O
65.7 O
63.5 O
Reg O
- O
qu O
- O
eucl O
78.7 O
65.7 O
63.6 O
Reg O
- O
li O
- O
eucl O
word O
vectors O
, O
obtained O
using O
linear O
kernel O
, O
compared O
using O
Euclidean O
distance O
; O
Reg O
- O
qu O
- O
cos O
word O
vectors O
, O
obtained O
using O
quadratic O
kernel O
, O
compared O
using O
cosine O
similarity O
; O
Reg O
- O
qu O
- O
eucl O
word O
vectors O
, O
obtained O
using O
quadratic O
kernel O
, O
compared O
using O
Euclidean O
distance O
; O
Reg O
- O
li O
- O
prod O
Gaussian O
word O
regions O
, O
obtained O
using O
linear O
kernel O
, O
compared O
using O
the O
inner O
productE O
; O
Reg O
- O
li O
- O
wprod O
Gaussian O
word O
regions O
estimated O
using O
the O
weighted B-HyperparameterName
variant O
, O
obtained O
using O
linear O
kernel O
, O
compared O
using O
the O
inner O
productE O
; O
Reg O
- O
li O
- O
JS O
Gaussian O
word O
regions O
, O
obtained O
using O
linear O
kernel O
, O
compared O
using O
the O
JensenShannon O
divergence O
; O
Reg O
- O
li O
- O
wJS O
Gaussian O
word O
regions O
estimated O
using O
the O
weighted B-HyperparameterName
variant O
, O
obtained O
using O
linear O
kernel O
, O
compared O
using O
Jensen O
- O
Shannon O
divergence O
. O

Recall B-MetricName
that O
the O
parameters O
of O
D O
- O
GloVe B-MethodName
were O
tuned O
on O
the O
Google O
Analogy O
Test O
Set O
, O
hence O
the O
results O
reported O
for O
this O
model O
for O
Gsem O
and O
Gsyn O
might O
be O
slightly O
higher O
than O
what O
would O
normally O
be O
obtained O
. O

We O
outperform O
SG O
and O
CBOW B-MethodName
for O
Gsem O
and O
Gsyn O
but O
not O
for O
MSR O
, O
and O
we O
outperform O
GloVe B-MethodName
and O
D O
- O
GloVe B-MethodName
for O
Gsyn O
and O
MSR O
but O
not O
for O
Gsem O
. O

For O
GloVe B-MethodName
, O
SG O
and O
CBOW B-MethodName
, O
we O
only O
show O
results O
for O
cosine O
, O
as O
this O
led O
to O
the O
best O
results O
. O

For O
D O
- O
GloVe B-MethodName
, O
we O
used O
the O
likelihood O
- O
based O
similarity O
measure O
proposed O
in O
the O
original O
paper O
, O
which O
was O
found O
to O
outperform O
both O
cosine O
and O
Euclidean O
distance O
for O
that O
model O
. O

Of O
the O
standard O
models O
SG O
and O
D O
- O
GloVe B-MethodName
obtain O
the O
strongest O
performance O
. O

S1 O
S2 O
S3 O
S4 O
S5 O
S6 O
S7 O
S8 O
S9 O
S10 O
S11 O
S12 O
SG O
0.656 O
0.773 O
0.789 O
0.648 O
0.709 O
0.459 O
0.500 O
0.415 O
0.435 O
0.773 O
0.655 O
0.731 O
CBOW B-MethodName
0.644 O
0.768 O
0.740 O
0.532 O
0.622 O
0.419 O
0.341 O
0.361 O
0.343 O
0.707 O
0.597 O
0.693 O
GloVe B-MethodName
0.595 O
0.755 O
0.746 O
0.515 O
0.577 O
0.318 O
0.533 O
0.382 O
0.354 O
0.690 O
0.652 O
0.724 O
D O
- O
GloVe B-MethodName
0.659 O
0.788 O
0.785 O
0.555 O
0.651 O
0.401 O
0.535 O
0.413 O
0.388 O
0.778 O
0.656 O
0.746 O
Gauss O
- O
D O
- O
cos O
0.591 O
0.622 O
0.661 O
0.403 O
0.501 O
0.249 O
0.388 O
0.337 O
0.411 O
0.640 O
0.599 O
0.643 O
Gauss O
- O
D O
- O
eucl O
0.591 O
0.623 O
0.661 O
0.403 O
0.501 O
0.250 O
0.388 O
0.338 O
0.411 O
0.641 O
0.599 O
0.643 O
Gauss O
- O
D O
- O
prod O
0.588 O
0.618 O
0.658 O
0.399 O
0.498 O
0.213 O
0.356 O
0.326 O
0.409 O
0.631 O
0.588 O
0.633 O
Gauss O
- O
D O
- O
JS O
0.598 O
0.619 O
0.665 O
0.403 O
0.532 O
0.288 O
0.381 O
0.339 O
0.410 O
0.643 O
0.599 O
0.644 O
Gauss O
- O
S O
- O
cos O
0.593 O
0.632 O
0.681 O
0.409 O
0.506 O
0.256 O
0.392 O
0.337 O
0.416 O
0.649 O
0.601 O
0.644 O
Gauss O
- O
S O
- O
eucl O
0.593 O
0.632 O
0.681 O
0.409 O
0.507 O
0.356 O
0.393 O
0.337 O
0.416 O
0.649 O
0.603 O
0.644 O
Gauss O
- O
S O
- O
prod O
0.591 O
0.619 O
0.659 O
0.403 O
0.505 O
0.312 O
0.389 O
0.328 O
0.412 O
0.633 O
0.591 O
0.633 O
Gauss O
- O
S O
- O
JS O
0.598 O
0.622 O
0.667 O
0.405 O
0.533 O
0.288 O
0.385 O
0.349 O
0.410 O
0.643 O
0.601 O
0.644 O
Reg O
- O
li O
- O
cos O
0.666 O
0.764 O
0.821 O
0.652 O
0.713 O
0.489 O
0.469 O
0.354 O
0.361 O
0.734 O
0.642 O
0.739 O
Reg O
- O
li O
- O
eucl O
0.668 O
0.766 O
0.821 O
0.654 O
0.715 O
0.489 O
0.469 O
0.359 O
0.361 O
0.734 O
0.643 O
0.739 O
Reg O
- O
li O
- O
prod O
0.661 O
0.759 O
0.818 O
0.634 O
0.710 O
0.481 O
0.445 O
0.358 O
0.360 O
0.724 O
0.641 O
0.729 O
Reg O
- O
li O
- O
wprod O
0.663 O
0.761 O
0.819 O
0.638 O
0.711 O
0.482 O
0.446 O
0.359 O
0.361 O
0.725 O
0.642 O
0.731 O
Reg O
- O
li O
- O
JS O
0.663 O
0.758 O
0.815 O
0.638 O
0.709 O
0.479 O
0.443 O
0.359 O
0.361 O
0.723 O
0.641 O
0.729 O
Reg O
- O
li O
- O
wJS O
0.665 O
0.760 O
0.816 O
0.638 O
0.710 O
0.481 O
0.445 O
0.359 O
0.361 O
0.725 O
0.641 O
0.731 O
Reg O
- O
qu O
- O
cos O
0.684 O
0.781 O
0.839 O
0.662 O
0.723 O
0.505 O
0.479 O
0.367 O
0.368 O
0.777 O
0.656 O
0.744 O
Reg O
- O
qu O
- O
eucl O
0.685 O
0.781 O
0.839 O
0.664 O
0.723 O
0.509 O
0.479 O
0.367 O
0.368 O
0.779 O
0.656 O
0.744 O
Reg O
- O
qu O
- O
prod O
0.681 O
0.780 O
0.831 O
0.658 O
0.719 O
0.501 O
0.478 O
0.355 O
0.331 O
0.778 O
0.653 O
0.741 O
Reg O
- O
qu O
- O
wprod O
0.684 O
0.788 O
0.831 O
0.663 O
0.721 O
0.501 O
0.475 O
0.370 O
0.365 O
0.778 O
0.653 O
0.739 O
Reg O
- O
qu O
- O
JS O
0.680 O
0.781 O
0.826 O
0.661 O
0.715 O
0.497 O
0.471 O
0.328 O
0.355 O
0.771 O
0.649 O
0.721 O
Reg O
- O
qu O
- O
wJS O
0.678 O
0.782 O
0.824 O
0.662 O
0.712 O
0.498 O
0.469 O
0.326 O
0.351 O
0.771 O
0.644 O
0.720 O
Table O
3 O
: O
Results O
for O
McRae O
feature O
norms O
( O
F1 B-MetricName
) O
. O

Taxonomic O
Attributive O
lin O
quad O
lin O
quad O
SG O
0.781 O
0.784 O
0.365 O
0.378 O
CBOW B-MethodName
0.775 O
0.781 O
0.361 O
0.371 O
GloVe B-MethodName
0.785 O
0.786 O
0.364 O
0.377 O
D O
- O
GloVe B-MethodName
0.743 O
0.749 O
0.342 O
0.364 O
Gauss O
- O
D O
0.787 O
0.789 O
0.406 O
0.414 O
Gauss O
- O
S O
0.781 O
0.784 O
0.401 O
0.406 O
Reg O
- O
li O
0.791 O
0.796 O
0.399 O
0.406 O
Reg O
- O
qu O
0.795 O
0.799 O
0.411 O
0.421 O
5 O
- O
fold O
cross O
- O
validation O
to O
train O
a O
binary O
SVM B-MethodName
for O
each O
property O
and O
compute O
the O
average B-MetricName
F O
- O
score B-MetricName
due O
to O
unbalanced O
class O
label O
distribution O
. O

We O
separately O
present O
results O
for O
SVMs B-MethodName
with O
a O
linear O
and O
a O
quadratic O
kernel O
. O

Therefore O
, O
we O
propose O
a O
zero O
- O
shot O
approach O
that O
implicitly O
integrates O
perceptual O
knowledge O
into O
pre O
- O
trained O
textual O
embeddings O
( O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
fastText O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
) O
via O
multitask O
training O
. O

Many O
of O
these O
approaches O
modify O
the O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
models O
by O
incorporating O
image O
features O
to O
the O
context O
for O
concrete O
words O
( O
Hill O
and O
Korhonen O
, O
2014 O
; O
Kottur O
et O
al O
. O
, O
2016 O
; O
Zablocki O
et O
al O
. O
, O
2017 O
; O
Ailem O
et O
al O
. O
, O
2018 O
) O
; O
minimizing O
the O
maxmargin O
loss O
between O
the O
image O
- O
vector O
and O
its O
corresponding O
word O
vectors O
( O
Lazaridou O
et O
al O
. O
, O
2015 O
) O
; O
providing O
social O
cues O
based O
on O
child O
- O
directed O
speech O
along O
with O
visual O
scenes O
( O
Lazaridou O
et O
al O
. O
, O
2016 O
) O
; O
or O
by O
extracting O
the O
relationship O
between O
words O
and O
images O
using O
multi O
- O
view O
spectral O
graphs O
( O
Fukui O
et O
al O
. O
, O
2017 O
) O
. O

Here O
, O
the O
grounded O
word O
vectors O
are O
usually O
the O
results O
of O
updating O
the O
textual O
word O
vectors O
during O
training O
( O
Mao O
et O
al O
. O
, O
2016 O
) O
or O
the O
output O
of O
sentence O
encoders O
such O
as O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

Let O
Te(w)2Rdbe O
a O
pretrained O
textual O
embedding O
of O
the O
word O
w O
, O
which O
has O
been O
trained O
on O
textual O
data O
only O
( O
e.g. O
, O
GloVe B-MethodName
) O
. O

Two O
popular O
pre O
- O
trained O
textual O
word O
embeddings O
namely O
GloVe B-MethodName
( O
crawl 300d 2:2M cased O
) O
and O
fastText O
( O
crawl 300d 2M SubW O
) O
are O
used O
for O
initialization B-HyperparameterName
of O
the O
embedding O
Te O
. O

For O
the O
regularization O
, O
R O
( O
 O
= O
0:001 O
; O
 O
= O
1 O
) O
for O
GloVe B-MethodName
andR O
( O
 O
= O
0:01 O
; O
 O
= O
0 O
) O
for O
fastText O
yielded O
the O
best O
relative O
results O
by O
meta O
parameter O
search O
. O

We O
refer O
to O
the O
second O
baseline O
as O
C_GloVe B-MethodName
and O
C_fastText O
for O
Glove O
and O
fastText O
trained O
only O
on O
captions O
. O

Model O
RW O
MEN O
WSim O
MTurk O
SimVerb O
SimLex O
Mean O
353 O
771 O
3500 O
999 O
GloVe B-MethodName
45.5 O
80.5 O
73.8 O
71.5 O
28.3 O
40.8 O
56.7 O
C_GloVe B-MethodName
46 O
82.1 O
74.1 O
72.3 O
29.3 O
43.3 O
57.85 O
VGE_G O
52.6 O
85.1 O
78.9 O
73.4 O
37.4 O
51.8 O
63.2 O
FastText O
56.1 O
81.5 O
72.2 O
75.1 O
37.8 O
47.1 O
61.6 O
C_fastText O
49.2 O
68.3 O
58.1 O
56.8 O
30.3 O
41.9 O
50.76 O
VGE_F O
57.8 O
83.6 O
73.9 O
76.1 O
39.2 O
49.0 O
63.2 O
Table O
1 O
: O
Intrinsic O
evaluation O
. O

In O
general O
, O
fastText O
performs O
better O
on O
word O
- O
level O
tasks O
compared O
to O
GloVe B-MethodName
, O
prob O
- O
ably O
because O
it O
provides O
more O
context O
for O
each O
word O
by O
leveraging O
from O
its O
sub O
- O
words O
. O

The O
results O
also O
validate O
the O
efﬁcacy O
of O
our O
proposed O
model O
since O
updating O
the O
embeddings O
on O
captions O
alone O
( O
C_fastText O
and O
C_GloVe B-MethodName
) O
brings O
subtle O
or O
no O
improvements O
. O

By O
the O
proposed O
visual O
grounding O
, O
signiﬁcant O
improvements O
are O
achieved O
on O
all O
datasets O
for O
both O
fastText O
and O
GloVe B-MethodName
. O

We O
limit O
our O
comparison O
to O
those O
who O
adopted O
the O
pre O
- O
trained O
GloVe B-MethodName
or O
fastText O
since O
these O
pre O
- O
trained O
models O
alone O
outperform O
many O
visually O
grounded O
embeddings O
such O
as O
( O
Hasegawa O
et O
al O
. O
, O
2017 O
; O
Zablocki O
et O
al O
. O
, O
2017 O
) O
on O
many O
of O
our O
evaluation O
datasets O
. O

Conceptually O
, O
Kiela O
et O
al O
. O
( O
2018 O
) O
also O
induces O
visual O
grounding O
on O
GloVe B-MethodName
by O
using O
the O
MSCOCO O
data O
set O
. O

Even O
though O
they O
used O
two O
pre O
- O
trained O
embeddings O
( O
GloVe B-MethodName
and O
Word2vec O
) O
and O
other O
resources O
, O
our O
model O
still O
outperforms O
their O
approach O
on O
MEN O
and O
WSim353 O
, O
but O
their O
approach O
is O
better O
on O
Simlex999 O
. O

We O
see O
a O
large O
improvement O
over O
GloVe B-MethodName
in O
all O
categories O
. O

Therefore O
, O
it O
outperforms O
GloVe B-MethodName
not O
only O
on O
concrete O
pairs O
( O
conc O
- O
q4 O
) O
but O
also O
on O
highly O
abstract O
pairs O
( O
conc O
- O
q1 O
) O
. O

We O
compared O
the O
results O
on O
SimLex999 O
with O
another O
recent O
visually O
grounded O
model O
called O
Picturebook O
( O
Kiros O
et O
al O
. O
, O
2018 O
) O
, O
which O
employs O
a O
multi O
- O
modal O
gating O
mechanism O
( O
similar O
to O
a O
LSTM B-MethodName
and O
GRU O
update O
gate O
) O
to O
fuse O
the O
Glove O
and O
Picturebook O
embeddings O
( O
Table O
3 O
) O
. O

Picturebook O
’s O
performance O
is O
highly O
biased O
toward O
concrete O
words O
( O
conc O
- O
q3 O
, O
conc O
- O
q4 O
) O
and O
performs O
worse O
than O
GloVe B-MethodName
by O
nearly O
29 O
% O
on O
highly O
abstract O
words O
( O
conc O
- O
q1 O
) O
. O

Picturebook O
+ O
GloVe B-MethodName
on O
the O
other O
hand O
shows O
better O
results O
but O
still O
performs O
worse O
on O
highly O
abstract O
words O
and O
adjectives O
. O

This O
is O
further O
demonstrated O
through O
inspection O
of O
nearest B-MethodName
neighbors I-MethodName
( O
Table O
5 O
) O
. O

Given O
the O
word O
‘ O
bird’,Model O
All O
Adjs O
Nouns O
Verbs O
Conc O
- O
q1 O
Conc O
- O
q2 O
Conc O
- O
q3 O
Conc O
- O
q4 O
Hard O
GloVe B-MethodName
40.8 O
62.2 O
42.8 O
19.6 O
43.3 O
41.6 O
42.3 O
40.2 O
27.2 O
VGE_G O
( O
ours O
) O
51.8 O
72.1 O
52.0 O
35 O
53.1 O
54.8 O
47.4 O
56.8 O
38.3 O
Picturebook O
37.3 O
11.7 O
48.2 O
17.3 O
14.4 O
27.5 O
46.2 O
60.7 O
28.8 O
Picturebook+GloVe B-MethodName
45.5 O
46.2 O
52.1 O
22.8 O
36.7 O
41.7 O
50.4 O
57.3 O
32.5 O
Table O
3 O
: O
SimLex999 O
( O
Spearman O
’s O
 O
) O
results O
. O

Model O
STS12 O
STS13 O
STS14 O
STS15 O
STS16 O
Mean O
GloVe B-MethodName
52.25 O
49.59 O
54.72 O
56.25 O
51.39 O
52.84 O
C_GloVe B-MethodName
53.27 O
50.56 O
56.72 O
57.86 O
52.11 O
54.10 O
VGE_G O
55.31 O
57.24 O
65.54 O
67.61 O
65.87 O
62.35 O
Fasttext O
22.95 O
24.63 O
31.37 O
37.71 O
29.34 O
29.2 O
C_fasttext O
29.69 O
23.80 O
37.58 O
45.29 O
29.34 O
33.14 O
VGE_F O
31.78 O
32.26 O
42.51 O
48.79 O
38.15 O
38.70 O
VGE_G O
( O
ours O
) O
55 O
57 O
66 O
68 O
66 O
62.40 O
Word2vec O
52 O
58 O
66 O
68 O
65 O
61.80 O
ELMo B-MethodName
( O
top_layer O
) O
54 B-MetricValue
49 O
62 O
67 O
63 O
59.00 O
ELMo B-MethodName
( O
all_layers O
) O
55 O
51 O
63 O
69 O
64 O
60.40 O
Power O
- O
mean B-MetricName
54 O
52 O
63 O
66 O
67 O
60.40 O
Table O
4 O
: O
Comparison O
( O
Pearson O
correlation B-MetricName
100 O
) O
of O
our O
embeddings O
( O
VGE O
_ O
* O
) O
with O
baselines O
( O
ﬁrst O
two O
sections O
) O
and O
other O
word O
embeddings O
( O
bottom O
) O
on O
STS O
. O

GloVe B-MethodName
returns O
‘ O
turtle O
’ O
and O
‘ O
nest O
’ O
while O
grounded O
GloVe B-MethodName
returns O
‘ O
sparrow O
’ O
and O
‘ O
avian O
’ O
, O
which O
both O
reference O
birds O
. O

For O
the O
word O
‘ O
happy O
’ O
for O
example O
, O
GloVe B-MethodName
suffers O
from O
a O
bias O
toward O
dissimilar O
words O
with O
high O
co O
- O
occurrence O
such O
as O
‘ O
everyone O
’ O
, O
‘ O
always O
’ O
, O
and O
‘ O
wish O
’ O
. O

A O
sample O
of O
nearest B-MethodName
neighbors I-MethodName
for O
FastText O
and O
VGE_F O
is O
available O
in O
Appendix O
B. O
However O
, O
since O
FastText O
already O
performs O
quite O
well O
on O
intrinsic O
tasks O
, O
the O
difference O
with O
its O
grounded O
version O
is O
subtle O
which O
also O
conﬁrms O
the O
results O
in O
Table O
1.Extrinsic O
Evaluation O
: O
Table O
4 O
shows O
the O
results O
on O
semantic O
similarity O
benchmarks O
. O

While O
fastText O
outperforms O
GloVe B-MethodName
on O
intrinsic O
tasks O
, O
GloVe B-MethodName
is O
superior O
here O
. O

The O
reason O
might O
be O
that O
unlike O
fastText O
GloVe B-MethodName
treats O
each O
word O
as O
a O
single O
unit O
and O
takes O
into O
account O
the O
global O
co O
- O
occurrences O
of O
words O
. O

Table O
4 O
( O
bottom O
) O
shows O
the O
results O
of O
our O
best O
model O
( O
VGE_G O
) O
with O
other O
textual O
word O
embeddings O
namely O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018b O
) O
, O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
and O
Power O
- O
Mean O
( O
Rücklé O
et O
al O
. O
, O
2018 O
) O
reported O
by O
( O
Perone O
et O
al O
. O
, O
2018 O
) O
. O

While O
the O
textual O
GloVe B-MethodName
is O
the O
second O
- O
worst O
model O
( O
by O
mean B-MetricName
score B-MetricName
: O
52.84 B-MetricValue
) O
in O
the O
table O
, O
its O
grounded O
version O
VGE_G O
is O
the O
best O
one O
. O

It O
would O
be O
interesting O
to O
see O
if O
our O
ﬁndings O
extend O
to O
grounded O
sentence O
embedding O
models O
( O
Sileo O
, O
2021 O
; O
Bordes O
et O
al O
. O
, O
2019 O
; O
Tan O
and O
Bansal O
, O
2020 O
) O
for O
instance O
by O
training O
transformer O
- O
based O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
on O
top O
of O
our O
embeddings O
. O

Dependency O
on O
the O
Encoding O
Dimension O
c O
: O
We O
train O
our O
model O
with O
different O
dimensions O
of O
thehappy O
sad O
big O
bird O
horse O
together O
smart O
G O
V O
G O
V O
G O
V O
G O
V O
G O
V O
G O
V O
G O
V O
lucky O
pleased O
sadly O
saddened O
hard O
humongous O
turtle O
sparrow O
dog O
racehorse O
well O
togeather O
sensible O
witty O
everyone O
delighted O
shame O
tragic O
little O
Big O
nest O
Birds O
riding O
Thoroughbred O
bring O
togheter O
dumb O
shrewd O
love O
merry O
horrible O
mournful O
squirrel O
avian O
ponies O
Horses O
both O
toegther O
sophisticated O
inteligent O
always O
thrilled O
scared O
saddening O
donkey O
steed O
they O
togather O
attractive O
resourceful O
wish O
joyful O
awful O
sorrowful O
apart O
togethor O
wise O
quick O
- O
witted O
hope O
hapy O
pity O
Sad O
up O
2gether O
kinda O
heartbreaking O
them O
togehter O
sorry O
heartbroken O
put O
togther O
along O
toghether O
with O
gether O
Table O
5 O
: O
Results O
of O
10 O
nearest B-MethodName
neighbors I-MethodName
for O
Glove O
( O
G O
) O
and O
VGE_G O
( O
V O
) O
. O

While O
GloVe B-MethodName
retrieves O
more O
related O
words O
, O
ours O
( O
VGE_G O
) O
focuses O
on O
similar O
words O
. O

= O
1 O
indicates O
no O
use O
of O
GloVe B-MethodName
and O
 O
= O
0means B-MetricName
no O
use O
of O
VGE_G. O

Table O
8 O
shows O
the O
results O
using O
GloVe B-MethodName
and O
VGE_G O
with O
different O
sizes O
. O

Signiﬁcant O
improvement O
is O
already O
achieved O
keeping O
the O
original O
dimension O
of O
GloVe B-MethodName
( O
300 O
) O
. O

Dependency O
on O
the O
Textual O
Embeddings O
: O
Further O
, O
we O
analyze O
how O
much O
of O
GloVe B-MethodName
’s O
original O
properties O
are O
maintained O
by O
the O
visual O
grounding O
. O

GivenVwandGwas O
the O
VGE_G O
and O
GloVe B-MethodName
vectors O
for O
the O
word O
w O
, O
we O
create O
a O
vector O
containing O
both O
embeddings O
Ew= O
[ O
( O
1  O
 O
) O
Gw O
; O
 O
Vw O
] O
. O

Signiﬁcant O
improvement O
is O
achieved O
even O
with O
the O
same O
size O
as O
the O
textual O
GloVe B-MethodName
. O

While O
both O
GloVe B-MethodName
and O
FastText O
show O
the O
same O
behaviour O
for O
language O
model O
tasks O
, O
fastText O
embeddings O
require O
more O
deviation O
( O
 O
= O
0inR O
( O
 O
; O
 O
) O
) O
to O
adapt O
to O
the O
binary O
discrimination O
task O
( O
LBW O
) O
. O

We O
constructed O
the O
visually O
grounded O
versions O
of O
GloVe B-MethodName
and O
fastText O
by O
learning O
a O
zero O
- O
shot O
transformation O
from O
textual O
to O
grounded O
space O
trained O
on O
the O
MSCOCO O
dataset O
. O

A O
closely O
related O
line O
of O
work O
with O
deterministic O
updates O
is O
deep O
knowledge O
tracing O
( O
DKT O
) O
( O
Piech O
et O
al O
. O
, O
2015 O
) O
, O
which O
applied O
a O
classical O
LSTM B-MethodName
model O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
to O
knowledge O
tracing O
and O
showed O
strong O
improvements O
over O
BKT O
. O

These O
vectors O
correspond O
respectively O
to O
the O
input O
gates O
andforget O
gates O
in O
recurrent O
neural O
network O
architectures O
such O
as O
the O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
or O
GRU O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
. O

As O
in O
those O
architectures O
, O
we O
will O
use O
neural B-MethodName
networks I-MethodName
to O
chooseαt O
, O
βtat O
each O
time O
step O
t O
, O
so O
that O
they O
may O
be O
sensitive O
in O
nonlinear O
ways O
to O
the O
context O
at O
round O
t. O

To O
consider O
the O
other O
end O
of O
the O
spectrum O
, O
we O
implemented O
a O
ﬂexible O
LSTM B-MethodName
model O
in O
the O
style O
of O
recent O
deep O
learning O
research O
. O

This O
alternative O
model O
predicts O
each O
response O
by O
a O
student O
( O
i.e. O
, O
on O
an O
MC O
or O
TP O
card O
) O
given O
the O
entire O
history O
of O
previous O
interactions O
with O
that O
student O
as O
summarized O
by O
an O
LSTM B-MethodName
. O

The O
LSTM B-MethodName
architecture O
is O
formally O
capable O
of O
capturing O
update O
rules O
exactly O
like O
those O
of O
PKT O
, O
but O
it O
is O
far O
from O
limited O
to O
such O
rules O
. O

Here O
ht∈Rddenotes O
the O
hidden O
state O
of O
the O
LSTM B-MethodName
, O
which O
evolves O
as O
the O
student O
interacts O
with O
the O
system O
and O
learns O
. O

htdepends O
on O
the O
LSTM B-MethodName
inputs O
for O
all O
times O
< O
t O
, O
just O
like O
the O
knowledge O
stateθtin O
equations O
( O
1)–(2 O
) O
. O

It O
also O
depends O
on O
the O
LSTM B-MethodName
input O
for O
time O
t O
, O
since O
that O
speciﬁes O
the O
ﬂash O
cardatto O
which O
we O
are O
predicting O
the O
response O
yt O
. O

When O
reading O
the O
history O
of O
past O
interactions O
, O
the O
LSTM B-MethodName
input O
at O
each O
time O
step O
tconcatenates O
the O
vector O
representation O
atof O
the O
current O
ﬂash O
card O
with O
vectors O
at−1,yt−1,ft−1that O
describe O
the O
student O
’s O
experience O
in O
round O
t−1 O
: O
these O
respectively O
encode O
the O
previous O
ﬂash O
card O
, O
the O
student O
’s O
response O
to O
it O
( O
a O
one O
- O
hot O
39 O
- O
dimensional O
vector O
) O
, O
and O
the O
resulting O
feedback O
( O
a O
39 O
- O
dimensional O
binary O
vector O
that O
indicates O
the O
remaining O
options O
after O
feedback O
) O
. O

Indicative O
feedback O
sets O
ft−1 O
= O
yt−1Model O
Parameters O
Accuracy(test O
) O
Cross O
- O
Entropy O
RNG+CM≈97 O
K O
0.449 O
1.888 O
LSTM≈25 B-MethodName
K O
0.429 O
1.992 O
Table O
4 O
: O
Comparison O
of O
our O
best O
- O
performing O
PKT O
model O
( O
RNG+CM O
) O
to O
our O
LSTM B-MethodName
model O
. O

On O
our O
dataset O
, O
PKT O
outperforms O
the O
LSTM B-MethodName
both O
in O
terms O
of O
accuracy B-MetricName
and O
cross O
- O
entropy B-MetricName
. O

Thus O
, O
ft−1gives O
the O
set O
of O
“ O
positive O
” O
options O
that O
we O
used O
in O
the O
RG O
update O
vector O
, O
while O
Ot−1gives O
the O
set O
of O
“ O
negative O
” O
options O
, O
allowing O
the O
LSTM B-MethodName
to O
similarly O
update O
its O
hidden O
state O
from O
ht−1tohtto O
reﬂect O
learning.11 O
As O
in O
section O
5.4 O
, O
we O
train O
the O
parameters O
by O
L2 O
- O
regularized O
maximum O
likelihood O
, O
with O
early O
stopping O
on O
development O
data O
. O

The O
weights B-HyperparameterName
for O
the O
LSTM B-MethodName
were O
initialized O
uniformly O
at O
random O
∼U(−δ,+δ O
) O
, O
whereδ= O
0.01 O
, O
and O
RMSProp O
was O
used O
for O
gradient O
descent O
. O

Interestingly O
, O
a O
dimensionality O
of O
just O
d=10 O
performed O
best O
on O
dev O
data:12at O
this O
size O
, O
the O
LSTM B-MethodName
has O
fewer O
parameters O
than O
our O
best O
model O
. O

We O
acknowledge O
that O
the O
LSTM B-MethodName
might O
perform O
better O
when O
a O
larger O
training O
set O
was O
available O
( O
which O
would O
allow O
a O
larger O
hidden B-HyperparameterName
layer I-HyperparameterName
) O
, O
or O
using O
a O
different O
form O
of O
regularization O
( O
Srivastava O
et O
al O
. O
, O
2014 B-HyperparameterValue
) O
. O

We O
would O
storein O
the O
LSTM B-MethodName
’s O
vector O
of O
cell O
activations B-HyperparameterName
, O
and O
conﬁgure O
the O
LSTM B-MethodName
’s O
“ O
input O
” O
and O
“ O
forget O
” O
gates O
to O
update O
this O
according O
to(2)where O
utis O
computed O
from O
the O
input O
. O

12We O
searched O
0.001,0.002,0.005,0.01,0.02,0.05for O
the O
regularization O
coefﬁcient O
, O
and O
5,10,15,20,50,100,200for O
the O
number O
of O
hidden B-HyperparameterName
units I-HyperparameterName
. O
Mcan O
be O
regarded O
as O
projecting O
φ(x O
, O
y)down O
to O
the O
LSTM B-MethodName
’s O
hidden O
dimension O
d O
, O
learning O
how O
to O
weight B-HyperparameterName
and O
use O
these O
features O
. O

In O
this O
variant O
, O
the O
LSTM B-MethodName
would O
no O
longer O
need O
to O
take O
atas O
part O
of O
its O
input O
at O
time O
t O
: O
rather O
, O
ht(just O
likeθtin O
PKT O
) O
would O
be O
a O
pure O
representation O
of O
the O
student O
’s O
knowledge O
state O
at O
time O
t O
, O
capable O
of O
predicting O
ytforanyat O
. O

This O
setup O
more O
closely O
resembles O
PKT O
— O
or O
the O
DKT O
LSTM B-MethodName
of O
Piech O
et O
al O
. O
( O
2015 O
) O
. O

We O
also O
found O
that O
in O
comparison O
to O
a O
less O
constrained O
LSTM B-MethodName
model O
, O
we O
can O
better O
ﬁt O
the O
human O
behavior O
by O
using O
weight B-HyperparameterName
update O
schemes O
that O
are O
broadly O
consistent O
with O
schemes O
used O
in O
machine O
learning O
. O

To O
this O
end O
, O
experiments O
were O
run O
using O
BERT B-MethodName
, O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
to O
compare O
its O
performance O
to O
other O
popular O
models O
that O
have O
been O
used O
for O
abusive O
language O
detection O
( O
Section O
6 O
) O
. O

In O
terms O
of O
models O
, O
popular O
classical O
classiﬁcation O
approaches O
include O
Logistic B-MethodName
Regression I-MethodName
and O
LSVM B-MethodName
( O
Linear O
Support O
Vector O
Machines O
) O
. O

In O
the O
‘ O
OffensEval O
’ O
shared O
task O
( O
Zampieri O
et O
al O
. O
, O
2019b O
) O
, O
the O
use O
of O
contextual O
embeddings O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
and O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
exhibited O
the O
best O
results O
. O

Karan O
and O
ˇSnajder O
( O
2018 O
) O
experimented O
with O
cross O
- O
domain O
training O
and O
testing O
, O
and O
opted O
to O
use O
the O
same O
model O
( O
LSVM B-MethodName
) O
with O
minimal O
features O
and O
to O
preprocess O
in O
favour O
of O
interpretability O
. O

4 O
Preliminary O
Feature O
and O
Model O
Study O
The O
ﬁrst O
set O
of O
experiments O
aimed O
to O
test O
the O
efﬁcacy O
of O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
when O
tackling O
the O
Abusive O
Language O
Detection O
task O
. O

For O
this O
, O
BERT B-MethodName
’s O
performance O
was O
compared O
to O
three O
other O
popular O
classiﬁers O
: O
Linear O
SVM B-MethodName
, O
an O
LSTM B-MethodName
( O
Long O
Short O
- O
Term O
Memory O
) O
Recurrent O
Neural O
Network O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
and O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

Information O
on O
the O
models O
themselves O
are O
provided O
below O
. O
DatasetLSVM B-MethodName
LSTM B-MethodName
ELMo B-MethodName
BERT B-MethodName
Acc O
. O

4.1 O
Linear O
SVM B-MethodName
The O
Linear O
SVM B-MethodName
( O
LSVM B-MethodName
) O
was O
modelled O
and O
trained O
in O
the O
Scikit O
- O
learn5library O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
, O
utilising O
a O
TF O
- O
IDF O
vector O
representation O
for O
the O
tweets O
. O

4.2 O
LSTM B-MethodName
Network O
The O
tested O
Deep O
Learning O
Model O
was O
built O
on O
a O
fairly O
simple O
LSTM B-MethodName
architecture O
using O
Keras6 O
with O
a O
TensorFlow7back O
end O
. O

The O
ﬁrst O
layer O
used O
a O
200 O
dimensional O
GloVe B-MethodName
embedding O
, O
8pre O
- O
trained O
on O
2 O
billion O
tweets O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
with O
embedding O
weights B-HyperparameterName
ﬁxed O
throughout O
the O
training O
. O

The O
Embedding O
Layer O
was O
followed O
by O
an O
LSTM B-MethodName
layer O
of O
200 O
units O
. O

4.3 O
ELMo B-MethodName
The O
third O
model O
tested O
used O
ELMo B-MethodName
for O
feature O
extraction O
and O
was O
implemented O
in O
the O
TensorFlow O
hub O
module9with O
1024 O
dimensional O
ELMo B-MethodName
5scikit-learn.org/stable/ O
6github.com/fchollet/keras O
7tensorflow.org/ O
8nlp.stanford.edu/projects/glove/ O
9tfhub.dev/google/elmo/2embeddings O
. O

This O
input O
was O
passed O
through O
an O
LSTM B-MethodName
layer O
of O
dimension O
256 O
and O
then O
a O
dense O
layer O
with O
a O
softmax O
activation B-HyperparameterName
function O
. O

ELMo B-MethodName
’s O
standalone O
performance O
was O
found O
to O
not O
be O
as O
impressive O
as O
hoped O
, O
with O
the O
batch B-HyperparameterName
size I-HyperparameterName
and O
usage O
of O
dropout B-HyperparameterName
signiﬁcantly O
affecting O
classiﬁcation O
rates O
. O

4.4 O
BERT B-MethodName
BERT B-MethodName
base O
, O
uncased O
was O
used O
as O
the O
underlying O
pre O
- O
trained O
model O
, O
in O
a O
ﬁne O
- O
tuning O
only O
approach O
with O
no O
statistical O
or O
linguistic O
features O
. O

The O
model O
built O
on O
the O
runclassifier O
API O
provided O
on O
the O
BERT B-MethodName
GitHub O
page10and O
the O
BERT B-MethodName
tokeniser O
, O
which O
simply O
lower O
- O
cases O
sentences O
and O
removes O
illegal O
characters O
. O

BERT B-MethodName
base O
, O
uncased O
trains O
a O
total O
of O
110 O
million O
parameters O
, O
and O
contains O
12 O
transformer O
blocks O
and O
12 O
self O
- O
attention O
heads O
with O
hidden B-HyperparameterName
layer I-HyperparameterName
dimension O
768 B-HyperparameterValue
. O

BERT B-MethodName
exhibits O
the O
best O
results O
for O
all O
datasets O
used O
in O
the O
experiments O
( O
with O
a O
signiﬁcance O
level O
of O
0:05 O
) O
. O

Surprisingly O
, O
ELMo B-MethodName
was O
neither O
competitive O
with O
BERT B-MethodName
nor O
with O
the O
GLoVE O
- O
embedding O
LSTM B-MethodName
recurrent O
neural O
network O
( O
when O
tested O
with O
the O
same O
statistical O
signiﬁcance O
level O
) O
. O

F O
1 B-MetricValue
Waseem O
and O
Hovy O
.9037 O
.8755 O
.5626 O
.5296 O
.7205 O
.5824 O
.6716 O
.5982 O
Davidson O
et O
al O
. O
.7719 O
.6928 O
.9639 O
.9351 O
.9261 O
.9157 O
.7514 O
.6847 O
Founta O
et O
al O
. O
.7278 O
.6049 O
.8324 O
.7559 O
.9421 O
.9340 O
.7862 O
.7447 O
OLID O
.7108 O
.6269 O
.8247 O
.7308 O
.9251 O
.9162 O
.8004 O
.7738 O
Table O
4 O
: O
Cross O
- O
dataset O
test O
results O
( O
accuracy B-MetricName
and O
macro O
- O
F O
1 O
) O
for O
all O
dataset O
combinations O
, O
using O
the O
BERT B-MethodName
models O
. O

For O
all O
datasets O
, O
these O
were O
BERT B-MethodName
models O
, O
but O
with O
varying O
hyper O
- O
parameter O
settings O
. O

Karan O
and O
ˇSnajder O
( O
2018 O
) O
used O
a O
simpler O
Linear O
SVM B-MethodName
model O
for O
all O
the O
datasets O
for O
the O
sake O
of O
interpretability O
, O
while O
the O
aim O
here O
, O
in O
contrast O
, O
was O
to O
see O
how O
well O
the O
best O
models O
( O
that O
may O
have O
learnt O
some O
datasetspeciﬁc O
biases O
) O
performed O
on O
other O
datasets O
. O

Similar O
trends O
were O
observed O
by O
Karan O
and O
ˇSnajder O
( O
2018 O
) O
when O
employing O
the O
Kolhatkar O
et O
al O
. O
( O
2018 O
) O
and O
TRAC-1 O
( O
Kumar O
et O
al O
. O
, O
2018a O
) O
datasets O
, O
that O
have O
62:7 O
% O
and56:6 O
% O
positive O
samples O
, O
respectively O
, O
and O
exhibited O
better O
results O
in O
cross O
- O
dataset O
testing O
than O
datasets O
with O
lower O
positive O
sample O
ratios O
. O
Subtask O
A O
Subtask O
B O
Subtask O
C O
BERT B-MethodName
Top O
BERT B-MethodName
Top O
BERT B-MethodName
Top O
F1 B-MetricName
.8168 B-MetricValue
.8286 O
.6997 O
.7545 O
.6162 O
.6597 O
Acc O
. O

.8546 O
.8628 O
.9000 O
.9250 O
.7136 O
.7277 O
Rank O
2 O
1 O
9 O
1 O
6 O
1 O
Table O
5 O
: O
BERT B-MethodName
test O
set O
results O
( O
macro O
- O
F O
1and O
accuracy B-MetricName
) O
compared O
to O
top O
OffensEval O
shared O
task O
performers O
. O

For O
the O
OLID O
classiﬁers O
, O
a O
BERT B-MethodName
model O
was O
used O
without O
any O
extra O
statistical O
features O
and O
with O
minimal O
preprocessing O
( O
only O
lower O
- O
casing O
of O
tweets O
) O
. O

Also O
in O
subtask O
C O
the O
model O
showed O
signiﬁcant O
signs O
of O
over-ﬁtting O
, O
with O
the O
BERT B-MethodName
approach O
only O
achieving O
an O
F O
1score B-MetricName
of O
0:52 O
. O

Founta O
et O
al O
. O
’s O
dataset O
shows O
a O
high O
number O
of O
hateful O
tweets O
classiﬁed O
as O
NOT O
, O
which O
may O
be O
due O
to O
the O
implicit O
nature O
of O
sexism O
or O
sarcasm O
in O
the O
tweets O
involved O
. O
Dataset O
TagSubtask O
A O
Subtask O
B O
Subtask O
C O
OFF O
NOT O
UNT O
TIN O
IND O
GRP O
OTH O
Waseem O
and O
Hovyracism O
52.94 O
47.06 O
0.00 O
100 O
64.70 O
23.52 O
11.76 O
sexism O
42.96 O
57.04 O
8.28 O
91.72 O
54.06 O
30.27 O
15.67 O
neither O
20.22 O
79.78 O
28.27 O
71.73 O
67.11 O
25.95 O
6.94 O
Davidson O
et O
al.hate O
speech O
84.13 O
15.87 O
3.35 O
96.65 O
63.22 O
24.61 O
12.17 O
offensive O
language O
86.89 O
13.11 O
7.45 O
92.55 O
71.89 O
20.39 O
7.72 O
neither O
30.63 O
69.37 O
20.44 O
79.56 O
63.29 O
5.48 O
31.23 O
Founta O
et O
al.hateful O
78.11 O
21.89 O
5.92 O
94.08 O
52.35 O
27.37 O
20.28 O
abusive O
97.34 O
2.66 O
26.04 O
73.96 O
75.56 O
10.42 O
14.02 O
normal O
9.23 O
90.77 O
24.82 O
75.18 O
60.24 O
35.84 O
3.92 O
spam O
8.72 O
91.28 O
43.59 O
56.41 O
50.98 O
1.71 O
47.31 O
Zampieri O
et O
al O
. O
( O
actual O
annotation O
fraction O
) O
32.91 O
67.09 O
11.88 O
88.12 O
61.31 O
28.17 O
10.52 O
Table O
6 O
: O
Results O
of O
using O
a O
BERT B-MethodName
model O
trained O
on O
OLID O
to O
tag O
other O
the O
datasets O
, O
for O
each O
OffensEval O
subtask O
. O

7 O
Discussion O
and O
Conclusion O
The O
paper O
makes O
two O
major O
contributions O
: O
First O
, O
an O
evaluation O
of O
the O
general O
effectiveness O
of O
BERT B-MethodName
in O
Abusive O
Language O
Classiﬁcation O
tasks O
and O
its O
ability O
to O
obtain O
results O
comparable O
to O
— O
or O
better O
than O
— O
the O
state O
- O
of O
- O
the O
- O
art O
by O
only O
ﬁne O
- O
tuning O
. O

Also O
, O
there O
are O
very O
few O
datasets O
that O
provide O
a O
large O
number O
of O
samples O
that O
can O
be O
taken O
advantage O
of O
by O
huge O
neural B-MethodName
networks I-MethodName
( O
Lee O
et O
al O
. O
, O
2018 O
) O
. O

•Word2Vec B-MethodName
: O
the O
sentence O
represented O
by O
taking O
the O
average B-MetricName
of O
every O
non O
- O
stopword O
word O
vector O
in O
the O
sentence O
. O

Word O
Vector O
Models O
: O
Word2Vec B-MethodName
and O
Word2VecAF B-MethodName
Both O
single O
layer O
networks O
. O

Word2Vec B-MethodName
takes O
as O
input O
the O
sentence O
represented O
as O
an O
averaged B-MetricName
word O
vector O
of O
100 O
numbers.4 O
Word2VecAF B-MethodName
takes O
the O
sentence O
average B-MetricName
vector O
, O
abstract O
average B-MetricName
vector O
and O
handcrafted O
features O
, O
giving O
a O
208 B-MetricValue
- O
dimensional O
vector O
for O
classiﬁcation O
. O

LSTM B-MethodName
- O
RNN O
Method O
: O
SNet O
Takes O
as O
input O
the O
ordered O
words O
of O
the O
sentence O
represented O
as O
100dimensional O
vectors O
and O
feeds O
them O
through O
a O
bi O
- O
directional O
RNN O
with O
Long O
- O
Short O
Term O
Memory O
( O
LSTM B-MethodName
, O
Hochreiter O
and O
Schmidhuber O
( O
1997 O
) O
) O
cells O
, O
with O
128 O
hidden B-HyperparameterName
units I-HyperparameterName
and O
dropout B-HyperparameterName
to O
prevent O
overﬁtting O
. O

The O
outputs O
of O
the O
LSTM B-MethodName
and O
features O
hidden B-HyperparameterName
layer I-HyperparameterName
are O
then O
concatenated O
and O
projected O
into O
two O
classes O
. O

The O
LSTM B-MethodName
encoding O
on O
its O
own O
outperforms O
models O
based O
on O
averaged B-MetricName
word O
embeddings O
by O
6.7 O
% O
accuracy B-MetricName
and O
2.1 B-MetricValue
ROUGE B-MetricName
points O
. O

State O
of O
the O
art O
in O
SRL B-TaskName
is O
held O
by O
the O
systems O
based O
on O
deep O
neural B-MethodName
networks I-MethodName
( O
Marcheggiani O
and O
Titov O
, O
2017 O
; O
He O
et O
al O
. O
, O
2017 O
) O
. O

Thisissue O
is O
aggravated O
by O
the O
fact O
that O
deep O
neural B-MethodName
networks I-MethodName
require O
signiﬁcant O
amounts O
of O
training O
data O
, O
and O
SRL B-TaskName
annotations O
are O
expensive O
to O
produce O
. O

More O
recently O
, O
low O
- O
dimensional O
word O
representations O
learned O
from O
text O
corpora O
using O
neural B-MethodName
networks I-MethodName
( O
i.e. O
, O
word O
embeddings O
) O
have O
emerged O
( O
Mikolov O
et O
al O
. O
, O
2013a O
; O
Pennington O
et O
al O
. O
, O
2014 O
; O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
stemming B-TaskName
from O
cognitive O
frameworks O
based O
on O
distributed O
representation O
( O
Hinton O
et O
al O
. O
, O
1986 O
; O
Feldman O
and O
Ballard,1982 O
) O
. O

To O
do O
so O
, O
standard O
word O
embedding O
models O
such O
as O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
, O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
or O
FastText O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
are O
often O
used O
. O

For O
word O
embedding O
models O
, O
we O
use O
both O
the O
CBOW B-MethodName
and O
Skip O
- O
Gram O
variants O
of O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
. O

We O
use O
standard O
hyperparameters O
for O
both O
CBOW B-MethodName
and O
Skip O
- O
Gram O
, O
with O
300dimensions O
and O
a O
window O
size O
of O
10 O
in O
both O
cases O
. O

Given O
the O
difference O
in O
speed O
( O
CBOW B-MethodName
being O
around O
ﬁve O
times O
faster O
to O
train O
) O
and O
the O
small O
performance O
difference O
, O
we O
considered O
CBOW B-MethodName
for O
all O
our O
main O
experiments O
, O
but O
included O
Skip O
- O
Gram O
results O
in O
the O
appendix O
. O

Recall@50 B-MetricName
, O
for O
example O
, O
reports O
the O
percentage O
of O
analogy O
completions O
in O
which O
the O
expected O
word O
was O
among O
the O
50 O
nearest B-MethodName
neighbors I-MethodName
to O
the O
three O
7We O
use O
the O
Wikipedia O
dump O
of O
November O
2016 O
. O

4.2 O
Results O
As O
explained O
in O
the O
previous O
section O
, O
our O
experiments O
are O
aimed O
at O
understanding O
the O
role O
of O
co O
- O
occurring O
words O
in O
a O
given O
relation O
type O
( O
e.g. O
9Likewise O
, O
for O
Skip O
- O
Gram O
we O
only O
considered O
the O
Google O
analogy O
dataset O
and O
a O
single O
strategy O
per O
corpus O
( O
results O
in O
the O
appendix O
) O
. O

237,221 O
2,397,131 O
21.3 O
20.0 O
19.8 O
17.1 O
22.1 O
46.2 O
43.3 O
43.6 O
40.4 O
46.9 O
65.1 O
61.5 O
61.6 O
59.1 O
65.4 O
auto O
- O
binary O
1,648,965 O
42,023,189 O
27.6 O
26.2 O
27.6 O
28.6 O
35.1 O
59.2 O
56.1 O
56.7 O
57.4 O
67.2 O
71.1 O
69.9 O
70.0 O
71.8 O
80.6 O
A O
VERAGE O
133,660 O
3,033,547 O
18.3 O
16.2 O
16.4 O
13.3 O
16.0 O
43.8 O
41.1 O
41.4 O
33.6 O
39.5 O
60.0 O
57.2 O
57.3 O
46.3 O
54.6 O
Table O
2 O
: O
UMBC O
corpus O
word O
analogy O
results O
using O
CBOW B-MethodName
with O
ﬁve O
different O
conﬁgurations O
: O
Default O
( O
Def O
) O
, O
Remove O
( O
Rm O
) O
, O
Replace O
( O
Rp O
) O
and O
their O
more O
aggressive O
counterparts O
removing O
all O
pairwise O
co O
- O
occurrences O
( O
Rm+ O
and O
Rp+ O
) O
. O

Table O
2 O
shows O
the O
main O
set O
of O
results O
of O
the O
CBOW B-MethodName
model O
on O
the O
UMBC O
corpus O
. O

We O
also O
compare O
, O
through O
principal B-MethodName
component I-MethodName
analysis I-MethodName
( O
PCA B-MethodName
) O
visualization O
, O
the O
structure O
of O
the O
relation O
with O
the O
highest O
point O
decrease O
, O
before O
and O
after O
co O
- O
occurrence O
removal O
( O
Section O
5.2 O
) O
. O

Following O
the O
previous O
example O
, O
the O
relative O
pairwise O
frequency O
Hpairof O
the O
instance O
composed O
of O
king O
- O
queen O
( O
5,498 O
joint O
co O
- O
occurrences O
in O
UMBC O
) O
and O
man O
- O
woman O
( O
36,189 O
) O
is O
0.068 O
. O
ally O
more O
robust O
to O
outliers O
( O
e.g. O
, O
a O
highly O
frequent O
word O
) O
than O
the O
usual O
arithmetic B-MetricName
mean B-MetricName
. O
Googlecapital O
- O
country O
162,671 O
521,038 O
61.9 B-MetricValue
54.6 O
55.5 O
93.3 O
89.5 O
90.3 O
97.6 O
94.9 O
96.1 O
capital O
- O
world O
320,106 O
1,102,291 O
66.8 O
48.1 O
51.4 O
92.9 O
80.6 O
82.1 O
97.2 O
90.3 O
91.1 O
currency O
4,327 O
223,588 O
31.4 O
28.5 O
25.3 O
60.9 O
58.2 O
51.3 O
77.9 O
78.4 O
71.0 O
city O
- O
in O
- O
state O
344,196 O
659,539 O
19.7 O
12.6 O
15.2 O
54.0 O
34.8 O
39.4 O
72.3 O
55.7 O
58.5 O
family O
375,904 O
1,012,935 O
78.1 O
73.3 O
72.3 O
94.3 O
92.9 O
89.7 O
95.7 O
94.5 O
93.9 O
nationality O
- O
adjective O
437,316 O
1,381,200 O
95.9 O
94.9 O
95.5 O
98.5 O
98.6 O
97.6 O
99.8 O
99.7 O
98.9 O
A O
VERAGE O
274,087 O
816,765 O
59.0 O
52.0 O
52.5 O
82.3 O
75.8 O
75.1 O
90.1 O
85.6 O
84.9BATScountry O
- O
capital O
267,095 O
885,924 O
83.8 O
79.5 O
78.7 O
91.1 O
90.0 O
88.9 O
93.6 O
91.8 O
91.3 O
country O
- O
language O
82,842 O
527,339 O
28.6 O
25.3 O
23.9 O
62.0 O
58.5 O
58.9 O
72.7 O
70.9 O
71.2 O
UKcity O
- O
county O
39,780 O
198,005 O
17.0 O
9.1 O
9.7 O
43.5 O
34.0 O
34.3 O
59.0 O
52.3 O
51.8 O
name O
- O
nationality O
31,980 O
390,864 O
31.4 O
29.5 O
30.7 O
56.0 O
53.9 O
56.5 O
67.8 O
65.2 O
69.1 O
name O
- O
occupation O
24,798 O
136,447 O
39.9 O
35.9 O
34.7 O
63.0 O
59.5 O
58.3 O
72.1 O
68.8 O
67.8 O
animal O
- O
young O
7,021 O
135,815 O
5.4 O
2.4 O
2.4 O
20.0 O
12.5 O
10.5 O
33.1 O
24.4 O
22.3 O
animal O
- O
sound O
2,921 O
121,416 O
5.3 O
2.8 O
1.7 O
15.2 O
10.5 O
7.6 O
24.2 O
18.6 O
14.5 O
animal O
- O
shelter O
23,832 O
262,708 O
2.0 O
1.3 O
0.9 O
8.7 O
5.4 O
4.6 O
17.5 O
12.1 O
10.7 O
things O
- O
color O
47,298 O
445,312 O
14.5 O
15.0 O
15.2 O
39.9 O
38.6 O
42.8 O
55.6 O
52.7 O
56.2 O
male O
- O
female O
400,035 O
1,387,664 O
51.8 O
47.4 O
46.5 O
77.6 O
73.7 O
71.6 O
84.5 O
82.4 O
80.7 O
hypernyms O
- O
animals O
11,723 O
77,632 O
20.1 O
16.8 O
14.4 O
57.6 O
52.7 O
47.4 O
75.4 O
71.0 O
66.6 O
hypernyms O
- O
misc O
16,825 O
164,097 O
7.4 O
6.7 O
6.2 O
33.1 O
29.9 O
30.3 O
55.8 O
53.0 O
51.9 O
hyponyms O
- O
misc O
90,811 O
594,541 O
12.4 O
12.4 O
10.9 O
52.2 O
51.2 O
49.4 O
72.7 O
70.6 O
69.8 O
meronyms O
- O
substance O
70,896 O
510,291 O
7.8 O
6.7 O
6.9 O
28.9 O
24.2 O
24.9 O
48.7 O
43.4 O
41.4 O
meronyms O
- O
member O
774,473 O
4,259,786 O
10.4 O
9.1 O
12.0 O
33.3 O
28.2 O
35.6 O
49.9 O
44.7 O
53.0 O
meronyms O
- O
part O
91,041 O
539,371 O
6.7 O
6.1 O
5.5 O
34.0 O
31.5 O
31.7 O
56.5 O
52.0 O
51.4 O
synonyms O
- O
intensity O
44,149 O
624,662 O
15.6 O
14.9 O
14.4 O
42.9 O
40.4 O
40.7 O
61.3 O
60.7 O
58.7 O
synonyms O
- O
exact O
83,651 O
1,308,183 O
23.2 O
20.7 O
25.4 O
52.6 O
50.0 O
54.4 O
69.1 O
67.1 O
70.2 O
antonyms O
- O
gradable O
282,330 O
1,233,454 O
21.6 O
18.5 O
22.3 O
49.0 O
46.3 O
49.6 O
67.1 O
64.6 O
66.5 O
antonyms O
- O
binary O
1,726,724 O
21,329,952 O
29.4 O
26.1 O
44.6 O
57.7 O
55.4 O
77.2 O
72.2 O
70.7 O
85.6 O
A O
VERAGE O
206,011 O
1,756,673 O
21.7 O
19.3 O
20.3 O
45.9 O
42.3 O
43.8 O
60.4 O
56.8 O
57.5 O
Table O
3 O
: O
Wikipedia O
corpus O
word O
analogy O
results O
using O
CBOW B-MethodName
with O
three O
different O
conﬁgurations O
: O
Default O
( O
Def O
) O
, O
Remove O
( O
Rm O
) O
, O
and O
its O
more O
aggressive O
setting O
removing O
all O
pairwise O
co O
- O
occurrences O
( O
Rm+ O
) O
With O
respect O
to O
performance O
drops O
, O
for O
each O
analogy O
completion O
instance O
we O
considered O
the O
ranks11 O
of O
the O
correct O
completion O
words O
in O
both O
the O
default O
and O
replace O
settings O
and O
computed O
the O
difference O
. O

6 O
Discussion O
6.1 O
The O
source O
of O
semantic O
regularities O
In O
this O
paper O
, O
we O
ﬁnd O
that O
neural O
word O
embeddings O
( O
i.e. O
, O
Word2Vec B-MethodName
models O
) O
do O
not O
require O
observation O
of O
instances O
of O
the O
relation O
( O
e.g. O
, O
Madrid O
is O
thecapital O
of O
Spain O
) O
in O
order O
to O
maintain O
nominal O
accuracy B-MetricName
in O
relation O
completion O
tasks O
. O

In O
Mikolov O
et O
al O
. O
( O
2013b O
) O
, O
where O
Word2Vec B-MethodName
models O
were O
ﬁrst O
introduced O
, O
the O
phrase O
” O
Linguistic O
Regularities O
” O
was O
used O
. O

Mostword O
embedding O
models O
( O
including O
the O
Skip O
- O
Gram O
model O
of O
Word2Vec B-MethodName
) O
are O
trained O
in O
pairwise O
fashion O
after O
- O
all O
, O
making O
predictions O
and O
calculating O
loss O
based O
on O
each O
pair O
of O
input O
and O
context O
words O
. O

In O
order O
to O
complement O
this O
analysis O
, O
for O
future O
work O
it O
would O
be O
interesting O
to O
analyze O
to O
what O
extent O
the O
conclusions O
of O
this O
analysis O
apply O
to O
purely O
distributional O
models O
, O
e.g. O
, O
PMI O
- O
based O
, O
as O
they O
have O
shown O
to O
share O
similarity O
properties O
with O
word O
embeddings O
( O
Levy O
et O
al O
. O
, O
2015 O
) O
, O
to O
the O
point O
of O
SkipGram B-MethodName
being O
viewed O
as O
an O
implicit O
co O
- O
occurrence O
matrix O
factorization O
( O
Levy O
and O
Goldberg O
, O
2014b).Moreover O
, O
the O
analysis O
could O
be O
extended O
to O
other O
types O
of O
relations O
, O
not O
only O
semantic O
. O

Among O
these O
works O
, O
many O
are O
landmark O
results O
that O
have O
revolutionized O
the O
ﬁeld O
of O
natural O
language O
processing O
, O
including O
Glove O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
BERT B-MethodName
embeddings O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
conditional O
random O
The O
last O
four O
authors O
have O
contributed O
equally O
. O

cites=17103810098319730115ﬁelds O
( O
Sutton O
and O
McCallum O
, O
2012 O
) O
, O
and O
bidirectional O
LSTM B-MethodName
models O
( O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

We O
also O
trained O
two O
other O
17 O
- O
model O
ensembles O
ourselves O
by O
applying O
Gaussian O
random O
projections O
to O
the O
BERT B-MethodName
embeddings O
space O
. O

0.650.700.750.800.850.900.95F1 B-MetricName
Score32 O
64 O
128 O
256 O
768 O
Number O
of O
Dimensions0.600.650.700.750.800.850.900.95F1 B-MetricName
Score O
( O
b O
) O
BERT B-MethodName
- O
based O
models O
. O

0.650.700.750.800.850.900.95F1 B-MetricName
Score32 O
64 O
128 O
256 O
768 O
Number O
of O
Dimensions0.600.650.700.750.800.850.900.95F1 B-MetricName
Score O
( O
c O
) O
BERT B-MethodName
+ O
cross O
- O
validation O
. O

We O
used O
a O
BERT B-MethodName
embeddings O
layer O
from O
the O
transformers O
open O
source O
library O
( O
Wolf O
et O
al O
. O
, O
2019 O
) O
, O
tuned O
on O
the O
CoNLL-2003 O
corpus O
, O
to O
produce O
BERT B-MethodName
embeddings O
over O
sliding O
windows O
of O
text O
from O
the O
train O
fold O
. O

We O
trained O
multinomial O
logistic B-MethodName
regression I-MethodName
classiﬁers O
over O
these O
random O
projections O
. O

Based O
on O
published O
results O
on O
BERT B-MethodName
embeddings O
for O
NER B-TaskName
, O
we O
expect O
that O
additional O
tuning O
would O
have O
raised O
the O
F1 B-MetricName
scores B-MetricName
of O
our O
models O
by O
about O
0.02 B-MetricValue
. O

This O
process O
involved O
training O
170 O
different O
models O
, O
but O
because O
we O
only O
needed O
to O
generate O
the O
BERT B-MethodName
embeddings O
once O
, O
we O
were O
able O
to O
perform O
all O
training O
in O
a O
few O
hours O
on O
a O
4 O
- O
year O
- O
old O
MacBook O
. O

The O
raw O
outputs O
of O
the O
two O
BERT B-MethodName
- O
based O
ensembles O
showed O
a O
high O
degreeEnsemble O
1 O
Ensemble O
2 O
Fold(s O
) O
Before O
Review O
After O
First O
Review O
After O
Second O
Review O
Original O
models O
Custom O
models O
dev O
/test O
0.5153 O
0.2500 O
0.2533 O
Original O
models O
Custom O
+ O
Cross O
- O
val O
. O

The O
original O
models O
ﬂagged O
a O
substantially O
different O
set O
of O
labels O
from O
our O
BERT B-MethodName
- O
based O
custom O
models O
, O
and O
this O
divergence O
increased O
after O
manual O
review O
. O

The O
original O
models O
ﬂagged O
a O
very O
different O
set O
of O
labels O
from O
the O
BERT B-MethodName
based O
models O
, O
especially O
after O
manual O
review O
. O

Our O
BERT B-MethodName
- O
based O
models O
found O
a O
higher O
fraction O
of O
Sentence O
type O
errors O
, O
largely O
because O
these O
models O
were O
able O
to O
express O
spans O
that O
cross O
sentence O
boundaries O
. O

We O
also O
suspect O
that O
many O
of O
these O
older O
models O
operated O
on O
one O
sentence O
at O
a O
time O
, O
while O
the O
document O
context O
feeding O
our O
BERT B-MethodName
embeddings O
could O
span O
multiple O
sentences O
. O

Had O
we O
been O
aiming O
to O
maximize O
the O
F1 B-MetricName
scores B-MetricName
of O
our O
BERT B-MethodName
- O
based O
models O
, O
we O
would O
have O
postprocessed O
the O
outputs O
of O
these O
models O
to O
split O
spans O
along O
sentence O
boundaries O
. O

Words O
are O
routinely O
represented O
by O
word O
embeddings O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
most O
commonly O
as O
ﬁxed O
- O
dimensional O
real O
- O
valued O
vectors O
, O
such O
as O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
fastText O
( O
Mikolov O
et O
al O
. O
, O
2018 O
) O
. O

On O
the O
other O
end O
of O
the O
spectrum O
, O
document O
embeddings O
are O
built O
with O
computationally O
extremely O
heavy O
deep O
learning O
models O
such O
as O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
ULMFiT O
( O
Howard O
and O
Ruder O
, O
2018 O
) O
, O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

We O
start O
by O
evaluating O
document O
sim1For O
example O
BERT B-MethodName
takes O
0.5 O
secs O
to O
process O
a O
sentence O
on O
a O
CPU O
( O
Nvidia O
blog O
, O
our O
experiments O
) O
, O
and O
getting O
good O
document O
representations O
may O
require O
ﬁne-tuning.ilarity O
as O
pairwise O
similarities O
between O
words O
and O
show O
that O
this O
induces O
a O
compact O
approximative O
representation O
for O
the O
documents O
themselves O
. O

A O
completely O
different O
approach O
is O
taken O
by O
the O
deep O
learning O
community O
, O
with O
the O
use O
of O
universal O
language O
and O
transformer O
models O
such O
as O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
ULMFiT O
( O
Howard O
and O
Ruder O
, O
2018 O
) O
, O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

For O
English O
we O
use O
fastText O
( O
Mikolov O
et O
al O
. O
, O
2018 O
) O
embeddings O
with O
d= O
300 O
, O
but O
note O
that O
similar O
results O
were O
achieved O
also O
with O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
. O

Johnson O
et O
al O
. O
( O
2017 O
) O
extracted O
a O
set O
of O
features O
from O
the O
MIMIC O
- O
III O
database O
for O
ICU O
mortality O
prediction O
and O
compared O
several O
stateof O
- O
the O
- O
art O
models O
against O
gradient B-MethodName
boosting I-MethodName
and O
logistic B-MethodName
regression I-MethodName
. O

Harutyunyan O
et O
al O
. O
( O
2017 O
) O
benchmarked O
their O
performance O
on O
four O
clinical O
prediction O
tasks O
on O
the O
MIMIC O
- O
III O
database O
using O
multitask O
recurrent O
neural B-MethodName
networks I-MethodName
. O

However O
, O
the O
authors O
used O
a O
logistic B-MethodName
regression I-MethodName
model O
to O
predict O
the O
mortality O
rate O
of O
the O
patients O
and O
did O
not O
evaluate O
their O
performance O
with O
the O
recent O
works O
and O
deep O
neural O
architectures O
. O

Furthermore O
, O
we O
benchmark O
the O
performance O
of O
our O
proposed O
topic O
models O
using O
two O
neural O
architectures O
, O
including O
Multi O
- O
Layer O
Perceptron O
( O
MLP O
) O
and O
Attention O
- O
based O
Long O
Short O
Term O
Memory O
( O
A O
- O
LSTM B-MethodName
) O
. O

3.1 O
Deep O
Neural O
Architectures O
We O
used O
two O
deep O
neural O
architectures O
, O
Multilayer O
Perceptron O
( O
MLP O
) O
and O
Attention O
- O
based O
LSTM B-MethodName
( O
A O
- O
LSTM B-MethodName
) O
, O
for O
the O
multi O
- O
label O
ICD-9 O
code O
group O
prediction O
task O
. O

3.1.2 O
Attention O
- O
based O
LSTM B-MethodName
The O
LSTM B-MethodName
effectively O
captures O
the O
long O
- O
term O
dependencies O
and O
overcomes O
the O
gradient O
vanishing O
problem O
which O
is O
crucial O
in O
the O
accurate O
risk O
stratiﬁcation O
using O
unstructured O
nursing O
notes O
. O

LSTMs B-MethodName
introduce O
an O
adaptive O
gating O
mechanism O
to O
determine O
the O
extent O
to O
which O
the O
LSTM B-MethodName
memory O
units O
must O
retain O
the O
previous O
state O
( O
ct 1 O
) O
and O
memorize O
the O
features O
in O
the O
current O
state O
( O
ct O
) O
. O

Typically O
, O
four O
gates O
composite O
an O
LSTM B-MethodName
network O
including O
the O
input O
gate O
i O
, O
the O
forget O
gate O
f O
, O
the O
output O
gate O
o O
, O
and O
the O
candidate O
value O
gfor O
the O
cell O
state O
. O

The O
precise O
form O
of O
an O
LSTM B-MethodName
update O
at O
a O
layer O
land O
time O
steptis O
computed O
as O
: O
0 O
BB@i O
f O
o O
g1 O
CCA=0 O
BB@sigm O
sigm O
sigm O
tanh1 O
CCAW(l O
) O
 O
h(l O
) O
t 1 O
h(l 1 O
) O
t O
! O

Let O
Hbe O
the O
matrix O
of O
output O
vectors O
[ O
h1;h2;:::;h O
T]produced O
from O
LSTM B-MethodName
. O

This O
study O
utilizes O
an O
attention O
- O
based O
LSTM B-MethodName
with O
dimension O
size O
of O
289for O
the O
embedding O
( O
17time O
steps O
) O
and O
300for O
the O
LSTM B-MethodName
hidden O
state O
. O

The O
multi O
- O
label O
classiﬁcation O
is O
facilitated O
using O
a O
sigmoid O
activation B-HyperparameterName
of O
the O
ﬁnal O
A O
- O
LSTM B-MethodName
output O
. O

Five O
standard O
evaluation O
metrics O
including O
Accuracy O
( O
ACC O
) O
, O
F1 B-MetricName
score B-MetricName
, O
MCC O
score B-MetricName
, O
Area O
Under O
the O
Precision B-MetricName
- O
Recall B-MetricName
Curve O
( O
AUPRC O
) O
, O
and O
Area O
Under O
the O
ROC O
Curve O
( O
AUROC O
) O
were O
employed O
to O
evaluate O
the O
performance O
of O
the O
proposed O
coherence O
- O
based O
modeling O
approaches O
, O
classiﬁed O
using O
MLP O
and O
A O
- O
LSTM B-MethodName
. O

The O
null O
hypothesis O
in O
Wilcoxon O
signed O
- O
rank O
test O
is O
that O
the O
two O
treatments O
are O
drawn O
from O
the O
same O
distribution O
, O
Data O
Model O
ClassiﬁerPerformance O
score B-MetricName
ACC O
F1 B-MetricName
MCC O
AUPRC O
AUROC O
C O
- O
LDA O
( O
140 B-MetricValue
; O
792100)MLP O
0:79540:0003 O
0:71750:0008 O
0:57430:0006 O
0:66920:0006 O
0:78570:0004 O
A O
- O
LSTM B-MethodName
0:79320:0002 O
0:71860:0002 O
0:57120:0007 O
0:66600:0007 O
0:78540:0013 O
C O
- O
NMF O
( O
140 O
; O
792100)MLP O
0:78260:0004 O
0:70110:0008 O
0:54800:0007 O
0:65300:0013 O
0:77350:0006 O
A O
- O
LSTM B-MethodName
0:78110:0005 O
0:69900:0040 O
0:54490:0007 O
0:65100:0009 O
0:77150:0026 O
LDA O
( O
140 O
; O
792100)MLP O
0:79500:0003 O
0:71680:0020 O
0:57350:0012 O
0:66850:0013 O
0:78480:0011 O
A O
- O
LSTM B-MethodName
0:79300:0007 O
0:71530:0034 O
0:57010:0022 O
0:66550:0013 O
0:78330:0020 O
NMF O
( O
140 O
; O
792100)MLP O
0:78290:0006 O
0:70290:0016 O
0:54980:0009 O
0:65300:0017 O
0:77440:0007 O
A O
- O
LSTM B-MethodName
0:78150:0008 O
0:69350:0052 O
0:54510:0024 O
0:65350:0014 O
0:76890:0031 O
Table O
1 O
: O
Experimental O
results O
for O
ICD-9 O
code O
group O
prediction O
using O
MLP O
and O
A O
- O
LSTM.Data B-MethodName
Model O
ClassiﬁerACC O
F1 B-MetricName
MCC O
AUPRC O
AUROC O
p O
z O
p O
z O
p O
z O
p O
z O
p O
z O
C O
- O
LDA O
( O
140 O
; O
792100)MLP O
  O
0:005 2:803  O
  O
  O
A O
- O
LSTM B-MethodName
0:005 2:803  O
0:005 2:803 O
0:005 2:803 O
0:009 2:599 O
C O
- O
NMF O
( O
140 O
; O
792100)MLP O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
A O
- O
LSTM B-MethodName
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
LDA O
( O
140 O
; O
792100)MLP O
0:005 2:803 O
0:009 2:599 O
0:007 2:701 O
0:016 2:395 O
0:009 2:599 O
A O
- O
LSTM B-MethodName
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
NMF O
( O
140 O
; O
792100)MLP O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
A O
- O
LSTM B-MethodName
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
0:005 2:803 O
Table O
2 O
: O
A O
paired O
samples O
Wilcoxon O
signed O
- O
rank O
test O
( O
two O
- O
tailed O
, O
p<0:05 O
) O
for O
the O
proposed O
model O
with O
the O
best O
performance O
against O
other O
modeling O
strategies O
. O

Data O
( O
size O
) O
ELMo B-MethodName
USE O
GloVe B-MethodName
WTMF O
WIM O
( O
20 O
) O
0.3873 O
-0.0290 O
0.7149 O
0.8525 O
TAC O
09 O
( O
54 O
) O
0.0515 O
0.2672 O
0.1713 O
0.4961 O
STS-14 O
( O
3750 O
) O
0.1636 O
0.5757 O
0.6129 O
0.7257 O
Table O
1 O
: O
Comparison O
of O
phrase O
embedding O
methods O
on O
correlation B-MetricName
to O
manual O
pyramd O
( O
WIM O
, O
TAC09 O
) O
or O
correlation B-MetricName
to O
human O
similarity O
judgements O
( O
STS-14 O
) O
. O

Here O
we O
report O
on O
a O
comparison O
of O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
the O
Universal O
Sentence O
Encoder O
for O
English O
( O
USE O
) O
( O
Cer O
et O
al O
. O
, O
2018 O
) O
with O
two O
conventional O
word O
embedding O
methods O
, O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
WTMF O
( O
Guo O
and O
Diab O
, O
2012).4 O
ELMo B-MethodName
is O
character O
- O
based O
rather O
than O
wordbased O
, O
relies O
on O
a O
many O
- O
layered O
bidirectional O
LSTM B-MethodName
, O
and O
incorporates O
word O
sequence O
( O
language O
model O
) O
information O
. O

To O
create O
meaning B-MetricName
vectors O
for O
strings O
of O
words O
, O
we O
use O
pretrained O
ELMo B-MethodName
vectors O
, O
taking O
the O
weighted B-HyperparameterName
sum O
of O
3 O
output O
layers O
as O
the O
word O
embeddings O
, O
then O
applying O
mean B-MetricName
pooling.5USE O
is O
intended O
for O
transfer O
learning O
tasks O
, O
based O
on O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 B-MetricValue
) O
or O
the O
( O
Iyyer O
et O
al O
. O
, O
2015 O
) O
deep O
averaging O
network O
( O
DAN O
) O
. O

We O
create O
meaning B-MetricName
vectors O
for O
word O
strings O
with O
the O
USE O
- O
DAN O
pretrained O
encoder.6We O
use O
the O
GloVe B-MethodName
download O
for O
100D O
vec4We O
do O
not O
show O
results O
for O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
where O
performance O
was O
similar O
to O
GloVe B-MethodName
. O

5We O
use O
ELMo B-MethodName
module O
from O
https://github O
. O

tors O
trained O
on O
the O
840B O
Common O
Crawl.7To O
combine O
the O
GloVe B-MethodName
word O
vectors O
into O
a O
phrase O
vector O
, O
we O
use O
the O
weighted B-HyperparameterName
averaging O
method O
from O
( O
Arora O
et O
al O
. O
, O
2016 O
) O
. O

The O
lower O
dimensionality O
of O
WTMF O
vectors O
compared O
to O
ELMo B-MethodName
or O
USE O
- O
DAN O
leads O
to O
higher O
maximum O
cosine O
values O
, O
thus O
better O
contrast O
between O
similar O
and O
dissimilar O
pairs O
. O

Experiments O
on O
Chinese O
Penn O
Treebank O
5.1 O
and O
7.0 O
show O
that O
our O
joint O
model O
consistently O
outperforms O
the O
pipeline O
approach O
on O
both O
settings O
of O
without O
and O
with O
BERT B-MethodName
, O
and O
achieves O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

Experiments O
on O
three O
Chinese O
Treebank O
( O
CTB O
) O
benchmark O
datasets O
show O
the O
joint O
framework O
is O
superior O
to O
the O
pipeline O
framework O
on O
both O
settings O
of O
without O
and O
with O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
and O
achieves O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

We O
employ O
three O
BiLSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
layers O
over O
the O
input O
vectors O
for O
context O
encoding O
. O

The O
ﬁnal O
representation O
ofi O
- O
th O
position O
is O
: O
hi O
= O
fibi+1 O
( O
8) O
where O
fiandbiare O
the O
output O
vectors O
of O
the O
toplayer O
forward O
and O
backward O
LSTMs B-MethodName
for O
the O
i O
- O
th O
position O
. O

As O
a O
word O
- O
level O
parser O
, O
its O
input O
is O
composed O
of O
two O
parts O
: O
1 O
) O
word O
embedding O
and O
2 O
) O
CharLSTM B-MethodName
word O
representation O
( O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

e(w O
) O
i O
= O
emb O
( O
wi)CharLSTM B-MethodName
( O
wi)(11 O
) O
Other O
components O
, O
such O
as O
encoding O
and O
span O
scoring O
, O
are O
the O
same O
with O
those O
in O
Section O
3.3 O
. O

In O
our O
joint O
framework O
, O
the O
dimensions O
of O
character O
and O
bichar O
embedding O
are O
100 O
, O
which O
are O
identical O
to O
the O
dimensions O
of O
word O
embedding O
and O
output O
vector O
of O
CharLSTM B-MethodName
in O
the O
pipeline O
framework O
. O

For O
experiments O
with O
BERT B-MethodName
, O
we O
adopt O
“ O
bertbase O
- O
chinese O
” O
to O
get O
contextual O
char O
/ O
word O
representations O
. O

Both O
pipeline O
and O
joint O
frameworks O
use O
3 O
BiLSTM B-MethodName
layers O
with O
the O
hidden B-HyperparameterName
size I-HyperparameterName
of O
400 B-HyperparameterValue
as O
their O
encoder O
. O

In O
this O
part O
, O
we O
compare O
pipeline O
framework O
and O
joint O
framework O
on O
the O
settings O
of O
without O
and O
with O
BERT B-MethodName
. O

Without O
using O
BERT B-MethodName
representations O
, O
our O
joint O
framework O
outperforms O
the O
pipeline O
framework O
by O
a O
large O
margin O
. O

After O
incorporating O
BERT B-MethodName
representations O
into O
two O
frameworks O
, O
our O
joint O
framework O
still O
achieves O
better O
performance O
except O
the O
word O
segmentation O
task O
on O
CTB5 O
. O

We O
owe O
it O
to O
the O
overﬁtting O
phenomenon O
of O
complicated O
model O
since O
CTB5 O
is O
relatively O
small O
. O
CTB5 O
CTB5 O
- O
big O
CTB7 O
Seg O
Tag O
Par O
Seg O
Tag O
Par O
Seg O
Tag O
Par O
Pipeline O
98.05 O
95.17 O
87.47 O
95.64 O
91.62 O
83.15 O
96.07 O
92.50 O
85.19 O
Joint O
98.41y95.64y87.57 O
96.18z92.72z84.28z96.55z93.30z85.89z O
Pipeline O
w/ O
BERT B-MethodName
98.72 O
96.51 O
91.39 O
97.36 O
94.73 O
89.39 O
97.27 O
94.70 O
89.76 O
Joint O
w/ O
BERT B-MethodName
98.59 O
96.53 O
91.51 O
97.39 O
94.97y89.48 O
97.49z95.11z90.14z O
Table O
5 O
: O
Results O
( O
F O
- O
score B-MetricName
) O
of O
pipeline O
and O
joint O
frameworks O
on O
different O
test O
datasets O
. O

Compared O
with O
their O
work O
, O
we O
simply O
use O
left O
binarization O
to O
decide O
the O
intra O
- O
word O
structures O
, O
and O
our O
model O
achieves O
much O
higher O
performance O
by O
adopting O
the O
state O
- O
of O
- O
the O
- O
art O
BiLSTM B-MethodName
- O
based O
parsing O
model O
. O

Kurita O
et O
al O
. O
( O
2017 O
) O
for O
the O
ﬁrst O
time O
apply O
neural B-MethodName
networks I-MethodName
to O
jont O
WS O
- O
POS B-TaskName
- O
DepPAR O
. O

They O
enhance O
the O
transition O
- O
based O
parser O
with O
char O
/ O
word O
embeddings O
and O
BiLSTM B-MethodName
which O
alleviate O
the O
efforts O
of O
feature O
engineering O
. O

Our O
model O
consists O
of O
a O
bidirectional O
LSTM B-MethodName
( O
BiLSTM B-MethodName
) O
based O
language O
model O
that O
is O
pre O
- O
trained O
to O
predict O
words O
in O
plain O
text O
, O
and O
a O
multi O
- O
layer O
perceptron O
( O
MLP O
) O
decision O
model O
that O
uses O
features O
from O
the O
language O
model O
to O
predict O
the O
correct O
actions O
for O
an O
ArcHybrid O
transition O
based O
parser O
. O

In O
this O
work O
we O
derive O
features O
for O
the O
parser O
from O
a O
bidirectional O
LSTM B-MethodName
language O
model O
trained O
with O
pre O
- O
tokenized O
text O
to O
predict O
words O
in O
a O
sentence O
using O
both O
the O
left O
and O
the O
right O
context O
. O

In O
this O
study O
, O
we O
derive O
context O
embeddings O
from O
the O
hidden O
states O
of O
the O
forward O
and O
backward O
LSTMs B-MethodName
of O
the O
language O
model O
that O
are O
generated O
while O
predicting O
a O
word O
. O

In O
( O
Alberti O
et O
al O
. O
, O
2017 O
) O
, O
a O
character O
- O
level O
LSTM B-MethodName
reads O
each O
word O
character O
by O
character O
and O
the O
last O
hidden O
state O
creates O
a O
word O
representation O
. O

The O
word O
representation O
is O
used O
as O
input O
to O
a O
wordlevel O
LSTM B-MethodName
whose O
hidden O
states O
constitute O
the O
lookahead O
representation O
of O
each O
word O
. O

Finally O
, O
the O
lookahead O
representation O
is O
used O
by O
a O
tagger O
LSTM B-MethodName
trained O
to O
predict O
POS B-TaskName
tags O
. O

Kuncoro O
et O
al O
. O
( O
2016 O
) O
use O
an O
LSTM B-MethodName
as O
the O
decision O
module O
instead O
, O
carrying O
internal O
state O
between O
actions O
. O

Dyer O
et O
al O
. O
( O
2015 O
) O
introduce O
stack O
- O
LSTMs B-MethodName
, O
which O
have O
the O
ability O
to O
recover O
earlier O
hidden O
states O
. O

They O
construct O
the O
parser O
state O
using O
three O
stack O
- O
LSTMs B-MethodName
, O
representing O
the O
buffer O
, O
the O
stack O
, O
and O
the O
action O
history O
. O

Kiperwasser O
and O
Goldberg O
( O
2016 O
) O
train O
a O
BiLSTM B-MethodName
whose O
input O
is O
word O
and O
POS B-TaskName
embeddings O
and O
whose O
hidden O
states O
are O
fed O
to O
an O
MLP O
that O
decides O
parsing O
actions O
. O

Word O
embeddings O
are O
generated O
by O
the O
character O
LSTM B-MethodName
. O

Our O
language O
models O
consist O
of O
two O
parts O
: O
a O
character O
based O
unidirectional O
LSTM B-MethodName
to O
produce O
word O
embeddings O
, O
and O
a O
word O
based O
bidirectional O
LSTM B-MethodName
to O
predict O
words O
and O
produce O
context O
embeddings O
. O

Next O
, O
the O
character O
based O
LSTM B-MethodName
reads O
each O
word O
left O
to O
right O
and O
the O
ﬁnal O
hidden B-HyperparameterName
layer I-HyperparameterName
is O
used O
as O
theword O
embedding O
. O

Next O
, O
those O
word O
embeddings O
become O
inputs O
to O
the O
BiLSTM B-MethodName
, O
which O
tries O
to O
predict O
each O
word O
based O
on O
its O
left O
and O
right O
contexts O
. O

Acontext O
embedding O
for O
a O
word O
is O
created O
by O
concatenating O
the O
hidden O
vectors O
of O
the O
forward O
and O
backward O
LSTMs B-MethodName
used O
in O
predicting O
that O
word O
. O

The O
unidirectional O
character O
LSTM B-MethodName
produces O
the O
word O
embeddings O
( O
shown O
for O
the O
word O
” O
Economic O
” O
in O
the O
Figure O
) O
which O
are O
fed O
as O
input O
to O
the O
bidirectional O
word O
LSTM B-MethodName
. O

The O
bidirectional O
LSTM B-MethodName
predicts O
a O
given O
word O
using O
the O
adjacent O
forward O
and O
backward O
hidden O
states O
at O
that O
position O
( O
e.g. O
the O
word O
“ O
news O
” O
is O
predicted O
using O
“ O
hf2 O
” O
and O
“ O
hb8 O
” O
) O
. O

The O
parser O
uses O
both O
word O
embeddings O
produced O
by O
the O
character O
LSTM B-MethodName
( O
350 O
dimensions O
) O
and O
the O
context O
embeddings O
produced O
by O
the O
word O
LSTM B-MethodName
( O
300 O
+ O
300 O
dimensions O
) O
as O
features O
. O

We O
did O
not O
ﬁne O
- O
tune O
the O
LM O
weights B-HyperparameterName
during O
parser O
training O
. O
The O
character O
and O
word O
LSTMs B-MethodName
were O
trained O
end O
- O
to O
- O
end O
using O
backpropagation O
through O
time O
( O
Werbos O
, O
1990 O
) O
using O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
with O
default O
parameters O
and O
with O
gradients O
clipped O
at O
5.0 O
. O

Whether O
our O
pre O
- O
trained O
language O
model O
and O
context O
embeddings O
would O
boost O
the O
scores B-MetricName
of O
more O
sophisticated O
approaches O
( O
e.g. O
stack O
- O
LSTMs B-MethodName
or O
global O
normalization O
) O
is O
an O
open O
question O
. O

We O
can O
make O
some O
observations O
consistent O
across O
all O
three O
languages O
based O
on O
the O
results O
in O
Table O
4 O
: O
•Word O
vectors O
from O
our O
BiLSTM B-MethodName
language O
model O
perform O
slightly O
better O
than O
Facebook O
vectors O
( O
p O
- O
v O
vs O
p O
- O
fb O
) O
. O

This O
is O
an O
expected O
result O
, O
as O
neural B-MethodName
networks I-MethodName
are O
known O
to O
require O
a O
huge O
amount O
of O
training O
instances O
. O

For O
instance O
, O
Gupta O
et O
al O
. O
( O
2015 O
) O
found O
that O
features O
of O
countries O
, O
such O
as O
their O
GDP O
, O
fertility O
rate O
or O
even O
level O
of O
CO O
2emissions O
, O
can O
be O
predicted O
from O
word O
embeddings O
using O
a O
linear B-MethodName
regression I-MethodName
model O
. O

Along O
similar O
lines O
, O
in O
this O
paper O
we O
will O
use O
shallow O
decision B-MethodName
trees I-MethodName
to O
evaluate O
the O
quality O
of O
our O
feature O
directions O
. O

Our O
method O
also O
somewhat O
relates O
to O
the O
debates O
in O
cognitive O
science O
on O
the O
relationship O
between O
similarity O
and O
rule O
based O
processes O
( O
Hahn O
and O
Chater O
, O
1998 O
) O
, O
in O
the O
sense O
that O
it O
allows O
us O
to O
explicitly O
link O
similarity O
based O
categorization O
methods O
( O
e.g. O
an O
SVM B-MethodName
classiﬁer O
trained O
on O
semantic O
space O
representations O
) O
with O
rule O
based O
categorization O
methods O
( O
e.g. O
the O
decision B-MethodName
trees I-MethodName
that O
we O
will O
learn O
from O
the O
feature O
directions O
) O
. O

Then O
, O
for O
each O
considered O
word O
w O
, O
a O
logistic B-MethodName
regression I-MethodName
classiﬁer20 O
Newsgroups O
: O
Accuracy O
Scored O
Movie O
Reviews O
: O
NDCG O
Scored O
Place O
- O
types O
: O
Kappa O
Scored O
fsins O
, O
sinful O
, O
jesus O
, O
moses O
g O
f O
environmentalist O
, O
wildlife O
, O
ecological O
g O
f O
smile O
, O
kid O
, O
young O
, O
female O
g O
fhitters O
, O
catcher O
, O
pitching O
, O
batting O
g O
f O
prophets O
, O
bibles O
, O
scriptures O
g O
f O
rust O
, O
rusty O
, O
broken O
, O
mill O
g O
fink O
, O
printers O
, O
printer O
, O
matrix O
g O
f O
assassinating O
, O
assasins O
, O
assasin O
g O
f O
eerie O
, O
spooky O
, O
haunted O
, O
ghosts O
g O
fjupiter O
, O
telescope O
, O
spacecraft O
, O
satellites O
g O
freanimated O
, O
undead O
, O
zombiﬁed O
g O
f O
religious O
, O
christian O
, O
chapel O
, O
carved O
g O
fﬁrearm O
, O
concealed O
, O
handgun O
, O
handguns O
g O
fufos O
, O
ufo O
, O
extraterrestrial O
, O
extraterrestrials O
g O
ffur O
, O
tongue O
, O
teeth O
, O
ears O
g O
fescaped O
, O
terror O
, O
wounded O
, O
ﬂed O
g O
f O
swordsman O
, O
feudal O
, O
swordﬁght O
, O
swordplay O
g O
fweeds O
, O
shed O
, O
dirt O
, O
gravel O
g O
fcellular O
, O
phones O
, O
phone O
g O
f O
scuba O
, O
divers O
, O
undersea O
g O
f O
stonework O
, O
archway O
, O
brickwork O
g O
fbrake O
, O
steering O
, O
tires O
, O
brakes O
g O
f O
regiment O
, O
armys O
, O
soliders O
, O
infantry O
g O
f O
rails O
, O
rail O
, O
tracks O
, O
railroad O
g O
friders O
, O
rider O
, O
ride O
, O
riding O
g O
f O
toons O
, O
animations O
, O
animating O
, O
animators O
g O
f O
dirty O
, O
trash O
, O
grunge O
, O
grafﬁti O
g O
fformats O
, O
jpeg O
, O
gif O
, O
tiff O
g O
f O
fundamentalists O
, O
doctrine O
, O
extremists O
g O
f O
tranquility O
, O
majestic O
, O
picturesque O
g O
fphysicians O
, O
treatments O
physician O
g O
f O
semitic O
, O
semitism O
, O
judaism O
, O
auschwitz O
g O
f O
monument O
, O
site O
, O
arch O
, O
cemetery O
g O
fbacteria O
, O
toxic O
, O
biology O
, O
tissue O
g O
f O
shipwrecked O
, O
ashore O
, O
shipwreck O
g O
f O
journey O
, O
traveling O
, O
travelling O
g O
fplanets O
, O
solar O
, O
mars O
, O
planetary O
g O
f O
planetary O
, O
earths O
, O
asteroid O
, O
spaceships O
g O
f O
mother O
, O
mom O
, O
children O
, O
child O
g O
fsymptoms O
, O
syndrome O
, O
diagnosis O
g O
f O
atheism O
, O
theological O
, O
atheists O
, O
agnostic O
g O
f O
frost O
, O
snowy O
, O
icy O
, O
freezing O
g O
funiversities O
, O
nonproﬁt O
, O
institution O
g O
f O
astronaut O
, O
nasa O
, O
spaceship O
, O
astronauts O
g O
f O
colourful O
, O
vivid O
, O
artistic O
, O
vibrant O
g O
Table O
1 O
: O
The O
ﬁrst O
clustered O
words O
of O
features O
for O
three O
different O
domains O
and O
three O
different O
scoring O
types O
. O

For O
example O
, O
we O
can O
use O
the O
classiﬁcation O
accuracy B-MetricName
to O
evaluate O
the O
quality O
in O
terms O
of O
the O
corresponding O
logistic B-MethodName
regression I-MethodName
classiﬁer O
: O
if O
this O
classiﬁer O
is O
sufﬁciently O
accurate O
, O
it O
must O
mean B-MetricName
that O
whether O
word O
wrelates O
to O
object O
o(i.e O
. O
whether O
it O
is O
used O
in O
the O
description O
of O
o O
) O
is O
important O
enough O
to O
affect O
the O
semantic O
space O
representation O
of O
o. O
In O
such O
a O
case O
, O
it O
seems O
reasonable O
to O
assume O
that O
wdescribes O
an O
important O
feature O
for O
the O
given O
domain O
. O

Note O
that O
by O
using O
a O
linear O
activation B-HyperparameterName
in O
the O
output O
layer O
, O
we O
can O
interpret O
the O
rows O
of O
the O
matrix O
Das O
theKfeature O
directions O
, O
with O
the O
components O
of O
the O
vector O
gi= O
( O
g1 O
i;:::;gK O
i)being O
the O
corresponding O
dot O
products.20 O
Newsgroups O
F1 B-MetricName
D1 O
F1 B-MetricName
D3 O
F1 B-MetricName
DN O
FT O
MDS O
0.50 B-MetricValue
0.47 O
0.44 O
MDS O
0.44 O
0.42 O
0.43 O
FT O
PCA B-MethodName
0.40 O
0.36 O
0.34 O
PCA B-MethodName
0.25 O
0.27 O
0.36 O
FT O
Doc2Vec O
0.44 O
0.42 O
0.41 O
Doc2Vec O
0.29 O
0.34 O
0.44 O
FT O
AWV O
0.47 O
0.45 O
0.40 O
AWV O
0.41 O
0.38 O
0.43 O
FT O
AWVw O
0.41 O
0.41 O
0.43 O
AWVw O
0.38 O
0.40 O
0.43 O
LDA O
0.40 O
0.37 O
0.35 O
Table O
3 O
: O
Results O
for O
20 B-DatasetName
Newsgroups I-DatasetName
. O

In O
particular O
, O
we O
learn O
decision B-MethodName
trees I-MethodName
which O
are O
limited O
to O
depth O
1 O
and O
3 O
, O
which O
use O
the O
rankings O
induced O
by O
the O
feature O
directions O
as O
input O
. O

Depth O
3 O
decision B-MethodName
trees I-MethodName
are O
able O
to O
model O
categories O
that O
can O
be O
characterized O
using O
at O
most O
three O
feature O
directions O
. O

IMDB O
Sentiment O
D1 O
D3 O
DN O
FT O
PCA B-MethodName
0.78 O
0.80 O
0.79 O
PCA B-MethodName
0.76 O
0.82 O
0.80 O
FT O
AWV O
0.72 O
0.76 O
0.71 O
AWV O
0.74 O
0.76 O
0.71 O
LDA O
0.79 O
0.80 O
0.79 O
Table O
5 O
: O
Results O
for O
IMDB O
Sentiment O
. O

We O
also O
consider O
PCA B-MethodName
, O
which O
directly O
uses O
the O
PPMI O
weighted B-HyperparameterName
BoW O
vectors O
as O
input O
, O
and O
which O
avoids O
the O
quadratic O
complexity O
of O
the O
MDS O
method O
. O

Finally O
, O
we O
also O
learn O
semantic O
spaces O
by O
averaging O
word O
vectors O
, O
using O
a O
pre O
- O
trained O
GloVe B-MethodName
word O
embeddings O
trained O
on O
the O
Wikipedia O
2014 O
+ O
Gigaword O
5 O
corpus6 O
. O

We O
used O
the O
logistic B-MethodName
regression I-MethodName
implementation O
from O
scikit O
- O
learn O
to O
ﬁnd O
the O
directions O
. O

As O
the O
performance O
of O
LDA O
can O
be O
sensitive O
to O
the O
number O
of O
topics O
and O
other O
parameters O
, O
we O
tuned O
the O
number O
of O
topics O
from O
f50;100;200;400 O
g O
, O
the O
topic O
word O
prior O
from O
f0:1;0:01;0:001gand O
the O
document O
topic O
prior O
f0:1;0:01;0:001g O
. O
To O
learn O
the O
decision B-MethodName
trees I-MethodName
, O
we O
use O
the O
scikitlearn O
implementation O
of O
CART O
, O
which O
allows O
us O
to O
limit O
the O
depth O
of O
the O
trees O
. O

Unbounded O
decision B-MethodName
trees I-MethodName
are O
far O
less O
sensitive O
to O
the O
quality O
of O
the O
directions O
, O
and O
can O
even O
perform O
reasonably O
on O
random O
directions O
. O

For O
example O
, O
the O
decision B-MethodName
tree I-MethodName
for O
the O
genre O
‘ O
Horror O
’ O
could O
use O
the O
feature O
direction O
forfgore;gory;horror;gruesomeg O
. O

This O
was O
shown O
in O
terms O
of O
an O
improved O
performance O
of O
low O
- O
depth O
decision B-MethodName
trees I-MethodName
in O
natural O
categorization O
tasks O
. O

We O
employ O
this O
method O
to O
the O
Generative O
Pre O
- O
trained O
Transformer O
( O
OpenAI B-MethodName
GPT B-MethodName
) O
( O
Radford O
et O
al O
. O
, O
2018 O
) O
by O
adding O
a O
weighted B-HyperparameterName
linear O
layer O
after O
the O
embedding O
layer O
. O

Compared O
with O
the O
original O
results O
in O
GPT B-MethodName
, O
our O
approach O
gains O
1.58 O
% O
absolute O
increase O
on O
Stories O
Cloze O
, O
6.4 O
% O
on O
RTE B-MetricName
, O
0.69 B-MetricValue
% O
on O
SciTail O
and O
0.8 O
% O
on O
SST-2 B-DatasetName
. O

We O
evaluate O
our O
method O
on O
a O
set O
of O
NLU O
tasks O
by O
applying O
it O
to O
GPT B-MethodName
. O

Recently O
, O
a O
novel O
language O
representation O
model O
called O
ERNIE B-MethodName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
has O
been O
proposed O
. O

ERNIE B-MethodName
introduces O
knowledge O
- O
related O
tasks O
in O
the O
pre O
- O
training O
process O
. O

There O
are O
many O
kinds O
of O
neural B-MethodName
networks I-MethodName
that O
can O
deal O
with O
NLU O
tasks O
. O

GPT B-MethodName
( O
Rad O
- O
ford O
et O
al O
. O
, O
2018 O
) O
applied O
BPE O
to O
its O
training O
process O
and O
got O
impressive O
achievements O
in O
a O
series O
of O
NLU O
tasks O
. O

3 O
Methods O
In O
this O
section O
, O
we O
introduce O
our O
reverse O
mapping O
bytepair O
encoding O
method O
and O
the O
procedure O
of O
applying O
it O
to O
GPT B-MethodName
. O

We O
employ O
them O
to O
the O
ﬁne O
- O
tuning O
process O
of O
GPT B-MethodName
by O
adding O
a O
weighted B-HyperparameterName
layer O
after O
the O
embedding O
layer O
. O

3.3 O
Training O
Process O
GPT B-MethodName
is O
followed O
by O
many O
studies O
for O
its O
excellent O
generalization O
performance O
and O
we O
will O
give O
a O
brief O
introduction O
to O
it O
in O
the O
ﬁrst O
subsubsection O
. O

Due O
to O
the O
expensive O
cost O
of O
pre O
- O
training O
, O
we O
reuse O
the O
OpenAI B-MethodName
pre O
- O
trained O
language O
model4parameters O
, O
and O
we O
train O
new O
embeddings O
and O
ﬁnetune O
all O
parameters O
during O
the O
ﬁne O
- O
tuning O
process O
. O

Generative O
Pre O
- O
trained O
Transformer O
The O
Generative O
Pre O
- O
trained O
Transformer O
( O
Radford O
et O
al O
. O
, O
2018 O
) O
, O
also O
known O
as O
OpenAI B-MethodName
GPT B-MethodName
or O
called O
as O
GPT B-MethodName
, O
is O
a O
language O
representation O
model O
which O
3https://spacy.io/api/entityrecognizer O
4https://github.com/openai/ﬁnetune-transformer-lmStart O
 O
Premise O
 O
Delim O
 O
Extract O
 O
Hypothesis O
Start O
 O
Premise O
 O
Delim O
 O
Extract O
 O
Hypothesis O
Start O
 O
Premise O
 O
Delim O
 O
Extract O
 O
Hypothesis O
Start O
 O
Premise O
 O
Delim O
 O
Extract O
 O
HypothesisWordpiece O
Channel O
NER B-TaskName
Channel O
POS B-TaskName
Channel[START O
] O
Oil O
prices O
fall O
back O
as O
Yukos O
oil O
threat O
lifted O
. O

Unless O
otherwise O
stated O
, O
the O
GPT B-MethodName
mentioned O
in O
this O
paper O
refers O
to O
a O
12layer O
decoder O
- O
only O
transformer O
with O
masked O
selfattention O
heads O
( O
768 O
dimensional O
states O
and O
12 O
attention O
heads O
) O
. O

Other O
hyperparameters O
are O
set O
as O
well O
as O
the O
GPT B-MethodName
paper O
( O
Radford O
et O
al O
. O
, O
2018 O
) O
for O
comparison O
. O

The O
following O
data O
ﬂow O
is O
the O
same O
as O
GPT B-MethodName
( O
Liu O
et O
al O
. O
, O
2018 O
) O
. O

We O
use O
the O
following O
objective O
to O
minimize O
: O
L3(C O
) O
= O
L2(C)+λL1(C O
) O
( O
3 O
) O
whereCrepresents O
the O
labeled O
dataset O
, O
L2(C O
) O
is O
the O
cross O
entropy B-MetricName
loss O
of O
classiﬁcation O
got O
on O
C O
, O
L1(C)is O
the O
cross O
entropy B-MetricName
loss O
of O
the O
language O
model O
got O
on O
Candλis O
a O
weight B-HyperparameterName
parameter O
between O
0 B-MetricValue
and O
1.Multi O
- O
channel O
Separate O
Transformer O
GPT B-MethodName
uses O
a O
multi O
- O
layer O
transformer O
decoder O
as O
the O
language O
model O
, O
and O
it O
learns O
patterns O
in O
the O
language O
by O
simply O
observing O
the O
token O
- O
level O
sequences O
. O

One O
is O
the O
12 O
- O
layer O
pre O
- O
trained O
transformer O
released O
by O
OpenAI B-MethodName
GPT B-MethodName
, O
and O
the O
other O
is O
an O
entirely O
new O
one O
which O
has O
different O
amount O
of O
layers O
with O
the O
former O
and O
takes O
responsibility O
for O
linguistic O
features O
. O

4 O
Experiments O
and O
Analysis O
4.1 O
Setup O
We O
follow O
the O
model O
hyperparameters O
mentioned O
in O
the O
GPT B-MethodName
( O
Liu O
et O
al O
. O
, O
2018 O
) O
paper O
. O

RTE B-MetricName
and O
SciTail O
RTE B-MetricName
( O
Bentivogli O
et O
al O
. O
, O
2009 B-MetricValue
) O
and O
SciTail O
( O
Khot O
et O
al O
. O
, O
2018 O
) O
are O
language O
in O
- O
Model O
Story O
Cloze(Acc% O
) O
RTE(Acc% B-MetricName
) O
SciTail(Acc% O
) O
SST-2(Acc% B-DatasetName
) O
jose.fonollosa O
’s O
model O
87.60 O
- O
- O
BiLSTM+ELMo+Attn B-MethodName
- O
58.9 O
- O
90.4 O
BigBird B-MethodName
- O
- O
93.84 O
GPT B-MethodName
86.5 O
56.0 O
88.3 O
91.3 O
NER B-TaskName
87.39 O
61.6 O
88.90 O
91.2 O
NER+POS B-TaskName
88.08 O
62.1 O
88.33 O
91.1 O
NER+DEP B-TaskName
87.55 O
62.0 O
88.99 O
90.2 O
NER+POS+DEP B-TaskName
87.07 O
60.4 O
87.54 O
92.1 O
POS B-TaskName
84.07 O
62.4 O
88.15 O
92.1 O
POS+DEP B-TaskName
86.05 O
61.5 O
87.63 O
91.7 O
DEP O
86.48 O
62.0 O
87.82 O
91.3 O
NPB O
- O
BPE O
87.76 O
58.8 O
88.43 O
91.1 O
NPB O
- O
BPE∗87.39 O
62.3 O
87.16 O
91.9 O
The O
details O
of O
the O
models O
for O
comparison O
in O
this O
table O
can O
be O
found O
in O
the O
leaderboards O
of O
the O
corresponding O
dataset O
ofﬁcial O
websites O
. O

Compared O
with O
GPT B-MethodName
, O
we O
achieved O
an O
absolute O
increase O
of O
1.58 O
% O
on O
Story O
Cloze O
Test O
, O
6.4 O
% O
on O
RTE B-MetricName
, O
0.69 B-MetricValue
% O
on O
SciTail O
and O
0.8 O
% O
on O
SST2 O
at O
our O
best O
. O
Evaluation O
of O
Named O
- O
entity O
Phrase O
Based O
Bytepair O
Encoding O
We O
evaluate O
NPB O
- O
BPE O
with O
the O
parameter O
- O
sharing O
training O
procedure O
. O

Evaluation O
of O
Different O
Training O
Processes O
The O
parameters O
in O
GPT B-MethodName
are O
all O
the O
same O
for O
all O
wordpiece O
embeddings O
in O
the O
vocabulary O
. O

The O
red O
dotted O
line O
in O
each O
subgraph O
represents O
the O
best O
result O
of O
LB O
- O
BPE+GPT B-MethodName
got O
on O
that O
dataset O
. O

Green O
arrows O
and O
boxes O
represent O
the O
weight B-HyperparameterName
changes O
compared O
with O
GPT B-MethodName
. O

4.3 O
Case O
Study O
We O
compare O
the O
attention O
weights B-HyperparameterName
of O
GPT B-MethodName
and O
our O
LB O
- O
BPE O
( O
NER+POS B-TaskName
) O
approach O
in O
a O
textual O
intailment O
example O
as O
shown O
in O
Figure O
6 O
. O

GPT B-MethodName
labels O
it O
asnotentailment O
while O
our O
approach O
labels O
it O
as O
entailment O
which O
is O
the O
right O
answer O
. O

By O
applying O
them O
to O
the O
ﬁne O
- O
tuning O
process O
of O
GPT B-MethodName
, O
we O
gain O
about O
16 O
% O
improvement O
on O
downsteam O
NLU O
tasks O
. O

Language O
models O
have O
manyforms O
, O
we O
only O
test O
our O
approach O
on O
GPT B-MethodName
, O
a O
follow O
up O
direction O
is O
ﬁnding O
if O
it O
is O
generic O
enough O
. O

Investigating O
Entity O
Knowledge O
in O
BERT B-MethodName
with O
Simple O
Neural O
End O
- O
To O
- O
End O
. O

In O
this O
study O
we O
investigate O
the O
following O
questions O
: O
( O
a O
) O
Can O
all O
those O
steps O
be O
learned O
jointly O
with O
a O
model O
for O
contextualized O
text O
- O
representations O
, O
i.e. O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
? O

( O
b O
) O
How O
much O
entity O
knowledge O
is O
already O
contained O
in O
pretrained O
BERT B-MethodName
? O

( O
c O
) O
Does O
additional O
entity O
knowledge O
improve O
BERT B-MethodName
’s O
performance O
in O
downstream B-TaskName
tasks I-TaskName
? O

We O
show O
on O
an O
entity O
linking O
benchmark O
that O
( O
i O
) O
this O
model O
improves O
the O
entity O
representations O
over O
plain O
BERT B-MethodName
, O
( O
ii O
) O
that O
it O
outperforms O
entity O
linking O
architectures O
that O
optimize O
the O
tasks O
separately O
and O
( O
iii O
) O
that O
it O
only O
comes O
second O
to O
the O
current O
state O
- O
of O
- O
the O
- O
art O
that O
does O
mention O
detection O
and O
entity O
disambiguation O
jointly O
. O

BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
is O
a O
deep O
selfattention O
- O
based O
architecture O
which O
is O
pretrained O
on O
large O
amounts O
of O
data O
with O
a O
language O
modelling O
objective O
. O

Since O
its O
appearance O
, O
BERT B-MethodName
is O
being O
analyzed O
and O
applied O
in O
various O
domains O
( O
Beltagy O
et O
al O
. O
, O
2019 O
; O
Lee O
et O
al O
. O
, O
2019 O
) O
. O

A O
recent O
study O
found O
that O
BERT B-MethodName
automatically O
learns O
the O
NLP O
pipeline O
( O
Tenney O
et O
al O
. O
, O
2019),i.e O
. O
a O
stack O
of O
increasingly O
higher O
level O
linguistic O
functions O
. O

Zhang O
et O
al O
. O
( O
2019 O
) O
investigated O
injecting O
entity O
knowledge O
from O
noisy1automatic O
entity O
linking O
into O
the O
pretraining O
of O
BERT B-MethodName
and O
they O
could O
show O
that O
this O
improves O
relation O
extraction O
. O

In O
this O
study O
we O
investigate O
the O
following O
questions O
: O
( O
a O
) O
Can O
BERT B-MethodName
’s O
architecture O
learn O
all O
entity O
linking O
steps O
jointly O
? O

We O
ﬁrst O
trained O
BERT B-MethodName
- O
base O
- O
uncased O
on O
English O
Wikipedia O
( O
dubbed O
BERT+Entity B-MethodName
) O
and O
then O
ﬁne O
- O
tuned O
and O
evaluated O
it O
on O
an O
entity O
linking O
benchmark O
. O

( O
b O
) O
How O
much O
entity O
knowledge O
is O
already O
contained O
in O
pretrained O
BERT B-MethodName
? O

To O
investigate O
this O
question O
, O
we O
froze O
BERT B-MethodName
and O
only O
trained O
the O
entity O
classiﬁer O
of O
BERT+Entity B-MethodName
on O
Wikipedia O
( O
dubbed O
Frozen O
- O
BERT+Entity B-MethodName
) O
, O
i.e. O
the O
resulting O
entity O
classiﬁer O
is O
adjusted O
for O
entity O
mentions O
for O
which O
plain O
BERT B-MethodName
already O
does O
assign O
distinct O
token O
representations O
, O
such O
that O
correct O
entity O
classiﬁcation O
is O
possible O
. O

Then O
we O
ﬁne O
- O
tuned O
and O
evaluated O
Frozen O
- O
BERT+Entity B-MethodName
on O
an O
entity O
linking O
benchmark O
. O

We O
ﬁnd O
that O
the O
performance O
of O
FrozenBERT+Entity B-MethodName
is O
6 O
% O
below O
BERT+Entity B-MethodName
, O
showing O
that O
BERT+Entity B-MethodName
has O
learned O
additional O
entity O
knowledge O
. O

( O
c O
) O
Does O
additional O
entity O
knowledge O
improve O
BERT B-MethodName
’s O
performance O
in O
downstream B-TaskName
tasks I-TaskName
? O

Due O
to O
training O
BERT+Entity B-MethodName
with O
a O
per O
token O
classiﬁcation O
, O
the O
model O
is O
forced O
to O
assign O
distinct O
entity O
speciﬁc O
features O
to O
each O
token O
of O
an O
entity O
mention O
. O

We O
evaluated O
BERT+Entity B-MethodName
in O
the O
natural O
1TagMe O
’s O
performance O
on O
various O
benchmark O
datasets O
ranges O
from O
37 O
% O
to O
72 O
% O
. O

BERT+Entity B-MethodName
predicts O
entity O
links O
per O
token O
, O
where O
” O
O O
” O
denotes O
a O
Nilprediction O
. O

The O
only O
exception O
is O
the O
RTE B-MetricName
task O
in O
GLUE B-DatasetName
in O
which O
BERT+Entity B-MethodName
improves O
2 B-MetricValue
% O
. O

Pretrained O
Language O
Models O
ULMFIT O
( O
Howard O
and O
Ruder O
, O
2018 O
) O
, O
ELMO O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
are O
modern O
language O
models O
that O
are O
very O
deep O
and O
wide O
( O
for O
NLP O
) O
and O
are O
pretrained O
on O
large O
amounts O
of O
data O
. O

In O
this O
study O
, O
we O
investigate O
the O
factual O
information O
in O
form O
of O
entities O
that O
is O
contained O
in O
BERT B-MethodName
, O
seeking O
to O
understand O
to O
what O
degree O
this O
information O
is O
already O
identiﬁable O
in O
BERT B-MethodName
and O
if O
the O
entity O
knowledge O
can O
be O
improved.3 O
End O
- O
To O
- O
End O
Neural O
Entity O
- O
Linking O
In O
this O
section O
we O
describe O
the O
BERT+Entity B-MethodName
, O
which O
is O
a O
is O
straightforward O
extension O
of O
BERT B-MethodName
, O
however O
, O
as O
with O
the O
original O
BERT B-MethodName
, O
the O
main O
challenge O
lies O
in O
designing O
the O
training O
scheme O
, O
i.e. O
in O
our O
case O
the O
creation O
of O
the O
training O
data O
. O

3.1 O
Model O
Our O
model O
is O
based O
on O
BERT B-MethodName
, O
which O
is O
a O
deep O
self O
- O
attention O
- O
based O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
that O
was O
trained O
on O
large O
amounts O
of O
text O
. O

Devlin O
et O
al O
. O
( O
2019 O
) O
made O
several O
pretrained O
BERT B-MethodName
models O
publicly O
available O
. O

BERT+Entity B-MethodName
is O
a O
straightforward O
extension O
on O
top O
of O
BERT B-MethodName
, O
i.e. O
we O
initialize O
BERT B-MethodName
with O
the O
publicly O
available O
weights B-HyperparameterName
from O
the O
BERT B-MethodName
- O
base O
- O
uncased O
model O
and O
add O
an O
output O
classiﬁcation O
layer O
on O
top O
of O
the O
architecture O
. O

Formally O
, O
let O
dbe O
BERT B-MethodName
’s O
token O
embedding O
size O
, O
andE2RjKBjdthe O
entity O
classiﬁcation O
layer O
, O
withjKBjbeing O
the O
number O
of O
entities O
in O
the O
KB O
, O
Vis O
the O
sub O
- O
word O
vocabulary O
, O
ci O
= O
BERT B-MethodName
( O
h)[i O
] O
is O
thei O
- O
thcontextualized O
token O
computed O
by O
BERT B-MethodName
from O
context O
h= O
[ O
v1;v2;:::;v O
i 1;vi;vi+1;:::;v O
m O
] O
with O
eachv2V. O

BERT B-MethodName
is O
originally O
trained O
with O
sentences O
. O

4 O
Entity O
Linking O
Experiments O
In O
the O
experiments O
we O
want O
to O
investigate O
how O
the O
simple O
neural O
end O
- O
to O
- O
end O
entity O
linking O
model O
BERT+Entity B-MethodName
performs O
, O
i.e. O
if O
it O
learns O
something O
additional O
on O
- O
top O
of O
BERT B-MethodName
. O

The O
jump O
at O
the O
4 O
- O
th O
epoch O
happens O
when O
we O
switch O
from O
training O
FrozenBERT+Entity B-MethodName
to O
BERT+Entity B-MethodName
, O
i.e. O
when O
we O
start O
ﬁnetuning O
BERT B-MethodName
. O

The O
loss O
for O
one O
fragment O
Cin O
batchbwas O
computed O
by O
L=1 O
jNbjjCjjCjX O
ijNbjX O
j [yijlog(^Ejci O
) O
+ O
( O
1 yij)log(1 (^Ejci O
) O
) O
] O
: O
For O
training O
on O
Wikipedia O
we O
used O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
mini O
batch B-HyperparameterName
size I-HyperparameterName
10 B-HyperparameterValue
, O
gradient O
accumulation O
over O
4batches O
, O
maximum O
label O
size O
10240 O
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
for O
BERT B-MethodName
was O
5e-5and O
for O
the O
entity O
classiﬁer O
0:01 O
. O

In O
the O
ﬁrst O
1.5 O
epochs B-HyperparameterName
we O
train O
Frozen O
- O
BERT+Entity B-MethodName
and O
then O
BERT+Entity B-MethodName
. O

In O
the O
ﬁrst O
3 O
epochs B-HyperparameterName
we O
train O
Frozen O
- O
BERT+Entity B-MethodName
and O
then O
BERT+Entity B-MethodName
. O

For O
training O
on O
CoNLL03 O
/ O
AIDA O
we O
used O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
mini O
batch B-HyperparameterName
size I-HyperparameterName
10 B-HyperparameterValue
, O
gradient O
accumulation O
over O
4batches O
, O
maximum O
label O
size O
1024 O
, O
learning B-HyperparameterName
rates I-HyperparameterName
for O
BERT B-MethodName
5e-5 O
, O
dropout B-HyperparameterName
in O
BERT B-MethodName
0:2 O
, O
and O
we O
freeze O
the O
token O
embeddings O
, O
the O
ﬁrst O
two O
layers O
of O
BERT B-MethodName
and O
the O
entity O
classiﬁer O
. O

baseline O
75.7 O
76.0 O
- O
73.3 O
73.9 O
Kolitsas O
et O
al O
. O
( O
2018 O
) O
86.6 O
87.2 O
92.4 O
82.6 O
83.2 O
89.1 O
BERT B-MethodName
63.3 O
66.6 O
67.6 O
49.6 O
52.4 O
52.8 O
Setting O
I O
Frozen O
- O
BERT+Entity B-MethodName
76.8 O
79.6 O
80.6 O
64.7 O
68.0 O
68.6 O
BERT+Entity B-MethodName
82.8 O
84.4 O
86.6 O
74.8 O
76.5 O
78.8 O
Setting O
II O
Frozen O
- O
BERT+Entity B-MethodName
76.5 O
80.1 O
79.6 O
67.8 O
71.9 O
67.8 O
BERT+Entity B-MethodName
86.0 O
87.3 O
92.3 O
79.3 O
81.1 O
87.9 O
Table O
1 O
: O
Comparing O
entity O
linking O
results O
on O
CoNLL03 O
/ O
AIDA O
. O

In O
Frozen O
- O
BERT+Entity B-MethodName
BERT B-MethodName
is O
not O
trained O
and O
only O
the O
entity O
classiﬁer O
on O
- O
top O
is O
trained O
. O

For O
the O
reported O
results O
denoted O
only O
with O
BERT B-MethodName
, O
the O
entity O
classiﬁer O
is O
trained O
from O
scratch O
on O
CoNLL03 O
/ O
AIDA O
and O
BERT B-MethodName
is O
ﬁnetuned O
. O

This O
shows O
the O
lower O
bound O
on O
this O
dataset O
, O
i.e. O
the O
amount O
of O
information O
that O
we O
can O
learn O
with O
BERT B-MethodName
only O
from O
the O
CoNLL03 O
/ O
AIDA O
training O
data O
. O

The O
difference O
between O
BERT B-MethodName
and O
Frozen O
- O
BERT+Entity B-MethodName
shows O
the O
amount O
of O
entity O
knowledge O
that O
plain O
BERT B-MethodName
already O
had O
, O
which O
it O
transferred O
in O
the O
entity O
classiﬁer O
during O
training O
on O
Wikipedia O
. O

Finally O
, O
BERT+Entity B-MethodName
is O
the O
proposed O
model O
, O
in O
which O
both O
BERT B-MethodName
and O
the O
entity O
classiﬁer O
have O
been O
trained O
on O
Wikipedia O
. O

4.3.2 O
Discussion O
Comparing O
BERT+Entity B-MethodName
and O
FrozenBERT+Entity B-MethodName
we O
see O
that O
there O
is O
a O
signiﬁcant O
amount O
of O
entity O
knowledge O
that O
BERT+Entity B-MethodName
learns O
additionally O
to O
Frozen O
- O
BERT+Entity B-MethodName
, O
i.e. O
training O
BERT+Entity B-MethodName
increases O
the O
scores B-MetricName
between O
6%-10 O
% O
depending O
on O
the O
score B-MetricName
and O
dataset O
. O

However O
, O
it O
should O
also O
be O
noted O
that O
Frozen O
- O
BERT+Entity B-MethodName
already O
shows O
an O
increase O
of O
13%-16 O
% O
over O
BERT B-MethodName
, O
thus O
it O
already O
learns O
for O
many O
entities O
distinct O
features O
that O
enable O
theReason O
for O
error O
# O
no O
prediction O
57 O
different O
than O
gold O
annotation O
no O
obvious O
reason O
13 O
semantic O
close O
4 O
lexical O
overlap O
5 O
nested O
entity O
5 O
gold O
annotation O
wrong O
12 O
span O
error O
3 O
unclear O
1 O
100 B-MetricValue
Table O
2 O
: O
Investigating O
the O
types O
of O
strong O
precision B-MetricName
errors O
of O
BERT+Entity B-MethodName
trained O
in O
Setting O
I O
on O
CoNLL03 O
/ O
AIDA O
( O
testa O
) O
on O
100 O
randomly O
sampled O
strong O
precision B-MetricName
errors O
from O
the O
validation O
dataset O
. O

The O
improvement O
of O
Frozen O
- O
BERT+Entity B-MethodName
in O
contrast O
to O
BERT B-MethodName
on O
CoNLL03 O
/ O
AIDA O
shows O
that O
this O
pretraining O
generalizes O
to O
validation O
and O
test O
data O
. O

This O
reduction O
of O
training O
data O
in O
Setting O
II O
— O
caused O
by O
capping O
the O
maximum O
amount O
of O
examples O
per O
entity O
— O
enabled O
us O
to O
run O
more O
epochs B-HyperparameterName
in O
less O
time O
, O
which O
might O
have O
improved O
the O
representations O
of O
less O
frequent O
entities O
. O
Task O
Metric O
BERT B-MethodName
- O
BERT B-MethodName
- O
Ensemble O
BERT+Entity B-MethodName
- O
Ensemble O
CoLA B-DatasetName
Matthew O
’s O
corr O
. O

89.90 O
89.60 O
QQP B-DatasetName
accuracy B-MetricName
91.64 B-MetricValue
91.21 O
MNLI B-MethodName
matched O
acc./mismatched O
acc O
. O

84.96 O
84.78 O
QNLI B-DatasetName
accuracy B-MetricName
91.21 B-MetricValue
91.15 O
RTE B-MetricName
accuracy B-MetricName
71.48 B-MetricValue
73.64 O
WNLI B-DatasetName
accuracy B-MetricName
56.33 B-MetricValue
56.33 O
SQUAD O
V2 O
matched O
/ O
mismatched O
76.89/73.83 O
76.36/73.46 O
SWAG O
accurracy O
80.70 O
80.76 O
WMT14 O
EN O
- O
DE O
BLEU B-MetricName
22.51 B-MetricValue
22.20 O
Table O
3 O
: O
Experiments O
on O
downstream B-TaskName
tasks I-TaskName
with O
BERT+Entity B-MethodName
trained O
in O
Setting O
I. O
The O
ﬁrst O
group O
are O
the O
GLUE B-DatasetName
tasks O
, O
then O
followed O
by O
SQUAD O
V2 O
and O
SWAG O
( O
for O
which O
only O
the O
dev O
set O
results O
are O
reported O
) O
, O
and O
the O
results O
for O
machine O
translation O
WMT14 O
EN O
- O
DE O
. O

When O
we O
compare O
BERT+Entity B-MethodName
with O
the O
two O
results O
from O
Kolitsas O
et O
al O
. O
( O
2018 O
) O
, O
we O
observe O
that O
BERT+Entity B-MethodName
improves O
over O
the O
baseline O
that O
models O
MD O
, O
CG O
and O
ED O
independently O
, O
and O
that O
BERT+Entity B-MethodName
comes O
second O
to O
the O
current O
state O
- O
ofthe O
- O
art O
in O
end O
- O
to O
- O
end O
entity O
linking O
. O

For O
BERT+Entity B-MethodName
, O
however O
, O
the O
drop O
is O
more O
severe O
, O
obviously O
the O
model O
overﬁts O
to O
some O
patterns O
in O
the O
training O
data O
that O
are O
present O
in O
the O
validation O
data O
, O
but O
not O
in O
the O
test O
data O
. O

In O
Table O
2 O
we O
performed O
an O
error O
analysis O
for O
the O
experiments O
for O
Setting O
I O
to O
learn O
what O
kind O
of O
strong O
precision B-MetricName
errors O
are O
responsible O
for O
the O
performance O
of O
BERT+Entity B-MethodName
. O

The O
largest O
source O
of O
errors O
was O
that O
BERT+Entity B-MethodName
did O
predict O
Nilinstead O
of O
an O
entity O
. O

We O
found O
that O
only O
in O
one O
of O
the O
subtasks O
of O
GLUE B-DatasetName
— O
the O
natural O
language O
inference O
tasks O
RTE B-MetricName
— O
BERT+Entity B-MethodName
performs O
better O
than O
BERT B-MethodName
, O
for O
all O
other O
we O
can O
observe O
no O
such O
effect O
. O

5.1 O
Model O
For O
the O
tasks O
GLUE B-DatasetName
, O
SQUAD O
V2 O
and O
SWAG O
we O
extend O
hugginface O
’s O
implementation5and O
concatenate O
the O
outputs O
of O
BERT B-MethodName
and O
BERT+Entity B-MethodName
( O
dubbed O
BERT+Entity B-MethodName
- O
Ensemble O
) O
or O
two O
BERTs B-MethodName
( O
dubbed O
BERT B-MethodName
- O
BERT B-MethodName
- O
Ensemble O
) O
. O

For O
EN O
- O
DE O
WMT14 O
we O
use O
BERT B-MethodName
( O
dubbed O
BERT-2Seq B-MethodName
) O
or O
BERT+Entity B-MethodName
( O
dubbed O
BERT+Entity-2Seq B-MethodName
) O
as O
encoder O
and O
use O
a O
Transformer O
decoder O
by O
adapting O
fairseqs O
Pytorch O
Seq2Seq O
Transformer O
implementation O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
. O

5https://github.com/huggingface/pytorch-pretrainedBERT5.2 B-MethodName
Training O
For O
the O
GLUE B-DatasetName
benchmark O
, O
SQUAD O
and O
SWAG O
we O
train O
the O
BERT+Entity B-MethodName
- O
Ensemble O
and O
BERT B-MethodName
- O
BERTEnsemble B-MetricName
for O
3 O
epochs B-HyperparameterName
and O
use O
the O
default O
hyperparameters O
from O
the O
implementation O
. O

The O
models O
BERT-2Seq B-MethodName
and O
BERT+Entity-2Seq B-MethodName
we O
train O
for O
4 O
epochs B-HyperparameterName
, O
with O
Adam O
as O
optimizer B-HyperparameterName
and O
learning B-HyperparameterName
rate5e-5 I-HyperparameterName
, O
max O
1000 B-HyperparameterValue
tokens O
per O
batch O
, O
clip O
gradient O
norm O
0:1 O
, O
dropout B-HyperparameterName
0:2 O
, O
label O
smoothing O
0:1 O
, O
and O
we O
keep O
the O
encoders O
BERT B-MethodName
and O
BERT+Entity B-MethodName
ﬁxed O
for O
the O
ﬁrst O
epoch O
and O
then O
train O
it O
together O
with O
the O
decoder O
. O

6 O
Conclusion O
In O
this O
study O
we O
investigated O
an O
extremely O
simpliﬁed O
approach O
to O
entity O
linking O
that O
worked O
surprisingly O
well O
and O
allowed O
us O
to O
investigate O
entity O
knowledge O
in O
BERT B-MethodName
. O

We O
found O
that O
with O
our O
approach O
we O
can O
learn O
additional O
entity O
knowledge O
in O
BERT B-MethodName
that O
helps O
in O
entity O
linking O
. O

4 O
and O
5 O
using O
GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
a O
pre O
- O
trained O
autoregressive O
Transformer O
language O
model O
, O
which O
allows O
us O
to O
obtain O
more O
accurate O
probability O
estimates O
than O
the O
n O
- O
gram O
models O
used O
in O
previous O
work O
( O
Genzel O
and O
Charniak O
, O
2002 O
, O
2003 O
; O
Doyle O
and O
Frank O
, O
2015a O
, O
b O
; O
Qian O
and O
Jaeger O
, O
2011 O
; O
Xu O
and O
Reitter O
, O
2018 O
) O
and O
to O
include O
discourse O
context O
in O
the O
computation O
. O

We O
rely O
on O
HuggingFace O
’s O
implementation O
of O
GPT-2 B-MethodName
with O
default O
tokenizers O
and O
parameters O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
and O
to O
adapt O
the O
language O
model O
to O
the O
idiosyncrasies O
of O
different O
types O
of O
language O
use O
, O
we O
ﬁnetune O
it O
separately O
on O
a O
70 O
% O
split O
of O
each O
target O
corpus O
. O

7We O
have O
tried O
to O
substitute O
GPT-2 B-MethodName
with O
the O
TransformerXL O
language O
model O
( O
Dai O
et O
al O
. O
, O
2019 O
) O
because O
of O
its O
unlimited O
context O
window O
size O
. O

In O
spite O
of O
its O
larger O
window O
, O
however O
, O
Transformer O
- O
XL O
yields O
higher O
perplexity B-MetricName
than O
GPT-2 B-MethodName
on O
all O
corpora O
, O
hence O
we O
sticked O
to O
GPT-2 B-MethodName
. O

Further O
reasons O
to O
discard O
Transformer O
- O
XL O
are O
discussed O
in O
Appendix O
B.1 O
8The O
length O
of O
the O
control O
contexts O
is O
always O
equal O
to O
the O
number O
of O
tokens O
in O
the O
true O
context O
. O
GPT-2 B-MethodName
pre O
- O
trained O
GPT-2 B-MethodName
ﬁnetuned O
Penn O
Treebank O
28.03 O
21.89 O
PhotoBook O
43.42 O
14.93 O
BNC O
Spoken O
66.47 O
8.69 O
Table O
2 O
: O
Word O
- O
level O
perplexity B-MetricName
of O
the O
GPT-2 B-MethodName
models O
on O
30 B-MetricValue
% O
held O
- O
out O
portions O
of O
the O
corpora O
. O

5 O
Analysis O
of O
Language O
Model O
Estimates O
In O
this O
section O
, O
we O
report O
the O
estimates O
and O
patterns O
of O
sentence O
information O
content O
directly O
computed O
with O
the O
ﬁnetuned O
GPT-2 B-MethodName
language O
models O
for O
our O
three O
corpora O
. O

Kasewa O
et O
al O
. O
( O
2018 O
) O
use O
a O
bidirectional O
LSTM B-MethodName
model O
for O
converting O
correct O
sentences O
into O
wrong O
sentences O
. O

Experimental O
results O
on O
topic O
classiﬁcation O
task O
and O
sentiment B-TaskName
analysis I-TaskName
task O
showed O
that O
the O
fully O
task O
- O
speciﬁc O
multilingual O
model O
obtained O
using O
our O
method O
outperformed O
the O
existing O
multilingual O
models O
with O
embedding O
layers O
ﬁxed O
to O
pre O
- O
trained O
cross O
- O
lingual O
word O
embeddings.1 O
1 O
Introduction O
Deep O
neural B-MethodName
networks I-MethodName
have O
improved O
the O
accuracy B-MetricName
of O
various O
natural O
language O
processing O
( O
NLP O
) O
tasks O
by O
performing O
representation O
learning O
with O
massive O
annotated O
datasets O
. O

These O
methods O
do O
not O
induce O
task O
- O
speciﬁc O
word O
embeddings O
, O
thereby O
failing O
to O
exert O
true O
potential O
of O
neural B-MethodName
networks I-MethodName
, O
as O
we O
conﬁrm O
inx5 O
. O

Our O
method O
assumes O
that O
the O
local O
topology O
among O
nearest B-MethodName
neighbors I-MethodName
will O
be O
consistent O
between O
two O
NLP O
tasks O
( O
here O
, O
language O
modeling O
and O
the O
target O
task O
) O
. O

First O
, O
for O
each O
word O
iin O
the O
target O
language O
, O
we O
2Although O
orthogonal O
mapping O
( O
Xing O
et O
al O
. O
, O
2015 O
) O
is O
reported O
to O
perform O
better O
for O
inducing O
cross O
- O
lingual O
word O
embeddings O
, O
it O
performed O
worse O
for O
our O
purpose O
in O
preliminary O
experiments O
probably O
due O
to O
the O
strong O
constraint.takeknearest O
neighbors O
( O
words O
) O
in O
the O
source O
language O
, O
Ngen O
i O
, O
in O
the O
semantic O
space O
of O
the O
general O
cross O
- O
lingual O
word O
embeddings O
where O
kis O
a O
hyperparameter O
, O
and O
the O
cosine O
similarity O
is O
the O
metric O
. O

To O
address O
this O
issue O
, O
we O
apply O
our O
cross O
- O
task O
projection O
to O
the O
source O
language O
with O
various O
hyperparameter O
k O
; O
namely O
, O
represent O
Xgen O
iconsideringknearest O
neighbors O
Xgen O
j(j6 O
= O
i O
) O
. O

Prior O
to O
reporting O
the O
results O
, O
we O
conﬁrm O
the O
impact O
of O
task O
- O
speciﬁc O
word O
embeddings O
in O
neural B-MethodName
networks I-MethodName
through O
experiments O
in O
a O
monolingual O
setting O
in O
English O
( O
Table O
5 O
) O
. O

We O
will O
further O
discuss O
the O
difference O
in O
the O
topologies O
of O
the O
general O
and O
task O
- O
speciﬁc O
embedding O
spaces O
in O
x5.3 O
by O
looking O
into O
nearest B-MethodName
neighbors I-MethodName
of O
some O
target O
words O
in O
the O
semantic O
space O
of O
general O
and O
task O
- O
speciﬁc O
crosslingual O
word O
embeddings O
( O
Table O
6 O
) O
. O

We O
ﬁrst O
analyze O
the O
task O
- O
speciﬁc O
crosslingual O
word O
embeddings O
through O
nearest B-MethodName
neighbors I-MethodName
of O
some O
words O
. O

We O
next O
investigate O
the O
distribution O
of O
the O
reconstruction O
weights B-HyperparameterName
to O
see O
the O
impact O
ofknearest O
neighbors O
other O
than O
the O
nearest B-MethodName
one I-MethodName
. O

For O
this O
purpose O
, O
we O
present O
nearest B-MethodName
neighbors I-MethodName
of O
frequent O
words O
in O
the O
tasks O
in O
various O
embeddings O
in O
English O
and O
French O
. O

Table O
6a O
shows O
nearest B-MethodName
neighbors I-MethodName
of O
“ O
excellent O
, O
” O
“ O
terrible O
, O
” O
and O
“ O
economic O
” O
in O
the O
general O
word O
embeddings O
, O
and O
the O
embedding O
layer O
of O
the O
models O
optimized O
for O
the O
training O
data O
in O
English O
. O

In O
contrast O
, O
the O
nearest B-MethodName
neighbors I-MethodName
of O
“ O
excellent O
” O
and O
“ O
terrible O
” O
are O
noisy O
since O
they O
do O
not O
contribute O
to O
the O
topic O
classiﬁcation O
task O
. O
The O
embedding O
layers O
optimized O
for O
sentiment B-TaskName
analysis I-TaskName
exhibit O
different O
properties O
. O

While O
the O
nearest B-MethodName
neighbors I-MethodName
of O
“ O
excellent O
” O
and O
“ O
terrible O
” O
are O
not O
semantically O
close O
, O
they O
all O
indicate O
positive O
and O
negative O
polarities O
in O
the O
respective O
domains O
. O

However O
, O
the O
nearest B-MethodName
neighbors I-MethodName
of O
“ O
economic O
” O
are O
noisy O
as O
they O
do O
not O
contribute O
to O
the O
task O
. O

Table O
6b O
shows O
nearest B-MethodName
neighbors I-MethodName
of O
“ O
excellent O
( O
excellent O
) O
, O
” O
“ O
terrible O
( O
terrible O
) O
, O
” O
and O
“ O
´ O
economie O
( O
economy O
) O
” O
in O
French O
; O
the O
general O
word O
embeddings O
( O
General O
) O
and O
the O
task O
- O
speciﬁc O
word O
embeddings O
obtained O
using O
our O
cross O
- O
task O
projection O
( O
LLM O
) O
.General O
embeddings O
exhibit O
similar O
properties O
as O
English O
ones O
. O

LLM O
embeddings O
of O
topic O
classiﬁcation O
task O
have O
“ O
fmi O
( O
IMF O
; O
International O
Monetary O
Fund O
) O
” O
and O
“ O
conjoncture O
( O
conjuncture O
) O
” O
as O
nearest B-MethodName
neighbors I-MethodName
of O
“ O
´ O
economie O
. O
” O
This O
indicates O
that O
our O
crosstask O
projection O
successfully O
obtains O
word O
embeddings O
optimized O
for O
the O
task O
since O
they O
are O
strong O
signals O
of O
the O
Economy O
label O
. O

For O
sentiment B-TaskName
analysis I-TaskName
, O
the O
word O
embeddings O
obtained O
by O
our O
crosstask O
projection O
of O
Amazon O
dataset O
captures O
“ O
extraordinary O
” O
and O
“ O
parfaite O
, O
” O
which O
strongly O
indicate O
positive O
polarity O
, O
as O
the O
nearest B-MethodName
neighbors I-MethodName
of O
“ O
excellent O
” O
In O
contrast O
, O
the O
words O
strongly O
associated O
with O
negative O
polarity O
, O
“ O
d O
´ O
ebile O
” O
and O
“ O
stupide O
, O
” O
are O
the O
nearest B-MethodName
neighbors I-MethodName
of O
“ O
terrible O
” O
in O
the O
embedding O
space O
. O

Distribution O
of O
the O
reconstruction O
weights B-HyperparameterName
To O
see O
how O
much O
the O
nearest B-MethodName
neighbors I-MethodName
for O
the O
target O
words O
contribute O
to O
the O
projection O
, O
we O
investigate O
the O
distribution O
of O
^ O
 O
induced O
by O
Eq O
. O

shows O
the O
distribution O
of O
the O
absolute O
value O
of O
^ O
 O
for O
the O
nearest B-MethodName
neighbors I-MethodName
of O
the O
target O
word O
and O
the O
other O
nearest B-MethodName
neighbors I-MethodName
. O

This O
observation O
indicates O
that O
all O
of O
the O
k O
- O
nearest B-MethodName
neighbors I-MethodName
contribute O
to O
the O
projection O
. O

Speciﬁcally O
, O
we O
learn O
deep O
associations O
between O
the O
sentence O
and O
aspect O
, O
and O
the O
long O
- O
term O
dependencies O
within O
the O
sentence O
by O
leveraging O
the O
pre O
- O
trained O
BERT B-MethodName
model O
. O

With O
the O
rapid O
development O
of O
deep O
learning O
, O
this O
task O
has O
been O
widely O
addressed O
by O
attentionbased O
neural B-MethodName
networks I-MethodName
( O
Wang O
et O
al O
. O
, O
2016 O
; O
Ma O
et O
al O
. O
, O
2017 O
; O
Cheng O
et O
al O
. O
, O
2017 O
; O
Tay O
et O
al O
. O
, O
2018 O
; O
Work O
performed O
while O
interning O
at O
IBM O
Research O
China O
. O

The O
attention O
weights B-HyperparameterName
of O
the O
aspect O
place O
are O
from O
the O
model O
ATAE O
- O
LSTM B-MethodName
( O
Wang O
et O
al O
. O
, O
2016 O
) O
, O
a O
typical O
attention O
mechanism O
used O
for O
soft O
- O
selection O
. O

Take O
Figure O
1 O
as O
an O
example O
, O
for O
the O
aspect O
place O
in O
the O
sentence O
“ O
the O
food O
is O
usually O
good O
but O
it O
certainly O
is O
not O
a O
relaxing O
place O
to O
go O
” O
, O
we O
visualize O
the O
attention O
weights B-HyperparameterName
from O
the O
model O
ATAE O
- O
LSTM B-MethodName
( O
Wang O
et O
al O
. O
, O
2016 O
) O
. O

To O
accurately O
determine O
the O
two O
positions O
of O
the O
opinion O
snippet O
of O
a O
particular O
aspect O
, O
we O
ﬁrst O
model O
the O
deep O
associations O
between O
the O
sentence O
and O
aspect O
, O
and O
the O
long O
- O
term O
dependencies O
within O
the O
sentence O
by O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
which O
is O
a O
pre O
- O
trained O
language O
model O
and O
achieves O
exciting O
results O
in O
many O
natural O
language O
tasks O
. O

Second O
, O
with O
the O
contextual O
representations O
from O
BERT B-MethodName
, O
the O
two O
positions O
are O
sequentially O
determined O
by O
self O
- O
critical O
reinforcement O
learning O
. O

We O
model O
deep O
associations O
between O
the O
sentence O
and O
aspect O
, O
and O
the O
long O
- O
term O
dependencies O
within O
the O
sentence O
by O
BERT B-MethodName
. O

Wang O
et O
al O
. O
( O
2016 O
) O
propose O
attention O
- O
based O
LSTM B-MethodName
networks O
which O
attend O
on O
different O
parts O
of O
the O
sentence O
for O
different O
aspects O
. O

We O
leverage O
BERT B-MethodName
to O
model O
the O
relationships O
between O
sentence O
words O
and O
a O
particular O
aspect O
. O

The O
sentence O
and O
aspect O
are O
packed O
together O
into O
a O
single O
sequence O
and O
fed O
into O
BERT B-MethodName
, O
in O
which O
Erepresents O
the O
input O
embedding O
, O
and O
Tirepresents O
the O
contextual O
representation O
of O
token O
i. O
With O
the O
contextual O
representations O
from O
BERT B-MethodName
, O
the O
start O
and O
end O
positions O
are O
sequentially O
sampled O
and O
then O
the O
framed O
content O
is O
used O
for O
sentiment O
prediction O
. O

In O
this O
paper O
, O
we O
employ O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
to O
model O
the O
deep O
associations O
between O
the O
sentence O
words O
and O
the O
aspect O
. O

BERT B-MethodName
is O
a O
powerful O
pre O
- O
trained O
model O
which O
has O
achieved O
remarkable O
results O
in O
many O
NLP O
tasks O
. O

The O
architecture O
of O
BERT B-MethodName
is O
a O
multi O
- O
layer O
bidirectional O
Transformer O
Encoder O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
which O
uses O
the O
self O
- O
attention O
mechanism O
to O
capture O
complex O
interaction O
and O
dependency O
between O
terms O
within O
a O
sequence O
. O

To O
leverage O
BERT B-MethodName
to O
model O
the O
relationships O
between O
the O
sentence O
and O
the O
aspect O
, O
we O
pack O
the O
sentence O
and O
aspect O
together O
into O
a O
single O
sequence O
and O
then O
feed O
it O
into O
BERT B-MethodName
, O
as O
shown O
in O
Figure O
2 O
. O

3.2 O
Soft O
- O
Selection O
Approach O
To O
fairly O
compare O
the O
performance O
of O
softselection O
approaches O
with O
hard O
- O
selection O
approaches O
, O
we O
use O
the O
same O
word O
- O
aspect O
fusion O
resultsTSfrom O
BERT B-MethodName
. O

L( O
) O
= O
 Rp(l)p(r O
) O
( O
6 O
) O
where O
is O
all O
the O
parameters O
in O
our O
architecture O
, O
which O
includes O
the O
base O
method O
BERT B-MethodName
, O
the O
position O
selection O
parameters O
fs;eg O
, O
and O
the O
parameters O
for O
sentiment O
prediction O
and O
then O
for O
reward O
calculation O
. O

The O
BERT B-MethodName
model O
is O
initialized O
from O
the O
pre O
- O
trained O
model O
, O
other O
parameters O
are O
initialized O
by O
sampling O
from O
normal O
distribution O
N(0;0:02 O
) O
. O

4.3 O
Compared O
Models O
LSTM B-MethodName
: O
it O
uses O
the O
average B-MetricName
of O
all O
hidden O
states O
as O
the O
sentence O
representation O
for O
sentiment O
prediction O
. O

TD O
- O
LSTM B-MethodName
( O
Tang O
et O
al O
. O
, O
2015 O
): O
it O
employs O
two O
LSTMs B-MethodName
and O
both O
of O
their O
outputs O
are O
applied O
to O
predict O
the O
sentiment O
polarity O
. O

2https://github.com/huggingface/pytorch-pretrainedBERTAT-LSTM B-MethodName
( O
Wang O
et O
al O
. O
, O
2016 O
): O
it O
utilizes O
the O
attention O
mechanism O
to O
produce O
an O
aspect O
- O
speciﬁc O
sentence O
representation O
. O

ATAE O
- O
LSTM B-MethodName
( O
Wang O
et O
al O
. O
, O
2016 O
): O
it O
also O
uses O
the O
attention O
mechanism O
. O

The O
difference O
with O
AT O
- O
LSTM B-MethodName
is O
that O
it O
concatenates O
the O
aspect O
embedding O
to O
each O
word O
embedding O
as O
the O
input O
to O
LSTM B-MethodName
. O

AF O
- O
LSTM(CORR B-MethodName
) O
( O
Tay O
et O
al O
. O
, O
2018 O
): O
it O
adopts O
circular O
correlation B-MetricName
to O
capture O
the O
deep O
fusion O
between O
sentence O
words O
and O
the O
aspect O
, O
which O
can O
learn O
rich O
, O
higher O
- O
order O
relationships O
between O
words O
and O
the O
aspect O
. O

AF O
- O
LSTM(CONV B-MethodName
) O
( O
Tay O
et O
al O
. O
, O
2018 O
): O
compared O
with O
AF O
- O
LSTM(CORR B-MethodName
) O
, O
this O
method O
applies O
circular O
convolution O
of O
vectors O
for O
performing O
word O
- O
aspect O
fusion O
to O
learn O
relationships O
between O
sentence O
words O
and O
the O
aspect O
. O

BERT B-MethodName
- O
Original O
: O
it O
makes O
sentiment O
prediction O
by O
directly O
using O
the O
ﬁnal O
hidden O
vector O
Cfrom O
BERT B-MethodName
with O
the O
sentence O
- O
aspect O
pair O
as O
input O
. O

4.4 O
Our O
Models O
BERT B-MethodName
- O
Soft O
: O
as O
described O
in O
Section O
3.2 O
, O
the O
contextual O
token O
representations O
from O
BERT B-MethodName
are O
processed O
by O
self O
attention O
mechanism O
( O
Lin O
et O
al O
. O
, O
2017 O
) O
and O
the O
attention O
- O
weighted B-HyperparameterName
sentence O
representation O
is O
utilized O
for O
sentiment O
classiﬁcation O
. O

BERT B-MethodName
- O
Hard O
: O
as O
described O
in O
Section O
3.3 O
, O
it O
takes O
the O
same O
input O
as O
BERT B-MethodName
- O
Soft O
. O

The O
best O
score B-MetricName
of O
each O
column O
is O
marked O
in O
bold O
. O
Term O
- O
Level O
Category O
- O
Level O
Laptops O
Restaurants O
Restaurants O
SemEval O
14 B-MetricValue
+ O
15 O
Model O
Aspect O
3 O
- O
way O
Binary O
3 O
- O
way O
Binary O
3 O
- O
way O
Binary O
3 O
- O
way O
Binary O
Avg O
LSTM B-MethodName
No O
61.75 O
78.25 O
67.94 O
82.03 O
73.38 O
79.97 O
75.96 O
79.92 O
74.90 O
TD O
- O
LSTM B-MethodName
Yes O
62.38 O
79.31 O
69.73 O
84.41 O
79.97 O
75.96 O
79.92 O
74.90 O
75.63 O
AT O
- O
LSTM B-MethodName
Yes O
65.83 O
78.25 O
74.37 O
84.74 O
77.90 O
84.87 O
76.16 O
81.28 O
77.93 O
ATAE O
- O
LSTM B-MethodName
Yes O
60.34 O
74.20 O
70.71 O
84.52 O
77.80 O
83.85 O
74.08 O
78.96 O
75.56 O
AF O
- O
LSTM(CORR B-MethodName
) O
Yes O
64.89 O
79.96 O
74.76 O
86.91 O
80.47 O
86.58 O
74.68 O
81.60 O
78.73 O
AF O
- O
LSTM(CONV B-MethodName
) O
Yes O
68.81 O
83.58 O
75.44 O
87.78 O
81.29 O
87.26 O
78.44 O
81.49 O
80.51 O
BERT B-MethodName
- O
Original O
Yes O
74.57 O
88.25 O
82.66 O
92.31 O
88.17 O
92.37 O
80.50 O
86.84 O
85.71 O
BERT B-MethodName
- O
Soft O
Yes O
74.92 O
90.41 O
82.68 O
91.98 O
87.05 O
91.92 O
80.02 O
86.75 O
85.72 O
BERT B-MethodName
- O
Hard O
Yes O
74.10 O
89.55 O
83.91 O
92.31 O
88.17 O
93.39 O
81.09 O
87.89 O
86.30 O
Table O
2 O
: O
Experimental O
results O
( O
accuracy B-MetricName
% O
) O
on O
all O
the O
datasets O
. O

The O
results O
in O
the O
ﬁrst O
part O
( O
except O
BERT B-MethodName
- O
Original O
) O
are O
obtained O
from O
the O
prior O
work O
( O
Tay O
et O
al O
. O
, O
2018 O
) O
. O

Firstly O
, O
we O
observe O
that O
BERT B-MethodName
- O
Original O
, O
BERT B-MethodName
- O
Soft O
, O
and O
BERT B-MethodName
- O
Hard O
outperform O
all O
soft O
attention O
baselines O
( O
in O
the O
ﬁrst O
part O
of O
Table O
2 O
) O
, O
which O
demonstrates O
the O
effectiveness O
of O
ﬁne O
- O
tuning O
the O
pre O
- O
trained O
model O
on O
the O
aspect O
- O
based O
sentiment O
classiﬁcation O
task O
. O

Particularly O
, O
BERT B-MethodName
- O
Original O
outperforms O
AF O
- O
LSTM(CONV B-MethodName
) O
by O
2.63 O
% O
9.57 O
% O
, O
BERT B-MethodName
- O
Soft O
outperforms O
AF O
- O
LSTM(CONV B-MethodName
) O
by O
2.01%9.60 O
% O
and O
BERT B-MethodName
- O
Hard O
improves O
AF O
- O
LSTM(CONV B-MethodName
) O
by O
3.38 O
% O
11.23 O
% O
in O
terms O
of O
accuracy B-MetricName
. O

Considering O
the O
average B-MetricName
score B-MetricName
across O
eight O
settings O
, O
BERT B-MethodName
- O
Original O
outperforms O
AF O
- O
LSTM(CONV B-MethodName
) O
by O
6.46 B-MetricValue
% O
, O
BERT B-MethodName
- O
Soft O
outperforms O
AF O
- O
LSTM(CONV B-MethodName
) O
by O
6.47 O
% O
and O
BERT B-MethodName
- O
Hard O
outperforms O
AF O
- O
LSTM(CONV B-MethodName
) O
by O
7.19 O
% O
respectively O
. O

Secondly O
, O
we O
compare O
the O
performance O
of O
three O
BERT B-MethodName
- O
related O
methods O
. O

The O
performance O
of O
BERT B-MethodName
- O
Original O
and O
BERT B-MethodName
- O
Soft O
are O
similar O
by O
comparing O
their O
average B-MetricName
scores B-MetricName
. O

The O
reason O
may O
be O
that O
the O
original O
BERT B-MethodName
has O
already O
modeled O
the O
deep O
relationships O
between O
the O
sentence O
and O
the O
aspect O
. O

BERT B-MethodName
- O
Original O
can O
be O
thought O
of O
as O
a O
kind O
of O
soft O
- O
selection O
approach O
as O
BERT B-MethodName
- O
Soft O
. O

However O
, O
the O
improvement O
of O
BERT B-MethodName
- O
Hard O
over O
BERT B-MethodName
- O
Soft O
is O
marginal O
. O

The O
average B-MetricName
score B-MetricName
of O
BERT B-MethodName
- O
Hard O
is O
better O
than O
BERT B-MethodName
- O
Soft O
by O
0.68 B-MetricValue
% O
. O

The O
improvement O
percentages O
are O
between O
0.36 O
% O
and O
1.49 O
% O
, O
while O
on O
the O
Laptop O
dataset O
, O
the O
performance O
of O
BERT B-MethodName
- O
Hard O
is O
slightly O
weaker O
thanBERT B-MethodName
- O
Soft O
. O

For O
simplicity O
, O
Type O
SameDiffTotal2 O
- O
3 O
More O
Total O
BERT B-MethodName
- O
Original O
73.33 O
57.10 O
60.86 O
58.35 O
68.42 O
BERT B-MethodName
- O
Soft O
75.31 O
57.25 O
57.19 O
57.23 O
69.39 O
BERT B-MethodName
- O
Hard O
76.90 O
60.15 O
64.53 O
61.61 O
71.89 O
Table O
5 O
: O
Experimental O
results O
( O
accuracy B-MetricName
% O
) O
on O
multiaspect O
sentences O
. O

The O
performance O
of O
BERT B-MethodName
- O
Hard O
is O
better O
than O
BERT B-MethodName
- O
Original O
and O
BERT B-MethodName
- O
Soft O
over O
all O
types O
of O
multi O
- O
aspect O
sentences O
. O

BERT B-MethodName
- O
Hard O
outperforms O
BERT B-MethodName
- O
Soft O
by O
2.11 O
% O
when O
the O
aspects O
have O
the O
same O
sentiment O
polarities O
. O

BERT B-MethodName
- O
Hard O
outperforms O
BERT B-MethodName
- O
Soft O
by O
7.65 O
% O
in O
total O
of O
Diff O
. O

Particularly O
, O
BERT B-MethodName
- O
Soft O
has O
the O
poorest O
performance O
on O
the O
subset O
Diff O
among O
the O
three O
methods O
, O
which O
proves O
that O
soft O
attention O
is O
more O
likely O
to O
cause O
attention O
distraction O
. O

The O
attention O
weights B-HyperparameterName
are O
visualized O
for O
BERT B-MethodName
- O
Soft O
, O
and O
the O
selected O
opinion O
snippets O
are O
marked O
for O
BERT B-MethodName
- O
Hard O
. O

4.7 O
Visualization O
In O
this O
section O
, O
we O
visualize O
the O
attention O
weights B-HyperparameterName
for O
BERT B-MethodName
- O
Soft O
and O
opinion O
snippets O
for O
BERTHard B-MethodName
. O

Firstly O
, O
the O
attention O
weights B-HyperparameterName
of O
BERT B-MethodName
- O
Soft O
scatter O
among O
the O
whole O
sentence O
and O
could O
attend O
to O
irrelevant O
words O
. O

For O
the O
aspect O
service O
, O
BERTSoft B-MethodName
attends O
to O
the O
word O
“ O
ok O
” O
with O
relatively O
high O
score B-MetricName
though O
it O
does O
not O
describe O
the O
aspect O
service O
. O

Secondly O
, O
our O
proposed O
method O
BERT B-MethodName
- O
Hard O
can O
detect O
the O
opinion O
snippet O
for O
a O
given O
aspect O
. O

As O
illustrated O
in O
Figure O
3 O
, O
the O
opinion O
snippets O
are O
selected O
by O
BERT B-MethodName
- O
Hard O
accurately O
. O

In O
the O
sentence O
“ O
the O
appetizers O
are O
ok O
, O
but O
the O
service O
is O
slow O
” O
, O
BERT B-MethodName
- O
Hard O
can O
exactly O
locate O
the O
opinion O
snippets O
“ O
ok O
” O
and“slow O
” O
for O
the O
aspect O
appetizers O
andservice O
respectively O
. O

At O
last O
, O
we O
enumerate O
some O
opinion O
snippets O
detected O
by O
BERT B-MethodName
- O
Hard O
in O
Table O
6 O
. O

ThesePositive O
Snippets O
Negative O
Snippets O
very O
good O
prompt O
attentive O
not O
great O
bland O
beautifully O
presented O
can O
not O
eat O
this O
well O
extremely O
tasty O
unbearable O
conversation O
as O
interesting O
as O
possible O
no O
idea O
how O
to O
use O
cool O
and O
soothing O
would O
never O
go O
there O
impressed O
by O
not O
above O
ordinary O
can O
not O
be O
beat O
for O
the O
quality O
not O
good O
Table O
6 O
: O
Examples O
of O
accurate O
opinion O
snippets O
detected O
by O
BERT B-MethodName
- O
Hard O
. O

Inaccurate O
Snippets O
are O
very O
large O
and O
and O
even O
greater O
food O
are O
not O
terrible O
tasty O
treat O
at O
everyone O
who O
works O
the O
money O
and O
said O
Table O
7 O
: O
Examples O
of O
inaccurate O
opinion O
snippets O
detected O
by O
BERT B-MethodName
- O
Hard O
. O

The O
deep O
associations O
between O
the O
sentence O
and O
aspect O
, O
and O
the O
long O
- O
term O
dependencies O
within O
the O
sentence O
are O
taken O
into O
consideration O
by O
leveraging O
the O
pretrained O
BERT B-MethodName
model O
. O

Indeed O
, O
we O
see O
that O
the O
majority O
of O
the O
top-10 O
nearest B-MethodName
neighbors I-MethodName
of O
the O
word O
gita O
in O
Italian O
( O
“ O
trip”-fem O
) O
are O
feminine O
words O
. O

Also O
, O
we O
notice O
that O
the O
word O
viaggio O
( O
“ O
journey O
” O
-masc O
) O
is O
not O
on O
the O
list O
, O
while O
in O
English O
, O
for O
comparison O
, O
we O
can O
ﬁnd O
journey O
in O
the O
top-10 O
nearest B-MethodName
neighbors I-MethodName
of O
trip O
. O

the O
list O
of O
nearest B-MethodName
neighbors I-MethodName
of O
“ O
dizionario O
” O
. O

For O
each O
pair O
we O
plot O
the O
new O
rank O
of O
the O
second O
word O
in O
the O
nearest B-MethodName
neighbors I-MethodName
list O
of O
the O
ﬁrst O
word O
as O
a O
function O
of O
its O
original O
rank O
before O
debiasing O
. O

Our O
baseline O
model O
has O
two O
parts O
: O
1 O
) O
Encoding O
the O
sequence O
of O
tokens O
by O
incorporating O
the O
preceding O
and O
following O
contexts O
using O
a O
bi O
- O
directional O
long O
short O
- O
term O
memory O
( O
BiLSTM B-MethodName
) O
( O
Graves O
et O
al O
. O
, O
2013 O
) O
, O
so O
each O
token O
is O
assigned O
a O
local O
contextual O
embedding O
. O

Here O
, O
following O
Ma O
and O
Hovy O
( O
2016a O
) O
, O
we O
use O
the O
concatenation O
of O
pre O
- O
trained O
word O
embeddings O
and O
character O
- O
level O
word O
representations O
composed O
by O
a O
convolutional O
neural O
network O
( O
CNN O
) O
as O
inputto O
the O
Bi O
- O
LSTM B-MethodName
. O

The O
Bi O
- O
LSTM B-MethodName
CRF O
network O
is O
a O
strong O
baseline O
due O
to O
its O
remarkable O
capability O
of O
modeling O
contextual O
information O
and O
label O
dependencies O
. O

Many O
recent O
efforts O
combine O
the O
Bi O
- O
LSTM B-MethodName
CRF O
network O
with O
language O
modeling O
( O
Liu O
et O
al O
. O
, O
2017 O
; O
Peters O
et O
al O
. O
, O
2017 O
, O
2018 O
) O
to O
boost O
the O
name O
tagging O
performance O
. O

Formally O
, O
given O
a O
document O
D O
= O
fs1;s2;:::g O
, O
wheresi O
= O
fwi1;wi2;:::gis O
a O
sequence O
of O
words O
, O
we O
apply O
a O
Bi O
- O
LSTM B-MethodName
to O
each O
word O
in O
si O
, O
generating O
local O
contextual O
representations O
hi= O
fhi1;hi2;:::g O
. O
Next O
, O
for O
each O
wij O
, O
we O
retrieve O
the O
sentences O
in O
the O
document O
that O
contain O
wij(e.g O
. O
,sqandsrin O
Figure O
2 O
) O
and O
select O
the O
local O
contextual O
representations O
of O
wijfrom O
these O
sentences O
as O
supporting O
evidence O
, O
~hij O
= O
f O
~ O
h1 O
ij;~h2 O
ij;:::g(e.g O
. O
,~hqj O
and O
~ O
hrkin O
Figure O
2 O
) O
, O
where O
hijand O
~ O
hijare O
obtained O
with O
the O
same O
Bi O
- O
LSTM B-MethodName
. O

The O
 O
list O
 O
includes O
 O
Cruzcampo O
, O
 O
Affligem O
 O
and O
 O
Zywiec O
 O
.Bidirectional O
 O
LSTM B-MethodName
 O
Encoder O
Bidirectional O
 O
LSTM B-MethodName
 O
EncoderBidirectional O
 O
LSTM B-MethodName
 O
Encoder O
Bidirectional O
 O
LSTM B-MethodName
 O
EncoderA+G O
A+GAAttentive O
 O
SummationGGated O
 O
Summation O
contextual O
 O
representaions O
 O
ofZywiec O
 O
from O
 O
topically O
 O
relateddocumentsBi­LSTM B-MethodName
 O
CRF O
 O
Layer O
Bi­LSTM B-MethodName
 O
CRF O
 O
LayerconcatenateAAttentive O
 O
SummationGGated O
 O
Summation O
Bidirectional O
 O
LSTM B-MethodName
 O
EncoderA+GAAttentive O
 O
SummationGGated O
 O
Summation O
contextual O
 O
representaions O
 O
of O
 O
Zywiec O
 O
fromtopically O
 O
related O
 O
documentsBi­LSTM B-MethodName
 O
CRF O
 O
Layer O
The O
 O
Zywiec O
 O
logo O
 O
includes O
 O
all O
 O
of O
 O
the O
 O
mostimportant O
 O
historical O
 O
symbols O
 O
of O
 O
the O
 O
brewery O
 O
andPoland O
 O
itself O
. O

Bidirectional O
 O
LSTM B-MethodName
 O
EncoderBidirectional O
 O
LSTM B-MethodName
 O
EncoderCorpus­level O
 O
supporting O
 O
sentencesDocument­level O
 O
supporting O
 O
sentences O
Figure O
3 O
: O
Corpus O
- O
level O
Attention O
Architecture.2.4 O
Tag O
Prediction O
For O
each O
word O
wijof O
sentencesi O
, O
we O
concatenate O
its O
local O
contextual O
representation O
hij O
, O
documentlevel O
gated O
supporting O
evidence O
representation O
Dij O
, O
and O
corpus O
- O
level O
gated O
supporting O
evidence O
representation O
Cijto O
obtain O
its O
ﬁnal O
representation O
. O

This O
representation O
is O
fed O
to O
another O
BiLSTM B-MethodName
to O
further O
encode O
the O
supporting O
evidence O
and O
local O
contextual O
features O
into O
an O
uniﬁed O
representation O
, O
which O
is O
given O
as O
input O
to O
an O
afﬁne O
- O
CRF O
layer O
for O
label O
prediction O
. O

4Both O
numbers O
are O
tuned O
from O
1 O
to O
10 O
and O
selected O
when O
the O
model O
performs O
best O
on O
the O
development O
set O
. O
Hyper O
- O
parameter O
Value O
CharCNN O
Filter O
Number O
25 O
CharCNN O
Filter O
Widths O
[ O
2 O
, O
3 O
, O
4 O
] O
Lower O
Bi O
- O
LSTM B-MethodName
Hidden O
Size O
100 O
Lower O
Bi O
- O
LSTM B-MethodName
Dropout O
Rate O
0.5 O
Upper O
Bi O
- O
LSTM B-MethodName
Hidden O
Size O
100 O
Learning O
Rate O
0.005 O
Batch O
Size O
N O
/ O
A O
Optimizer O
SGD O
( O
Bottou O
, O
2010 O
) O
Each O
batch O
is O
a O
document O
. O

For O
each O
model O
with O
an O
attention O
, O
since O
the O
BiLSTM B-MethodName
encoder O
must O
encode O
the O
local O
, O
documentlevel O
, O
and/or O
corpus O
- O
level O
contexts O
, O
we O
pre O
- O
train O
a O
Bi O
- O
LSTM B-MethodName
CRF O
model O
for O
50 O
epochs B-HyperparameterName
, O
add O
our O
document O
- O
level O
attention O
and/or O
corpus O
- O
level O
attention O
, O
and O
then O
ﬁne O
- O
tune O
the O
augmented O
model O
. O

3.3 O
Performance O
Comparison O
We O
compare O
our O
methods O
to O
three O
categories O
of O
baseline O
name O
tagging O
methods O
: O
Vanilla O
Name O
Tagging O
Without O
any O
additional O
resources O
and O
supervision O
, O
the O
current O
state O
- O
ofthe O
- O
art O
name O
tagging O
model O
is O
the O
Bi O
- O
LSTMCRF B-MethodName
network O
reported O
by O
Lample O
et O
al O
. O
( O
2016 O
) O
and O
Ma O
and O
Hovy O
( O
2016b O
) O
, O
whose O
difference O
lies O
in O
using O
a O
LSTM B-MethodName
or O
CNN O
to O
encode O
characters O
. O

We O
ﬁrst O
pretrain O
a O
baseline O
Bi O
- O
LSTM B-MethodName
CRF O
model O
from O
epoch O
1 O
to O
50 O
. O

The O
score B-MetricName
of O
document O
- O
level O
propagation O
on O
English O
is O
90.21 B-MetricValue
% O
( O
F1 B-MetricName
) O
, O
and O
the O
corpus O
- O
level O
propagation O
is O
89.02 B-MetricValue
% O
which O
are O
both O
lower O
than O
the O
BiLSTMCRF B-MethodName
baseline O
90.97 O
% O
. O

Huang O
et O
al O
. O
( O
2015 O
) O
and O
Lample O
et O
al O
. O
( O
2016 O
) O
proposed O
a O
neural O
architecture O
consisting O
of O
a O
bi O
- O
directional O
long O
short O
- O
term O
memory O
network O
( O
Bi O
- O
LSTM B-MethodName
) O
encoder O
and O
a O
conditional O
random O
ﬁeld O
( O
CRF O
) O
output O
layer O
( O
Bi O
- O
LSTM B-MethodName
CRF O
) O
. O

Efforts O
incorporated O
character O
level O
compositional O
word O
embeddings O
, O
language O
modeling O
, O
and O
CRF O
re O
- O
ranking O
into O
the O
Bi O
- O
LSTM B-MethodName
CRF O
architecture O
which O
improved O
the O
performance O
( O
Ma O
and O
Hovy O
, O
2016a O
; O
Liu O
et O
al O
. O
, O
2017 O
; O
Sato O
et O
al O
. O
, O
2017 O
; O
Peters O
et O
al O
. O
, O
2017 O
, O
2018 O
) O
. O

Similar O
to O
these O
studies O
, O
our O
approach O
is O
also O
based O
on O
a O
Bi O
- O
LSTM B-MethodName
CRF O
architecture O
. O

To O
enrich O
language O
models O
with O
some O
of O
this O
missing O
experience O
, O
we O
leverage O
two O
sources O
of O
information O
: O
( O
1 O
) O
the O
Lancaster O
Sensorimotor O
norms O
, O
which O
provide O
ratings O
( O
means B-MetricName
and O
standard O
deviations O
) O
for O
over O
40,000 O
English O
words O
along O
several O
dimensions O
of O
embodiment O
, O
and O
which O
capture O
the O
extent O
to O
which O
something O
is O
experienced O
across O
11 O
different O
sensory O
modalities O
, O
and O
( O
2 O
) O
vectors O
from O
coefﬁcients O
of O
binary O
classiﬁers O
trained O
on O
images O
for O
the O
BERT B-MethodName
vocabulary O
. O

We O
pre O
- O
trained O
the O
ELECTRA B-MethodName
model O
and O
ﬁne O
- O
tuned O
the O
RoBERTa B-MethodName
model O
with O
these O
two O
sources O
of O
information O
then O
evaluate O
using O
the O
established O
GLUE B-DatasetName
benchmark O
and O
the O
Visual O
Dialog O
benchmark O
. O

In O
particular O
, O
what O
does O
this O
mean B-MetricName
for O
language O
models O
that O
are O
trained O
purely O
on O
text O
( O
likely O
largely O
written O
by O
adults O
) O
, O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 B-MetricValue
) O
or O
GPT-3 B-MethodName
? O

We O
explore O
how O
these O
embeddings O
can O
be O
used O
to O
enrich O
the O
ELECTRA B-MethodName
language O
model O
’s O
pre O
- O
training O
and O
ﬁne O
- O
tuning O
, O
and O
evaluate O
on O
the O
GLUE B-DatasetName
benchmark O
( O
Experiment O
1 O
, O
Section O
4 O
) O
, O
and O
how O
they O
can O
be O
used O
to O
replace O
input O
embeddings O
for O
a O
pre O
- O
trained O
RoBERTa B-MethodName
model O
for O
the O
Visual O
Dialog O
task O
( O
Experiment O
2 O
, O
Section O
5 O
) O
. O

Rogers O
et O
al O
. O
( O
2020 O
) O
provides O
a O
recent O
primer O
and O
overview O
of O
research O
that O
has O
attempted O
to O
uncover O
strengths O
and O
weaknesses O
of O
BERT B-MethodName
and O
related O
language O
models O
( O
so O
- O
called O
BERTology B-MethodName
) O
. O

This O
criticism O
is O
born O
out O
in O
Forbes O
et O
al O
. O
( O
2019 O
) O
which O
showed O
that O
BERT B-MethodName
can O
guess O
affordances O
and O
properties O
of O
objects O
because O
that O
information O
can O
be O
found O
in O
text O
( O
e.g. O
, O
a O
typical O
chair O
has O
the O
affordance O
of O
being O
sittable O
, O
and O
a O
property O
of O
having O
legs O
) O
, O
but O
has O
no O
notion O
of O
how O
objects O
are O
related O
semantically O
to O
each O
other O
, O
and O
Da O
and O
Kasai O
( O
2019 O
) O
further O
showed O
that O
real O
- O
world O
perceptual O
properties O
are O
likely O
to O
be O
assumed O
instead O
of O
inferred O
. O

Furthermore O
, O
Bender O
and O
Koller O
( O
2020 O
) O
make O
a O
strong O
case O
that O
BERT B-MethodName
learns O
form O
instead O
of O
meaning B-MetricName
, O
and O
while O
the O
fact O
that O
BERT B-MethodName
performs O
so O
well O
on O
many O
tasks O
is O
difﬁcult O
to O
dispute O
, O
models O
trained O
on O
text O
are O
missing O
semantic O
information O
crucial O
for O
holistic O
language O
understanding O
. O

Since O
before O
BERT B-MethodName
which O
has O
proven O
powerful O
in O
many O
language O
processing O
tasks O
, O
efforts O
have O
been O
made O
to O
encode O
multimodal O
( O
i.e. O
, O
more O
than O
just O
text O
as O
a O
learning O
modality O
) O
information O
into O
embeddings O
and O
language O
models O
( O
Takano O
and O
Utsumi O
, O
2016 O
; O
Kiros O
et O
al O
. O
, O
2014 O
; O
Zellers O
et O
al O
. O
, O
2021 O
) O
and O
recent O
, O
continued O
efforts O
towards O
bridging O
grounded O
visual O
representations O
to O
distributional O
representations O
of O
word O
meanings B-MetricName
give O
credence O
to O
the O
claim O
that O
text O
- O
only O
models O
like O
BERT B-MethodName
are O
missing O
crucial O
semantic O
information O
because O
enriching O
BERT B-MethodName
with O
visual O
information O
improves O
performance O
in O
several O
known O
tasks O
( O
Kim O
et O
al O
. O
, O
2019 O
; O
Lu O
et O
al O
. O
, O
2019 O
; O
Li O
et O
al O
. O
, O
2019 O
) O
. O

We O
train O
a O
binary O
logistic B-MethodName
regression I-MethodName
classiﬁer O
, O
then O
extract O
the O
coefﬁcients O
for O
the O
redvector O
. O

We O
follow O
Kiros O
et O
al O
. O
( O
2018 O
) O
and O
use O
Google O
Image O
Search O
to O
ﬁnd O
images O
using O
the O
BERT B-MethodName
vocabulary O
, O
resulting O
in O
27,152 O
words O
and O
corresponding O
images O
( O
some O
words O
did O
not O
result O
in O
images O
, O
and O
we O
did O
not O
download O
images O
for O
ﬁller O
words O
) O
. O

We O
then O
use O
a O
logistic B-MethodName
regression I-MethodName
classiﬁer O
( O
C=0.25 O
, O
max O
iterations=1000 O
) O
, O
one O
for O
each O
word O
, O
trained O
on O
the O
images O
for O
each O
word O
. O

4 O
Experiment O
1 O
: O
Tying O
embedding O
weights B-HyperparameterName
and O
pre O
- O
training O
ELECTRA B-MethodName
, O
ﬁne O
- O
tuning O
on O
GLUE B-DatasetName
In O
this O
experiment O
, O
and O
crucially O
for O
our O
ongoing O
work O
that O
aligns O
with O
child O
- O
inspired O
language O
acquisition O
, O
we O
use O
ELECTRA B-MethodName
( O
Clark O
et O
al O
. O
, O
2020 O
) O
as O
a O
language O
model O
because O
it O
has O
been O
shown O
to O
be O
trainable O
with O
smaller O
amounts O
of O
data O
than O
other O
language O
models O
, O
yet O
yield O
respectable O
results O
and O
can O
be O
trained O
using O
a O
single O
GPU O
. O

WAC O
classiﬁer O
weights B-HyperparameterName
are O
tied O
to O
the O
embedding O
layer O
for O
the O
Generator O
and O
Discriminator O
for O
ELECTRA B-MethodName
. O

We O
only O
report O
the O
results O
for O
the O
MRPC B-DatasetName
( O
a O
paraphrase O
task O
that O
uses O
accuracy B-MetricName
and O
f1 O
metrics O
) O
, O
COLA O
( O
a O
grammatical O
acceptibility O
task O
; O
uses O
Matthew O
’s O
Correlation B-MetricName
) O
, O
and O
WNLI B-DatasetName
( O
ambiguity O
resolution O
; O
uses O
an O
accuracy B-MetricName
metric O
) O
tasks O
because O
they O
are O
sufﬁcient O
to O
illustrate O
the O
utility O
of O
our O
method O
when O
applied O
to O
ELECTRA B-MethodName
. O

To O
give O
ELECTRA B-MethodName
knowledge O
about O
additional O
modalities O
from O
the O
Lancaster O
and O
WAC O
vectors O
, O
we O
tie O
the O
vectors O
to O
the O
the O
weights B-HyperparameterName
of O
the O
generator O
and O
discriminator O
of O
ELECTRA B-MethodName
depicted O
in O
Figure O
2 O
, O
and O
vary O
whether O
the O
embeddings O
are O
2GLUE B-DatasetName
has O
a O
public O
leader O
board O
found O
at O
https:// O
gluebenchmark.com/leaderboard O
3We O
build O
off O
of O
the O
implementation O
of O
https:// O
github.com/lucidrains/electra-pytorchfrozen O
or O
not O
during O
pre O
- O
training O
, O
then O
train O
for O
100,000 O
steps.4We O
then O
ﬁne O
- O
tune O
the O
resulting O
ELECTRA B-MethodName
model O
on O
the O
GLUE B-DatasetName
tasks O
using O
the O
multimodal O
vectors O
following O
standard O
ﬁne O
- O
tuning O
protocols O
; O
that O
is O
, O
we O
add O
a O
linear O
layer O
with O
a O
softmax O
to O
the O
pre O
- O
trained O
model O
and O
use O
the O
ADAM O
solver O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e-5 O
for O
3 B-HyperparameterValue
epochs B-HyperparameterName
. O

As O
the O
WAC O
vectors O
were O
larger O
than O
ELECTRA B-MethodName
’s O
expected O
embedding O
size O
of O
128 O
, O
we O
applied O
UMAP B-MetricName
to O
reduce O
the O
dimensionality O
to O
128 O
; O
similarly O
for O
the O
WAC O
and O
Lancaster O
concatenated O
embeddings O
. O

For O
Lancaster O
vectors O
, O
we O
set O
the O
ELECTRA B-MethodName
embedding O
size O
to O
39 O
. O

For O
a O
broader O
comparison O
, O
we O
also O
compared O
to O
GloVE O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
several O
ablations O
where O
we O
concatenate O
multimodal O
vectors O
with O
the O
GloVe B-MethodName
vectors O
( O
we O
used O
the O
evaluation O
script O
for O
GloVE O
provided O
by O
Wang O
et O
al O
. O
( O
2018 O
) O
) O
. O

We O
also O
use O
the O
same O
training O
and O
evaluation O
regime O
for O
the O
WAC O
and O
Lancaster O
vectors O
, O
and O
a O
concatenation O
of O
the O
two O
, O
on O
their O
own O
treating O
them O
as O
word O
- O
level O
embeddings O
similar O
to O
GloVe B-MethodName
. O

The O
word O
- O
level O
embeddings O
of O
GloVe B-MethodName
, O
WAC O
, O
and O
Lancaster O
are O
shown O
in O
the O
top O
5 O
rows O
of O
the O
table O
. O

Interestingly O
, O
the O
best O
performing O
model O
for O
COLA O
was O
GloVE O
and O
Lancaster O
word O
- O
level O
embeddings O
; O
COLA O
is O
4This O
takes O
about O
12 O
hours O
of O
training O
on O
our O
12 O
GB O
GPU O
, O
which O
we O
opted O
for O
because O
it O
represents O
more O
data O
and O
train O
time O
than O
ELECTRA B-MethodName
- O
small O
, O
but O
still O
a O
small O
enough O
amount O
of O
time O
to O
establish O
using O
this O
model O
in O
a O
co O
- O
located O
, O
interactive O
learning O
setting O
similar O
to O
the O
setting O
where O
children O
learn O
their O
ﬁrst O
language O
. O
MRPC B-DatasetName
MRPC B-DatasetName
COLA O
WNLI B-DatasetName
acc O
f1 O
corr O
acc O
GloVE O
0.745 O
0.807 O
0.691 O
0.563 O
GloVE+lan O
0.735 O
0.799 O
0.449 O
0.563 O
lan O
0.711 O
0.778 O
0.691 O
0.596 O
wac O
0.748 O
0.812 O
0.313 O
0.563 O
lan+wac O
0.619 O
0.670 O
0.382 O
0.535 O
ELECTRA B-MethodName
0.730 O
0.835 O
0.449 O
0.563 O
EL O
- O
wac O
0.730 O
0.835 O
0.440 O
0.563 O
EL O
- O
wacf0.760 O
0.833 O
0.0 O
0.563 O
EL O
- O
lan O
0.708 O
0.819 O
0.39 O
0.535 O
EL O
- O
lan O
- O
wacf0.792 O
0.859 O
0.459 O
0.563 O
Table O
1 O
: O
GLUE B-DatasetName
development O
set O
results O
with O
GloVe B-MethodName
, O
ELECTRA B-MethodName
, O
Lancaster O
( O
lan O
) O
, O
WAC O
models O
and O
several O
combinations O
, O
with O
( O
fand O
without O
weight B-HyperparameterName
freezing O
during O
ELECTRA B-MethodName
pre O
- O
training O
. O

All O
other O
rows O
show O
the O
ELECTRA B-MethodName
baseline O
and O
ELECTRA B-MethodName
that O
uses O
some O
variation O
of O
WAC O
, O
Lancaster O
, O
or O
both O
as O
embeddings O
( O
denoted O
with O
the O
EL- O
preﬁx O
) O
. O

The O
bottom O
part O
of O
the O
table O
compares O
ELECTRA B-MethodName
with O
a O
variant O
of O
ELECTRA B-MethodName
that O
uses O
WAC O
embeddings O
( O
both O
with O
and O
without O
freezing O
the O
embedding O
weights B-HyperparameterName
) O
, O
ELECTRA B-MethodName
with O
lancaster O
embeddings O
and O
ELECTRA B-MethodName
with O
WAC O
embeddings O
concatenated O
with O
the O
Lancaster O
embeddings O
( O
where O
the O
length O
of O
the O
WAC O
embeddings O
plus O
the O
size O
of O
ELECTRA B-MethodName
is O
128 O
) O
. O

Contrary O
to O
our O
hypothesis O
, O
we O
observe O
that O
when O
ELECTRA B-MethodName
uses O
WAC O
with O
frozen O
weights B-HyperparameterName
, O
the O
performance O
on O
the O
benchmark O
performs O
better O
than O
all O
others O
, O
including O
the O
ELECTRA B-MethodName
baseline O
. O

This O
could O
suggest O
that O
ELECTRA B-MethodName
can O
make O
effective O
use O
of O
the O
visual O
and O
Lancaster O
embeddings O
by O
adjusting O
weights B-HyperparameterName
in O
the O
other O
layers O
of O
the O
model O
. O

The O
EL O
- O
lan O
- O
wac O
variant O
performed O
well O
above O
the O
ELECTRA B-MethodName
baseline O
, O
substantiating O
the O
hypothesis O
that O
enriching O
the O
model O
with O
multimodal O
knowledge O
can O
improve O
results O
. O

Taken O
together O
, O
we O
ﬁnd O
the O
results O
encouraging O
because O
the O
relatively O
short O
training O
regime O
still O
yielded O
respectable O
results O
, O
suggesting O
that O
ELECTRA B-MethodName
with O
a O
visual O
or O
other O
multimodal O
embedding O
can O
be O
useful O
with O
less O
training O
as O
is O
the O
case O
when O
children O
learn O
language.5 O
Experiment O
2 O
: O
Replacing O
RoBERTa B-MethodName
embeddings O
, O
ﬁne O
- O
tuninng O
on O
Visual O
Dialog O
The O
evaluation O
in O
Experiment O
1 O
was O
made O
up O
of O
text O
- O
based O
tasks O
. O

In O
this O
experiment O
, O
we O
evaluate O
using O
a O
fully O
pre O
- O
trained O
RoBERTa B-MethodName
model O
by O
replacing O
its O
embeddings O
with O
theWAC O
and O
Lancaster O
vectors O
. O

Baseline O
and O
Procedure O
We O
report O
the O
values O
for O
the O
model O
described O
in O
Murahari O
et O
al O
. O
( O
2019 O
) O
for O
our O
baseline O
– O
work O
which O
builds O
on O
VilBERT B-MethodName
( O
Lu O
et O
al O
. O
, O
2019 O
) O
, O
a O
parallel O
model O
of O
vision O
and O
language O
used O
for O
the O
visual O
dialogue O
task O
– O
and O
leverage O
their O
model O
with O
our O
custom O
, O
multimodal O
embeddings O
. O

Murahari O
et O
al O
. O
( O
2019 O
) O
adapted O
the O
VilBERT B-MethodName
model O
for O
the O
visdial O
task O
by O
using O
a O
pre O
- O
trained O
language O
model O
trained O
on O
English O
Wikipedia O
and O
the O
BooksCorpus O
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
using O
masked O
language O
modeling O
and O
next O
sentence O
prediction O
losses O
. O

They O
then O
frame O
the O
task O
as O
a O
next O
- O
sentence O
prediction O
task O
( O
whereas O
the O
original O
VilBERT B-MethodName
was O
modeled O
to O
generate O
descriptions O
of O
images O
) O
. O

The O
underlying O
architecture O
uses O
a O
pre O
- O
trained O
BERT B-MethodName
language O
model O
( O
i.e. O
, O
bert O
- O
base O
- O
uncased O
) O
as O
a O
starting O
point O
before O
training O
on O
the O
Wikipedia O
, O
BooksCorpus O
, O
Conceptual O
Captions O
, O
and O
VQA O
datasets O
. O

We O
altered O
their O
architecture O
by O
replacing O
the O
RoBERTa B-MethodName
pre O
- O
trained O
embedding O
layer O
with O
the O
Lancaster O
and O
WAC O
vectors O
, O
as O
depicted O
in O
Figure O
3 O
. O

Vocabulary O
: O
RoBERTa B-MethodName
& O
AoA O
Abstract O
words O
do O
not O
have O
concrete O
, O
visual O
denotations O
, O
such O
as O
utopia O
orjustice O
, O
so O
it O
does O
not O
make O
theoretical O
sense O
to O
include O
a O
WAC O
embedding O
for O
words O
that O
are O
clearly O
abstract O
because O
whatever O
set O
of O
images O
represents O
those O
concepts O
may O
not O
have O
useful O
semantic O
information O
. O

To O
explore O
if O
RoBERTa B-MethodName
could O
make O
use O
of O
a O
WAC O
embedding O
that O
uses O
words O
that O
are O
more O
aimed O
at O
a O
child O
vocabulary O
, O
we O
report O
results O
of O
ﬁltering O
out O
words O
not O
in O
the O
the O
Age O
- O
of O
- O
Acquisition O
( O
AoA O
) O
list O
( O
Kuperman O
et O
al O
. O
, O
2012b O
) O
. O

During O
the O
ﬁnal O
ﬁne O
- O
tuning O
on O
the O
Visual O
Dialog O
data O
, O
however O
, O
we O
replace O
the O
RoBERTa B-MethodName
embeddings O
with O
the O
Lancaster O
and O
WAC O
embeddings O
. O

Lancaster O
vectors O
Similar O
to O
AoA O
, O
the O
Lancaster O
Norms O
has O
a O
predeﬁned O
vocabulary O
, O
which O
, O
when O
compared O
to O
the O
RoBERTa B-MethodName
vocabulary O
results O
in O
11,402 O
words O
in O
both O
. O

For O
each O
word O
in O
the O
RoBERTa B-MethodName
vocabulary O
that O
was O
also O
in O
the O
Lancaster O
norms O
, O
we O
replaced O
the O
RoBERTa B-MethodName
embedding O
with O
the O
Lancaster O
vector O
for O
that O
word O
; O
otherwise O
words O
retained O
the O
original O
RoBERTa B-MethodName
embedding O
. O

As O
their O
model O
expects O
vectors O
of O
size O
768 O
( O
the O
embedding O
size O
for O
RoBERTa B-MethodName
) O
, O
but O
the O
Lancaster O
vectors O
are O
only O
size O
39 O
, O
we O
padded O
the O
rest O
of O
the O
vector O
with O
zeros O
. O

WAC O
vectors O
We O
use O
the O
vocabulary O
from O
the O
RoBERTa B-MethodName
tokenizer O
as O
with O
the O
Lancaster O
Vectors O
, O
which O
results O
in O
a O
a O
27,152 O
- O
word O
overlap O
with O
the O
WAC O
vectors O
. O

As O
the O
WAC O
vectors O
have O
a O
dimensionality O
of O
513 O
, O
smaller O
than O
the O
required O
size O
of O
RoBERTa B-MethodName
’s O
768 O
, O
we O
padded O
zeros O
after O
each O
vector O
. O

We O
report O
the O
results O
of O
the O
baseline O
model O
and O
the O
variants O
of O
our O
above O
changes.6Compared O
to O
Experiment O
1 O
with O
the O
GLUE B-DatasetName
benchmark O
, O
the O
approach O
taken O
in O
this O
section O
fundamentally O
changes O
how O
the O
Lancaster O
and O
WAC O
embeddings O
are O
applied O
to O
RoBERTa B-MethodName
; O
here O
the O
Lancaster O
and O
WAC O
embeddings O
are O
used O
on O
a O
pre O
- O
trained O
model O
. O

This O
is O
partially O
due O
to O
the O
fact O
that O
our O
training O
regime O
was O
altered O
due O
to O
hardware O
limitations O
( O
i.e. O
, O
we O
could O
only O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
8 B-HyperparameterValue
on O
a O
single O
12 O
GB O
GPU).no O
freeze O
MRR B-MetricName
R@1 O
R@5 O
NDCG O
baseline O
64.92 B-MetricValue
50.52 O
82.98 O
56.82 O
lan O
64.61 O
49.98 O
82.6 O
58.10 O
2 O
- O
freeze O
MRR B-MetricName
R@1 O
R@5 O
NDCG O
lan O
63.77 B-MetricValue
49.03 O
81.63 O
57.53 O
wac O
66.38 O
52.25 O
83.8 O
61.47 O
lanwac O
63.93 O
49.025 O
82.65 O
57.73 O
wac O
- O
aoa O
66.79 O
52.75 O
84.05 O
60.44 O
Table O
2 O
: O
Experiment O
2 O
results O
for O
the O
visdial O
task O
: O
baseline O
RoBERTa B-MethodName
embedding O
, O
Lancaster O
norms O
( O
lan O
) O
, O
WAC O
vectors O
, O
and O
concatenated O
( O
lanwac O
) O
, O
not O
frozen O
, O
and O
only O
frozen O
for O
2 O
epochs B-HyperparameterName
( O
bottom O
section O
) O
. O

pothesize O
that O
RoBERTa B-MethodName
will O
improve O
with O
WAC O
embeddings O
, O
as O
well O
as O
the O
Lancaster O
concatenated O
toWAC O
( O
denoted O
lanwac O
) O
, O
though O
the O
Lancaster O
embedding O
on O
its O
own O
may O
be O
too O
small O
to O
make O
a O
difference O
. O

As O
words O
that O
are O
learned O
earlier O
in O
a O
child O
’s O
life O
are O
generally O
more O
concrete O
, O
we O
hypothesize O
that O
RoBERTa B-MethodName
will O
improve O
when O
WAC O
only O
uses O
words O
from O
the O
AoA O
data O
as O
more O
abstract O
terms O
are O
represented O
by O
zero O
vectors O
. O

Though O
it O
is O
clear O
that O
RoBERTa B-MethodName
is O
doing O
the O
heavy O
lifting O
, O
when O
added O
to O
RoBERTa B-MethodName
, O
the O
Lancaster O
and O
WAC O
vectors O
show O
improvements O
over O
the O
RoBERTa B-MethodName
baseline O
for O
some O
metrics O
. O

RoBERTA B-MethodName
that O
uses O
the O
WAC O
embedding O
especially O
shows O
respectable O
results O
in O
the O
visdial O
task O
, O
particularly O
when O
the O
embedding O
uses O
the O
AoA O
vocabulary O
( O
we O
only O
considered O
AoA O
for O
WAC O
because O
WAC O
peformed O
better O
than O
lanwac O
in O
this O
experiment O
) O
. O

Lancaster O
norms O
performed O
well O
on O
their O
own O
in O
one O
GLUE B-DatasetName
task O
compared O
to O
other O
word O
embeddings O
like O
GloVe B-MethodName
, O
and O
coupled O
with O
the O
WAC O
vectors O
as O
the O
embedding O
in O
an O
ELECTRA B-MethodName
model O
, O
they O
performed O
respectably O
on O
the O
GLUE B-DatasetName
task O
. O

The O
WAC O
vectors O
, O
when O
used O
as O
embeddings O
in O
the O
RoBERTa B-MethodName
model O
performed O
well O
on O
the O
Visual O
Dialog O
task O
, O
particularly O
when O
the O
vocabulary O
was O
more O
restricted O
to O
the O
Age O
of O
Acquisition O
vocabulary O
. O

Moreover O
, O
standard O
language O
models O
can O
not O
actually O
identify O
denotations O
when O
they O
are O
present O
; O
i.e. O
, O
ELECTRA B-MethodName
and O
RoBERTa B-MethodName
are O
not O
actually O
capable O
of O
determining O
if O
an O
object O
is O
redorsoftfrom O
observing O
that O
object O
– O
a O
basic O
ability O
for O
a O
language O
learning O
child O
– O
simply O
because O
those O
models O
can O
not O
observe O
the O
world O
outside O
of O
text O
, O
though O
the O
purpose O
of O
the O
WAC O
( O
and O
models O
like O
VilBERT B-MethodName
) O
model O
is O
to O
do O
just O
that O
: O
identify O
denotations O
; O
by O
coupling O
WAC O
with O
ELECTRA B-MethodName
and O
RoBERTa B-MethodName
, O
both O
models O
can O
make O
use O
of O
that O
capability O
. O

In O
particular O
, O
our O
knowledge O
from O
this O
paper O
informs O
us O
that O
the O
ELECTRA B-MethodName
model O
with O
embeddings O
tied O
to O
WAC O
classiﬁer O
weights B-HyperparameterName
is O
a O
good O
candidate O
for O
live O
interaction O
of O
a O
robot O
that O
is O
learning O
words O
from O
a O
human O
collaborator O
because O
the O
ELECTRA- B-MethodName
WAC O
model O
can O
function O
with O
small O
amounts O
of O
data O
and O
the O
embedding O
layer O
can O
successfully O
be O
tied O
to O
weights B-HyperparameterName
of O
the O
WAC O
classiﬁers O
. O

ELECTRA B-MethodName
: O
Pretraining O
text O
encoders O
as O
discriminators O
rather O
than O
generators O
. O

BERT B-MethodName
: O
Pre O
- O
training O
of O
Deep O
Bidirectional O
Transformers O
for O
Language O
Understanding O
. O
Ezequiel O
A O
Di O
Paolo O
, O
Elena O
Clare O
Cuffari O
, O
and O
Hanne O
De O
Jaegher O
. O

ViLBERT B-MethodName
: O
Pretraining O
TaskAgnostic O
Visiolinguistic O
Representations O
for O
Visionand O
- O
Language O
Tasks O
. O

A O
Primer O
in O
BERTology B-MethodName
: O
What O
we O
know O
about O
how O
BERT B-MethodName
works O
. O

The O
system O
uses O
a O
simple O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
based O
encoder O
- O
decoder O
based O
model O
. O

Akin O
to O
machine O
translation O
systems O
, O
this O
system O
uses O
an O
encoder O
- O
decoder O
LSTM B-MethodName
model O
as O
proposed O
by O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

The O
encoder O
is O
a O
bidirectional O
LSTM B-MethodName
, O
while O
the O
decoder O
LSTM B-MethodName
feeds O
into O
a O
softmax O
layer O
for O
every O
character O
position O
in O
the O
target O
string O
. O

The O
major O
change O
in O
our O
model O
is O
the O
size O
of O
LSTM B-MethodName
layers O
which O
is O
kept O
variable O
( O
depending O
on O
vocabulary O
size O
) O
as O
opposed O
to O
ﬁxed O
as O
in O
system O
proposed O
by O
( O
Faruqui O
et O
al O
. O
, O
2016 O
) O
based O
on O
assumption O
that O
bigger O
vocabulary O
would O
require O
bigger O
layers O
to O
extract O
features O
and O
system O
is O
trained O
for O
more O
epochs B-HyperparameterName
. O

Keras O
API O
( O
Chollet O
et O
al O
. O
, O
2015 O
) O
was O
used O
for O
writing O
neural B-MethodName
networks I-MethodName
. O

One O
is O
that O
different O
conﬁgurations O
of O
deep O
neural B-MethodName
networks I-MethodName
work O
well O
for O
different O
languages O
. O

If O
deep O
learning O
is O
augmented O
with O
other O
transduction O
, O
rule O
- O
based O
or O
knowledge O
- O
based O
methods O
, O
the O
results O
for O
low O
- O
size O
could O
perhaps O
be O
improved O
. O
Very O
high O
accuracies B-MetricName
( O
> O
95 B-MetricValue
% O
) O
are O
observed O
for O
some O
languages O
in O
high O
sized O
datasets O
, O
neural B-MethodName
networks I-MethodName
is O
probably O
the O
best O
choice O
for O
processing O
such O
languages O
. O

The O
second O
work O
extends O
this O
idea O
and O
uses O
an O
LSTM B-MethodName
on O
a O
restricted O
form O
of O
the O
argument O
labeling O
task O
. O

The O
main O
contributions O
of O
this O
paper O
are O
1 O
. O
integratingaBiLSTMmodelintotheshallow B-MethodName
discourse O
pipeline O
architecture O
, O
and O
2 O
. O
addressing O
the O
problem O
of O
jointly O
predicting O
connective O
and O
arguments O
with O
a O
movingwindow O
approach O
for O
handling O
overlapping O
relations O
in O
running O
text O
. O

Argument O
labeling O
with O
recurrent O
neural B-MethodName
networkswasdonebyWangetal I-MethodName
. O

Thus O
, O
the O
main O
inspiration O
for O
our O
approach O
is O
the O
recent O
work O
of O
Hooda O
and O
Kosseim O
( O
2017 O
) O
, O
who O
use O
an O
LSTM B-MethodName
network O
on O
spans O
of O
text O
for O
labeling O
arguments O
. O
Theauthorsexaminetheirapproachonpreextracted O
argument O
spans O
where O
the O
size O
of O
spans O
is O
determined O
by O
the O
maximal O
argument O
pair O
distance O
. O
Thisshowsthefeasibilityofneuralnetworks O
to O
label O
discourse O
arguments O
in O
a O
restricted O
problem O
setting O
. O

Speciﬁcally O
, O
it O
operatesonawindowofwords O
, O
whichwebuildaround O
a O
connective O
that O
we O
assume O
to O
have O
been O
identiﬁedbyapreviousmoduleinthepipeline O
. O
Then O
, O
for O
each O
token O
the O
prediction O
task O
is O
deﬁned O
as O
a O
fourway O
classiﬁcation O
problem O
, O
i.e. O
, O
the O
label O
is O
one O
word O
windowBiLSTMDense B-MethodName
( O
relu)Dropout O
( O
0.2 O
) O
Embedding O
( O
Glove)Dense O
( O
softmax)label O
windowFigure3 O
: O
BidirectionalLSTMmodelarchitecture B-MethodName
. O
First O
, O
tokensinawindowareprocessedsequentiallyusingthe O
recurrent O
network O
. O

Because O
of O
the O
small O
size O
of O
the O
PDTB O
corpus O
, O
we O
use O
pretrained O
wordembeddings O
, O
andthesearefurtherprocessed O
withabidirectionalLongShort O
- O
TermMemorynetwork O
( O
BiLSTM B-MethodName
) O
. O

LSTMs B-MethodName
have O
shown O
better O
performancecomparedtosimplerecurrentneuralnetworks O
due O
to O
their O
higher O
capacity O
for O
storing O
important O
information O
over O
longer O
distances O
. O

For O
our O
BiLSTMmodel B-MethodName
, O
the O
hidden O
states O
are O
processed O
independently O
by O
a O
dense O
layer O
and O
ﬁnally O
aregiventothetopoutputlayer(seeFigure3).Between O
the O
two O
dense O
layers O
, O
an O
additional O
dropout B-HyperparameterName
layer O
is O
used O
for O
better O
generalization O
. O

Each O
LSTM B-MethodName
has O
a O
hidden B-HyperparameterName
layer I-HyperparameterName
of O
size O
512 B-HyperparameterValue
. O

The O
output O
of O
each O
LSTM B-MethodName
is O
concatenated O
per O
step O
, O
thus O
, O
the O
output O
of O
the O
BiLSTM B-MethodName
results O
in O
1024 O
dimensions O
per O
step O
. O

The O
dense O
layer O
with O
ReLU O
activation B-HyperparameterName
function O
on O
top O
of O
the O
BiLSTM B-MethodName
has O
64 B-HyperparameterValue
dimensions O
, O
and O
the O
dropout B-HyperparameterName
works O
with O
a O
0.2 B-HyperparameterValue
probability O
. O

In O
our O
experiments O
, O
we O
use O
global O
vectors O
for O
word O
representations O
( O
GloVe B-MethodName
) O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
similar O
as O
done O
by O
Hooda O
and O
Kosseim O
( O
2017 O
) O
. O

In O
general O
, O
GloVe B-MethodName
embeddings O
have O
yielded O
promising O
results O
for O
a O
varietyofrelatedtasks O
. O
Hence O
, O
inthepresentstudy O
, O
we O
use O
GloVe B-MethodName
without O
comparatively O
evaluating O
diﬀerent O
pretrained O
embeddings O
. O

This O
means B-MetricName
that O
although O
two O
words O
look O
similar O
in O
terms O
of O
characters O
, O
they O
might O
have O
diﬀerent O
meanings B-MetricName
depending O
on O
the O
words O
in O
their O
context O
. O
Asaconsequence O
, O
wefollowtheideaofusing O
LSTMsontopofthewordembeddingsinorderto B-MethodName
account O
for O
the O
context O
in O
the O
individual O
representation O
. O

6 O
Conclusions O
and O
Future O
Work O
In O
this O
work O
, O
we O
integrate O
a O
BiLSTM B-MethodName
model O
in O
the O
shallow O
discourse O
parsing O
framework O
. O

Our O
system O
is O
based O
on O
an O
attentional O
sequence O
- O
to O
- O
sequence O
neural O
network O
model O
using O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
cells O
, O
with O
joint O
training O
of O
morphological O
inﬂection O
and O
the O
inverse O
transformation O
, O
i.e. O
lemmatization O
and O
morphological O
analysis O
. O

3 O
Method O
Our O
system O
is O
based O
on O
the O
attentional O
sequenceto O
- O
sequence O
model O
of O
Bahdanau O
et O
al O
. O
( O
2014 O
) O
with O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
cells O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
and O
variational O
dropout B-HyperparameterName
Gal O
and O
Ghahramani O
( O
2016 B-HyperparameterValue
) O
. O

( O
lemmatizer O
) O
model O
has O
separate O
parameters O
, O
except O
the O
embeddings O
, O
but O
is O
structurally O
identical O
except O
for O
two O
details O
: O
instead O
of O
passing O
the O
morphological O
feature O
information O
to O
the O
decoder O
( O
via O
a O
single O
fully O
connected O
layer O
) O
, O
we O
predict O
the O
features O
from O
the O
ﬁnal O
state O
of O
the O
encoder O
LSTM B-MethodName
( O
via O
a O
separate O
fully O
connected O
layer O
) O
. O

4 O
Model O
conﬁguration O
For O
the O
ofﬁcial O
submission O
, O
we O
use O
128 O
LSTM B-MethodName
cells O
for O
the O
( O
unidirectional O
) O
encoder O
, O
decoder O
, O
attention O
mechanism O
, O
character O
embeddings O
, O
as O
well O
as O
for O
the O
fully O
connected O
layers O
for O
morphological O
features O
encoding O
/ O
prediciton O
. O

6 O
Conclusions O
We O
implemented O
a O
system O
using O
an O
attentional O
sequence O
- O
to O
- O
sequence O
model O
with O
Long O
ShortTerm O
Memory O
( O
LSTM B-MethodName
) O
cells O
. O

2 O
Background O
2.1 O
Autoregressive O
models O
( O
AM O
) O
These O
are O
currently O
the O
standard O
for O
neural O
seq2seq O
processing O
, O
with O
such O
representatives O
as O
RNN O
/ O
LSTMs B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Sutskever O
et O
al O
. O
, O
2014 O
) O
, O
ConvS2S O
( O
Gehring O
et O
al O
. O
, O
2017 O
) O
, O
Transformer O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
) O
. O

( O
Andor O
et O
al O
. O
, O
2016 O
) O
consider O
transition O
- O
based O
neural B-MethodName
networks I-MethodName
, O
and O
contrast O
local O
to O
global O
normalization O
of O
decision O
sequences O
, O
showing O
how O
the O
global O
approach O
avoids O
the O
label O
bias O
problem O
in O
such O
tasks O
as O
tagging O
or O
parsing O
. O

A O
simple O
formal O
example O
is O
to O
consider O
one O
real O
- O
valued O
( O
non O
binary O
) O
feature O
for O
the O
length O
, O
and O
one O
for O
the O
square O
of O
the O
length O
, O
an O
experiment7.3 O
Implementation O
aspects O
7.3.1 O
Autoregressive O
models O
The O
AMs O
are O
implemented O
in O
PyTorch4(Paszke O
et O
al O
. O
, O
2017 O
) O
using O
a O
2 O
- O
layered O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
with O
hidden O
- O
state O
size O
200 O
. O

These O
LSTMs B-MethodName
are O
optimized O
with O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
, O
with O
learning B-HyperparameterName
rate I-HyperparameterName
↵ O
= O
0.001 B-HyperparameterValue
, O
and O
that O
we O
did O
recently O
but O
do O
not O
report O
here O
; O
by O
matching O
the O
data O
expectations O
of O
these O
two O
additional O
features O
, O
the O
model O
is O
able O
to O
represent O
the O
mean B-MetricName
and O
variance O
of O
length O
in O
the O
data O
. O

Therefore O
, O
we O
seek O
to O
jointly O
learn O
POS B-TaskName
tagging O
and O
dependency O
parsing O
. O
As O
Long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
networks O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
have O
shown O
signiﬁcant O
representational O
effectiveness O
to O
a O
wide O
range O
of O
NLP O
tasks O
, O
we O
leverage O
bidirectional O
LSTMs B-MethodName
( O
BiLSTM B-MethodName
) O
to O
learn O
shared O
representations O
for O
both O
POS B-TaskName
tagging O
and O
dependency O
parsing O
. O

In O
our O
system O
, O
we O
follow O
the O
bidirectional O
LSTM B-MethodName
- O
CNN O
architecture O
( O
BiLSTMCNNs B-MethodName
) O
( O
Chiu O
and O
Nichols O
, O
2016 O
; O
Ma O
and O
Hovy O
, O
2016 O
) O
, O
where O
CNNs O
encode O
word O
information O
into O
character O
- O
level O
representation O
and O
BiLSTM B-MethodName
models O
context O
information O
of O
each O
word O
. O

The O
tagger O
uses O
a O
BiLSTM B-MethodName
over O
the O
concatenation O
of O
word O
embeddings O
and O
character O
embeddings O
: O
spos O
i O
= O
BiLSTMpos(ew B-MethodName
i O
 O
ec O
i O
) O
Then O
we O
calculate O
the O
probability O
of O
tag O
for O
each O
type O
using O
afﬁne O
classiﬁers O
as O
follows O
: O
hpos O
i O
= O
MLPpos(spos O
i O
) O
rpos O
i O
= O
Wposhpos O
i+bpos O
ypos O
i O
= O
argmax(ri O
) O
The O
tag O
classiﬁer O
is O
trained O
jointly O
using O
crossentropy B-MetricName
losses O
that O
are O
summed O
together O
with O
the O
dependency O
parser O
loss O
during O
optimization O
. O

Context O
- O
sensitive O
Representation O
In O
order O
to O
integrate O
contextual O
information O
, O
we O
concatenate O
the O
character O
embedding O
ec O
, O
pre O
- O
trained O
word O
embeddingewand O
UPOS B-TaskName
tag O
embedding O
epos O
, O
then O
feed O
them O
into O
the O
BiLSTM B-MethodName
. O

We O
take O
the O
bidirectional O
vectors O
at O
the O
ﬁnal O
layer O
as O
the O
contextsensitive O
representation O
: O
  O
! O
si O
= O
LSTMforward B-MethodName
( O
ew O
i O
 O
ec O
i O
 O
epos O
i O
) O
 O
 si O
= O
LSTMbackward B-MethodName
( O
ew O
i O
 O
ec O
i O
 O
epos O
i O
) O
si=  O
! O
si O
 O
 si O
Notably O
, O
we O
use O
the O
UPOS B-TaskName
tag O
from O
the O
output O
of O
our O
POS B-TaskName
tagging O
model O
. O
RepresentationLSTM B-MethodName
LSTMLSTM B-MethodName
LSTMLSTM B-MethodName
LSTMLSTM B-MethodName
LSTM+ B-MethodName
+ O
+ O
+ O
┴ O
. O

The O
encoder O
based O
on O
BiLSTM B-MethodName
- O
CNNs O
architecture O
takes O
the O
sequence O
of O
tokens O
and O
their O
POS B-TaskName
tags O
as O
input O
, O
then O
encodes O
it O
into O
encoder O
hidden O
state O
si O
. O

The O
parser O
consists O
of O
a O
bidirectional O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
encoder O
and O
a O
stack O
- O
LSTM B-MethodName
( O
Dyer O
et O
al O
. O
, O
2015 O
) O
decoder O
, O
introduced O
as O
follows O
. O

Bidirectional O
- O
LSTM B-MethodName
Encoder O
The O
bidirectional O
LSTM B-MethodName
encodes O
a O
variable O
- O
length O
utterance O
x= O
( O
x1;;xn)into O
a O
list O
of O
token O
representations O
[ O
h1;;hn O
] O
, O
where O
each O
representation O
is O
the O
concatenation O
of O
the O
corresponding O
forward O
and O
backward O
LSTM B-MethodName
states O
. O

Stack O
- O
LSTM B-MethodName
Decoder O
After O
the O
utterance O
is O
encoded O
, O
the O
logical O
form O
is O
generated O
with O
a O
stackLSTM B-MethodName
decoder O
. O

The O
stack O
- O
LSTM B-MethodName
makes O
two O
types O
of O
updates O
based O
on O
the O
functions O
it O
predicts O
: O
Update-1 O
: O
when O
a O
Class-1 O
orClass-2 O
function O
is O
called O
, O
a O
non O
- O
terminal O
or O
terminal O
tokenltwill O
be O
generated O
, O
At O
this O
point O
, O
the O
stack O
- O
LSTM B-MethodName
state O
, O
denoted O
by O
gt O
, O
is O
updated O
from O
its O
older O
state O
gt 1as O
in O
an O
ordinary O
LSTM B-MethodName
: O
gt O
= O
LSTM B-MethodName
( O
lt;gt 1 O
) O
( O
1)The O
new O
state O
is O
additionally O
pushed O
onto O
the O
stack O
marking O
whether O
it O
corresponds O
to O
a O
non O
- O
terminal O
or O
terminal O
. O

Update-2 O
: O
when O
the O
reduce O
function O
is O
called O
( O
Class-3 O
) O
, O
the O
states O
of O
the O
stack O
- O
LSTM B-MethodName
are O
recursively O
popped O
from O
the O
stack O
until O
a O
non O
- O
terminal O
is O
encountered O
. O

This O
nonterminal O
state O
is O
popped O
as O
well O
, O
after O
which O
the O
stack O
- O
LSTM B-MethodName
reaches O
an O
intermediate O
state O
denoted O
bygt 1 O
: O
t. O
At O
this O
point O
, O
we O
compute O
the O
representation O
of O
the O
completed O
subtree O
zt O
as O
: O
zt O
= O
Wz[pz O
: O
cz O
] O
( O
2 O
) O
wherepzdenotes O
the O
parent O
( O
non O
- O
terminal O
) O
embedding O
of O
the O
subtree O
, O
and O
czdenotes O
the O
average B-MetricName
embedding O
of O
the O
children O
( O
terminals O
or O
already O
- O
completed O
subtrees O
) O
. O

Finally O
, O
ztserves O
as O
input O
for O
updatinggt 1 O
: O
ttogt O
: O
gt O
= O
LSTM B-MethodName
( O
zt;gt 1 O
: O
t O
) O
( O
3 O
) O
Prediction O
At O
each O
time O
step O
of O
the O
decoding O
, O
the O
parser O
ﬁrst O
predicts O
a O
subsequent O
function O
ft+1 O
conditioned O
on O
the O
decoder O
state O
gtand O
the O
encoder O
states O
h1hn O
. O

Stack O
- O
LSTM B-MethodName
Encoder O
To O
reconstruct O
utterancex O
, O
logical O
form O
yis O
ﬁrst O
encoded O
with O
a O
stackLSTM B-MethodName
encoder O
. O

1In O
Section O
2.1 O
, O
we O
used O
a O
different O
notation O
for O
the O
output O
distribution O
of O
the O
semantic O
parser O
as O
p(yjx).The O
stack O
- O
LSTM B-MethodName
sequentially O
processes O
the O
functions O
and O
updates O
its O
states O
based O
on O
the O
class O
of O
each O
function O
, O
following O
the O
same O
principle O
( O
Update-1 O
andUpdate-2 O
) O
described O
in O
Section O
2.1 O
. O

We O
save O
a O
list O
of O
terminal O
, O
non O
- O
terminal O
and O
subtree O
representations O
[ O
g1;;gs O
] O
, O
where O
each O
representation O
is O
the O
stack O
- O
LSTM B-MethodName
state O
at O
the O
corresponding O
time O
step O
of O
encoding O
. O

LSTM B-MethodName
Decoder O
Utterancexis O
reconstructed O
with O
a O
standard O
LSTM B-MethodName
decoder O
attending O
to O
tree O
nodes O
and O
subtree O
representations O
. O

6.2 O
Training O
Across O
training O
regimes O
, O
the O
dimensions O
of O
word O
vector O
, O
logical O
form O
token O
vector O
, O
and O
LSTM B-MethodName
hidden O
states O
( O
for O
the O
semantic O
parser O
and O
the O
inverse O
parser O
) O
are O
50 O
, O
50 O
, O
and O
150 O
, O
respectively O
. O

We O
used O
oneLSTM B-MethodName
layer O
in O
the O
forward O
and O
backward O
directions O
. O

Our O
results O
suggest O
that O
neural B-MethodName
networks I-MethodName
are O
powerful O
tools O
for O
generating O
candidate O
logical O
forms O
in O
a O
weakly O
- O
supervised O
setting O
, O
due O
to O
their O
ability O
of O
encoding O
and O
utilizing O
sentential O
context O
and O
generation O
history O
. O

In O
CLIP O
, O
the O
authors O
train O
the O
joint O
embedding O
space O
of O
a O
visual O
network O
( O
hereafter O
called O
simply O
CLIP O
) O
and O
a O
language O
network O
( O
hereafter O
called O
CLIP O
- O
T O
) O
using O
contrastive O
learning O
on O
400 O
M O
imagecaption O
pairs O
. O

Finally O
, O
we O
also O
use O
two O
text O
- O
only O
language O
models O
, O
GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
in O
our O
feature O
- O
space O
comparisons O
. O

In O
a O
similar O
way O
, O
the O
language O
stream O
of O
the O
CLIP O
model O
( O
CLIP O
- O
T O
) O
can O
be O
treated O
as O
a O
third O
language O
model O
for O
our O
comparisons O
. O

Then O
, O
multimodal O
models O
stand O
a O
bit O
further O
( O
CLIP O
, O
TSM O
, O
VirTex O
, O
ICMLM O
) O
; O
and O
ﬁnally O
, O
CLIP O
- O
T O
and O
the O
language O
models O
( O
BERT B-MethodName
, O
GPT2 B-MethodName
) O
are O
the O
furthest O
away O
. O

Finally O
, O
thelanguage O
models O
( O
BERT B-MethodName
, O
GPT2 B-MethodName
and O
CLIP O
- O
T O
) O
are O
separated O
from O
the O
rest O
, O
along O
a O
distinct O
direction O
. O

To O
test O
this O
, O
for O
each O
vision O
model O
, O
we O
collect O
the O
ImageNet O
features O
for O
each O
image O
class O
, O
and O
train O
a O
standard O
word O
embedding O
( O
Skip O
- O
Gram O
method O
) O
while O
constraining O
the O
class O
label O
words O
to O
these O
visual O
feature O
vectors O
. O

5.1 O
Method O
Architecture O
We O
train O
Skip O
- O
Gram O
models O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
on O
Wikipedia O
using O
the O
Gensim O
library O
( O
ˇReh˚ O
u O
ˇrek O
and O
Sojka O
, O
2010 O
) O
. O

Embedding O
dimension O
Since O
the O
vision O
models O
do O
not O
all O
share O
the O
same O
feature O
dimensions O
, O
in O
order O
to O
compare O
all O
Skip O
- O
Gram O
models O
, O
we O
reduce O
the O
dimensionality O
of O
the O
feature O
spaces O
of O
all O
vision O
models O
to O
300 O
dimensions O
using O
a O
PCA B-MethodName
. O

The O
PCA B-MethodName
is O
computed O
using O
the O
visual O
features O
of O
all O
images O
in O
the O
ImageNet O
validation O
set O
. O

Consequently O
, O
the O
Skip O
- O
Gram O
word O
embeddings O
are O
trained O
with O
300 O
dimensions O
. O

Training O
We O
train O
the O
Skip O
- O
Gram O
models O
for O
5 O
epochs B-HyperparameterName
, O
using O
the O
standard O
negative O
sampling O
strategy O
. O

5.2 O
Evaluation O
We O
evaluate O
our O
Skip O
- O
Gram O
embeddings O
on O
two O
tasks O
: O
word O
analogies O
and O
word O
pair O
similarities O
. O

5.3 O
Results O
The O
baseline O
Skip O
- O
Gram O
produces O
the O
best O
word O
embeddings O
overall O
( O
black O
bars O
in O
Fig O
10 O
) O
. O

We O
use O
these O
visual O
feature O
vectors O
as O
class O
prototypes O
and O
evaluate O
the O
corresponding O
nearest O
- O
neighbor O
classiﬁcation O
accuracy B-MetricName
on O
the O
ImageNet O
validation O
set5with O
a O
method O
similar O
to O
4We O
here O
test O
the O
300d O
vectors O
after O
the O
PCA B-MethodName
dimensionality O
reduction O
. O

The O
Baseline O
is O
a O
vanilla O
Skip O
- O
Gram O
model O
( O
300 O
dimensions O
) O
where O
all O
20,456 O
word O
embeddings O
are O
free O
to O
be O
learned O
. O

In O
the O
last O
ﬁve O
years O
, O
systems O
based O
on O
artiﬁcial O
neural B-MethodName
networks I-MethodName
( O
NMT O
) O
have O
become O
the O
new O
state O
of O
the O
art O
. O

LSTM B-MethodName
Embeddings O
The O
ﬁnal O
output O
state O
of O
an O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
is O
used O
as O
the O
referring O
expression O
embedding O
. O

BiLSTM B-MethodName
Embeddings O
For O
the O
bidirectional O
LSTM B-MethodName
, O
forward O
and O
backward O
LSTMs B-MethodName
are O
run O
over O
the O
input O
sequence O
, O
and O
their O
outputs O
for O
each O
word O
are O
concatenated O
. O

Attention O
Embeddings O
Attention O
encoders O
( O
Lin O
et O
al O
. O
, O
2017 O
) O
learn O
a O
weighted B-HyperparameterName
average B-MetricName
of O
BiLSTM B-MethodName
outputs O
as O
the O
referring O
expression O
representation O
. O
They O
output O
an O
attention O
score B-MetricName
that O
is O
used O
to O
compute O
a O
weighted B-HyperparameterName
average B-MetricName
of O
the O
BiLSTM B-MethodName
outputs O
. O

InferSent O
is O
similar O
to O
the O
BiLSTM B-MethodName
model O
but O
was O
trained O
using O
the O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
task O
with O
the O
intuition O
that O
this O
task O
would O
require O
the O
sentence O
embeddings O
to O
contain O
semantically O
meaningful B-MetricName
information O
. O

We O
evaluate O
the O
LSTM B-MethodName
, O
BiLSTM B-MethodName
- O
Max O
, O
Attention O
, O
and O
InferSent O
Encoders O
. O

We O
use O
GloVe B-MethodName
vectors O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
a O
pretrained O
VGG19 O
model O
to O
extract O
visual O
features O
( O
Simonyan O
and O
Zisserman O
, O
2015 O
) O
. O

4.4 O
) O
SAME O
USERACROSS O
USERS0 O
1 O
All O
0 O
1 O
All O
0 O
1 O
2 O
All O
57 O
% O
43 O
% O
56 O
% O
44 O
% O
49 O
% O
33 O
% O
15 O
% O
Vision O
64.2 O
66.1 O
65.0 O
59.6 O
60.6 O
60.0 O
32.1 O
32.1 O
29.4 O
31.5 O
31.5UnsupervisedWord O
Overlap O
3.5 O
74.2 O
34.1 O
4.0 O
72.6 O
34.3 O
5.3 O
85.4 O
87.3 O
37.6 O
58.1 O
Word O
Averaging O
3.5 O
69.6 O
32.1 O
4.0 O
67.8 O
32.2 O
5.3 O
77.8 O
80.2 O
42.7 O
51.8 O
Paragram O
Phrase O
3.5 O
71.3 O
32.8 O
4.0 O
71.3 O
33.7 O
5.3 O
81.1 O
83.7 O
44.3 O
56.9 O
InferSent O
8.8 O
73.8 O
36.9 O
9.2 O
73.6 O
37.6 O
5.9 O
85.0 O
85.7 O
46.3 O
47.7SupervisedLSTM B-MethodName
28.2 O
41.1 O
33.8 O
28.2 O
40.3 O
33.5 O
6.0 O
69.4 O
63.9 O
37.2 O
46.8 O
BiLSTM B-MethodName
- O
Max O
30.0 O
60.8 O
43.3 O
27.0 O
57.8 O
40.6 O
7.0 O
75.7 O
74.6 O
41.8 O
53.1 O
Attention O
29.2 O
61.6 O
43.2 O
26.8 O
56.0 O
39.7 O
10.1 O
67.0 O
63.9 O
38.4 O
47.4 O
InferSent O
27.9 O
70.5 O
46.3 O
28.8 O
68.6 O
46.4 O
5.8 O
82.5 O
85.3 O
45.4 O
61.3JointAddition O
65.3 O
69.4 O
67.1 O
62.0 O
62.3 O
62.1 O
22.3 O
63.8 O
60.3 O
42.6 O
48.7 O
Concatenation O
68.6 O
71.3 O
69.8 O
58.0 O
70.7 O
63.6 O
19.0 O
84.7 O
86.1 O
52.7 O
62.7 O
Table O
2 O
: O
Grounding O
accuracy B-MetricName
under O
different O
conditions O
. O

So O
as O
to O
make O
our O
evaluation O
more O
robust O
, O
we O
run O
the O
above O
experiments O
for O
three O
popular O
embedding O
methods O
, O
using O
large O
pre O
- O
trained O
models O
released O
by O
their O
respective O
authors O
as O
follows O
: O
Word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
is O
the O
original O
implementation O
of O
the O
CBOW B-MethodName
and O
skip O
- O
gram O
architectures O
that O
popularized O
neural O
word O
embeddings O
. O

Glove O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
is O
a O
global O
logbilinear O
regression O
model O
to O
train O
word O
embeddings O
designed O
to O
explicitly O
enforce O
the O
model O
properties O
needed O
to O
solve O
word O
analogies O
. O

Speciﬁcally O
, O
it O
outperforms O
a O
strong O
RoBERTa B-MethodName
- O
based O
baseline O
by O
2.4 O
- O
3.0 O
folds O
in O
recall@1 B-MetricName
. O

3.1 O
Process O
Representation O
We O
use O
the O
ofﬁcially O
released O
RoBERTa B-MethodName
- O
base O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
for O
representations O
of O
event O
processes O
. O

RoBERTa B-MethodName
improves O
the O
original O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
with O
a O
modiﬁed O
training O
procedure O
. O

Then O
those O
contents O
of O
all O
primitive O
events O
in O
Pare O
sequentially O
concatenated O
, O
while O
the O
separator O
token O
of O
RoBERTa B-MethodName
< O
/s O
> O
is O
added O
between O
the O
contents O
of O
every O
consecutive O
two O
events O
. O

Then O
a O
RoBERTa B-MethodName
lan O
- O
guage O
model O
captures O
the O
event O
process O
, O
and O
separatelygenerates O
gloss O
- O
based O
representations O
for O
positive O
andnegative O
sampled O
labels O
. O

3.3 O
Learning O
Objective O
Let(P O
, O
A O
, O
O O
) O
be O
a O
process O
Pdenoted O
by O
action O
and O
object O
labels O
AandO O
, O
our O
model O
captures O
the O
semantic O
associations O
between O
a O
RoBERTa B-MethodName
encoded O
process O
Pand O
label O
glosses O
SAandSOby O
optimizing O
a O
ranking O
task O
objective O
. O

Besides O
one O
based O
on O
RoBERTa B-MethodName
( O
marked O
as O
S2LRoBERTa B-MethodName
) O
, O
the O
two O
others O
are O
the O
BiGRU O
encoder O
( O
S2L O
- O
BiGRU O
) O
and O
mean B-MetricName
- O
pooling O
encoder O
( O
S2Lmean B-MetricName
) O
with O
Skip O
- O
Gram O
word O
embeddings O
used O
by O
Rashkin O
et O
al O
. O
( O
2018 B-MetricValue
) O
. O

Note O
that O
to O
train O
S2L O
models O
, O
the O
original O
paper O
uses O
an O
cross O
- O
entropy B-MetricName
lossType O
axes O
Action O
Object O
Metrics O
MRR B-MetricName
recall@1 B-MetricName
recall@10 B-MetricName
MRR B-MetricName
recall@1 B-MetricName
recall@10 B-MetricName
S2L O
- O
mean B-MetricName
- O
pool O
3.72 B-MetricValue
1.96 O
5.95 O
1.01 O
0.80 O
1.66 O
S2L O
- O
BiGRU O
7.94 O
4.40 O
12.71 O
4.20 O
2.72 O
6.19 O
S2L O
- O
RoBERTa B-MethodName
8.36 O
5.31 O
14.69 O
4.88 O
3.24 O
8.10 O
Single O
P2GT O
-MFS O
( O
partial O
event O
) O
18.03 O
14.36 O
17.16 O
10.36 O
6.37 O
17.64 O
Single O
P2GT O
-WSD O
( O
partial O
event O
) O
18.07 O
14.05 O
17.82 O
10.72 O
6.68 O
18.03 O
Single O
P2GT O
-MFS O
24.10 O
19.67 O
32.40 O
13.71 O
8.86 O
23.09 O
Single O
P2GT O
-WSD O
25.83 O
19.93 O
37.50 O
14.19 O
9.32 O
24.84 O
JointP2GT O
-MFS O
28.57 O
20.63 O
43.14 O
15.26 O
10.62 O
25.01 O
JointP2GT O
-WSD O
29.11 O
21.21 O
42.84 O
15.70 O
11.07 O
25.51 O
Table O
1 O
: O
Results O
( O
in O
percentage O
) O
for O
multi O
- O
axis O
event O
process O
typing O
. O

For O
the O
training O
setting O
with O
WSD O
, O
we O
use O
the O
BERT B-MethodName
- O
NN O
model O
( O
Hadiwinoto O
et O
al O
. O
, O
2019 O
) O
, O
which O
is O
one O
of O
the O
SOTA O
WSD O
methods O
that O
is O
trained O
on O
the O
SemCor O
corpus O
( O
Langone O
et O
al O
. O
, O
2004 O
) O
. O

The O
results O
by O
the O
S2L O
baseline O
methods O
show O
that O
incorporating O
pre O
- O
trained O
RoBERTa B-MethodName
offers O
noticeably O
better O
performance O
than O
other O
encoding O
techniques O
. O

When O
typing O
the O
action O
, O
with O
the O
same O
representation O
of O
event O
processes O
, O
P2GT O
supercedes O
S2L O
- O
RoBERTa B-MethodName
with O
an O
absolute O
increase O
ofMRR B-MetricName
by O
15.47 O
% O
( O
ca O
. O

GlossBERT B-MethodName
( O
Huang O
et O
al O
. O
, O
2019 O
) O
thereof O
formalizes O
the O
WSD O
problem O
as O
classifying O
context O
- O
gloss O
pairs O
. O

The O
proposed O
P2GT O
framework O
ﬁne O
- O
tunes O
RoBERTa B-MethodName
to O
capture O
the O
association O
of O
process O
- O
gloss O
pairs O
. O

We O
use O
a O
CRF O
sequence O
tagger O
fromAllenNLP O
( O
Gardner O
et O
al O
. O
, O
2018 O
) O
, O
and O
experiment O
with O
two O
variants O
: O
( O
1 O
) O
NER B-TaskName
- O
M O
ULTILANG O
: O
A O
BiLSTM B-MethodName
CRF O
model O
( O
20 O
K O
parameters O
) O
with O
40 O
dimensional O
multi O
- O
lingual O
word O
embeddings O
( O
Ammar O
et O
al O
. O
, O
2016 O
) O
, O
and O
( O
2 O
) O
NER B-TaskName
- O
L O
INEAR O
: O
A O
linear O
CRF O
model O
which O
was O
originally O
used O
by O
Liu O
et O
al O
. O
( O
2018 O
) O
. O

We O
measure O
the O
MRR B-MetricName
in O
3 B-MetricValue
settings O
: O
( O
1 O
) O
NER B-TaskName
- O
LINEAR O
, O
a O
linear O
CRF O
model O
for O
NER B-TaskName
which O
replicates O
the O
experimental O
settings O
in O
( O
Liuet O
al O
. O
, O
2018 O
) O
, O
where O
LTAL O
works O
, O
( O
2 O
) O
N O
ERMULTILANG O
, O
a O
BiLSTM B-MethodName
- O
CRF O
sequence O
tagger O
from O
AllenNLP O
( O
Gardner O
et O
al O
. O
, O
2018 O
) O
with O
40 O
dimensional O
multi O
- O
lingual O
word O
embeddings O
of O
Ammar O
et O
al O
. O
( O
2016 O
) O
, O
and O
( O
3 O
) O
B O
ASEORACLE O
, O
the O
baseline O
model O
for O
span O
detection O
task O
. O

We O
apply O
our O
metrics O
to O
a O
set O
of O
recent O
neural O
models O
for O
negation O
resolution O
, O
including O
NegBERT B-MethodName
, O
tagging O
- O
based O
, O
and O
dependency O
- O
parsing O
based O
approaches O
, O
providing O
a O
competitive O
range O
of O
baselines O
for O
future O
work O
to O
compare O
with O
. O
Our O
contributions O
are O
as O
follows O
. O

Several O
works O
using O
neural B-MethodName
networks I-MethodName
( O
e.g. O
, O
Fancellu O
et O
al O
. O
, O
2016 O
, O
2018 O
; O
Lazib O
et O
al O
. O
, O
2020 O
) O
train O
BiLSTMs B-MethodName
, O
or O
syntactically O
structured O
BiLSTMs B-MethodName
or O
GCNs O
. O

Recently O
, O
a O
range O
of O
papers O
has O
explored O
BERT B-MethodName
- O
based O
models O
for O
negation O
resolution O
( O
Khandelwal O
and O
Sawant O
, O
2020 O
; O
Khandelwal O
and O
Britto O
, O
2020 O
; O
Britto O
and O
Khandelwal O
, O
2020 O
; O
Shaitarova O
and O
Rinaldi O
, O
2021 O
) O
. O

The O
NegBERT B-MethodName
system O
( O
Khandelwal O
and O
Sawant O
, O
2020 O
; O
Britto O
and O
Khandelwal O
, O
2020 O
) O
, O
which O
we O
run O
using O
XLM O
- O
R O
, O
modiﬁes O
the O
input O
for O
the O
second O
step O
, O
adding O
artiﬁcial O
tokens O
to O
indicate O
cues O
. O

In O
addition O
, O
we O
implement O
a O
BiLSTMTagger B-MethodName
following O
the O
architecture O
proposed O
by O
Fancellu O
et O
al O
. O
( O
2016 O
) O
, O
but O
using O
XLM O
- O
R O
as O
the O
underlying O
language O
model.6Cues O
are O
predicted O
using O
a O
single O
linear O
layer O
with O
softmax O
on O
top O
of O
XLM O
- O
R. O

Scopes O
are O
predicted O
by O
feeding O
, O
for O
each O
negation O
instance O
, O
the O
XLM O
- O
R O
embeddings O
of O
the O
sentences O
concatenated O
with O
a O
cue O
/ O
notcue O
embedding O
to O
a O
single O
- O
layer O
BiLSTM B-MethodName
and O
once O
again O
using O
a O
tokenwise O
linear+softmax O
layer O
for O
classiﬁcation O
. O

In O
addition O
, O
we O
conduct O
experiments O
on O
the O
BioScope O
corpus O
( O
Szarvas O
et O
al O
. O
, O
2008 O
) O
using O
the O
abstracts O
subset O
( O
11,871 O
sentences O
) O
, O
as O
well O
as O
the O
7https://github.com/boschresearch/ O
steps O
- O
parser O
8The O
original O
annotation O
may O
be O
seen O
as O
corresponding O
to O
an O
( O
equivalent O
) O
formalization O
of O
:( O
like(m O
; O
p)_like(s O
; O
p)).Cues O
- O
B O
SCM O
SCM O
- O
B O
( O
NIS O
ex O
) O
ST O
ST O
ST O
NIS O
tok O
NIS O
tok O
NIS O
tok O
System O
F1 B-MetricName
F1 B-MetricName
F1 B-MetricName
P O
R O
F O
1 B-MetricValue
P O
R O
F O
1 O
Punct O
- O
BL O
100.0 O
17.0 O
9.9 O
89.8 O
62.0 O
73.3 O
93.3 O
58.8 O
72.1 O
NegBERT B-MethodName
91.1 O
78.8 O
70.1 O
86.1 O
87.8 O
86.9 O
83.9 O
88.7 O
86.3 O
BiLSTM B-MethodName
- O
Tagger O
92.3 O
79.6 O
70.0 O
90.8 O
85.4 O
88.0 O
90.3 O
86.3 O
88.2 O
DP O
- O
direct O
92.8 O
73.2 O
61.4 O
85.4 O
87.4 O
86.4 O
84.6 O
87.7 O
86.1 O
DP O
- O
nested O
93.3 O
72.8 O
60.6 O
86.6 O
83.0 O
84.8 O
85.6 O
85.4 O
85.5 O
DP O
- O
direct O
- O
synt O
92.8 O
79.1 O
69.2 O
87.4 O
86.5 O
86.9 O
86.2 O
88.3 O
87.2 O
DP O
- O
nested O
- O
synt O
93.4 O
79.2 O
69.2 O
88.0 O
84.6 O
86.2 O
87.2 O
86.6 O
86.9 O
Table O
1 O
: O
Comparison O
of O
Evaluation O
Scores O
on O
when O
training O
and O
testing O
on O
CD O
- O
neg O
. O

Underlined O
: O
Comparison O
NegBERT B-MethodName
vs. O
DP O
- O
direct O
- O
synt O
. O

We O
ran O
the O
NegBERT B-MethodName
system O
for O
end O
- O
to O
- O
end O
negation O
resolution O
using O
the O
original O
code O
, O
but O
our O
evaluation O
scripts.9In O
this O
uniﬁed O
evaluation O
setup O
, O
in O
the O
SCM O
metrics O
, O
we O
can O
only O
see O
that O
the O
DP O
models O
based O
on O
standard O
XLM O
- O
R O
underperform O
; O
all O
other O
9Khandelwal O
and O
Sawant O
( O
2020 O
) O
report O
results O
for O
scope O
resolution O
that O
appear O
to O
be O
based O
on O
gold O
cues.systems O
perform O
roughly O
similarly O
. O

The O
BiLSTM B-MethodName
- O
Tagger O
turns O
out O
to O
be O
the O
most O
accurate O
model O
for O
scope O
resolution O
, O
reﬂected O
similarly O
in O
the O
ST O
and O
the O
NIS O
tokscores B-MetricName
. O

Our O
architecture O
similar O
to O
the O
one O
of O
Fancellu O
et O
al O
. O
( O
2018 O
) O
seems O
to O
outperform O
NegBERT B-MethodName
, O
which O
adds O
artiﬁcial O
tokens O
to O
indicate O
cues.10 O
As O
expected O
to O
some O
extent O
, O
the O
NIS O
tokand O
ST O
scores B-MetricName
are O
similar O
in O
general O
. O

Both O
scores B-MetricName
identify O
the O
BiLSTM B-MethodName
- O
Tagger O
as O
the O
most O
precise O
system O
and O
NegBERT B-MethodName
as O
having O
the O
highest O
recall B-MetricName
, O
with O
the O
Tagger O
achieving O
the O
best O
F O
1 B-MetricValue
. O

For O
example O
, O
ST O
assigns O
the O
same O
summary O
statistic O
( O
F O
1 O
) O
to O
NegBERT B-MethodName
and O
DP O
- O
direct O
- O
synt O
, O
while O
in O
terms O
of O
NIS O
tok O
, O
the O
F O
1of O
DP O
- O
direct O
- O
synt O
is O
more O
than O
1 O
point O
higher O
. O

Comparing O
ST O
and O
NIS O
tokscores B-MetricName
, O
we O
can O
see O
that O
NIS O
tokgenerally O
assigns O
higher O
recall B-MetricName
, O
but O
slightly O
lower O
precision B-MetricName
to O
systems O
such O
as O
NegBERT B-MethodName
or O
DP O
; O
the O
BiLSTMTagger B-MethodName
’s O
precision B-MetricName
drops O
less O
. O

Hence O
, O
the O
ST O
scores B-MetricName
for O
models O
such O
as O
NegBERT B-MethodName
or O
DP O
slightly O
underestimate O
recall B-MetricName
because O
the O
systems O
failed O
more O
often O
on O
longer O
instances O
; O
and O
in O
turn O
they O
slightly O
over O
- O
estimate O
precision B-MetricName
, O
e.g. O
, O
because O
wrongly O
predicted O
instances O
often O
have O
short O
scopes O
. O

Likewise O
, O
NegBERT B-MethodName
scores B-MetricName
were O
produced O
by O
running O
the O
system O
off O
- O
the O
- O
shelf O
without O
optimizing O
the O
cue O
tagger O
separately O
. O

Cues O
- O
B O
NIS O
ex O
ST O
NIS O
tok O
NIS O
tok O
NIS O
tok O
System O
F1 B-MetricName
F1 B-MetricName
F1 B-MetricName
P O
R O
F O
1BioScopePunct O
- O
BL O
100.0 B-MetricValue
43.2 O
65.7 O
77.0 O
69.7 O
73.5 O
NegBERT B-MethodName
90.5 O
79.5 O
84.5 O
83.3 O
91.5 O
87.2 O
BiLSTM B-MethodName
- O
Tagger O
94.3 O
83.0 O
88.6 O
91.7 O
91.7 O
91.7 O
DP O
- O
nested O
- O
synt O
95.4 O
82.7 O
90.4 O
92.7 O
92.1 O
92.4SFUPunct O
- O
BL O
100.0 O
45.9 O
66.5 O
75.5 O
87.4 O
81.0 O
NegBERT B-MethodName
81.9 O
69.5 O
71.0 O
67.9 O
86.3 O
75.9 O
BiLSTM B-MethodName
- O
Tagger O
86.7 O
73.2 O
77.9 O
74.9 O
89.8 O
81.7 O
DP O
- O
nested O
- O
synt O
86.5 O
71.1 O
77.1 O
73.7 O
88.9 O
80.5 O
Table O
2 O
: O
In O
- O
Domain O
Comparison O
of O
Systems O
on O
the O
BioScope O
and O
SFU O
datasets O
. O

Train O
TestCues O
- O
B O
NIS O
ex O
ST O
NIS O
tok O
System O
F1 B-MetricName
F1 B-MetricName
F1 B-MetricName
F1 B-MetricName
NegBERT B-MethodName
Bio O
CD65.2 O
8.9 B-MetricValue
55.2 O
51.1 O
BiLSTM B-MethodName
- O
Tagger O
61.6 O
10.1 O
46.0 O
46.8 O
DP O
- O
nested O
- O
synt O
64.8 O
9.2 O
51.8 O
47.9 O
NegBERT B-MethodName
SFU O
CD69.3 O
9.7 O
56.7 O
53.6 O
BiLSTM B-MethodName
- O
Tagger O
68.8 O
10.1 O
58.0 O
53.8 O
DP O
- O
nested O
- O
synt O
68.1 O
9.6 O
55.2 O
51.6 O
NegBERT B-MethodName
CD O
Bio60.6 O
19.0 O
49.3 O
46.2 O
BiLSTM B-MethodName
- O
Tagger O
64.3 O
17.7 O
56.7 O
54.3 O
DP O
- O
nested O
- O
synt O
65.3 O
18.7 O
54.7 O
50.9 O
NegBERT B-MethodName
SFU O
Bio78.7 O
55.5 O
63.2 O
71.0 O
BiLSTM B-MethodName
- O
Tagger O
81.0 O
53.4 O
65.9 O
72.8 O
DP O
- O
nested O
- O
synt O
82.0 O
57.2 O
67.2 O
74.0 O
NegBERT B-MethodName
CD O
SFU51.3 O
10.0 O
37.2 O
37.5 O
BiLSTM B-MethodName
- O
Tagger O
50.2 O
9.0 O
37.5 O
37.8 O
DP O
- O
nested O
- O
synt O
50.3 O
9.0 O
36.2 O
36.5 O
NegBERT B-MethodName
Bio O
SFU63.4 O
48.8 O
53.7 O
58.6 O
BiLSTM B-MethodName
- O
Tagger O
64.5 O
53.1 O
57.0 O
61.4 O
DP O
- O
nested O
- O
synt O
56.9 O
40.1 O
47.5 O
51.5 O
Table O
3 O
: O
Cross O
- O
Domain O
Comparison O
of O
Systems O
between O
CD O
- O
neg O
, O
BioScope O
, O
and O
SFU O
. O

On O
both O
datasets O
, O
the O
DP O
- O
nested O
- O
synt O
models O
outperform O
NegBERT B-MethodName
. O

On O
BioScope O
, O
the O
parsing O
- O
based O
approach O
clearly O
outperforms O
NegBERT B-MethodName
and O
the O
BiLSTM B-MethodName
- O
Tagger O
; O
on O
SFU O
, O
the O
BiLSTM B-MethodName
- O
Tagger O
performs O
best O
. O

Overall O
, O
the O
BiLSTM B-MethodName
- O
Tagger O
seems O
most O
robust O
. O

However O
, O
the O
NegBERT B-MethodName
system O
performs O
close O
to O
or O
better O
than O
the O
BiLSTM B-MethodName
- O
Taggerwhen O
moving O
from O
another O
to O
the O
ConanDoyle O
- O
neg O
dataset O
, O
and O
the O
DP O
- O
nested O
- O
synt O
model O
has O
an O
advantage O
when O
moving O
from O
SFU O
to O
BioScope O
. O

We O
test O
the O
relation O
module O
on O
the O
SQuAD B-DatasetName
2.0 O
dataset O
using O
both O
the O
BiDAF O
and O
BERT B-MethodName
models O
as O
baseline O
readers O
. O

We O
obtain1:8%gain O
of O
F1 B-MetricName
accuracy B-MetricName
on O
top O
of O
the O
BiDAF O
reader O
, O
and O
1:0%on O
top O
of O
the O
BERT B-MethodName
base O
model O
. O

Highlighted O
words O
are O
the O
output O
from O
the O
BERT B-MethodName
base O
model O
. O

Our O
results O
show O
improvement O
on O
top O
of O
the O
baseline O
BiDAF O
model O
and O
the O
state O
- O
of O
- O
the O
- O
art O
reader O
based O
on O
BERT B-MethodName
, O
on O
the O
SQuAD B-DatasetName
2.0 O
task O
. O

The O
recently O
releasedBERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
greatly O
increased O
the O
F1 B-MetricName
scores B-MetricName
on O
the O
SQuAD B-DatasetName
2.0 B-MetricValue
leaderboard O
. O

BERT B-MethodName
consists O
of O
stacked O
Transformers O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
that O
are O
pre O
- O
trained O
on O
vast O
amounts O
of O
unlabeled O
data O
with O
a O
masked O
language O
model O
. O

BERT B-MethodName
models O
contains O
a O
special O
CLS O
token O
which O
is O
helpful O
for O
the O
SQuAD B-DatasetName
2.0 O
task O
. O

Due O
to O
a O
strong O
masked O
language O
model O
to O
help O
predict O
answers O
and O
a O
strong O
CLS O
token O
to O
encode O
entailment O
, O
BERT B-MethodName
models O
are O
the O
current O
state O
- O
of O
- O
the O
art O
for O
SQuAD B-DatasetName
2.0 O
. O

3.1 O
Augmenting O
Inputs O
Figure O
2 O
shows O
our O
relation O
module O
on O
top O
of O
the O
base O
reader O
BERT B-MethodName
. O

The O
context O
output O
Cfrom O
BERT B-MethodName
is O
projected O
into O
two O
hidden B-HyperparameterName
state I-HyperparameterName
layers O
S O
andE O
, O
whereC O
, O
SandE2RLh O
, O
Lis O
the O
context O
length O
and O
his O
the O
hidden B-HyperparameterName
size I-HyperparameterName
. O

4 O
Question B-TaskName
Answering I-TaskName
Baselines O
We O
test O
the O
relation O
module O
on O
top O
of O
our O
own O
PyTorch O
implementation O
of O
the O
BiDAF O
model O
( O
Seo O
et O
al O
. O
, O
2017 O
) O
, O
as O
well O
as O
the O
recent O
released O
BERT B-MethodName
base O
model O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
for O
the O
SQuAD B-DatasetName
2.0 O
task O
. O

We O
also O
do O
not O
test O
on O
top O
of O
BERT B-MethodName
+ O
Synthetic O
Self O
Training O
( O
Devlin O
, O
2019 O
) O
due O
to O
lack O
of O
computational O
resources O
available O
. O

4.2 O
BERT B-MethodName
BERT B-MethodName
is O
a O
masked O
language O
model O
pre O
- O
trained O
on O
large O
amounts O
of O
data O
that O
is O
the O
core O
component O
of O
all O
of O
the O
current O
state O
- O
of O
- O
the O
- O
art O
models O
on O
the O
SQuAD B-DatasetName
2.0 O
task O
. O

The O
input O
to O
BERT B-MethodName
is O
the O
concatenation O
of O
a O
question O
and O
context O
pair O
in O
the O
form O
of O
[ O
“ O
CLS O
” O
; O
question O
; O
“ O
SEP O
” O
; O
context O
; O
“ O
SEP O
” O
] O
. O

BERT B-MethodName
comes O
with O
its O
own O
special O
“ O
CLS O
” O
token O
, O
which O
is O
pre O
- O
trained O
on O
a O
next O
sentence O
pair O
objective O
in O
order O
to O
encode O
entailment O
information O
between O
the O
two O
sentences O
during O
the O
pretraining O
scheme O
. O

We O
allow O
gradients O
to O
be O
passed O
through O
all O
layers O
of O
BERT B-MethodName
, O
and O
ﬁnetune O
the O
initialized O
weights B-HyperparameterName
with O
the O
SQuAD B-DatasetName
2.0 O
dataset O
. O

The O
results O
are O
reported O
on O
the O
SQuAD B-DatasetName
2.0 O
development O
set O
. O
Model O
EM(% B-MetricName
) O
F1(% B-MetricName
) O
( O
Clark O
and O
Gardner O
, O
2018 O
) O
61.9 O
64.8 O
Our O
Implementation O
of O
BiDAF O
65.7 O
68.6 O
BiDAF O
+ O
Relation O
Module O
67.7 O
70.4 O
BERT B-MethodName
- O
base O
73.6 O
76.6 O
BERT B-MethodName
- O
base O
+ O
Relation O
Module O
74.2 O
77.6 O
BERT B-MethodName
- O
large O
78.9 O
82.1 O
BERT B-MethodName
- O
large O
+ O
Relation O
Module O
79.2 O
82.6 O
Table O
1 O
: O
Model O
performance O
on O
SQuAD B-DatasetName
2.0 O
development O
set O
averaged B-MetricName
over O
three O
random B-HyperparameterName
seeds I-HyperparameterName
. O

Model O
EM(% B-MetricName
) O
F1(% B-MetricName
) O
BERT B-MethodName
- O
base O
70.7 O
74.4 O
BERT B-MethodName
- O
base O
+ O
Answer O
Veriﬁer O
71.7 O
75.5 O
BERT B-MethodName
- O
base O
+ O
Relation O
Module O
73.2 O
76.8 O
Table O
2 O
: O
SQuAD B-DatasetName
2.0 O
leaderboard O
numbers O
on O
the O
BERT B-MethodName
- O
base O
Models O
. O

Our O
model O
shows O
improvement O
over O
the O
public O
BERT B-MethodName
- O
base O
models O
on O
the O
ofﬁcial O
evaluation O
. O

BERT B-MethodName
comes O
in O
two O
different O
sizes O
, O
a O
BERTbase B-MethodName
model O
( O
comprising O
of O
roughly O
110 O
million O
parameters O
) O
, O
and O
a O
BERT B-MethodName
- O
large O
model O
( O
comprising O
of O
roughly O
340 O
million O
parameters O
) O
. O

We O
use O
the O
BERT B-MethodName
- O
base O
model O
to O
run O
our O
experiments O
due O
to O
the O
limited O
computing O
resources O
that O
training O
the O
BERT B-MethodName
- O
large O
model O
would O
take O
. O

We O
only O
use O
the O
BERT B-MethodName
- O
large O
model O
to O
show O
that O
we O
still O
get O
improvements O
with O
the O
relation O
module O
. O

The O
relation O
module O
on O
top O
of O
the O
BERT B-MethodName
- O
base O
model O
only O
contains O
roughly O
10 O
million O
parameters O
. O

We O
use O
the O
BERT B-MethodName
- O
base O
model O
to O
run O
our O
experiments O
with O
the O
same O
hyper O
- O
parameters O
given O
on O
the O
ofﬁcial O
BERT B-MethodName
GitHub O
repository O
. O

We O
also O
show O
that O
on O
top O
of O
the O
BERT B-MethodName
- O
large O
model O
, O
on O
the O
development O
set O
, O
our O
relation O
module O
still O
obtains O
performance O
gain1 O
. O

1We O
do O
not O
have O
enough O
time O
to O
get O
ofﬁcial O
SQuAD B-DatasetName
2.0 O
evaluation O
results O
for O
the O
large O
BERT B-MethodName
models O
. O
Answerable O
Non O
- O
Answerable O
BERT B-MethodName
- O
base O
81.5 O
78.3 O
+ O
Relation O
Module O
82.1 O
82.1 O
Table O
3 O
: O
Prediction O
accuracies B-MetricName
on O
answerable O
and O
nonanswerable O
questions O
on O
development O
set O
. O

Our O
proposed O
relation O
module O
improves O
the O
overall O
F1 B-MetricName
and O
EM B-MetricName
accuracy B-MetricName
: O
2:0 O
% O
gain O
on O
EM B-MetricName
and O
1:8 O
% O
gain O
on O
F1 B-MetricName
on O
the O
BiDAF O
, O
as O
well O
as O
0:8 O
% O
gain O
on O
EM B-MetricName
and O
1:0 O
% O
gain O
on O
F1 B-MetricName
on O
the O
BERT B-MethodName
- O
base O
model O
. O

The O
module O
obtains O
less O
gain O
( O
0:5%gain O
of O
F1 B-MetricName
) O
on O
BERT B-MethodName
large O
model O
due O
to O
the O
better O
performance O
of O
BERT B-MethodName
large O
model O
. O

Table O
2 O
presents O
performance O
of O
three O
BERTbase B-MethodName
models O
with O
minimum O
additions O
taken O
from O
the O
ofﬁcial O
SQuAD B-DatasetName
2.0 O
leaderboard O
. O

We O
see O
that O
our O
relation O
module O
gives O
more O
gain O
than O
an O
Answer O
Veriﬁer O
on O
top O
of O
the O
BERT B-MethodName
- O
base O
model O
. O

Table O
3 O
compares O
these O
accuracy B-MetricName
numbers O
for O
these O
questions O
with O
and O
without O
the O
relation O
module O
on O
top O
of O
the O
BERT B-MethodName
- O
base O
model O
. O

6 O
Ablation O
Study O
We O
conduct O
an O
ablation O
study O
to O
show O
how O
different O
components O
of O
the O
relation O
module O
affects O
the O
overall O
performance O
for O
the O
BERT B-MethodName
- O
base O
model O
. O

First O
we O
test O
only O
adding O
plausible O
answers O
on O
top O
of O
the O
BERT B-MethodName
- O
base O
model O
, O
in O
order O
to O
quantify O
the O
gain O
in O
span O
prediction O
that O
adding O
these O
extra O
answers O
in O
would O
give O
. O

We O
show O
that O
with O
just O
adding O
plausible O
answers O
, O
the O
average B-MetricName
of O
theModel O
EM(% B-MetricName
) O
F1(% B-MetricName
) O
BERT B-MethodName
- O
base O
73.6 B-MetricValue
76.6 O
BERT B-MethodName
- O
base+Plausible O
Answers O
73.5 O
76.9 O
BERT B-MethodName
- O
base+RM O
- O
Plausible O
Answers O
73.6 O
76.9 O
BERT B-MethodName
- O
base+RM O
( O
4 O
heads O
) O
74.1 O
77.4 O
BERT B-MethodName
- O
base+RM O
( O
16 O
heads O
) O
74.2 O
77.6 O
BERT B-MethodName
- O
base+RM O
( O
64 O
heads O
) O
74.0 O
77.2 O
Table O
4 O
: O
Ablation O
study O
on O
our O
Relation O
Module O
. O

This O
gain O
in O
F1 B-MetricName
is O
due O
to O
the O
BERT B-MethodName
layers O
being O
ﬁne O
- O
tuned O
on O
more O
answer O
span O
data O
that O
we O
provide O
. O

We O
feed O
the O
output O
of O
our O
BERT B-MethodName
- O
base O
model O
directly O
into O
the O
object O
extractor O
and O
subsequently O
to O
the O
relation O
network O
. O

In O
Example O
1 O
, O
the O
BERT B-MethodName
- O
base O
model O
incorrectly O
outputs O
“ O
Northridge O
earthquake O
” O
( O
in O
red O
) O
as O
the O
answer O
. O

The O
BERTbase B-MethodName
model O
incorrectly O
outputs O
“ O
input O
encoding O
” O
( O
in O
red O
) O
as O
its O
prediction O
, O
while O
adding O
our O
relation O
module O
on O
the O
BERT B-MethodName
- O
base O
model O
predicts O
correctly O
that O
the O
question O
is O
not O
answerable O
. O

Our O
results O
on O
the O
SQuAD B-DatasetName
2.0 O
dataset O
using O
the O
relation O
module O
on O
both O
BiDAF O
and O
BERT B-MethodName
models O
show O
improvements O
from O
the O
relation O
module O
. O

Compared O
to O
classical O
transformers O
, O
pre O
- O
trained O
transformers O
such O
as O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018)and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
capture O
rich O
world O
and O
linguistic O
knowledge O
from O
large O
- O
scale O
external O
corpora O
, O
and O
signiﬁcant O
improvements O
are O
obtained O
by O
ﬁne O
- O
tuning O
these O
pre O
- O
trained O
models O
on O
a O
variety O
of O
downstream B-TaskName
tasks I-TaskName
. O

We O
follow O
this O
promising O
direction O
by O
ﬁne O
- O
tuning O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
) O
on O
a O
target O
task O
. O

GPT B-MethodName
takes O
as O
input O
Xiand O
produces O
the O
ﬁnal O
hidden O
state O
hM O
iof O
the O
last O
token O
in O
Xi O
, O
which O
is O
further O
fed O
into O
a O
linear O
layer O
followed O
by O
a O
softmax O
layer O
to O
generate O
the O
probability O
: O
Pi O
= O
exp(WyhM O
i)P O
1iNexp(WyhM O
i)(2 O
) O
where O
Wyis O
weight B-HyperparameterName
matrix O
of O
the O
output O
layer O
. O

As O
shown O
in O
Figure O
2 O
, O
during O
training O
, O
our O
evidence O
sentence O
extractor O
contains O
two O
components O
: O
a O
probabilistic O
graph O
containing O
various O
sources O
of O
indirect O
supervision O
used O
as O
a O
supervision O
module O
and O
a O
ﬁne O
- O
tuned O
GPT B-MethodName
used O
as O
a O
prediction O
module O
. O

We O
update O
the O
model O
by O
alternatively O
optimizing O
GPT B-MethodName
and O
the O
probabilistic O
graph O
so O
that O
they O
reach O
an O
agreement O
on O
latent O
variables O
. O

After O
training O
, O
only O
the O
ﬁne O
- O
tuned O
GPT B-MethodName
is O
kept O
tomake O
predictions O
for O
a O
new O
instance O
during O
testing O
. O

We O
use O
the O
pre O
- O
trained O
transformer O
( O
i.e. O
, O
GPT B-MethodName
) O
released O
by O
Radford O
et O
al O
. O
( O
2018 O
) O
with O
the O
same O
preprocessing O
procedure O
. O

When O
GPT B-MethodName
is O
used O
as O
the O
neural O
reader O
, O
we O
set O
training O
epochs B-HyperparameterName
to O
4 B-HyperparameterValue
, O
use O
eight O
P40 O
GPUs O
for O
experiments O
on O
RACE O
, O
and O
use O
one O
GPU O
for O
experiments O
on O
other O
datasets O
. O

When O
GPT B-MethodName
is O
used O
as O
the O
evidence O
sentence O
extractor O
, O
we O
set O
batch B-HyperparameterName
size I-HyperparameterName
1per O
GPU O
and O
dropout B-HyperparameterName
rate I-HyperparameterName
0:3 O
. O

The O
ﬁne O
- O
tuned O
transformer O
( O
GPT B-MethodName
) O
baseline O
, O
which O
takes O
as O
input O
the O
full O
document O
, O
achieves O
an O
improvement O
of O
2:2%in O
macroaverage B-MetricName
F1 B-MetricName
( O
F1 B-MetricName
m O
) O
over O
the O
previous O
highest O
score B-MetricName
, O
66:5 O
% O
. O

Besides O
the O
ﬁne O
- O
tuned O
GPT B-MethodName
baseline O
, O
we O
use O
CoMatching O
( O
Wang O
et O
al O
. O
, O
2018b O
) O
, O
another O
state O
- O
ofthe O
- O
art O
neural O
reader O
on O
the O
RACE O
dataset O
. O

As O
shown O
in O
Table O
3 O
, O
by O
using O
the O
evidence O
sentences O
selected O
by O
InferSent O
, O
we O
suffer O
up O
to O
a O
1:9%drop O
in O
accuracy B-MetricName
with O
Co O
- O
Matching O
and O
up O
to O
a4:2%drop O
with O
the O
ﬁne O
- O
tuned O
GPT B-MethodName
. O

In O
comparison O
, O
by O
using O
the O
sentences O
extracted O
by O
our O
sentence O
extractor O
, O
which O
is O
trained O
with O
DPL O
as O
aApproach O
F1 B-MetricName
m O
F1a B-MetricName
EM0 B-MetricName
All O
- O
ones O
baseline O
( O
Khashabi O
et O
al O
. O
, O
2018 B-MetricValue
) O
61.0 O
59.9 O
0.8 O
Lucene O
world O
baseline O
( O
Khashabi O
et O
al O
. O
, O
2018 O
) O
61.8 O
59.2 O
1.4 O
Lucene O
paragraphs O
baseline O
( O
Khashabi O
et O
al O
. O
, O
2018 O
) O
64.3 O
60.0 O
7.5 O
Logistic O
regression O
( O
Khashabi O
et O
al O
. O
, O
2018 O
) O
66.5 O
63.2 O
11.8 O
Full O
context O
+ O
Fine O
- O
Tuned O
Transformer O
( O
GPT B-MethodName
, O
Radford O
et O
al O
. O
( O
2018 O
) O
) O
68.7 O
66.7 O
11.0 O
Random O
5 O
sentences O
+ O
GPT B-MethodName
65.3 O
63.1 O
7.2 O
Top O
5 O
sentences O
by O
ESE O
DS+ O
GPT B-MethodName
70.2 O
68.6 O
12.7 O
Top O
5 O
sentences O
by O
ESE O
DPL+ O
GPT B-MethodName
70.5 O
67.8 O
13.3 O
Top O
5 O
sentences O
by O
ESE O
gt+ O
GPT B-MethodName
72.3 O
70.1 O
19.2 O
Ground O
truth O
evidence O
sentences O
+ O
GPT B-MethodName
78.1 O
74.0 O
28.6 O
Human O
Performance O
( O
Khashabi O
et O
al O
. O
, O
2018 O
) O
86.4 O
83.8 O
56.6 O
Table O
2 O
: O
Performance O
of O
various O
settings O
on O
the O
MultiRC O
development O
set O
. O

We O
use O
the O
ﬁne O
- O
tuned O
GPT B-MethodName
as O
the O
evidence O
sentence O
extractor O
( O
ESE O
) O
and O
the O
neural O
reader O
( O
ESE O
DS O
: O
ESE O
trained O
on O
the O
silver O
standard O
evidence O
sentences O
; O
ESE O
DPL O
: O
ESE O
trained O
with O
DPL O
as O
a O
supervision O
module O
; O
ESE O
gt O
: O
ESE O
trained O
using O
ground O
truth O
evidence O
sentences O
; O
F1 B-MetricName
mmacro O
- O
average B-MetricName
F1 B-MetricName
; O
F1 B-MetricName
a O
: O
micro O
- O
average B-MetricName
F1 B-MetricName
; O
EM B-MetricName
0 B-MetricValue
: O
exact O
match O
) O
. O

ApproachDev O
Test O
Middle O
High O
All O
Middle O
High O
All O
Sliding O
Window O
( O
Richardson O
et O
al O
. O
, O
2013 O
; O
Lai O
et O
al O
. O
, O
2017 O
) O
- O
- O
- O
37.3 O
30.4 O
32.2 O
Co O
- O
Matching O
( O
Wang O
et O
al O
. O
, O
2018b O
) O
- O
- O
- O
55.8 O
48.2 O
50.4 O
Full O
context O
+ O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
) O
- O
- O
- O
62.9 O
57.4 O
59.0 O
Full O
context O
+ O
GPT B-MethodName
55.6 O
56.5 O
56.0 O
57.5 O
56.5 O
56.8 O
Random O
3 O
sentences O
+ O
GPT B-MethodName
50.3 O
51.1 O
50.9 O
50.9 O
49.5 O
49.9 O
Top O
3 O
sentences O
by O
InferSent O
( O
question O
) O
+ O
Co O
- O
Matching O
49.8 O
48.1 O
48.5 O
50.0 O
45.5 O
46.8 O
Top O
3 O
sentences O
by O
InferSent O
( O
question O
+ O
all O
options O
) O
+ O
Co O
- O
Matching O
52.6 O
49.2 O
50.1 O
52.6 O
46.8 O
48.5 O
Top O
3 O
sentences O
by O
ESE O
DS+ O
Co O
- O
Matching O
58.1 O
51.6 O
53.5 O
55.6 O
48.2 O
50.3 O
Top O
3 O
sentences O
by O
ESE O
DPL+ O
Co O
- O
Matching O
57.5 O
52.9 O
54.2 O
57.5 O
49.3 O
51.6 O
Top O
3 O
sentences O
by O
InferSent O
( O
question O
) O
+ O
GPT B-MethodName
55.0 O
54.7 O
54.8 O
54.6 O
53.4 O
53.7 O
Top O
3 O
sentences O
by O
InferSent O
( O
question O
+ O
all O
options O
) O
+ O
GPT B-MethodName
59.2 O
54.6 O
55.9 O
57.2 O
53.8 O
54.8 O
Top O
3 O
sentences O
by O
ESE O
DS+ O
GPT B-MethodName
62.5 O
57.7 O
59.1 O
64.1 O
55.4 O
58.0 O
Top O
3 O
sentences O
by O
ESE O
DPL+ O
GPT B-MethodName
63.2 O
56.9 O
58.8 O
64.3 O
56.7 O
58.9 O
Top O
3 O
sentences O
by O
ESE O
DS+ O
full O
context O
+ O
GPT B-MethodName
63.4 O
58.6 O
60.0 O
63.7 O
57.7 O
59.5 O
Top O
3 O
sentences O
by O
ESE O
DPL+ O
full O
context O
+ O
GPT B-MethodName
64.2 O
58.5 O
60.2 O
62.4 O
58.7 O
59.8 O
Silver O
standard O
evidence O
sentences O
+ O
GPT B-MethodName
73.2 O
73.9 O
73.7 O
74.1 O
72.3 O
72.8 O
Amazon O
Turker O
Performance O
( O
Lai O
et O
al O
. O
, O
2017 O
) O
- O
- O
- O
85.1 O
69.4 O
73.3 O
Ceiling O
Performance O
( O
Lai O
et O
al O
. O
, O
2017 O
) O
- O
- O
- O
95.4 O
94.2 O
94.5 O
Table O
3 O
: O
Accuracy O
( O
% O
) O
of O
various O
settings O
on O
the O
RACE O
dataset O
. O

supervision O
module O
, O
we O
observe O
a O
much O
smaller O
decrease O
( O
0:1 O
% O
) O
in O
accuracy B-MetricName
with O
the O
ﬁne O
- O
tuned O
GPT B-MethodName
baseline O
, O
and O
we O
slightly O
improve O
the O
accuracy B-MetricName
with O
the O
Co O
- O
Matching O
baseline O
. O

The O
ﬁne O
- O
tuned O
GPT B-MethodName
baseline O
, O
which O
taks O
as O
input O
the O
full O
document O
, O
achieves O
55:1%in O
accuracy B-MetricName
on O
the O
test O
set O
. O

If O
we O
train O
our O
evidence O
sentence O
extractor O
with O
DPL O
as O
a O
supervision O
module O
and O
feed O
the O
extracted O
evidence O
sentences O
to O
the O
ﬁne O
- O
tuned O
GPT B-MethodName
, O
we O
get O
test O
accuracy B-MetricName
57:7 O
% O
. O

Approach O
Dev O
Test O
Full O
context O
+ O
GPTy(Sun B-MethodName
et O
al O
. O
, O
2019 B-MetricValue
) O
55.9 O
55.5 O
Full O
context O
+ O
GPT B-MethodName
55.1 O
55.1 O
Top O
3 O
sentences O
by O
ESE O
silver O
- O
gt O
+ O
GPT B-MethodName
50.1 O
50.4 O
Top O
3 O
sentences O
by O
ESE O
DS+ O
GPT B-MethodName
55.1 O
56.3 O
Top O
3 O
sentences O
by O
ESE O
DPL+ O
GPT B-MethodName
57.3 O
57.7 O
Silver O
standard O
evidence O
sentences O
+ O
GPT B-MethodName
60.5 O
59.8 O
Human O
Performancey93.9 O
95.5 O
Table O
4 O
: O
Performance O
in O
accuracy B-MetricName
( O
% O
) O
on O
the O
DREAM O
dataset O
( O
Results O
marked O
withyare O
taken O
from O
Sun O
et O
al O
. O
( O
2019 O
) O
; O
ESE O
silver O
- O
gt O
: O
ESE O
trained O
using O
silver O
standard O
evidence O
sentences O
) O
. O

We O
tried O
two O
different O
ways O
to O
convert O
the O
pretrained O
word O
vectors O
to O
binary O
features O
: O
( O
1 O
) O
ﬁnd O
theknearest O
neighbors O
( O
k= O
3 O
in O
experiments O
) O
in O
the O
embedding O
space O
, O
and O
use O
these O
neighors O
as O
features O
; O
( O
2 O
) O
cluster O
the O
words O
into O
kclusters O
, O
( O
k= O
8,16 O
, O
... O
, O
2048,10000,100000 O
) O
, O
and O
used O
the O
cluster O
features O
. O

For O
example O
, O
using O
stacked O
word O
and O
character O
bi O
- O
LSTMCRFs B-MethodName
( O
Lample O
et O
al O
. O
, O
2016 O
) O
achieved O
95.75 O
% O
POS B-TaskName
tagging O
accuracy B-MetricName
, O
and O
96.00 B-MetricValue
% O
using O
word+preﬁx O
/ O
sufﬁx O
embedding O
. O

The O
Wikipedia O
data O
were O
POS B-TaskName
- O
tagged O
with O
universal O
POS B-TaskName
( O
UPOS B-TaskName
) O
tags O
( O
Petrov O
et O
al O
. O
, O
2012 O
) O
using O
the O
state O
- O
of O
- O
the O
art O
TurboTagger O
( O
Martins O
et O
al O
. O
, O
2013).9 O
The O
parser O
was O
trained O
using O
default O
settings O
( O
SVM B-MethodName
MIRA O
with O
20 O
iterations O
, O
no O
further O
parameter O
tuning O
) O
on O
the O
TRAIN O
+ O
DEV O
portion O
of O
the O
UD O
treebank O
annotated O
with O
UPOS B-TaskName
tags O
. O

Given O
a O
sentence O
( O
unstructured O
text O
) O
and O
its O
graph O
, O
we O
use O
contrastive O
learning O
to O
impose O
relation O
- O
related O
structure O
on O
the O
tokenlevel O
representations O
of O
the O
sentence O
obtained O
with O
a O
CharacterBERT B-MethodName
( O
El O
Boukkouri O
et O
al O
. O
, O
2020 O
) O
model O
. O

1 O
Introduction O
Pretrained O
language O
models O
( O
LMs O
) O
, O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
and O
GPT-3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
, O
capture O
contextualized O
information O
effectively O
and O
are O
used O
in O
a O
wide O
variety O
of O
natural O
language O
processing O
( O
NLP O
) O
tasks O
. O

In O
this O
study O
, O
we O
present O
a O
novel O
contrastive O
learning O
( O
CL O
) O
framework O
to O
leverage O
the O
embedding O
space O
of O
CharacterBERT B-MethodName
and O
impose O
a O
relation O
structure O
on O
the O
embeddings O
. O

The O
proposed O
framework O
receives O
a O
sentence O
and O
a O
graph O
that O
represents O
the O
text O
relations O
in O
a O
structured O
way O
, O
and O
the O
CL O
paradigm O
is O
applied O
to O
impose O
this O
structure O
on O
the O
token O
embeddings O
of O
the O
CharacterBERT B-MethodName
text O
encoder O
. O

To O
evaluate O
the O
efﬁcacy O
of O
our O
approach O
, O
a O
simple O
baseline O
neural O
network O
classiﬁer O
for O
RE O
, O
us O
- O
ing O
the O
pretrained O
CharacterBERT B-MethodName
medical O
version O
representations O
, O
is O
trained O
. O

The O
representations O
of O
the O
CharacterBERT B-MethodName
tuned O
version O
after O
applying O
CL O
are O
used O
to O
train O
the O
same O
classiﬁer O
, O
which O
vastly O
outperforms O
the O
baseline O
classiﬁer O
. O

To O
prepare O
the O
input O
for O
CharacterBERT B-MethodName
, O
tokenization O
is O
applied O
to O
each O
sentence O
using O
the O
character O
- O
CNN O
module O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

The O
BERT B-MethodName
tokenizer O
handles O
out O
- O
of O
- O
vocabulary O
( O
OOV O
) O
words O
by O
splitting O
these O
words O
into O
word O
pieces O
. O

Hence O
, O
CharacterBERT B-MethodName
is O
chosen O
instead O
of O
BERT B-MethodName
. O

The O
graph O
nodes O
are O
initialized O
with O
embeddings O
that O
are O
extracted O
by O
the O
ﬁnal O
layer O
of O
the O
pretrained O
medical O
version O
of O
CharacterBERT B-MethodName
. O

The O
sentence O
is O
passed O
to O
the O
text O
encoder O
( O
CharacterBERT B-MethodName
) O
, O
which O
has O
the O
ﬁrst O
six O
layers O
frozen O
. O

CharacterBERT B-MethodName
is O
initialized O
with O
the O
pretrained O
weights B-HyperparameterName
( O
medical O
version O
) O
. O

CharacterBERT B-MethodName
captures O
contextualised O
information O
very O
well O
. O

Hence O
, O
only O
one O
dense O
layer O
is O
added O
after O
CharacterBERT B-MethodName
. O

Following O
previous O
research O
on O
representation O
learning O
( O
Henaff O
, O
2020 O
; O
Chen O
et O
al O
. O
, O
2020 O
; O
He O
et O
al O
. O
, O
2020 O
; O
Zhang O
et O
al O
. O
, O
2020 O
) O
, O
we O
evaluate O
the O
tuned O
CharacterBERT B-MethodName
text O
encoder O
, O
taken O
from O
the O
trained O
CLGS O
andCLDR O
models O
, O
in O
a O
linear O
classiﬁcation O
setting O
, O
where O
all O
the O
candidate O
relations O
( O
concatenation O
of O
the O
token O
embeddings O
) O
are O
created O
, O
and O
a O
linear O
classiﬁcation O
layer O
is O
trained O
for O
the O
RE O
task O
. O

As O
a O
baseline O
model O
, O
we O
use O
the O
pretrained O
medical O
CharacterBERT B-MethodName
to O
create O
the O
representation O
for O
the O
relations6 O
. O

Model O
Precision B-MetricName
Recall B-MetricName
F1 B-MetricName
Baseline O
69.96 B-MetricValue
64.39 O
66.79 O
CharacterBERT B-MethodName
CLGS O
56.82 O
59.42 O
58.09 O
CharacterBERT B-MethodName
CLDR O
79.51 O
84.39 O
81.73 O
Table O
3 O
: O
RE O
- O
linear O
classiﬁcation O
setting O
Using O
the O
tuned O
CharacterBERT B-MethodName
representation O
from O
the O
CLGS O
model O
( O
mean B-MetricName
graph O
and O
text O
pooling O
) O
results O
in O
poor O
performance O
. O

In O
contrast O
, O
when O
we O
use O
the O
tuned O
CharacterBERT B-MethodName
of O
the O
CLDR O
model O
, O
the O
basic O
classiﬁer O
vastly O
outperforms O
the O
baseline O
model O
. O

Using O
the O
tuned O
CharacterBERT B-MethodName
of O
the O
CLDR O
model O
, O
the O
relation O
representation O
space O
is O
created O
. O

For O
the O
RE O
task O
, O
we O
utilize O
the O
tuned O
CharacterBERT B-MethodName
of O
theCLDR O
model O
to O
create O
the O
candidate O
relation O
representations O
. O

At O
the O
inference O
step O
, O
for O
each O
candidate O
relation O
, O
we O
decide O
whether O
it O
is O
positive O
based O
on O
the O
labels O
of O
the O
k O
- O
nearest B-MethodName
neighbors I-MethodName
in O
the O
learned O
embedding O
space O
. O

First O
, O
we O
determine O
whether O
a O
candidate O
relation O
( O
concatenation O
of O
the O
tokens O
) O
is O
predicted O
as O
positive O
in O
the O
relation O
representation O
space O
, O
which O
is O
obtained O
by O
the O
tuned O
CharacterBERT B-MethodName
of O
the O
CLDR O
model O
. O

Eberts O
and O
Ulges O
( O
2020 O
) O
present O
a O
span O
- O
based O
model O
that O
its O
core O
module O
is O
pretrained O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

3)ALIGN O
: O
A O
bag O
- O
of O
- O
words O
alignment O
model O
inspired O
by O
MacCartney O
( O
2009).10 O
4)CBOW B-MethodName
: O
A O
simple O
bag O
- O
of O
- O
embeddings O
sentence O
representation O
model O
( O
wil O
) O
. O

5)BiLSTM B-MethodName
: O
The O
simple O
BiLSTM B-MethodName
model O
described O
by O
wil O
. O

6)Chen O
( O
CH O
): O
Stacked O
BiLSTM B-MethodName
- O
RNNs O
with O
shortcut O
connections O
and O
character O
word O
embeddings O
( O
Chen O
et O
al O
. O
, O
2017b O
) O
. O

7)InferSent O
: O
A O
single O
- O
layer O
BiLSTM B-MethodName
- O
RNN O
model O
with O
max O
- O
pooling O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
. O

8)SSEN O
: O
Stacked O
BiLSTM B-MethodName
- O
RNNs O
with O
shortcut O
connections O
( O
Nie O
and O
Bansal O
, O
2017 O
) O
. O

9)ESIM O
: O
Sequential O
inference O
model O
proposed O
by O
Chen O
et O
al O
. O
( O
2017a O
) O
which O
uses O
BiLSTMs B-MethodName
with O
an O
attention O
mechanism O
. O

10)OpenAI B-MethodName
GPT B-MethodName
: O
Transformer O
- O
based O
language O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
with O
ﬁnetuning O
on O
NLI O
( O
Radford O
et O
al O
. O
, O
2018 O
) O
. O

11)BERT B-MethodName
: O
Transformer O
- O
based O
language O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
with O
a O
cloze O
- O
style O
and O
nextsentence O
prediction O
objective O
, O
and O
ﬁnetuning O
on O
NLI O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

 O
MAJ O
57.8 O
0.0 O
50.7 O
0.0 O
58.4 O
0.0 O
33.3 O
0.0 O
50.0 O
0.0 O
+0.0 O
+0.0 O
+0.0 O
HYP O
49.4 O
-8.4 O
52.5 O
+1.8 O
40.8 O
-17.6 O
31.2 O
-2.1 O
50.1 O
+0.1 O
-8.1 O
-1.0 O
-5.2 O
ALIGN O
62.1 O
+4.3 O
56.0 O
+5.3 O
34.8 O
-23.6 O
22.6 O
-10.7 O
47.2 O
-2.8 O
-4.7 O
-6.8 O
-5.5 O
CBOW B-MethodName
47.0 O
-10.8 O
61.8 O
+11.1 O
42.4 O
-16.0 O
30.2 O
-3.1 O
50.7 O
+0.7 O
-5.2 O
-1.2 O
-3.6 O
BiLSTM B-MethodName
51.2 O
-6.6 O
63.3 O
+12.6 O
50.8 O
-7.6 O
31.2 O
-2.1 O
50.7 O
+0.7 O
-0.5 O
-0.7 O
-0.6 O
CH O
54.2 O
-3.6 O
64.0 O
+13.3 O
55.2 O
-3.2 O
30.3 O
-3.0 O
50.7 O
+0.7 O
+2.2 O
-1.2 O
+0.9 O
InferSent O
66.3 O
+8.5 O
65.3 O
+14.6 O
29.6 O
-28.8 O
28.8 O
-4.5 O
50.7 O
+0.7 O
-1.9 O
-1.9 O
-1.9 O
SSEN O
58.4 O
+0.6 O
65.1 O
+14.4 O
49.2 O
-9.2 O
28.4 O
-4.9 O
50.7 O
+0.7 O
+1.9 O
-2.1 O
+0.3 O
ESIM O
54.8 O
-3.0 O
62.0 O
+11.3 O
45.6 O
-12.8 O
21.8 O
-11.5 O
50.1 O
+0.1 O
-1.5 O
-5.7 O
-3.2 O
GPT B-MethodName
68.1 O
+10.3 O
72.2 O
+21.5 O
52.4 O
-6.0 O
36.4 O
+3.1 O
50.0 O
+0.0 O
+8.6 O
+1.6 O
+5.8 O
BERT B-MethodName
57.2 O
-0.6 O
72.8 O
+22.1 O
49.6 O
-8.8 O
36.9 O
+3.6 O
42.2 O
-7.8 O
+4.2 O
-2.1 O
+1.7 O
Q O
- O
REAS O
56.6 O
-1.2 O
61.1 O
+10.4 O
50.8 O
-7.6 O
63.3 O
+30 O
71.5 O
+21.5 O
+0.5 O
+25.8 O
+10.6 O
Table O
6 O
: O
Accuracies(% O
) O
of O
9 O
NLI O
Models O
on O
ﬁve O
tests O
for O
quantitiative O
reasoning O
in O
entailment O
. O

We O
observe O
that O
neural O
models O
, O
particularly O
OpenAI B-MethodName
GPT B-MethodName
excel O
at O
verbal O
aspects O
of O
quantitative O
reasoning O
( O
RTE B-MetricName
- O
Quant O
, O
NewsNLI O
) O
, O
whereas O
Q O
- O
REAS O
excels O
at O
numerical O
aspects O
( O
Stress O
Test O
, O
AwpNLI O
) O
. O

On O
this O
set O
, O
GPT25 B-MethodName
achieves O
an O
accuracy B-MetricName
of O
51.18 B-MetricValue
% O
, O
as O
compared O
to O
72.04 O
% O
on O
the O
unperturbed O
set O
, O
suggesting O
the O
model O
relies O
on O
verbal O
cues O
rather O
than O
numerical O
reasoning O
. O

Closer O
examination O
reveals O
that O
OpenAI B-MethodName
switches O
to O
predicting O
the O
‘ O
neutral O
’ O
category O
for O
perturbed O
samples O
instead O
of O
entailment O
, O
accounting O
for O
42.7 O
% O
of O
its O
errors O
, O
possibly O
symptomatic O
of O
lexical O
bias O
issues O
( O
Naik O
et O
al O
. O
, O
2018 O
) O
. O

We O
use O
this O
baseline O
implementation O
as O
a O
starting O
point O
and O
achieve O
the O
best O
overall O
accuracy B-MetricName
of O
49.87 B-MetricValue
on O
Task O
2 O
by O
introducing O
three O
augmentations O
to O
the O
provided O
baseline O
system O
: O
( O
1 O
) O
We O
use O
an O
LSTM B-MethodName
to O
encode O
the O
entire O
available O
context O
; O
( O
2 O
) O
We O
employ O
a O
multitask O
learning O
approach O
with O
the O
auxiliary O
objective O
of O
MSD O
prediction O
; O
and O
( O
3 O
) O
We O
train O
the O
auxiliary O
component O
in O
a O
multilingual O
fashion O
, O
over O
sets O
of O
two O
to O
three O
languages O
. O

The O
resulting O
sequence O
of O
vectors O
is O
encoded O
using O
an O
LSTM B-MethodName
encoder O
. O

Subsequently O
, O
an O
LSTM B-MethodName
decoder O
generates O
the O
characters O
in O
the O
output O
word O
form O
using O
encoder O
states O
and O
an O
attention O
mechanism O
. O

2.2.1 O
Entire O
Context O
Encoded O
with O
LSTMs B-MethodName
The O
idea O
behind O
this O
modiﬁcation O
is O
to O
provide O
the O
encoder O
with O
access O
to O
all O
morpho O
- O
syntactic O
cues O
present O
in O
the O
sentence O
. O

However O
, O
we O
now O
reduce O
the O
entire O
past O
context O
to O
a O
ﬁxed O
- O
size O
vector O
by O
encoding O
it O
with O
a O
forward O
LSTM B-MethodName
, O
and O
we O
similarly O
represent O
the O
future O
context O
by O
encoding O
it O
with O
a O
backwards O
LSTM B-MethodName
. O

Tags O
are O
generated O
with O
an O
LSTM B-MethodName
one O
component O
at O
a O
time O
, O
e.g. O
the O
tag O
PRO;NOM;SG;1 O
is O
predicted O
as O
a O
sequence O
of O
four O
components O
, O
/angbracketleftPRO O
, O
NOM O
, O
SG O
, O
1 O
/angbracketright O
. O

Entire O
Context O
Encoded O
with O
LSTMs B-MethodName
Encoding O
the O
entire O
context O
with O
an O
LSTM B-MethodName
highly O
increases O
the O
variance O
of O
the O
observed O
results O
. O

The O
results O
indicate O
that O
encoding O
the O
full O
context O
with O
an O
LSTM B-MethodName
highly O
enhances O
the O
performance O
of O
the O
model O
, O
by O
11.15 O
% O
on O
average B-MetricName
. O

LSTM B-MethodName
Enc O
refers O
to O
a O
model O
that O
encodes O
the O
full O
context O
with O
an O
LSTM B-MethodName
; O
Multi O
- O
task O
builds O
on O
LSTM B-MethodName
Enc O
with O
an O
auxiliary O
objective O
of O
MSD O
prediction O
; O
Multilingual O
refers O
to O
a O
model O
with O
an O
auxiliary O
component O
trained O
in O
a O
multilingual O
fashion O
; O
Finetuned O
refers O
to O
a O
multilingual O
model O
topped O
with O
monolingual O
ﬁnetuning O
. O

Inspired O
by O
Mao O
et O
al O
. O
( O
2016 O
) O
and O
Yu O
et O
al O
. O
( O
2017 O
) O
, O
we O
instead O
use O
a O
linear O
combination O
of O
maximum O
likeli6We O
do O
not O
train O
from O
scratch O
because O
each O
model O
( O
for O
each O
strategy O
) O
takes O
several O
days O
to O
converge O
. O
Model O
Activity O
F1 B-MetricName
Entity O
F1 B-MetricName
LSTM B-MethodName
1.18 B-MetricValue
0.87 O
HRED O
4.34 O
2.22 O
VHRED O
4.63 O
2.53 O
VHRED O
( O
w/ O
attn O
. O
) O

LSTM B-MethodName
, O
HRED O
and O
VHRED O
are O
results O
reported O
in O
Serban O
et O
al O
. O
( O
2017a O
) O
. O

Here O
, O
we O
focus O
on O
multiple O
choiceQA O
due O
to O
its O
prevalence O
in O
human O
evaluations O
of O
reading O
comprehension O
, O
and O
use O
RoBERTa B-MethodName
due O
to O
its O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
this O
task O
. O

Although O
RACE O
has O
been O
widely O
used O
in O
NLP O
, O
it O
was O
recently O
shown O
that O
it O
has O
substantial O
quality O
assurance O
drawbacks O
; O
47 O
% O
of O
its O
questions O
are O
guessable O
by O
RoBERTa B-MethodName
without O
the O
passage O
, O
and O
18 O
% O
do O
not O
have O
a O
unique O
correct O
answer O
( O
Berzak O
et O
al O
. O
, O
2020 O
) O
. O

5.1 O
Model O
We O
utilize O
the O
RoBERTa B-MethodName
transformer O
architecture O
, O
which O
has O
shown O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
the O
multiple O
choice O
reading O
comprehension O
task(Liu O
et O
al O
. O
, O
2019 O
) O
. O

To O
allow O
RoBERTa B-MethodName
to O
beneﬁt O
from O
the O
gaze O
data O
, O
we O
use O
multi O
- O
task O
learning O
with O
hard O
parameter O
sharing O
( O
Caruana O
, O
1993 O
) O
, O
and O
modify O
RoBERTa B-MethodName
to O
jointly O
predict O
the O
answer O
to O
each O
question O
and O
the O
human O
gaze O
times O
allocated O
to O
each O
passage O
word O
. O

We O
follow O
the O
standard O
procedure O
for O
using O
transformer O
architectures O
for O
multiple O
- O
choice O
tasks O
, O
concatenating O
the O
passage O
, O
question O
, O
and O
answer O
[ O
CLS;d;SEP;Q;y O
] O
for O
each O
possible O
answer O
y. O
The O
resulting O
string O
is O
encoded O
through O
RoBERTa B-MethodName
. O

RT(w O
) O
= O
1 O
SX O
sTFs(w)P O
w0TFs(w0)(2 O
) O
In O
cases O
where O
RoBERTa B-MethodName
’s O
byte O
pair O
tokenizer O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
splits O
a O
single O
word O
into O
multiple O
tokens O
, O
we O
evenly O
split O
the O
gaze O
time O
associated O
with O
the O
word O
among O
the O
resulting O
tokens O
. O

We O
take O
the O
encoding O
of O
each O
passage O
word O
at O
the O
last O
layer O
of O
RoBERTa B-MethodName
for O
each O
candidate O
answer O
yand O
add O
a O
linear O
layer O
parameterized O
by O
a O
weight B-HyperparameterName
vectorv2Rdshared O
across O
all O
passage O
word O
positions O
, O
where O
dis O
the O
RoBERTa B-MethodName
embedding O
dimension O
. O

It O
is O
therefore O
also O
modular O
– O
the O
RoBERTa B-MethodName
model O
can O
be O
substituted O
with O
any O
QA O
model O
which O
provides O
passage O
word O
representations O
. O

5.3 O
Conditions O
We O
test O
two O
initial O
models O
: O
1.No O
RACE O
ﬁne O
- O
tuning O
using O
RoBERTa B-MethodName
that O
has O
not O
been O
ﬁne O
- O
tuned O
for O
QA O
on O
RACE O
. O

2.With O
RACE O
ﬁne O
- O
tuning O
using O
RoBERTa B-MethodName
that O
has O
been O
ﬁne O
- O
tuned O
on O
RACE O
to O
perform O
multiple O
choice O
question B-TaskName
answering I-TaskName
, O
following O
the O
procedure O
in O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O

We O
encode O
the O
question O
and O
the O
passage O
separately O
with O
an O
off O
- O
the O
- O
shelf O
encoder O
( O
here O
, O
RoBERTa B-MethodName
that O
has O
not O
been O
ﬁne O
- O
tuned O
for O
question O
- O
answering O
) O
and O
compute O
the O
dot O
- O
product O
between O
each O
encoded O
passage O
word O
and O
the O
ﬁnal O
encoding O
of O
the O
question O
’s O
CLS O
token O
. O

LossQuestion O
– O
Passage O
Similarity O
54.7 O
78.9 O
72.3 O
87.3 O
41.2 O
52.6 O
68.2 O
82.9 O
Gaze O
Gathering O
Dundee O
55.9 O
78.2 O
72.5 O
86.6 O
39.3 O
52.5 O
68.4 O
82.9 O
Gaze O
Gathering O
OneStopQA O
54.7 O
80.2 O
71.5 O
87.0 O
39.6 O
48.4 O
68.4 O
83.0 O
Critical O
Spans O
OneStopQA O
54.7 O
80.7 O
70.1 O
86.5 O
40.4 O
51.6 O
68.4 O
82.9 O
Gaze O
Hunting O
OneStopQA O
57.1 O
80.5 O
73.1 O
88.0 O
41.1 O
53.0 O
68.5 O
83.0 O
Table O
2 O
: O
Question B-TaskName
Answering I-TaskName
accuracy B-MetricName
for O
RoBERTa B-MethodName
Base O
and O
Large O
on O
OneStopQA O
and O
RACE O
. O

Models O
CSQA O
OBQA O
MCScript2.0 O
ConceptNet O
SWOW O
ConceptNet O
SWOW O
ConceptNet O
SWOW O
ALBERT B-MethodName
73.78 O
( O
0.79 O
) O
63.47 O
( O
1.42 O
) O
93.62 O
( O
0.44 O
) O
+ O
GconAttn O
74.03 O
( O
0.46 O
) O
74.05 O
( O
0.50 O
) O
65.13 O
( O
2.16 O
) O
65.87 O
( O
1.21 O
) O
93.91 O
( O
0.50 O
) O
93.84 O
( O
0.35 O
) O
+ O
RN O
75.64 O
( O
0.70 O
) O
74.40 O
( O
0.37 O
) O
64.73 O
( O
2.10 O
) O
66.40 O
( O
1.00 O
) O
93.53 O
( O
0.11 O
) O
93.49 O
( O
0.22 O
) O
Table O
3 O
: O
Test O
accuracy B-MetricName
on O
CSQA O
, O
OBQA O
and O
MCScript2.0 O
. O

We O
report O
performance O
of O
ALBERT B-MethodName
, O
and O
augment O
it O
with O
two O
KG O
- O
aware O
models O
using O
either O
ConceptNet O
orSWOW O
. O

We O
use O
RoBERTa B-MethodName
- O
Large O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
to O
obtain O
a O
node O
embedding O
matrix O
, O
separately O
for O
ConceptNet O
andSWOW O
. O

Speciﬁcally O
, O
for O
each O
node O
Ei2 O
E O
, O
we O
feed O
the O
sequence O
of O
[ O
CLS]+Ei+[SEP]to O
RoBERTa B-MethodName
and O
use O
the O
last O
layer O
representation O
of O
[ O
CLS O
] O
as O
its O
embedding O
. O

We O
do O
not O
use O
TransE O
node O
embeddings O
, O
as O
they O
were O
outperformed O
by O
RoBERTa B-MethodName
embeddings O
in O
preliminary O
tests.2020 O
) O
, O
we O
select O
31 O
out O
of O
ConceptNet O
’s O
47 O
relation O
types O
which O
proved O
helpful O
for O
CQA O
, O
and O
merge O
the O
remaining O
relations O
into O
17 O
types O
. O

We O
ﬁx O
the O
text O
encoder O
to O
ALBERT B-MethodName
-xxlargev2(Lan O
et O
al O
. O
, O
2020 O
) O
, O
which O
performed O
competitively O
in O
recent O
work O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
. O

Fig11https://coinnlp.github.io/task1.html O
12The O
only O
exception O
is O
ALBERT B-MethodName
vs O
RN O
with O
ConceptNet O
on O
OBQA O
where O
p=0:07 O
. O

We O
use O
character O
- O
based O
bidirectional O
long O
shortterm O
memory O
( O
LSTM B-MethodName
) O
networks O
for O
both O
tokenization O
and O
POS B-TaskName
tagging O
. O

Afterwards O
, O
we O
employ O
a O
list O
- O
based O
transitionbased O
algorithm O
for O
general O
non O
- O
projective O
parsing O
and O
present O
an O
improved O
StackLSTM B-MethodName
- O
based O
architecture O
for O
representing O
each O
transition O
state O
and O
making O
predictions O
. O

To O
handle O
the O
nonprojective O
dependencies O
in O
most O
of O
the O
languages O
( O
or O
treebanks O
) O
provided O
in O
the O
task O
, O
we O
employ O
the O
list O
- O
based O
transition O
parsing O
algorithm O
( O
Choi O
and O
McCallum O
, O
2013 O
) O
, O
equipped O
with O
an O
improved O
Stack O
- O
LSTMbased B-MethodName
model O
for O
representing O
the O
transition O
states O
, O
i.e. O
, O
conﬁgurations O
( O
Section O
2.3 O
) O
. O

Therefore O
, O
we O
develop O
a O
hierarchical O
LSTM B-MethodName
- O
based O
model O
, O
as O
illustrated O
in O
Figure O
1 O
, O
in O
which O
characters O
in O
each O
token O
are O
composed O
using O
a O
character O
- O
based O
bidirectional O
LSTM B-MethodName
( O
BiLSTM B-MethodName
) O
network O
and O
then O
concatenated O
with O
additional O
token O
- O
level O
features O
( O
e.g. O
, O
token O
embedding O
, O
the O
ﬁrst O
character O
of O
this O
token O
, O
etc O
. O
) O
and O
passed O
through O
a O
token O
- O
level O
Bi O
- O
LSTM B-MethodName
. O

The O
hidden O
states O
of O
the O
token O
- O
level O
Bi O
- O
LSTM B-MethodName
are O
ﬁnally O
used O
for O
classiﬁcation O
through O
a O
softmax O
layer O
. O

plicit O
word O
boundary O
markers O
, O
i.e. O
, O
white O
spaces O
, O
including O
Chinese O
, O
Japanese O
andVietnamese O
.1 O
Our O
word O
segmentation O
model O
is O
also O
built O
on O
Bi O
- O
LSTM B-MethodName
networks O
, O
and O
incorporates O
rich O
statistics O
- O
based O
features O
gathered O
from O
large O
- O
scale O
unlabeled O
data O
. O

In O
our O
system O
, O
we O
use O
a O
model O
similar O
to O
the O
one O
for O
sentence O
segmentation O
( O
Section O
2.1.1 O
) O
, O
i.e. O
, O
a O
hierarchical O
Bi O
- O
LSTM B-MethodName
model O
which O
outperforms O
UDPipe O
on O
most O
of O
datasets O
with O
much O
fewer O
features O
. O

Concretely O
, O
each O
word O
is O
modeled O
using O
a O
characterbased O
Bi O
- O
LSTM B-MethodName
, O
so O
that O
word O
preﬁx O
and O
sufﬁx O
features O
can O
be O
effectively O
incorporated O
, O
which O
is O
particularly O
important O
for O
morphologically O
rich O
languages O
. O

The O
character O
- O
based O
compositional O
embedding O
of O
each O
word O
is O
then O
concatenated O
with O
a O
pretrained O
word O
embedding O
and O
a O
Brown O
cluster O
embedding O
, O
resulting O
in O
the O
ﬁnal O
word O
representation O
which O
is O
fed O
as O
input O
of O
a O
word O
- O
level O
Bi O
- O
LSTM B-MethodName
for O
POS B-TaskName
tagging O
. O

We O
base O
our O
parser O
mainly O
on O
the O
Stack O
- O
LSTM B-MethodName
model O
proposed O
by O
Dyer O
et O
al O
. O
( O
2015 O
) O
, O
where O
three O
Stack O
- O
LSTMs B-MethodName
are O
utilized O
to O
incrementally O
obtain O
the O
representations O
of O
the O
bufferβ O
, O
the O
stackσand O
the O
transition O
action O
sequenceA. O

However O
, O
compared O
with O
the O
arc O
- O
standard O
algorithm O
( O
Nivre O
, O
2004 O
) O
used O
by O
Dyer O
et O
al O
. O
( O
2015 O
) O
, O
the O
list O
- O
based O
arc O
- O
eager O
transition O
system O
has O
an O
extra O
component O
in O
each O
conﬁguration O
, O
i.e. O
, O
the O
deque O
δ O
. O
So O
we O
use O
an O
additional O
Stack O
- O
LSTM B-MethodName
to O
learn O
the O
representation O
of O
δ O
. O
More O
importantly O
, O
we O
introduce O
two O
LSTM B-MethodName
- O
based O
techniques O
, O
namely O
Bi O
- O
LSTM B-MethodName
Subtraction O
andIncremental O
Tree O
- O
LSTM B-MethodName
( O
explained O
below O
) O
for O
modeling O
the O
buffer O
and O
sub O
- O
tree O
representations O
in O
our O
model O
. O

2.3.1 O
Bi O
- O
LSTM B-MethodName
Subtraction O
We O
regard O
the O
buffer O
as O
a O
segment O
and O
use O
the O
subtraction O
between O
LSTM B-MethodName
hidden O
vectors O
of O
the O
segment O
head O
and O
tail O
as O
its O
representation O
. O

The O
buffer O
βis O
represented O
by O
Bi O
- O
LSTM B-MethodName
Subtraction O
, O
the O
sub O
- O
trees O
are O
computed O
by O
Incremental O
Tree O
- O
LSTM B-MethodName
. O

hf O
( O
* O
) O
andhb O
( O
* O
) O
indicate O
the O
hidden O
vectors O
of O
forward O
and O
backward O
LSTM B-MethodName
respectively O
. O

2016 O
; O
Kiperwasser O
and O
Goldberg O
, O
2016 O
; O
Cross O
and O
Huang O
, O
2016 O
) O
, O
thus O
called O
Bi O
- O
LSTM B-MethodName
Subtraction O
. O

The O
forward O
and O
backward O
subtractions O
are O
calculated O
independently O
, O
i.e. O
, O
bf O
= O
hf(l)−hf(f)and O
bb O
= O
bb(f)−bb(l O
) O
, O
wherehf(f)andhf(l)are O
the O
hidden O
vectors O
of O
the O
ﬁrst O
and O
the O
last O
words O
in O
the O
forward O
LSTM B-MethodName
, O
hb(f)andhb(l)are O
the O
hidden O
vectors O
of O
the O
ﬁrst O
and O
the O
last O
words O
in O
the O
backward O
LSTM B-MethodName
. O

2.3.2 O
Incremental O
Tree O
- O
LSTM B-MethodName
We O
use O
a O
Tree O
- O
LSTM B-MethodName
( O
Tai O
et O
al O
. O
, O
2015 O
; O
Zhu O
et O
al O
. O
, O
2015 O
) O
in O
our O
parser O
to O
model O
the O
sub O
- O
trees O
during O
parsing O
. O

The O
example O
in O
Figure O
6 O
shows O
the O
differences O
between O
RecNN O
( O
Dyer O
et O
al O
. O
, O
2015 O
) O
and O
Tree O
- O
LSTM B-MethodName
. O

Whereas O
in O
Tree O
- O
LSTM B-MethodName
, O
a O
head O
is O
combined O
with O
all O
of O
its O
modiﬁers O
simultaneously O
in O
each O
LSTM B-MethodName
unit O
. O

However O
, O
our O
implementation O
of O
Tree O
- O
LSTM B-MethodName
is O
different O
from O
the O
conventional O
one O
. O

Unlike O
traditional O
bottom O
- O
up O
Tree O
- O
LSTMs B-MethodName
in O
which O
each O
head O
and O
all O
of O
its O
modiﬁers O
are O
combined O
simultaneously O
, O
the O
modiﬁers O
are O
found O
incrementally O
during O
our O
parsing O
procedure O
. O

Therefore O
, O
we O
propose O
Incremental O
Tree O
- O
LSTM B-MethodName
, O
which O
obtains O
sub O
- O
tree O
representations O
incrementally O
. O

4.1 O
Experimental O
Settings O
4.1.1 O
Model O
Selection O
Strategies O
Forsentence O
segmentation O
, O
we O
apply O
our O
own O
models O
for O
a O
subset O
of O
languages O
on O
which O
UDPipe O
yields O
poor O
performance O
, O
and O
use O
UDPipe O
for O
the O
rest O
languages.3Speciﬁcally O
, O
we O
use O
the O
rulebased O
model O
for O
la O
ittb O
and O
cs O
cltt,4and O
use O
the O
BiLSTM B-MethodName
- O
based O
model O
( O
Figure O
1 O
) O
for O
sk O
, O
en O
, O
en O
lines O
, O
ﬁftb O
, O
got O
, O
nl O
lassysmall O
, O
grc O
proiel O
, O
la O
ittb O
, O
cu O
, O
3We O
use O
the O
same O
hyper O
- O
parameter O
settings O
as O
provided O
by O
the O
organizers O
to O
train O
the O
UDPipe O
models O
. O

For O
word O
segmentation O
, O
we O
use O
our O
Bi O
- O
LSTM B-MethodName
- O
based O
model O
for O
zh O
, O
ja O
, O
ja O
pud O
and O
vi O
, O
which O
do O
n’t O
have O
explicit O
word O
boundary O
markers O
, O
i.e. O
, O
white O
spaces O
. O

For O
cross O
- O
lingual O
transfer O
parsing O
of O
lowresource O
languages O
, O
we O
use O
parallel O
data O
from O
OPUS O
to O
derive O
cross O
- O
lingual O
word O
embeddings.6 O
Thefastalign O
toolkit O
( O
Dyer O
et O
al O
. O
, O
2013 O
) O
is O
used O
for O
word O
alignment.7 O
We O
use O
the O
Dynet O
toolkit O
for O
the O
implementation O
of O
all O
our O
neural O
models.8 O
4.2 O
Effects O
of O
Different O
Parts O
in O
Dependency O
Parsing O
We O
conduct O
experiments O
on O
the O
development O
sets O
of O
4 O
treebanks O
to O
investigate O
the O
contributions O
of O
the O
two O
architectures O
we O
proposed O
( O
i.e. O
, O
the O
Incremental O
Tree O
- O
LSTM B-MethodName
and O
the O
Bi O
- O
LSTM B-MethodName
Subtraction O
) O
and O
the O
Brown O
cluster O
. O

B O
: O
BiLSTM B-MethodName
Subtraction O
, O
T O
: O
Incremental O
Tree O
- O
LSTM B-MethodName
, O
C O
: O
Brown O
cluster O
. O

We O
develop O
effective O
neural O
models O
for O
each O
task O
, O
with O
particular O
utilization O
of O
bidirectional O
LSTM B-MethodName
networks O
. O

Residual O
has O
been O
widely O
applied O
to O
build O
deep O
neural B-MethodName
networks I-MethodName
with O
enhanced O
feature O
propagation O
and O
improved O
accuracy B-MetricName
. O

Residual O
structure O
, O
which O
alleviates O
the O
so O
- O
called O
gradient O
exploding O
or O
vanishing O
problem O
in O
optimization O
( O
He O
et O
al O
. O
, O
2016a O
) O
, O
enables O
the O
training O
of O
neural B-MethodName
networks I-MethodName
with O
great O
depth O
by O
building O
skip O
connections O
between O
layers O
. O

2 O
Related O
Work O
In O
recent O
years O
, O
the O
application O
of O
residual O
structure O
to O
deep O
neural B-MethodName
networks I-MethodName
has O
become O
an O
active O
research O
topic O
( O
He O
et O
al O
. O
, O
2016a O
; O
Srivastava O
et O
al O
. O
, O
2015 O
; O
He O
et O
al O
. O
, O
2016b O
; O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Szegedy O
et O
al O
. O
, O
2017 O
) O
. O

Highway O
network O
( O
Srivastava O
et O
al O
. O
, O
2015 O
) O
is O
among O
the O
very O
ﬁrst O
endeavors O
to O
implant O
residual O
structures O
into O
deep O
neural B-MethodName
networks I-MethodName
. O

very O
helpful O
to O
information O
transfer O
and O
eases O
the O
optimization O
of O
deep O
neural B-MethodName
networks I-MethodName
. O

This O
is O
because O
that O
in O
the O
underlying O
static O
blocks O
of O
deep O
neural B-MethodName
networks I-MethodName
, O
the O
guidance O
from O
error O
signal O
is O
weak O
and O
the O
optimization O
is O
unstable O
, O
thus O
making O
the O
introduction O
of O
layer O
normalization O
necessary O
. O

Detailed O
analyses O
prove O
that O
the O
proposed O
approach O
can O
also O
promote O
the O
optimization O
ability O
of O
deep O
neural B-MethodName
networks I-MethodName
, O
and O
is O
conducive O
to O
exerting O
the O
expressive O
power O
of O
existing O
models O
. O

Existing O
work O
used O
recurrent O
neural B-MethodName
networks I-MethodName
and O
convolutional O
neural B-MethodName
networks I-MethodName
to O
model O
input O
essays O
, O
giving O
grades O
based O
on O
a O
single O
vector O
representation O
of O
the O
essay O
. O

Both O
recurrent O
neural B-MethodName
networks I-MethodName
( O
Williams O
and O
Zipser O
, O
1989 O
; O
Mikolov O
et O
al O
. O
, O
2010 O
) O
and O
convolutional O
neural B-MethodName
networks I-MethodName
( O
LeCun O
et O
al O
. O
, O
1998 O
; O
Kim O
, O
2014 O
) O
have O
been O
used O
for O
modelling O
input O
essays O
. O

In O
particular O
, O
Alikaniotis O
et O
al O
. O
( O
2016 O
) O
and O
Taghipour O
and O
Ng O
( O
2016 O
) O
use O
a O
single O
- O
layer O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
over O
the O
word O
sequence O
to O
model O
the O
essay O
, O
and O
Dong O
and O
Zhang O
( O
2016 O
) O
use O
a O
two O
- O
level O
hierarchical O
CNN O
structure O
to O
model O
sentences O
and O
documents O
separately O
. O

It O
has O
been O
commonly O
understood O
that O
CNNs O
can O
capture O
local O
ngram O
information O
effectively O
, O
while O
LSTMs B-MethodName
are O
strong O
in O
modelling O
long O
history O
. O

No O
previous O
work O
has O
compared O
the O
effectiveness O
of O
LSTMs B-MethodName
and O
CNNs O
under O
the O
same O
settings O
for O
AES O
. O

To O
better O
understand O
the O
contrast O
, O
we O
adopt O
the O
two O
- O
layer O
structure O
of O
Dong O
and O
Zhang O
( O
2016 O
) O
, O
comparing O
CNNs O
and O
LSTMs B-MethodName
for O
modelling O
sentences O
and O
documents O
. O

We O
adopt O
the O
neural O
attention O
model O
( O
Xu O
et O
al O
. O
, O
2015 O
; O
Luong O
et O
al O
. O
, O
2015 O
) O
to O
automatically O
calculate O
weights B-HyperparameterName
for O
convolution O
features O
of O
CNNs O
and O
hidden O
state O
values O
of O
LSTMs B-MethodName
, O
which O
has O
been O
used O
for O
obtaining O
the O
most O
pertinent O
information O
for O
machine O
translation O
( O
Luong O
et O
al O
. O
, O
2015 O
) O
, O
sentiment B-TaskName
analysis I-TaskName
( O
Shin O
et O
al O
. O
, O
2016 O
; O
Wang O
et O
al O
. O
, O
2016 O
; O
Liu O
and O
Zhang O
, O
2017 O
) O
and O
other O
tasks O
. O

Results O
show O
that O
CNN O
is O
relatively O
more O
effective O
for O
modelling O
sentences O
, O
and O
LSTMs B-MethodName
are O
relatively O
more O
effective O
for O
modelling O
documents O
. O

In O
the O
classiﬁcation O
scenario O
, O
scores B-MetricName
are O
divided O
into O
several O
categories O
, O
each O
score B-MetricName
or O
score B-MetricName
range O
is O
regarded O
as O
one O
class O
and O
the O
ordinary O
classiﬁcation O
models O
are O
employed O
such O
as O
Naive B-MethodName
Bayes I-MethodName
( O
NB O
) O
and O
SVMs B-MethodName
( O
Larkey O
, O
1998 B-MetricValue
; O
Rudner O
and O
Liang O
, O
2002 O
) O
. O

In O
the O
regression O
scenario O
, O
each O
score B-MetricName
is O
treated O
as O
continous O
values O
for O
the O
essay O
and O
regression O
models O
are O
considered O
, O
like O
linear B-MethodName
regression I-MethodName
, O
Bayesian O
linear O
ridge O
regression O
( O
Attali O
and O
Burstein O
, O
2004 B-MetricValue
; O
Phandi O
et O
al O
. O
, O
2015 O
) O
. O

Different O
from O
their O
model O
, O
our O
neural O
model O
learns O
text O
representation O
with O
LSTMs B-MethodName
, O
which O
could O
model O
the O
coherence O
and O
coreference B-TaskName
among O
sequences O
of O
sentences O
( O
i.e. O
capturing O
more O
global O
information O
compared O
to O
CNNs O
) O
. O

Alikaniotis O
et O
al O
. O
( O
2016 O
) O
use O
score B-MetricName
- O
speciﬁc O
word O
embeddings O
as O
word O
features O
and O
take O
the O
last O
hidden O
state O
of O
LSTM B-MethodName
as O
text O
representation O
. O

Taghipour O
and O
Ng O
( O
2016 O
) O
take O
the O
average B-MetricName
value O
over O
all O
the O
hidden O
states O
of O
LSTM B-MethodName
as O
text O
representation O
. O

In O
contrast O
to O
the O
previous O
LSTM B-MethodName
models O
, O
we O
use O
LSTM B-MethodName
to O
learn O
from O
sentence O
sequences O
and O
attention O
pooling O
on O
the O
hidden O
states O
of O
LSTM B-MethodName
to O
obtain O
the O
contribution O
of O
each O
sentence O
to O
the O
ﬁnal O
quality O
of O
essays O
. O

The O
structure O
of O
a O
text O
representation O
using O
LSTM B-MethodName
is O
depicted O
in O
Figure O
2 O
. O

LSTMs B-MethodName
use O
gates O
to O
control O
information O
ﬂow O
, O
preserving O
or O
forgetting O
information O
for O
each O
cell O
units O
. O

Assuming O
that O
an O
essay O
script O
consists O
ofTsentences O
, O
s1,s2 O
, O
... O
, O
sTwithstbeing O
the O
feature O
representation O
of O
t O
- O
th O
wordst O
, O
we O
have O
LSTM B-MethodName
cell O
units O
addressed O
in O
the O
following O
equations O
: O
it O
= O
σ(Wi·st+Ui·ht−1+bi O
) O
ft O
= O
σ(Wf·st+Uf·ht−1+bf O
) O
˜ct O
= O
tanh O
( O
Wc·st+Uc·ht−1+bc O
) O
ct O
= O
it O
◦ O
˜ct+ft O
◦ O
ct−1 O
ot O
= O
σ(Wo·st+Uo·ht−1+bo O
) O
ht O
= O
ot O
◦ O
tanh O
( O
ct),(14 O
) O
where O
standhtare O
the O
input O
sentence O
and O
output O
sentence O
vectors O
at O
time O
t O
, O
respectively O
. O

The O
symbol O
◦ O
denotes O
element O
- O
wise O
multiplication O
and O
σrepresents O
the O
sigmoid O
function O
. O
After O
obtaining O
the O
intermediate O
hidden O
states O
of O
LSTM B-MethodName
h1,h2, O
... O
,hT O
, O
we O
use O
another O
attention O
pooling O
layer O
over O
the O
sentences O
to O
learn O
the O
ﬁnal O
text O
representation O
. O

During O
the O
training O
process O
, O
character O
embeddings O
are O
ﬁne O
- O
tuned O
. O
Layer O
Parameter O
Name O
Parameter O
Value O
Lookupchar O
embedding O
dim O
30 O
word O
embedding O
dim O
50 B-HyperparameterValue
CNNwindow O
size O
5 O
number O
of O
ﬁlters O
100 B-HyperparameterValue
LSTM B-MethodName
hidden B-HyperparameterName
units I-HyperparameterName
100 O
Dropout O
dropout B-HyperparameterName
rate I-HyperparameterName
0.5 B-HyperparameterValue
epochs B-HyperparameterName
50 O
batch B-HyperparameterName
size I-HyperparameterName
10 B-HyperparameterValue
initial O
learning B-HyperparameterName
rate I-HyperparameterName
η O
0.001 B-HyperparameterValue
momentum B-HyperparameterName
0.9 B-HyperparameterValue
Table O
1 O
: O
Hyper O
- O
parameters O
Set O
# O
Essays O
Genre O
Avg O
Len O
. O

Word O
Embeddings O
We O
take O
the O
Stanford O
’s O
publicly O
available O
GloVe B-MethodName
50 O
- O
dimensional O
embeddings2as O
word O
pretrained O
embeddings O
, O
which O
are O
trained O
on O
6 O
billion O
words O
from O
Wikipedia O
and O
web O
text O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

Baseline O
models O
We O
take O
LSTM B-MethodName
with O
Meanover O
- O
Time O
Pooling O
( O
LSTM B-MethodName
- O
MoT O
) O
( O
Taghipour O
and O
Ng O
, O
2016 O
) O
and O
hierarchical O
CNN O
( O
CNN O
- O
CNNMoT O
) O
( O
Dong O
and O
Zhang O
, O
2016 O
) O
as O
our O
baselines O
. O

LSTM B-MethodName
- O
MoT O
uses O
one O
layer O
of O
LSTM B-MethodName
over O
the O
word O
sequences O
, O
and O
takes O
the O
average B-MetricName
pooling O
over O
all O
time O
- O
step O
states O
as O
the O
ﬁnal O
text O
representation O
, O
which O
is O
called O
Mean O
- O
over O
- O
Time O
( O
MoT O
) O
pooling O
( O
Taghipour O
and O
Ng O
, O
2016 B-MetricValue
) O
. O

LSTM B-MethodName
- O
MoT O
is O
the O
current O
state O
- O
of O
- O
the O
- O
art O
neural O
model O
on O
the O
text O
- O
level O
and O
CNN O
- O
CNN O
- O
MoT O
is O
a O
state O
- O
of O
- O
the O
- O
art O
model O
on O
the O
sentence O
- O
level O
. O

Besides O
, O
LSTM B-MethodName
- O
LSTM B-MethodName
- O
MoT O
and O
LSTM B-MethodName
- O
CNN O
- O
MoT O
are O
adopted O
as O
another O
two O
baseline O
models O
. O

The O
former O
model O
takes O
LSTMs B-MethodName
to O
represent O
both O
sentences O
and O
texts O
, O
and O
the O
latter O
uses O
CNN O
representing O
sentences O
and O
LSTM B-MethodName
representing O
texts O
. O

We O
compare O
our O
model O
( O
LSTMCNN B-MethodName
- O
attent O
) O
with O
the O
baseline O
models O
to O
study O
CNN O
representing O
sentences O
and O
LSTM B-MethodName
representing O
texts O
. O

Our O
model O
LSTM B-MethodName
- O
CNN O
- O
attent O
outperforms O
the O
baseline O
model O
CNN O
- O
CNN O
- O
MoT O
by O
3.0 O
% O
, O
LSTMMoT B-MethodName
by O
2.2 O
% O
on O
average B-MetricName
quadratic O
weighted B-HyperparameterName
3http://www.nltk.orgPrompts O
LSTMMoTCNNCNN B-MethodName
- O
MoTLSTMCNN B-MethodName
- O
att O
1 B-MetricValue
0.818 O
0.805 O
0.822 O
2 O
0.688 O
0.613 O
0.682 O
3 O
0.679 O
0.662 O
0.672 O
4 O
0.805 O
0.778 O
0.814 O
5 O
0.808 O
0.800 O
0.803 O
6 O
0.817 O
0.809 O
0.811 O
7 O
0.797 O
0.758 O
0.801 O
8 O
0.527 O
0.644 O
0.705 O
Avg O
. O

LSTM B-MethodName
- O
CNN O
- O
attent O
Average B-MetricName
QWK O
char O
0.738 B-MetricValue
word O
0.764 O
word O
+ O
char O
0.761 O
Table O
4 O
: O
Comparison O
of O
quadratic O
weight B-HyperparameterName
kappa O
using O
different O
features O
on O
the O
test O
data O
. O

Even O
compared O
with O
the O
ensemble O
model O
used O
by O
Taghipour O
and O
Ng O
( O
2016 O
) O
, O
which O
ensembles O
10 O
instances O
of O
CNN O
and O
LSTM B-MethodName
of O
different O
initializations B-HyperparameterName
, O
our O
model O
still O
achieves O
0.3 O
% O
improvement O
on O
QWK O
. O

5.3 O
Analysis O
We O
perform O
several O
development O
experiments O
to O
verify O
the O
effectiveness O
of O
sentence O
- O
document O
model O
and O
text O
representation O
with O
LSTM B-MethodName
and O
attention O
pooling O
. O

Empirical O
results O
show O
that O
with O
only O
character O
embedding O
features O
, O
the O
performance O
of O
our O
model O
outperforms O
CNNCNN O
- O
MoT O
, O
and O
is O
close O
to O
LSTM B-MethodName
- O
MoT. O

One O
possible O
explanation O
is O
that O
the O
ASAP O
dataset O
is O
rather O
small O
given O
the O
model O
parameters O
, O
which O
has O
a O
potential O
for O
overﬁtting O
if O
both O
words O
and O
characters O
are O
used O
. O
Model O
Model O
Type O
Pooling O
Avg O
QWK O
LSTM B-MethodName
- O
MoT O
document O
- O
level O
MoT O
0.742 O
LSTM B-MethodName
- O
attent O
document O
- O
level O
attention O
0.731 O
CNN O
- O
CNNMoTsentence O
- O
level O
MoT O
0.734 O
LSTM B-MethodName
- O
LSTMMoTsentence B-MethodName
- O
level O
MoT O
0.758 O
LSTM B-MethodName
- O
CNNMoTsentence O
- O
level O
MoT O
0.759 O
LSTM B-MethodName
- O
LSTMattentsentence B-MethodName
- O
level O
attention O
0.762 O
LSTM B-MethodName
- O
CNNattentsentence O
- O
level O
attention O
0.764 O
Table O
5 O
: O
Comparison O
between O
different O
model O
types O
and O
pooling O
methods O
on O
the O
test O
data O
( O
only O
word O
embeddings O
used O
) O
. O

Granularity O
The O
previous O
model O
LSTM B-MethodName
- O
MoT O
tackles O
the O
AES O
task O
by O
treating O
each O
essay O
script O
as O
a O
sequence O
of O
words O
, O
which O
makes O
an O
essay O
an O
extra O
long O
sequence O
. O

The O
word O
number O
of O
one O
essay O
usually O
exceeds O
several O
hundreds O
, O
which O
makes O
it O
difﬁcult O
to O
directly O
use O
LSTM B-MethodName
to O
learn O
text O
representation O
if O
only O
last O
hidden O
state O
is O
used O
. O

It O
has O
been O
veriﬁed O
by O
Taghipour O
and O
Ng O
( O
2016 O
) O
that O
LSTM B-MethodName
with O
Meanover O
- O
Time O
pooling O
outperforms O
LSTM B-MethodName
with O
only O
last O
state O
. O

Both O
LSTM B-MethodName
- O
CNN O
- O
MoT O
and O
LSTM B-MethodName
- O
LSTM B-MethodName
- O
MoT O
are O
sentence O
- O
document O
models O
. O

The O
former O
explores O
CNN O
for O
sentence O
representation O
and O
LSTM B-MethodName
for O
text O
representation O
, O
and O
the O
latter O
use O
both O
LSTMs B-MethodName
for O
sentence O
and O
text O
representation O
with O
MoT O
pooling O
. O

In O
Table O
5 O
, O
LSTM B-MethodName
- O
CNN O
- O
MoT O
and O
LSTM B-MethodName
- O
LSTM B-MethodName
- O
MoT O
obtain O
large O
improvements O
compared O
to O
LSTM B-MethodName
- O
MoT O
, O
especially O
for O
prompt O
8 O
essays O
, O
of O
which O
the O
average B-MetricName
script O
length O
is O
the O
biggest O
. O

Local O
vs O
Global O
In O
Table O
5 O
, O
we O
compare O
LSTMCNN B-MethodName
- O
MoT O
with O
CNN O
- O
CNN O
- O
MoT O
to O
analyze O
the O
effectiveness O
of O
LSTM B-MethodName
for O
text O
representation O
over O
CNN O
. O

Both O
CNN O
- O
CNN O
- O
MoT O
and O
LSTM B-MethodName
- O
CNNMoT O
learn O
hierarchical O
sentence O
- O
document O
representations O
. O

The O
latter O
employs O
a O
CNN O
to O
learn O
sentence O
representation O
at O
the O
bottom O
, O
stacks O
one O
layer O
of O
LSTM B-MethodName
above O
to O
learntext O
representation O
, O
and O
mean B-MetricName
- O
over O
- O
time O
pooling O
is O
also O
used O
after O
CNN O
and O
LSTM B-MethodName
. O

Compared O
with O
CNN O
- O
CNN O
- O
MoT O
in O
Table O
5 O
, O
LSTM B-MethodName
- O
CNNMoT O
gives O
a O
big O
improvement O
. O

We O
believe O
that O
on O
text O
representation O
layer O
, O
LSTMs B-MethodName
can O
learn O
more O
global O
information O
, O
such O
as O
sentence O
coherence O
, O
while O
CNNs O
learn O
more O
local O
features O
, O
such O
as O
ngrams O
and O
bag O
- O
of O
- O
words O
. O

LSTM B-MethodName
- O
LSTM B-MethodName
- O
MoT O
outperforms O
CNN O
- O
CNN O
- O
MoT O
and O
gets O
slightly O
worse O
than O
LSTM B-MethodName
- O
CNN O
- O
MoT O
, O
which O
also O
shows O
that O
LSTM B-MethodName
is O
relatively O
more O
effective O
for O
modeling O
the O
documents O
. O

The O
pooling O
layers O
are O
used O
after O
both O
CNN O
and O
LSTM B-MethodName
layer O
to O
get O
sentence O
representation O
and O
text O
representation O
respectively O
. O

Taghipour O
and O
Ng O
( O
2016 O
) O
tried O
to O
attend O
over O
words O
on O
their O
one O
- O
layer O
LSTM B-MethodName
model O
, O
but O
failed O
to O
beat O
the O
baseline O
model O
that O
employs O
mean B-MetricName
- O
over O
- O
time O
pooling O
, O
because O
of O
that O
text O
- O
level O
model O
contains O
a O
quite O
long O
sequence O
of O
words O
, O
which O
may O
weaken O
the O
effect O
of O
attention O
. O

Step O
- O
wise O
linear B-MethodName
regression I-MethodName
is O
employed O
in O
the O
e O
- O
rater O
systems O
along O
with O
grammatical O
errors O
, O
lexical O
complexity O
as O
handcrafted O
features O
. O

In O
the O
research O
literature O
, O
Larkey O
( O
1998 O
) O
uses O
Naive B-MethodName
Bayes I-MethodName
model O
and O
takes O
AES O
as O
a O
classiﬁcation O
model O
. O

Rudner O
and O
Liang O
( O
2002 O
) O
explore O
multinomial O
Bernoulli O
Naive B-MethodName
Bayes I-MethodName
models O
to O
classify O
texts O
into O
several O
categories O
of O
text O
quality O
based O
on O
content O
and O
style O
features O
. O

It O
leverages O
scorespeciﬁc B-MetricName
word O
embeddings O
( O
SSWEs O
) O
for O
word O
representations O
, O
and O
takes O
the O
last O
hidden O
states O
of O
a O
two O
- O
layer O
bidirectional O
LSTM B-MethodName
for O
essay O
representations O
. O

Taghipour O
and O
Ng O
( O
2016 O
) O
also O
adopt O
a O
LSTM B-MethodName
model O
for O
AES O
, O
but O
use O
ordinary O
word O
embedding O
and O
take O
the O
average B-MetricName
pooling O
value O
of O
all O
the O
hidden O
states O
of O
LSTM B-MethodName
layer O
as O
the O
essay O
representations O
. O

Our O
work O
contributes O
to O
the O
research O
literature O
by O
systematically O
investigating O
CNN O
and O
LSTM B-MethodName
on O
sentence O
- O
level O
and O
text O
- O
level O
modeling O
, O
and O
the O
effectiveness O
of O
attention O
network O
on O
automatically O
selecting O
more O
relevant O
ngrams O
and O
sentences O
for O
the O
task O
. O

Li O
et O
al O
. O
( O
2015 O
) O
build O
a O
hierarchical O
LSTM B-MethodName
auto O
- O
encoder O
for O
documents O
. O

Yang O
et O
al O
. O
( O
2016 O
) O
build O
hierarchical O
LSTM B-MethodName
models O
with O
attention O
for O
document O
and O
Tang O
et O
al O
. O
( O
2015 O
) O
use O
a O
hierarchical O
Gated O
RNN O
for O
sentiment O
classiﬁcation O
. O

Ren O
and O
Zhang O
( O
2016 O
) O
use O
hierarchical O
CNN O
- O
LSTM B-MethodName
model O
for O
spam O
detection O
. O

We O
use O
a O
hierarchical O
CNNLSTM B-MethodName
model O
for O
essay O
scoring O
, O
which O
is O
a O
regression O
task.7 O
Conclusion O
We O
investigated O
a O
recurrent O
convolutional O
neural O
network O
to O
learn O
text O
representation O
and O
grade O
essays O
automatically O
. O

We O
evaluate O
our O
models O
on O
49 O
languages O
and O
show O
that O
the O
neural B-MethodName
architecture I-MethodName
that O
models O
the O
morphological O
labels O
as O
sequences O
of O
morphological O
category O
values O
performs O
signiﬁcantly O
better O
than O
both O
baselines O
establishing O
state O
- O
of O
- O
the O
- O
art O
results O
in O
morphological O
tagging O
for O
most O
languages.1 O
1 O
Introduction O
The O
common O
approach O
to O
morphological O
tagging O
combines O
the O
set O
of O
word O
’s O
morphological O
features O
into O
a O
single O
monolithic O
tag O
and O
then O
, O
similar O
to O
POS B-TaskName
tagging O
, O
employs O
multiclass O
sequence O
classiﬁcation O
models O
such O
as O
CRFs O
( O
Müller O
et O
al O
. O
, O
2013 O
) O
or O
recurrent O
neural B-MethodName
networks I-MethodName
( O
Labeau O
et O
al O
. O
, O
2015 O
; O
Heigold O
et O
al O
. O
, O
2017 O
) O
. O

All O
our O
models O
share O
the O
same O
neural O
encoder O
architecture O
based O
on O
bidirectional O
LSTMs B-MethodName
to O
construct O
contextual O
representations O
for O
words O
( O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

Inoue O
et O
al O
. O
( O
2017 O
) O
combined O
these O
classiﬁers O
into O
a O
multitask O
neural O
model O
sharing O
the O
same O
encoder O
, O
and O
predicted O
both O
POS B-TaskName
tag O
and O
morphological O
category O
values O
given O
the O
same O
contextual O
representation O
computed O
by O
a O
bidirectional O
LSTM B-MethodName
. O

In O
neural B-MethodName
networks I-MethodName
, O
this O
idea O
can O
be O
implemented O
with O
recurrent O
sequence O
modeling O
. O

Indeed O
, O
one O
of O
our O
proposed O
models O
generates O
morphological O
tags O
with O
an O
LSTM B-MethodName
network O
. O

While O
Heigold O
et O
al O
. O
( O
2017 O
) O
found O
that O
their O
neural O
model O
with O
bidirectional O
LSTM B-MethodName
encoder O
surpasses O
the O
CRF O
baseline O
, O
the O
results O
of O
Yu O
et O
al O
. O
( O
2017 O
) O
are O
mixed O
with O
the O
convolutional O
encoder O
being O
slightly O
better O
or O
on O
par O
with O
the O
CRF O
but O
the O
LSTM B-MethodName
encoder O
being O
worse O
than O
the O
CRF O
baseline O
. O

Most O
previous O
work O
on O
neural O
POS B-TaskName
and O
morphological O
tagging O
has O
shared O
the O
general O
idea O
of O
using O
bidirectional O
LSTM B-MethodName
for O
computing O
contextual O
features O
for O
words O
( O
Ling O
et O
al O
. O
, O
2015 O
; O
Huang O
et O
al O
. O
, O
2015 O
; O
Labeau O
et O
al O
. O
, O
2015 O
; O
Ma O
and O
Hovy O
, O
2016 O
; O
Heigold O
et O
al O
. O
, O
2017 O
) O
. O

We O
adopt O
the O
general O
encoder O
architecture O
from O
these O
works O
, O
constructing O
word O
representations O
from O
characters O
and O
using O
another O
bidirectional O
LSTM B-MethodName
to O
encode O
the O
context O
vectors O
. O

For O
each O
word O
in O
a O
sentence O
, O
the O
decoder O
uses O
a O
unidirectional O
LSTM B-MethodName
network O
( O
Figure O
1 O
( O
c O
) O
) O
to O
generate O
a O
sequence O
of O
morphological O
category O
- O
value O
pairs O
based O
on O
the O
context O
vector O
h O
and O
the O
previous O
predictions O
. O

It O
consists O
of O
a O
bidirectional O
LSTM B-MethodName
network O
that O
maps O
words O
in O
a O
sentence O
into O
context O
vectors O
using O
character O
and O
wordlevel O
embeddings O
. O

Character O
- O
level O
word O
embeddings O
are O
constructed O
with O
a O
bidirectional O
LSTM B-MethodName
network O
and O
they O
capture O
useful O
information O
about O
words O
’ O
morphology O
and O
shape O
. O

The O
character O
and O
word O
- O
level O
embeddings O
are O
concatenated O
and O
passed O
as O
inputs O
to O
the O
bidirectional O
LSTM B-MethodName
encoder O
. O

The O
proposed O
model O
consists O
of O
an O
LSTM B-MethodName
- O
based O
encoder O
, O
identical O
to O
the O
one O
described O
above O
in O
section O
3.3 O
, O
and O
a O
softmax O
classiﬁer O
over O
the O
full O
tagset O
. O

4.3 O
Training O
and O
Parametrisation O
Since O
tuning O
model O
hyperparameters O
for O
each O
of O
the O
69 O
datasets O
individually O
is O
computationally O
demanding O
, O
we O
optimise O
parameters O
on O
Finnish O
— O
a O
morphologically O
complex O
language O
with O
a O
reasonable O
dataset O
size O
— O
and O
apply O
the O
resulting O
values O
to O
6http://cistern.cis.lmu.de/marmot/SEQOTHER O
NN O
Encoder O
Word O
embedding O
size O
300 O
300 O
Character O
embedding O
size O
100 O
100 O
Character O
LSTM B-MethodName
hidden B-HyperparameterName
layer I-HyperparameterName
size O
150 B-HyperparameterValue
150 O
Word O
embedding O
dropout B-HyperparameterName
0.5 B-HyperparameterValue
0.5 O
LSTM B-MethodName
layers O
1 O
1 O
LSTM B-MethodName
hidden O
state O
size O
400 B-HyperparameterValue
400 O
LSTM B-MethodName
input O
dropout B-HyperparameterName
0.5 O
0.5 O
LSTM B-MethodName
state O
dropout B-HyperparameterName
0.3 B-HyperparameterValue
0.3 O
LSTM B-MethodName
output O
dropout B-HyperparameterName
0.5 O
0.5 O
Decoder O
LSTM B-MethodName
hidden O
state O
size O
800 O
800 O
Tag O
embedding O
size O
150 O
– O
Training O
Initial O
learning B-HyperparameterName
rate I-HyperparameterName
1.0 B-HyperparameterValue
1.0 O
Batch O
size O
5 O
20 O
Maximum O
epochs B-HyperparameterName
400 O
400 O
Learning O
rate O
decay O
factor O
– O
0.98 O
Table O
2 O
: O
Hyperparameters O
for O
neural O
models O
. O

We O
ﬁrst O
tuned O
the O
character O
embedding O
size O
and O
character O
- O
LSTM B-MethodName
hidden B-HyperparameterName
layer I-HyperparameterName
size O
of O
the O
encoder O
on O
the O
SEQmodel O
and O
reused O
the O
obtained O
values O
with O
all O
other O
models O
. O

We O
ﬁrst O
independently O
optimised O
the O
dropout B-HyperparameterName
rates I-HyperparameterName
on O
word O
embeddings O
, O
encoder O
’s O
LSTM B-MethodName
inputs O
and O
outputs O
, O
as O
well O
as O
the O
number O
of O
LSTM B-MethodName
layers O
. O

Parameter O
Values O
Word O
embedding O
dropout B-HyperparameterName
f0;0:1 O
; O
: O
: O
: O
; O
0:5 O
g O
LSTM B-MethodName
input O
dropout B-HyperparameterName
f0;0:1 O
; O
: O
: O
: O
; O
0:5 O
g O
LSTM B-MethodName
input O
dropout B-HyperparameterName
f0;0:1 O
; O
: O
: O
: O
; O
0:5 O
g O
Number O
of O
LSTM B-MethodName
layers O
f1;2 O
g O
Initial O
learning B-HyperparameterName
rate I-HyperparameterName
f0:01;0:1;1;2 O
g O
Learning O
rate O
decay O
factor O
f0:97;0:98;0:99;1 O
g O
Decay O
step O
f1250 O
; O
2500 B-HyperparameterValue
; O
5000 O
g O
Table O
6 O
: O
The O
grid O
values O
for O
hyperparameter O
tuning O
. O

Dropout O
is O
used O
to O
avoid O
overﬁtting O
by O
randomly O
dropping O
units O
from O
the O
neural B-MethodName
networks I-MethodName
during O
training O
. O

Inspired O
by O
dropout B-HyperparameterName
, O
this O
paper O
presents O
GI O
- O
Dropout O
, O
a O
novel O
dropout B-HyperparameterName
method O
integrating O
with O
global O
information O
to O
improve O
neural B-MethodName
networks I-MethodName
for O
text O
classiﬁcation O
. O

1 O
Introduction O
Recently O
, O
neural B-MethodName
networks I-MethodName
have O
achieved O
remarkable O
results O
in O
natural O
language O
processing O
( O
NLP O
) O
. O

However O
, O
with O
the O
consideration O
of O
computational O
complexity O
and O
spatial O
limitation O
, O
neural B-MethodName
networks I-MethodName
are O
often O
trained O
via O
mini O
- O
batch O
in O
which O
global O
information O
is O
gathered O
implicitly O
rather O
yHengru O
Xu O
and O
Shen O
Li O
contributed O
equally O
to O
this O
work O
. O

Unlike O
most O
of O
machine O
learning O
methods O
, O
the O
advantage O
of O
neural B-MethodName
networks I-MethodName
is O
extracting O
features O
with O
less O
need O
of O
feature O
engineering O
. O

However O
, O
during O
the O
training O
process O
, O
neural B-MethodName
networks I-MethodName
tend O
to O
focus O
on O
some O
distinctive O
words O
or O
phrases O
but O
ignore O
other O
noteworthy O
patterns O
, O
which O
may O
result O
in O
overﬁtting O
, O
especially O
in O
a O
small O
dataset O
. O

With O
this O
dropout B-HyperparameterName
method O
, O
neural B-MethodName
networks I-MethodName
tend O
to O
extract O
not O
only O
the O
obvious O
features O
but O
also O
the O
unobvious O
features O
which O
are O
also O
helpful O
for O
the O
classiﬁcation O
. O

2 O
Related O
Work O
Recently O
, O
neural B-MethodName
networks I-MethodName
dominate O
the O
state O
- O
ofthe O
- O
art O
results O
on O
a O
wide O
range O
of O
NLP O
tasks O
. O

Following O
this O
work O
, O
Yin O
and O
Sch O
¨utze O
( O
2015 O
) O
introduce O
multichannel O
variable O
- O
size O
convolution O
, O
and O
Zhang O
et O
al O
. O
( O
2016b O
) O
exploit O
different O
pre O
- O
trained O
word O
embeddings O
( O
e.g. O
word2vec O
and O
GloVe B-MethodName
) O
. O

Hinton O
et O
al O
. O
( O
2012 O
) O
introduce O
Binary O
( O
regular O
) O
Dropout O
, O
showing O
that O
it O
can O
prevent O
co O
- O
adaptation O
of O
neurons O
by O
randomly O
dropping O
units O
from O
the O
neural B-MethodName
networks I-MethodName
during O
training O
, O
so O
as O
to O
reduce O
overﬁtting O
. O

Ba O
and O
Frey O
( O
2013 O
) O
present O
standout O
, O
an O
adaptive O
dropout B-HyperparameterName
method O
, O
where O
each O
variable O
’s O
dropout B-HyperparameterName
probability O
is O
calculated O
by O
a O
binary O
belief O
network O
, O
which O
can O
be O
trained O
jointly O
with O
the O
neural B-MethodName
networks I-MethodName
. O

Hence O
, O
neural B-MethodName
networks I-MethodName
are O
able O
to O
extract O
not O
only O
the O
obvious O
features O
but O
also O
the O
unobvious O
features O
which O
are O
also O
helpful O
for O
the O
classiﬁcation O
. O

Since O
neural B-MethodName
networks I-MethodName
aim O
to O
capture O
semantic O
features O
and O
classify O
sentences O
by O
the O
features O
, O
we O
encourage O
models O
to O
share O
more O
attention O
to O
unobvious O
features O
by O
dropping O
words O
according O
to O
their O
importance O
. O

4.3 O
Self O
- O
attentive O
RNN O
Model O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
is O
a O
speciﬁc O
recurrent O
neural O
network O
( O
RNN O
) O
architecture O
which O
is O
good O
at O
modeling O
temporal O
sequences O
and O
6A O
widely O
used O
publicly O
available O
word2vec O
300dimension O
vectors O
which O
were O
trained O
on O
100 O
billion O
words O
from O
Google O
News O
in O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
way O
. O
Parameters O
Values O
Word O
embeddings O
GoogleNews O
- O
negative3006 O
Fine O
- O
tune O
Yes O
Convolution O
1 O
- O
d O
Filter O
size O
[ O
3 O
, O
4 O
, O
5 O
] O
Filter O
numbers O
300 O
Activation O
function O
ReLU O
Pooling O
method O
max O
- O
over O
- O
time O
MLP O
dropout B-HyperparameterName
rate I-HyperparameterName
0.5 B-HyperparameterValue
Table O
2 O
: O
CNN O
conﬁguration O
. O

The O
self O
- O
attentive O
RNN O
proposed O
by O
Lin O
et O
al O
. O
( O
2017 O
) O
consists O
of O
a O
bidirectional O
LSTM B-MethodName
( O
biLSTM B-MethodName
) O
and O
the O
selfattention O
mechanism O
. O

Self O
- O
attention O
mechanism O
is O
used O
to O
replace O
the O
max O
pooling O
or O
averaging O
step O
after O
the O
biLSTM B-MethodName
. O

In O
brief O
, O
suppose O
we O
have O
a O
sentence O
of O
ntokens O
, O
and O
let O
the O
hidden B-HyperparameterName
unit I-HyperparameterName
number O
for O
each O
unidirectional O
LSTM B-MethodName
be O
u. O
After O
the O
biLSTM B-MethodName
layer O
, O
we O
can O
get O
H O
, O
which O
have O
the O
size O
of O
n O
- O
by-2u O
. O

The O
attention O
mechanism O
takes O
the O
whole O
LSTM B-MethodName
hidden O
states O
Has O
input O
, O
and O
outputs O
a O
vector O
of O
weightsa B-HyperparameterName
, O
a O
= O
softmax O
( O
ws2tanh(Ws1HT O
) O
) O
( O
5 O
) O
whereWs1is O
a O
weight B-HyperparameterName
matrix O
with O
a O
shape O
of O
daby-2u O
, O
andWs2is O
a O
vector O
of O
parameters O
with O
size O
dawhich O
is O
a O
hyperparameter O
. O

In O
the O
end O
, O
the O
annotation O
vector O
a O
becomes O
annotation O
matrix O
A. O
A O
= O
softmax O
( O
Ws2tanh(Ws1HT O
) O
) O
( O
6 O
) O
The O
sentence O
embedding O
is O
: O
M O
= O
AH O
( O
7 O
) O
Then O
the O
paper O
uses O
two O
layer O
2 O
- O
layer O
MLP O
with O
ReLU O
activation B-HyperparameterName
function O
to O
predict O
the O
label O
of O
𝑊𝑠1𝑑𝑎 O
2𝑢ℎ1ℎ2ℎ3ℎ4⋯ℎ𝑛⋯𝑒1𝑒2𝑒3𝑒5𝑒4𝑒6 O
Dropout O
𝐻biLSTM× B-MethodName
GI O
- O
DropoutThestoryissadandveryboring0 O
ThestoryissadandveryboringFigure O
5 B-HyperparameterValue
: O
Self O
- O
attentive O
RNN O
architectures O
. O

This O
model O
uses O
a O
bidirectional O
LSTM B-MethodName
with O
300dimensions O
of O
hidden O
states O
in O
each O
direction O
. O

In O
self O
attention O
part O
, O
dais350and O
the O
coefﬁcient O
of O
the O
penalization O
term O
is O
1.ris O
set O
to4considering O
the O
size O
of O
datasets O
and O
the O
lengthParameters O
Values O
Word O
embeddings O
Glove-3007 O
Fine O
- O
tune O
Yes O
biLSTM B-MethodName
hidden B-HyperparameterName
units I-HyperparameterName
300 B-HyperparameterValue
da O
350 O
r O
4 O
MLP O
Activation O
ReLU O
MLP O
dropout B-HyperparameterName
rate I-HyperparameterName
0.5 B-HyperparameterValue
Table O
3 O
: O
Self O
- O
attentive O
RNN O
conﬁguration O
. O

7A O
widely O
used O
publicly O
available O
300 O
- O
dimension O
word O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014).Model O
MR O
SST-1 O
SST-2 B-DatasetName
Subj O
TREC O
CR O
MPQA O
CNN O
- O
non O
- O
static O
81.5 O
48.0 O
87.2 O
93.4 O
93.6 O
84.3 O
89.5 O
CNN O
- O
reproduce O
81.4 O
47.8 O
87.5 O
93.0 O
92.4 O
84.3 O
89.6 O
CNN O
- O
Dropout O
- O
same O
( O
p O
) O
81.5(0.1 O
) O
48.5(0.1 O
) O
87.6(0.1 O
) O
93.5(0.2 O
) O
92.9(0.1 O
) O
84.5(0.5 O
) O
87.4(0.1 O
) O
CNN O
- O
GI O
- O
Dropout O
( O
 O
) O
81.9(0.87 O
) O
49.0(0.95 O
) O
88.1(0.98 O
) O
93.4(0.91 O
) O
93.2(0.83 O
) O
85.1(0.87 O
) O
89.8(0.98 O
) O
RNN O
- O
baseline O
82.1 O
49.7 O
89.7 O
93.6 O
92.6 O
84.1 O
89.6 O
RNN O
- O
Dropout O
- O
same O
( O
p O
) O
82.2(0.2 O
) O
51.9(0.1 O
) O
90.1(0.1 O
) O
93.9(0.1 O
) O
93.4(0.2 O
) O
84.2(0.1 O
) O
89.7(0.1 O
) O
RNN O
- O
GI O
- O
Dropout O
( O
 O
) O
82.5(0.87 O
) O
54.1(0.95 O
) O
90.4(0.95 O
) O
94.2(0.98 O
) O
94.8(0.95 O
) O
84.7(0.91 O
) O
89.7(0.98 O
) O
MVCNN O
- O
49.6 O
89.4 O
93.9 O
- O
- O
MGNC O
- O
CNN O
- O
48.7 O
88.3 O
94.1 O
95.5 O
- O
CNN O
- O
Rule O
81.7 O
- O
89.3 O
- O
- O
85.3 O
Semantic O
- O
CNN O
82.1 O
50.8 O
89.0 O
93.7 O
94.4 O
86.0 O
89.3 O
combine O
- O
skip O
76.5 O
- O
- O
93.6 O
92.2 O
80.1 O
87.1 O
DSCNN O
82.2 O
50.6 O
88.7 O
93.9 O
95.6 O
- O
Paragraph O
Vector O
74.8 O
48.7 O
87.8 O
90.5 O
91.8 O
78.1 O
74.2 O
NBSVM B-MethodName
79.4 O
- O
- O
93.2 O
- O
81.8 O
86.3 O
Tree O
LSTM B-MethodName
- O
51.0 O
88.0 O
- O
- O
- O
Table O
4 O
: O
Effectiveness O
of O
GI O
- O
Dropout O
. O

Results O
also O
include O
: O
MVCNN O
( O
Yin O
and O
Sch O
¨utze O
, O
2015 O
) O
, O
MGNC O
- O
CNN O
( O
Zhang O
et O
al O
. O
, O
2016b O
) O
, O
CNN O
- O
Rule O
( O
Hu O
et O
al O
. O
, O
2016 O
) O
, O
Semantic O
- O
CNN O
( O
Li O
et O
al O
. O
, O
2017 O
) O
, O
combine O
- O
skip O
( O
Kiros O
et O
al O
. O
, O
2015 O
) O
, O
combine O
- O
skip O
( O
Kiros O
et O
al O
. O
, O
2015 O
) O
, O
DSCNN O
( O
Zhang O
et O
al O
. O
, O
2016a O
) O
, O
Paragraph O
Vector O
( O
Le O
and O
Mikolov O
, O
2014 O
) O
, O
NBSVM B-MethodName
( O
Wang O
and O
Manning O
, O
2012 O
) O
and O
Tree O
LSTM B-MethodName
( O
Tai O
et O
al O
. O
, O
2015 O
) O
. O

5 O
Conclusion O
This O
paper O
proposes O
GI O
- O
Dropout O
, O
a O
novel O
dropout B-HyperparameterName
method O
which O
utilizes O
global O
information O
and O
guides O
neural B-MethodName
networks I-MethodName
to O
extract O
not O
only O
obvious O
features O
but O
also O
unobvious O
features O
. O

2 O
Related O
Work O
Recently O
, O
the O
RNN O
- O
based O
generators O
have O
shown O
improving O
results O
in O
tackling O
the O
NLG O
problems O
in O
task O
oriented O
- O
dialogue O
systems O
with O
varied O
proposed O
methods O
, O
such O
as O
HLSTM B-MethodName
( O
Wen O
et O
al O
. O
, O
2015a O
) O
, O
SCLSTM B-MethodName
( O
Wen O
et O
al O
. O
, O
2015b O
) O
, O
or O
espe O
- O
cially O
RNN O
Encoder O
- O
Decoder O
models O
integrating O
with O
attention O
mechanism O
, O
such O
as O
Enc O
- O
Dec O
( O
Wen O
et O
al O
. O
, O
2016b O
) O
, O
and O
RALSTM B-MethodName
( O
Tran O
and O
Nguyen O
, O
2017 O
) O
. O

A O
hierarchical O
multi O
scale O
recurrent O
neural B-MethodName
networks I-MethodName
was O
proposed O
to O
learn O
both O
hierarchical O
and O
temporal O
representation O
( O
Chung O
et O
al O
. O
, O
2016 O
) O
, O
while O
Bowman O
et O
al O
. O
( O
2015 O
) O
presented O
a O
variational O
autoencoder O
for O
unsupervised O
generative O
language O
model O
. O

We O
then O
employ O
deep O
neural B-MethodName
networks I-MethodName
to O
approximate O
the O
prior O
p(zjd O
) O
, O
true O
posteriorp(zjd;u O
) O
, O
and O
decoder O
p(ujz;d O
) O
. O

According O
to O
( O
Sohn O
et O
al O
. O
, O
2015 O
) O
, O
the O
variational O
lower O
bound O
can O
be O
recomputed O
as O
: O
L( O
; O
 O
; O
d;u O
) O
= O
 KL(q O
 O
( O
zjd;u)jjp(zjd O
) O
) O
+ O
Eq O
 O
( O
zjd;u)[logp(ujz;d)]logp(ujd)(2 O
) O
3.1.1 O
Variational O
Encoder O
Network O
The O
encoder O
consists O
of O
two O
networks O
: O
( O
i O
) O
a O
Bidirectional O
LSTM B-MethodName
( O
BiLSTM B-MethodName
) O
which O
encodes O
the O
sequence O
of O
slot O
- O
value O
pairs O
fsvigTDA O
i=1by O
separate O
parameterization O
of O
slots O
and O
values O
( O
Wen O
et O
al O
. O
, O
2016b O
) O
; O
and O
( O
ii O
) O
a O
shared O
CNN O
/ O
RNN O
Utterance O
Encoder O
which O
encodes O
the O
corresponding O
utterance O
. O

3.1.2 O
Variational O
Inference O
Network O
This O
section O
models O
both O
the O
prior O
p(zjd)and O
the O
posteriorq O
 O
( O
zjd;u)by O
utilizing O
neural B-MethodName
networks I-MethodName
. O

3.1.3 O
Variational O
Decoder O
Network O
Given O
a O
DA O
dand O
the O
latent O
variable O
z O
, O
the O
decoder O
calculates O
the O
probability O
over O
the O
generation O
uas O
a O
joint O
probability O
of O
ordered O
conditionals O
: O
p(ujz;d O
) O
= O
TUY O
t=1p(utju O
< O
t;z;d O
) O
( O
8) O
wherep(utju O
< O
t;z;d)=g0(RALSTM B-MethodName
( O
ut;ht 1;dt O
) O
. O

The O
RALSTM B-MethodName
cell O
( O
Tran O
and O
Nguyen O
, O
2017 O
) O
is O
slightly O
modiﬁed O
in O
order O
to O
integrate O
the O
representation O
of O
latent O
variable O
, O
i.e. O
,he O
, O
into O
the O
computational O
cell O
( O
see O
Suppl O
. O

Speciﬁcally O
, O
after O
having O
the O
vector O
representation O
hU O
, O
we O
apply O
another O
linear B-MethodName
regression I-MethodName
to O
obtain O
the O
distribution O
parameter O
2 O
= O
W2hU+b2andlog2 O
2= O
W2hU+b2 O
. O

4.4 O
Joint O
Cross O
Training O
Dual O
VAE O
Model O
To O
allow O
the O
dual O
V O
AE O
model O
explore O
and O
encode O
useful O
information O
of O
the O
Dialogue O
Act O
into O
the O
latent O
variable O
, O
we O
further O
take O
a O
cross O
training O
between O
two O
V O
AEs O
by O
simply O
replacing O
the O
RALSTM B-MethodName
Decoder O
of O
the O
VNLG O
model O
with O
the O
DCNN O
Utterance O
Decoder O
and O
its O
objective O
trainingLDA O
- O
DCNN O
as O
: O
L(0 O
; O
 O
; O
d;u)' KL(q O
 O
( O
zjd;u)jjp0(zjd O
) O
) O
+ O
Eq O
 O
( O
zjd;u)[logp0(ujz;d O
) O
] O
; O
( O
12 O
) O
and O
a O
joint O
cross O
training O
objective O
is O
employed O
: O
LCrossVAE O
= O
LVNLG O
+ O
 O
( O
LCNN O
- O
DCNN O
+ O
LDA O
- O
DCNN O
) O
( O
13)5 O
Experiments O
We O
assessed O
the O
proposed O
models O
on O
four O
different O
original O
NLG O
domains O
: O
ﬁnding O
a O
restaurant O
and O
hotel O
( O
Wen O
et O
al O
. O
, O
2015a O
) O
, O
or O
buying O
a O
laptop O
and O
television O
( O
Wen O
et O
al O
. O
, O
2016b O
) O
. O

We O
compared O
the O
proposed O
models O
against O
strong O
baselines O
which O
have O
been O
recently O
published O
as O
NLG O
benchmarks O
of O
those O
datasets O
, O
including O
( O
i O
) O
gating O
models O
such O
as O
HLSTM B-MethodName
( O
Wen O
et O
al O
. O
, O
2015a O
) O
, O
and O
SCLSTM B-MethodName
( O
Wen O
et O
al O
. O
, O
2015b O
) O
; O
and O
( O
ii O
) O
attention O
models O
such O
as O
Enc O
- O
Dec O
( O
Wen O
et O
al O
. O
, O
2016b O
) O
, O
RALSTM B-MethodName
( O
Tran O
and O
Nguyen O
, O
2017 O
) O
. O

6.1 O
Integrating O
Variational O
Inference O
We O
compare O
the O
encoder O
- O
decoder O
RALSTM B-MethodName
model O
to O
its O
modiﬁcation O
by O
integrating O
with O
variational O
inference O
( O
R O
- O
VNLG O
and O
C O
- O
VNLG O
) O
as O
demonstrated O
in O
Figure O
3 O
and O
Table O
1 O
. O

It O
clearly O
shows O
that O
the O
variational O
generators O
not O
only O
provide O
a O
compelling O
evidence O
on O
adapting O
to O
a O
new O
, O
unseen O
domain O
when O
the O
target O
domain O
data O
is O
scarce O
, O
i.e. O
, O
from O
1 O
% O
to7 O
% O
( O
Figure O
3 O
) O
but O
also O
preserve O
the O
power O
of O
the O
original O
RALSTM B-MethodName
on O
generation O
task O
since O
their O
performances O
are O
very O
competitive O
to O
those O
of O
RALSTM B-MethodName
( O
Table O
1 O
, O
scr100 O
) O
. O

Table O
1 O
, O
scr10 O
further O
shows O
the O
necessity O
of O
the O
integrating O
in O
which O
the O
VNLGs O
achieved O
a O
signiﬁcant O
improvement O
over O
the O
RALSTM B-MethodName
in O
scr10 O
scenario O
where O
the O
models O
trained O
from O
scratch O
with O
only O
a O
limited O
amount O
of O
training O
data O
( O
10 O
% O
) O
. O

Take O
Hotel O
domain O
, O
for O
example O
, O
the O
C O
- O
VNLG O
model O
( O
79:98BLEU B-MetricName
, O
8:67 O
% O
ERR O
) O
has O
better O
results O
in O
comparison O
to O
the O
RVNLG O
( O
73:78BLEU B-MetricName
, O
15:43 O
% O
ERR O
) O
and O
RAL O
- O
ModelHotel O
Restaurant O
Tv O
Laptop O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERRscr100HLSTM B-MethodName
0.8488 B-MetricValue
2.79 O
% O
0.7436 O
0.85 O
% O
0.5240 O
2.65 O
% O
0.5130 O
1.15 O
% O
SCLSTM B-MethodName
0.8469 O
3.12 O
% O
0.7543 O
0.57 O
% O
0.5235 O
2.41 O
% O
0.5109 O
0.89 O
% O
ENCDEC O
0.8537 O
4.78 O
% O
0.7358 O
2.98 O
% O
0.5142 O
3.38 O
% O
0.5101 O
4.24 O
% O
RALSTM B-MethodName
0.8965 O
0.58 O
% O
0.7779 O
0.20 O
% O
0.5373 O
0.49 O
% O
0.5231 O
0.50 O
% O
R O
- O
VNLG O
( O
Ours O
) O
0.8851 O
0.57 O
% O
0.7709 O
0.36 O
% O
0.5356 O
0.73 O
% O
0.5210 O
0.59 O
% O
C O
- O
VNLG O
( O
Ours O
) O
0.8811 O
0.49 O
% O
0.7651 O
0.06 O
% O
0.5350 O
0.88 O
% O
0.5192 O
0.56 O
% O
DualV O
AE O
( O
Ours O
) O
0.8813 O
0.33 O
% O
0.7695 O
0.29 O
% O
0.5359 O
0.81 O
% O
0.5211 O
0.91 O
% O
CrossV O
AE O
( O
Ours O
) O
0.8926 O
0.72 O
% O
0.7786 O
0.54 O
% O
0.5383 O
0.48 O
% O
0.5240 O
0.50 O
% O
scr10HLSTM B-MethodName
0.7483 O
8.69 O
% O
0.6586 O
6.93 O
% O
0.4819 O
9.39 O
% O
0.4813 O
7.37 O
% O
SCLSTM B-MethodName
0.7626 O
17.42 O
% O
0.6446 O
16.93 O
% O
0.4290 O
31.87 O
% O
0.4729 O
15.89 O
% O
ENCDEC O
0.7370 O
23.19 O
% O
0.6174 O
23.63 O
% O
0.4570 O
21.28 O
% O
0.4604 O
29.86 O
% O
RALSTM B-MethodName
0.6855 O
22.53 O
% O
0.6003 O
17.65 O
% O
0.4009 O
22.37 O
% O
0.4475 O
24.47 O
% O
R O
- O
VNLG O
( O
Ours O
) O
0.7378 O
15.43 O
% O
0.6417 O
15.69 O
% O
0.4392 O
17.45 O
% O
0.4851 O
10.06 O
% O
C O
- O
VNLG O
( O
Ours O
) O
0.7998 O
8.67 O
% O
0.6838 O
6.86 O
% O
0.5040 O
5.31 O
% O
0.4932 O
3.56 O
% O
DualV O
AE O
( O
Ours O
) O
0.8022 O
6.61 O
% O
0.6926 O
7.69 O
% O
0.5110 O
3.90 O
% O
0.5016 O
2.44 O
% O
CrossV O
AE O
( O
Ours O
) O
0.8103 O
6.20 O
% O
0.6969 O
4.06 O
% O
0.5152 O
2.86 O
% O
0.5085 O
2.39 O
% O
scr30HLSTM B-MethodName
0.8104 O
6.39 O
% O
0.7044 O
2.13 O
% O
0.5024 O
5.82 O
% O
0.4859 O
6.70 O
% O
SCLSTM B-MethodName
0.8271 O
6.23 O
% O
0.6825 O
4.80 O
% O
0.4934 O
7.97 O
% O
0.5001 O
3.52 O
% O
ENCDEC O
0.7865 O
9.38 O
% O
0.7102 O
13.47 O
% O
0.5014 O
9.19 O
% O
0.4907 O
10.72 O
% O
RALSTM B-MethodName
0.8334 O
4.23 O
% O
0.7145 O
2.67 O
% O
0.5124 O
3.53 O
% O
0.5106 O
2.22 O
% O
C O
- O
VNLG O
( O
Ours O
) O
0.8553 O
2.64 O
% O
0.7256 O
0.96 O
% O
0.5265 O
0.66 O
% O
0.5117 O
2.15 O
% O
DualV O
AE O
( O
Ours O
) O
0.8534 O
1.54 O
% O
0.7301 O
2.32 O
% O
0.5288 O
1.05 O
% O
0.5107 O
0.93 O
% O
CrossV O
AE O
( O
Ours O
) O
0.8585 O
1.37 O
% O
0.7479 O
0.49 O
% O
0.5307 O
0.82 O
% O
0.5154 O
0.81 O
% O
Table O
1 O
: O
Results O
evaluated O
on O
four O
domains O
by O
training O
models O
from O
scratch O
with O
10%,30 O
% O
, O
and O
100 O
% O
in O
- O
domain O
data O
, O
respectively O
. O

6.2 O
Ablation O
Studies O
The O
ablation O
studies O
( O
Table O
1 O
) O
demonstrate O
the O
contribution O
of O
each O
model O
components O
, O
in O
which O
we O
incrementally O
train O
the O
baseline O
RALSTM B-MethodName
, O
the O
C O
- O
VNLG O
(= O
RALSTM B-MethodName
+ O
Variational O
inference O
) O
, O
the O
DualV O
AE O
(= O
C O
- O
VNLG O
+ O
Variational O
CNNDCNN O
) O
, O
and O
the O
CrossV O
AE O
(= O
DualV O
AE O
+ O
Cross O
training O
) O
models O
. O

In O
contrast O
, O
by O
integrating O
the O
variational O
inference O
, O
the O
C O
- O
VNLG O
model O
, O
for O
example O
in O
Hotel O
domain O
, O
can O
significantly O
improve O
the O
BLEU B-MetricName
score B-MetricName
from O
68:55to O
79:98 O
, O
and O
also O
reduce O
the O
slot O
error O
rate O
ERR O
by O
a O
large O
margin O
, O
from O
22:53to8:67 O
, O
compared O
to O
the O
RALSTM B-MethodName
baseline O
. O

The O
CrossV O
AE O
model O
trained O
on O
scr10 O
scenario O
, O
in O
some O
cases O
, O
achieved O
results O
which O
close O
to O
those O
of O
the O
HLSTM B-MethodName
, O
SCLSTM B-MethodName
, O
and O
ENCDEC O
models O
trained O
on O
all O
training O
data O
( O
scr100 O
) O
scenario O
. O

Take O
, O
for O
example O
, O
the O
most O
challenge O
dataset O
Laptop O
, O
in O
which O
the O
DualV O
AE O
and O
CrossV O
AE O
obtained O
competitive O
results O
regarding O
the O
BLEU B-MetricName
score B-MetricName
, O
at O
50:16and50:85respectively O
, O
which O
close O
to O
those O
of O
the O
HLSTM B-MethodName
( O
51:30BLEU B-MetricName
) O
, O
SCLSTM B-MethodName
( O
51:09BLEU B-MetricName
) O
, O
and O
ENCDEC O
( O
51:01 O
BLEU B-MetricName
) O
, O
while O
the O
results O
regardless O
the O
slot O
error O
rate O
ERR O
scores B-MetricName
are O
also O
close O
to O
those O
of O
the O
previous O
or O
even O
better O
in O
some O
cases O
, O
for O
example O
DualV O
AE O
( O
2:44ERR O
) O
, O
CrossV O
AE O
( O
2:39ERR),Figure O
4 B-MetricValue
: O
Performance O
comparison O
of O
the O
models O
trained O
on O
Laptop O
domain O
. O

Take O
Tv O
domain O
, O
for O
example O
, O
in O
which O
the O
CrossV O
AE O
inscr30 O
achieves O
a O
good O
result O
regarding O
BLEU B-MetricName
and O
slot O
error O
rate O
ERR O
score B-MetricName
, O
at O
53:07BLEU B-MetricName
and0:82ERR O
, O
that O
are O
not O
only O
competitive O
to O
the O
RALSTM B-MethodName
( O
53:73BLEU B-MetricName
, O
0:49ERR O
) O
, O
but O
also O
outperform O
the O
previous O
models O
in O
scr100 O
training O
scenario O
, O
such O
as O
HLSTM B-MethodName
( O
52:40BLEU B-MetricName
, O
2:65 O
ERR O
) O
, O
SCLSTM B-MethodName
( O
52:35BLEU B-MetricName
, O
2:41ERR O
) O
, O
and O
ENCDEC O
( O
51:42BLEU B-MetricName
, O
3:38ERR O
) O
. O

6.3 O
Model O
comparison O
on O
unseen O
domain O
In O
this O
experiment O
, O
we O
trained O
four O
models O
( O
ENCDEC O
, O
SCLSTM B-MethodName
, O
RALSTM B-MethodName
, O
and O
CrossV O
AE O
) O
from O
scratch O
in O
the O
most O
difﬁcult O
unseen O
Laptop O
domain O
with O
an O
increasingly O
varied O
proportion O
of O
training O
data O
, O
start O
from O
1 O
% O
to O
100 O
% O
. O

The O
CrossV O
AE O
model O
is O
clearly O
better O
than O
the O
previous O
models O
( O
ENCDEC O
, O
SCLSTM B-MethodName
, O
RALSTM B-MethodName
) O
in O
all O
cases O
. O

While O
the O
performance O
of O
the O
CrossV O
AE O
, O
RALSTM B-MethodName
model O
starts O
to O
saturate O
around O
30 O
% O
and O
50 O
% O
, O
respectively O
, O
the O
ENCDEC O
modelseems O
to O
continue O
getting O
better O
as O
providing O
more O
training O
data O
. O

6.4 O
Domain O
Adaptation O
We O
further O
examine O
the O
domain O
scalability O
of O
the O
proposed O
methods O
by O
training O
the O
CrossV O
AE O
and O
SCLSTM B-MethodName
models O
on O
adaptation O
scenarios O
, O
in O
which O
we O
ﬁrst O
trained O
the O
models O
on O
out O
- O
ofdomain O
data O
, O
and O
then O
ﬁne O
- O
tuned O
the O
model O
parameters O
by O
using O
a O
small O
amount O
( O
10 O
% O
) O
of O
indomain O
data O
. O

Both O
SCLSTM B-MethodName
and O
CrossV O
AE O
models O
can O
take O
advantage O
of O
“ O
close O
” O
dataset O
pairs O
, O
i.e. O
, O
Restaurant$Hotel O
, O
and O
Tv$Laptop O
, O
to O
achieve O
better O
performances O
compared O
to O
those O
of O
the O
“ O
different O
” O
dataset O
pairs O
, O
i.e. O
Latop$Restaurant O
. O

Moreover O
, O
Table O
2 O
clearly O
shows O
that O
the O
SCLSTM B-MethodName
( O
denoted O
by O
[ O
) O
is O
limited O
to O
scale O
to O
another O
domain O
in O
terms O
of O
having O
very O
low O
BLEU B-MetricName
and O
high O
ERR O
scores B-MetricName
. O

This O
adaptation O
scenario O
along O
with O
the O
scr10 O
and O
scr30 O
in O
Table O
1 O
demonstrate O
that O
the O
SCLSTM B-MethodName
can O
not O
work O
when O
having O
a O
low O
- O
resource O
setting O
of O
in O
- O
domain O
training O
data O
. O

Preliminary O
experiments O
on O
semi O
- O
supervised O
training O
were O
also O
conducted O
, O
in O
which O
we O
trained O
the O
CrossV O
AE O
model O
with O
the O
same O
10 O
% O
indomain O
labeled O
data O
as O
in O
the O
other O
scenarios O
andSource O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
Hotel[- O
- O
0.6243 B-MetricValue
11.20 O
% O
0.4325 O
29.12 O
% O
0.4603 O
22.52 O
% O
Restaurant[0.7329 O
29.97 O
% O
- O
- O
0.4520 O
24.34 O
% O
0.4619 O
21.40 O
% O
Tv[0.7030 O
25.63 O
% O
0.6117 O
12.78 O
% O
- O
- O
0.4794 O
11.80 O
% O
Laptop[0.6764 O
39.21 O
% O
0.5940 O
28.93 O
% O
0.4750 O
14.17 O
% O
- O
Hotel]- O
- O
0.7138 O
2.91 O
% O
0.5012 O
5.83 O
% O
0.4949 O
1.97 O
% O
Restaurant]0.7984 O
4.04 O
% O
- O
- O
0.5120 O
3.26 O
% O
0.4947 O
1.87 O
% O
Tv]0.7614 O
5.82 O
% O
0.6900 O
5.93 O
% O
- O
- O
0.4937 O
1.91 O
% O
Laptop]0.7804 O
5.87 O
% O
0.6565 O
6.97 O
% O
0.5037 O
3.66 O
% O
- O
Hotel- O
- O
0.6926 O
3.56 O
% O
0.4866 O
11.99 O
% O
0.5017 O
3.56 O
% O
Restaurant0.7802 O
3.20 O
% O
- O
- O
0.4953 O
3.10 O
% O
0.4902 O
4.05 O
% O
Tv0.7603 O
8.69 O
% O
0.6830 O
5.73 O
% O
- O
- O
0.5055 O
2.86 O
% O
Laptop0.7807 O
8.20 O
% O
0.6749 O
5.84 O
% O
0.4988 O
5.53 O
% O
- O
CrossV O
AE O
( O
scr10 O
) O
0.8103 O
6.20 O
% O
0.6969 O
4.06 O
% O
0.5152 O
2.86 O
% O
0.5085 O
2.39 O
% O
CrossV O
AE O
( O
semi O
- O
U50 O
- O
L10 O
) O
0.8144 O
6.12 O
% O
0.6946 O
3.94 O
% O
0.5158 O
2.95 O
% O
0.5086 O
1.31 O
% O
Table O
2 O
: O
Results O
evaluated O
on O
Target O
domains O
by O
adaptation O
training O
SCLSTM B-MethodName
model O
from O
100 O
% O
( O
denoted O
as O
[ O
) O
of O
Source O
data O
, O
and O
the O
CrossV O
AE O
model O
from O
30 O
% O
( O
denoted O
as O
] O
) O
, O
100 O
% O
( O
denoted O
as O
 O
) O
of O
Source O
data O
. O

The O
ENCDEC O
, O
HLSTM B-MethodName
and O
SCLSTM B-MethodName
models O
in O
Table O
3- O
DA O
1 O
, O
for O
example O
, O
tend O
to O
generate O
outputs O
with O
redundant O
slots O
( O
i.e. O
,SLOT O
HDMIPORT O
, O
SLOT O
NAME O
, O
SLOT O
FAMILY O
) O
, O
missing O
slots O
( O
i.e. O
, O
[ O
l7 O
family O
] O
, O
[ O
4 O
hdmi O
port O
-s O
] O
) O
, O
or O
even O
in O
some O
cases O
produce O
irrelevant O
slots O
( O
i.e. O
,SLOT O
AUDIO O
, O
eco O
rating O
) O
, O
resulting O
in O
inadequate O
utterances O
. O

HLSTM-10 B-MethodName
the O
typhon O
45 O
is O
a O
great O
eco O
rating O
, O
the O
hades O
48 O
is O
in O
the O
l2 O
family O
with O
2 O
hdmi O
port O
-s O
. O

[ O
l7 O
family O
] O
[ O
4 O
hdmi O
port O
-s O
] O
SCLSTM-10 B-MethodName
the O
typhon O
45 O
is O
the O
hades O
48 O
with O
2 O
hdmi O
port O
in O
the O
l2 O
family O
, O
the O
SLOT O
NAME O
has4 O
hdmi O
port O
-s O
andSLOT O
HDMIPORT O
hdmi O
port O
. O

[ O
l2 O
family O
] O
[ O
l7 O
family O
] O
HLSTM-30 B-MethodName
the O
typhon O
45 O
is O
in O
the O
l2 O
product O
family O
with O
2 O
hdmi O
port O
-s O
, O
whereas O
the O
hades O
48 O
has4 O
hdmi O
port O
. O

[ O
l7 O
family O
] O
SCLSTM-30 B-MethodName
the O
typhon O
45 O
has2 O
hdmi O
port O
-s O
, O
the O
hades O
48 O
is O
in O
the O
l2 O
product O
family O
. O

its O
price O
is O
1500 O
dollars O
, O
its O
audio O
is O
nicam O
stereo O
, O
and O
it O
has O
2 O
hdmi O
port O
-s O
. O
ENCDEC-10 O
the O
proteus O
73 O
is O
a O
great O
television O
with O
a O
nicam O
stereo O
and2 O
hdmi O
port O
-s O
[ O
1500 O
dollars O
] O
HLSTM-10 B-MethodName
the O
proteus O
73 O
is O
atelevision O
with O
2 O
hdmi O
port O
-s O
and O
comes O
with O
a O
nicam O
stereo O
andcosts O
1500 O
dollars O
[ O
OK O
] O
SCLSTM-10 B-MethodName
the O
proteus O
73 O
is O
a O
nice O
television O
with O
nicam O
stereo O
and2 O
hdmi O
port O
-s O
[ O
1500 O
dollars O
] O
C O
- O
VNLG-10 O
the O
proteus O
73 O
television O
has O
a O
nicam O
stereo O
and2 O
hdmi O
port O
-s O
andcosts O
1500 O
dollars O
[ O
OK O
] O
DualV O
AE-10 O
the O
proteus O
73 O
television O
has O
a O
nicam O
stereo O
and2 O
hdmi O
port O
-s O
andcosts O
1500 O
dollars O
[ O
OK O
] O
CrossV O
AE-10 O
the O
proteus O
73 O
television O
has2 O
hdmi O
port O
-s O
and O
a O
nicam O
stereo O
andcosts O
1500 O
dollars O
[ O
OK O
] O
ENCDEC-30 O
the O
proteus O
73 O
television O
has2 O
hdmi O
port O
-s O
andnicam O
stereo O
audio O
for1500 O
dollars O
[ O
OK O
] O
HLSTM-30 B-MethodName
the O
proteus O
73 O
television O
has O
a O
nicam O
stereo O
and2 O
hdmi O
port O
-s O
and O
is O
priced O
at O
1500 O
dollars O
[ O
OK O
] O
SCLSTM-30 B-MethodName
the O
proteus O
73 O
is O
a O
nice O
television O
with O
nicam O
stereo O
and2 O
hdmi O
port O
-s O
. O
it O
is O
priced O
at O
1500 O
dollars O
[ O
OK O
] O
C O
- O
VNLG-30 O
the O
proteus O
73 O
television O
has2 O
hdmi O
port O
-s O
, O
nicam O
stereo O
audio O
, O
and O
costs O
1500 O
dollars O
[ O
OK O
] O
DualV O
AE-30 O
the O
proteus O
73 O
television O
has2 O
hdmi O
port O
-s O
andnicam O
stereo O
audio O
andcosts O
1500 O
dollars O
[ O
OK O
] O
CrossV O
AE-30 O
the O
proteus O
73 O
television O
has2 O
hdmi O
port O
-s O
andnicam O
stereo O
audio O
andcosts O
1500 O
dollars O
[ O
OK O
] O
Table O
3 O
: O
Comparison O
of O
top O
Tvresponses O
generated O
for O
different O
models O
in O
different O
scenarios O
. O

which O
one O
do O
you O
prefer O
Enc O
- O
Dec-10 O
the O
satellite O
pallas O
21 O
has O
a O
500 O
gb O
drive O
, O
the O
satellite O
dinlas O
18 O
has O
a O
4 O
hour O
battery O
life O
and O
a O
3.5 O
hour O
battery O
life O
and O
a O
SLOT O
BATTERY O
battery O
life O
and O
a O
1 O
tb O
drive O
HLSTM-10 B-MethodName
the O
satellite O
pallas O
21 O
has O
a O
4 O
hour O
battery O
life O
and O
a O
500 O
gb O
drive O
. O

which O
one O
do O
you O
prefer O
[ O
satellite O
pallas O
18 O
] O
[ O
3.5 O
hour O
battery O
] O
[ O
1 O
tb O
drive O
] O
SCLSTM-10 B-MethodName
the O
satellite O
pallas O
21 O
has O
a O
4 O
hour O
battery O
, O
and O
has O
a O
3.5 O
hour O
battery O
life O
and O
a O
500 O
gb O
drive O
and O
a O
1 O
tb O
drive O
[ O
satellite O
dinlas O
18 O
] O
C O
- O
VNLG-10 O
the O
satellite O
pallas O
21 O
has O
a O
500 O
gb O
drive O
and O
a O
4 O
hour O
battery O
life O
. O

which O
one O
do O
you O
prefer O
[ O
3.5 O
hour O
battery O
] O
Enc O
- O
Dec-30 O
the O
satellite O
pallas O
21 O
has O
a O
500 O
gb O
drive O
with O
a O
1 O
tb O
drive O
and O
is O
the O
satellite O
dinlas O
18 O
with O
a O
SLOT O
DRIVE O
drive O
for O
4 O
hour O
-s O
. O
which O
one O
do O
you O
prefer O
[ O
3.5 O
hour O
battery O
] O
HLSTM-30 B-MethodName
the O
satellite O
pallas O
21 O
is O
a500 O
gb O
drive O
with O
a O
4 O
hour O
battery O
life O
. O

which O
one O
do O
you O
prefer O
[ O
1 O
tb O
drive O
] O
SCLSTM-30 B-MethodName
the O
satellite O
pallas O
21 O
has O
a O
500 O
gb O
drive O
. O

In O
our O
experiments O
on O
DUC O
2004 O
, O
we O
consider O
three O
types O
of O
sentence O
relation O
graphs O
and O
demonstrate O
the O
advantage O
of O
combining O
sentence O
relations O
in O
graphs O
with O
the O
representation O
power O
of O
deep O
neural B-MethodName
networks I-MethodName
. O

Despite O
their O
popularity O
, O
neural B-MethodName
networks I-MethodName
still O
have O
issues O
when O
dealing O
with O
multi O
- O
document O
summarization O
( O
MDS O
) O
. O

This O
work O
proposes O
a O
multi O
- O
document O
summarization O
system O
that O
exploits O
the O
representational O
power O
of O
deep O
neural B-MethodName
networks I-MethodName
and O
the O
sentence O
relation O
information O
encoded O
in O
graph O
representations O
of O
document O
clusters O
. O

These O
features O
, O
listed O
in O
Table O
1 O
, O
are O
used O
as O
input O
for O
linear B-MethodName
regression I-MethodName
, O
as O
per O
Christensen O
et O
al O
. O
( O
2013 O
) O
. O

The O
function O
f(X O
, O
A)takes O
a O
form O
of O
layer O
- O
wise O
propagation O
based O
on O
neural B-MethodName
networks I-MethodName
. O

We O
aim O
to O
show O
that O
our O
model O
, O
by O
combining O
sentence O
relations O
in O
graphs O
with O
the O
representation O
power O
of O
deep O
neural B-MethodName
networks I-MethodName
, O
can O
improve O
upon O
other O
traditional O
graphbased O
extractive O
approaches O
and O
the O
vanilla O
GRU O
model O
which O
does O
not O
use O
any O
graph O
. O

This O
indicates O
the O
advantage O
of O
the O
representation O
power O
of O
neural B-MethodName
networks I-MethodName
used O
in O
our O
model O
. O

5 O
Conclusion O
In O
this O
paper O
, O
we O
presented O
a O
novel O
multi O
- O
document O
summarization O
system O
that O
exploits O
the O
representational O
power O
of O
neural B-MethodName
networks I-MethodName
and O
graph O
representations O
of O
sentence O
relationships O
. O

The O
parser O
uses O
features O
from O
a O
bidirectional O
LSTM B-MethodName
to O
produce O
a O
distribution O
over O
possible O
heads O
for O
each O
word O
in O
the O
sentence O
. O

To O
do O
this O
, O
the O
parser O
uses O
a O
bidirectional O
LSTM B-MethodName
( O
biLSTMs B-MethodName
) O
, O
which O
have O
shown O
to O
be O
effective O
in O
capturing O
long O
- O
term O
dependencies O
. O

xi= O
[ O
e(wi);e(ti O
) O
] O
( O
1 O
) O
These O
representations O
are O
the O
input O
to O
a O
bi O
- O
LSTM B-MethodName
, O
which O
produces O
a O
sentence O
- O
speciﬁc O
representation O
of O
token O
wicomputed O
by O
concatenating O
the O
hidden O
states O
of O
a O
forward O
and O
a O
backward O
LSTM B-MethodName
: O
ai= O
[ O
hf O
i;hb O
i O
] O
( O
2 O
) O
where O
hf O
iandhb O
idenotes O
the O
hidden O
states O
of O
the O
forward O
and O
backward O
LSTMs B-MethodName
. O

Each O
feature O
is O
represented O
by O
its O
vector O
representations O
and O
we O
concatenate O
them O
together O
to O
represent O
each O
input O
token O
which O
will O
be O
fed O
into O
the O
bi O
- O
LSTMs B-MethodName
. O

We O
use O
two O
- O
layer O
bi O
- O
LSTMs B-MethodName
with O
150 O
hidden B-HyperparameterName
units I-HyperparameterName
, O
and O
set O
embedding O
size O
for{words O
, O
UPOS B-TaskName
, O
XFEATS O
, O
LID O
} O
to O
{ O
300 B-HyperparameterValue
, O
30 O
, O
40 O
, O
10 O
} O
, O
respectively O
. O

While O
learning O
embedding O
models O
has O
yielded O
fruitful O
results O
in O
several O
NLP O
subfields O
, O
most O
notably O
Word2Vec B-MethodName
, O
embedding O
correspondence O
has O
relatively O
not O
been O
well O
explored O
especially O
in O
the O
context O
of O
natural O
language O
understanding O
( O
NLU O
) O
, O
a O
task O
that O
typically O
extracts O
structured O
semantic O
knowledge O
from O
a O
text O
. O

In O
this O
study O
, O
we O
used O
long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
for O
encoding O
input O
sequences O
. O

Stacked O
LSTM B-MethodName
layer O
is O
used O
to O
read O
the O
sequences O
of O
slot O
- O
tags O
and O
slot O
- O
values O
. O

SF O
, O
I O
, O
S O
and O
J O
stand O
for O
semantic O
frame O
, O
intent O
, O
slot O
- O
tag O
and O
joint O
of O
intent O
and O
slottag O
. O
Identifier O
Combinations O
C1 O
CRF O
C2 O
CNN O
RNN O
C3 O
RNN+CRF O
C4 O
CRF O
C5 O
MaxEnt O
RNN O
C6 O
RNN+CRF O
C7 O
CRF O
C8 O
SVM B-MethodName
RNN O
C9 O
RNN+CRF O
C10 O
Joint O
Liu O
( O
Liu O
and O
Lane O
, O
2016 O
) O
C11 O
Joint O
Tur O
( O
Hakkani O
- O
T O
¨ur O
et O
al O
. O
, O
2016 O
) O
Table O
4 O
: O
Multiple O
NLU O
systems O
for O
re O
- O
ranking O
. O

A O
maximum O
- O
entropy B-MetricName
( O
MaxEnt)- O
and O
a O
support O
vector O
machine O
( O
SVM)-based B-MethodName
intent O
classifier O
were O
implemented O
as O
a O
traditional O
sentence O
classification O
method O
. O

Bidirectional O
LSTMs B-MethodName
were O
used O
to O
build O
the O
simple O
RNNbased O
classifier O
. O

By O
placing O
a O
CRF O
layer O
on O
top O
of O
the O
bidirectional O
LSTM B-MethodName
network O
( O
Lee O
, O
2017 O
) O
, O
an O
RNN+CRF O
- O
based O
network O
was O
implemented O
. O

The O
combinations O
of O
CRF O
and O
neural B-MethodName
networks I-MethodName
have O
also O
been O
explored O
by O
Xu O
and O
Sarikaya O
( O
2013 O
) O
. O

Harmonic O
mean B-MetricName
is O
used O
because O
it O
is O
dominated O
by O
its O
lowest O
constituents O
, O
Word O
Embedding O
Based O
tf O
- O
idf O
Based O
Character O
Based O
LSTM B-MethodName
- O
RNN O
Adaboost O
3 B-MetricValue
layer O
CNN O
GRU O
- O
RNN O
Gaussian O
Naive B-MethodName
Bayes I-MethodName
( O
GNB O
) O
Bidirectional O
LSTM B-MethodName
- O
RNN O
5 O
- O
Nearest O
Neighbors O
Bidirectional O
GRU O
- O
RNN O
( O
Multinomial O
) O
Logistic B-MethodName
Regression I-MethodName
Multilayer O
Perceptron O
( O
MLP O
) O
Random B-MethodName
Forest I-MethodName
- O
Support O
Vector O
Machine O
Table O
2 O
: O
Models O
summary O
organised O
by O
which O
input O
type O
they O
use O
. O

LSTM B-MethodName
( O
Yogatama O
et O
al O
. O
, O
2017)92.1 O
94.9 O
92.6 O
59.6 O
98.7 O
73.7 O
- O
- O
-0.87 O
Genertv O
. O

LSTM B-MethodName
( O
Yogatama O
et O
al O
. O
, O
2017)90.6 O
90.3 O
88.2 O
52.7 O
95.4 O
69.3 O
- O
- O
-0.88 O
Kneser O
- O
Ney O
Bayes O
( O
Yogatama O
et O
al O
. O
, O
2017)89.3 O
94.6 O
81.8 O
41.7 O
95.4 O
69.3 O
- O
- O
-0.79 O
FastText O
Lin O
. O

Given O
that O
the O
difﬁculty O
measure O
lacks O
precision B-MetricName
and O
may O
be O
better O
suited O
to O
classiﬁcation O
than O
regression O
as O
discussed O
in O
Section O
3.1 B-MetricValue
, O
can O
not O
take O
account O
of O
statistics O
without O
a O
convenient O
range O
of O
values O
and O
that O
the O
difﬁculty O
measure O
must O
be O
interpretable O
, O
we O
suggest O
that O
future O
work O
could O
look O
at O
combining O
statistics O
with O
a O
white O
- O
box O
, O
nonlinear O
algorithm O
like O
a O
decision B-MethodName
tree I-MethodName
. O

Future O
work O
should O
ﬁrstly O
experiment O
with O
nonlinear O
but O
interpretable O
methods O
of O
combining O
statistics O
into O
a O
difﬁculty O
measure O
such O
as O
decision B-MethodName
trees I-MethodName
. O

Both O
contain O
an O
encoder O
- O
decoder O
with O
LSTMs B-MethodName
. O

The O
latter O
uses O
additional O
LSTMs B-MethodName
storing O
representations O
of O
the O
predicted O
inﬂected O
form O
, O
action O
history O
and O
deleted O
lemma O
characters O
. O

At O
the O
same O
time O
, O
convolutional O
neural B-MethodName
networks I-MethodName
have O
been O
leveraged O
to O
extract O
features O
from O
root O
words O
( O
Ostling O
, O
2016 O
) O
. O

1 O
Introduction O
Dependency O
parsing O
has O
achieved O
new O
states O
of O
the O
art O
using O
distributed O
word O
representations O
in O
neural B-MethodName
networks I-MethodName
, O
trained O
with O
large O
amounts O
of O
annotated O
data O
( O
Dozat O
and O
Manning O
, O
2017 O
; O
Dozat O
et O
al O
. O
, O
2017 O
; O
Ma O
et O
al O
. O
, O
2018 O
; O
Che O
et O
al O
. O
, O
2018 O
) O
. O

In O
our O
decontextualization O
framework O
, O
we O
use O
a O
single O
LSTM B-MethodName
cell O
without O
recurrence O
to O
obtain O
a O
context O
- O
independent O
vector O
, O
thereby O
allowing O
for O
a O
direct O
probe O
into O
the O
LSTM B-MethodName
networks O
in O
- O
dependent O
of O
a O
particular O
corpus O
. O

Retroﬁtting O
Approach O
Following O
Schuster O
et O
al O
. O
( O
2019 O
) O
, O
we O
ﬁrst O
train O
a O
bidirectional O
LM O
with O
two O
- O
layer O
LSTMs B-MethodName
on O
top O
of O
character O
CNNs O
for O
each O
language O
( O
ELMo B-MethodName
, O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
and O
then O
align O
the O
monolingual O
LMs O
across O
languages O
. O

2Multilingual O
BERT B-MethodName
is O
documented O
in O
https O
: O
//github.com O
/ O
google O
- O
research O
/ O
bert O
/ O
blob/ O
master O
/ O
multilingual.md O
.two O
LSTM B-MethodName
layers O
) O
to O
compute O
the O
contextual O
representation O
ei O
, O
cfor O
the O
word O
: O
ei O
, O
c O
= O
P2 O
j=0 jh(j O
) O
i O
, O
c O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
.3In O
the O
ﬁrst O
step O
, O
we O
compute O
an O
“ O
anchor O
” O
h(j O
) O
ifor O
each O
word O
by O
averaging O
h(j O
) O
i O
, O
c O
over O
all O
occurrences O
in O
an O
LM O
corpus O
. O

We O
train O
a O
single O
bidirectional O
LM O
with O
charater O
CNNs O
and O
two O
- O
layer O
LSTMs B-MethodName
on O
multiple O
languages O
( O
Rosita O
, O
Mulcaire O
et O
al O
. O
, O
2019 O
) O
. O

3Schuster O
et O
al O
. O
( O
2019 O
) O
only O
used O
the O
ﬁrst O
LSTM B-MethodName
layer O
, O
but O
we O
found O
a O
performance O
beneﬁt O
from O
using O
all O
layers O
in O
preliminary O
results O
. O

We O
use O
a O
strong O
graph O
- O
based O
dependency O
parser O
with O
BiLSTM B-MethodName
and O
biafﬁne O
attention O
( O
Dozat O
and O
Manning O
, O
2017 O
) O
, O
which O
is O
also O
used O
in O
related O
work O
( O
Schuster O
et O
al O
. O
, O
2019 O
; O
Mulcaire O
et O
al O
. O
, O
2019 O
) O
. O

The O
joint O
training O
approach O
( O
Rosita O
) O
, O
which O
uses O
no O
dictionaries O
, O
consistently O
outperforms O
the O
dictionary O
- O
dependent O
retroﬁtting O
approach O
( O
ELMos+Alignment B-MethodName
) O
. O

Note O
that O
the O
degraded O
overall O
performance O
of O
our O
ELMo+Alignment B-MethodName
compared O
to O
Schuster O
et O
al O
. O
( O
2019 O
) O
’s O
reported O
results O
( O
71.2 O
vs. O
73.1 O
) O
is O
likely O
9See O
Appendix O
for O
a O
list O
of O
UD O
treebanks O
used O
. O

10System O
outputs O
for O
all O
shared O
task O
systems O
are O
available O
athttps://lindat.mff.cuni.cz/repository/ O
xmlui O
/ O
handle/11234/1 O
- O
2885Model O
DEU O
SPA O
FRA O
ITA O
POR O
SWE O
AVG O
Schuster O
et O
al O
. O
( O
2019 O
) O
( O
retroﬁtting O
) O
61.4 O
77.5 O
77.0 O
77.6 O
73.9 O
71.0 O
73.1 O
Schuster O
et O
al O
. O
( O
2019 O
) O
( O
retroﬁtting O
, O
no O
dictionaries O
) O
61.7 O
76.6 O
76.3 O
77.1 O
69.1 O
54.2 O
69.2 O
fastText O
+ O
Alignment O
45.2 O
68.5 O
62.8 O
58.9 O
61.1 O
50.4 O
57.8 O
ELMos B-MethodName
+ O
Alignment O
( O
retroﬁtting O
) O
57.3 O
75.4 O
73.7 O
71.6 O
75.1 O
74.2 O
71.2 O
Rosita O
( O
joint O
training O
, O
no O
dictionaries O
) O
58.0 O
81.8 O
75.6 O
74.8 O
77.1 O
76.2 O
73.9 O
Rosita O
+ O
Reﬁnement O
( O
joint O
training O
+ O
retroﬁtting O
) O
61.7 O
79.7 O
75.8 O
76.0 O
76.8 O
76.7 O
74.5 O
Table O
2 O
: O
Zero O
- O
target O
results O
in O
LAS O
. O

due O
to O
the O
signiﬁcantly O
reduced O
amount O
of O
LM O
data O
we O
used O
in O
all O
of O
our O
experiments O
( O
50 O
M O
words O
per O
language O
, O
an O
order O
of O
magnitude O
reduction O
from O
the O
full O
Wikipedia O
dumps O
used O
in O
Schuster O
et O
al O
. O
( O
2019 O
) O
) O
.Schuster O
et O
al O
. O
( O
2019 O
) O
( O
no O
dictionaries O
) O
is O
the O
same O
retroﬁtting O
approach O
as O
ELMos+Alignment B-MethodName
except O
that O
the O
transformation O
matrices O
are O
learned O
in O
an O
unsupervised O
fashion O
without O
dictionaries O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
. O

( O
ELMo B-MethodName
) O
+ O
ENG O
( O
Rosita O
) O
+ O
rel O
. O

Consistent O
with O
our O
simulations O
, O
our O
parsers O
on O
top O
of O
Rosita O
( O
multilingual O
CWR O
s O
from O
the O
joint O
training O
approach O
) O
substantially O
outperform O
the O
parsers O
with O
ELMos B-MethodName
( O
monolingual O
CWR O
s O
) O
in O
all O
languages O
, O
and O
establish O
a O
new O
state O
of O
the O
artModel O
gold O
pred O
. O

Hungarian O
( O
HUN O
) O
Che O
et O
al O
. O
( O
2018 O
) O
( O
HUN O
, O
ensemble O
) O
– O
82.66 O
Che O
et O
al O
. O
( O
2018 O
) O
( O
HUN O
) O
– O
80.96 O
ELMo B-MethodName
( O
HUN O
) O
81.89 O
81.54 O
Rosita O
( O
HUN O
+ O
ENG O
) O
85.34 O
84.89 O
Rosita O
( O
HUN O
+ O
FIN O
) O
85.40 O
84.96 O
Vietnamese O
( O
VIE O
) O
Che O
et O
al O
. O
( O
2018 O
) O
( O
VIE O
, O
ensemble O
) O
– O
55.22 O
ELMo B-MethodName
( O
VIE O
) O
62.67 O
55.72 O
Rosita O
( O
VIE+ENG O
) O
63.07 O
56.42 O
Uyghur O
( O
UIG O
) O
Che O
et O
al O
. O
( O
2018 O
) O
( O
UIG O
, O
ensemble O
) O
– O
67.05 O
Che O
et O
al O
. O
( O
2018 O
) O
( O
UIG O
) O
– O
66.20 O
ELMo B-MethodName
( O
UIG O
) O
66.64 O
63.98 O
Rosita O
( O
UIG+ENG O
) O
67.85 O
65.55 O
Rosita O
( O
UIG+TUR O
) O
68.08 O
65.73 O
Kazakh O
( O
KAZ O
) O
Rosa O
and O
Mare O
ˇcek(2018 O
) O
( O
KAZ O
+ O
TUR O
) O
– O
26.31 O
Smith O
et O
al O
. O
( O
2018 O
) O
( O
KAZ O
+ O
TUR O
) O
– O
31.93 O
Schuster O
et O
al O
. O
( O
2019 O
) O
( O
KAZ O
+ O
TUR O
) O
– O
36.98 O
Rosita O
( O
KAZ O
+ O
ENG O
) O
48.02 O
46.03 O
Rosita O
( O
KAZ O
+ O
TUR O
) O
53.98 O
51.96 O
Table O
4 O
: O
LAS O
( O
F1 B-MetricName
) O
comparison O
for O
truly O
low O
- O
resource O
languages O
. O

Schuster O
et O
al O
. O
( O
2019 O
) O
aligned O
the O
monolingual O
ELMos B-MethodName
for O
Kazakh O
and O
Turkish O
using O
the O
KAZ O
- O
TUR O
dictionary O
that O
Rosa O
and O
Mare O
ˇcek(2018 O
) O
derived O
from O
parallel O
text O
. O

4.3 O
Comparison O
to O
Multilingual O
BERT B-MethodName
Embeddings O
We O
also O
evaluate O
the O
diverse O
low O
- O
resource O
language O
pairs O
using O
pretrained O
multilingual O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
text O
embeddings O
( O
Figure O
3 O
) O
. O

Here O
, O
the O
same O
language O
model O
( O
multilingual O
cased O
BERT,12covering B-MethodName
104 O
languages O
) O
is O
used O
for O
all O
parsers O
, O
with O
the O
only O
variation O
being O
in O
the O
training O
treebanks O
provided O
to O
each O
parser O
. O

Parsers O
are O
trained O
using O
the O
same O
hyperparameters O
and O
data O
as O
in O
Section O
3.2.13 O
There O
are O
two O
critical O
differences O
from O
our O
previous O
experiments O
: O
multilingual O
BERT B-MethodName
is O
trained O
on O
much O
larger O
amounts O
of O
Wikipedia O
data O
compared O
to O
other O
LMs O
used O
in O
this O
work O
, O
and O
the O
WordPiece O
vocabulary O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
used O
in O
the O
cased O
multilingual O
BERT B-MethodName
model O
has O
been O
shown O
to O
have O
a O
distribution O
skewed O
toward O
Latin O
alphabets O
( O
´ O
Acs,2019 O
) O
. O

These O
results O
are O
thus O
not O
directly O
comparable O
to O
those O
in O
Figure O
1 O
; O
nevertheless O
, O
it O
is O
interesting O
to O
see O
that O
the O
results O
obtained O
with O
ELMo B-MethodName
- O
like O
LMs O
are O
comparable O
to O
and O
in O
some O
cases O
better O
than O
results O
using O
a O
BERT B-MethodName
model O
trained O
on O
over O
a O
hundred O
languages O
. O

Our O
results O
broadly O
ﬁt O
with O
those O
of O
Pires O
et O
al O
. O
( O
2019 O
) O
, O
who O
found O
that O
multilingual O
BERT B-MethodName
was O
useful O
for O
zero O
- O
shot O
crosslingual O
syntactic O
transfer O
. O

In O
particular O
, O
we O
ﬁnd O
nearly O
no O
performance O
beneﬁt O
from O
cross O
- O
script O
transfer O
using O
BERT B-MethodName
in O
a O
language O
pair O
( O
English O
- O
Japanese O
) O
for O
which O
they O
reported O
12Available O
at O
https://github.com/googleresearch/bert/ O
13AllenNLP O
version O
0.9.0 O
was O
used O
for O
these O
experiments O
. O
ARA O
HEB O
HRV O
RUS O
NLD O
DEU O
SPA O
ITA O
CMN O
JPN50607080mono.+ENG+rel O
. O

5.1 O
Decontextualization O
Recall B-MetricName
from O
Section O
2that O
we O
produce O
CWR O
s O
from O
bidirectional O
LMs O
with O
character O
CNNs O
and O
twolayer O
LSTMs B-MethodName
. O

We O
propose O
a O
method O
to O
remove O
the O
dependence O
on O
context O
cfor O
the O
two O
LSTM B-MethodName
layers O
( O
the O
CNN O
layer O
is O
already O
context O
- O
independent O
by O
design O
) O
. O

During O
LM O
training O
, O
the O
hidden O
states O
of O
each O
layer O
htare O
computed O
by O
the O
standard O
LSTM B-MethodName
equations O
: O
it= (Wixt++Uiht 1+bi O
) O
ft= (Wfxt+Ufht 1+bf O
) O
˜ct= O
tanh O
( O
Wcxt+Ucht 1+bc O
) O
ot= (Woxt+Uoht 1+bo O
) O
ct O
= O
ft ct 1+it ˜ct O
ht O
= O
ot tanh O
( O
ct)Representations O
UD O
SRL B-TaskName
NER B-TaskName
GloVe B-MethodName
83.78 O
80.01 O
83.90 O
fastText O
83.93 O
80.27 O
83.40 O
Decontextualization O
86.88 O
81.41 O
87.72 O
ELMo B-MethodName
88.71 O
82.12 O
88.65 O
Table O
5 O
: O
Context O
independent O
vs. O
dependent O
performance O
in O
English O
. O

We O
also O
concatenate O
128 O
- O
dimensional O
character O
LSTM B-MethodName
representations O
with O
the O
word O
vectors O
in O
every O
conﬁguration O
to O
ensure O
all O
models O
have O
character O
input O
. O

Table O
5compares O
the O
decontextualized O
vectors O
with O
the O
original O
CWR O
s O
( O
ELMo B-MethodName
) O
and O
the O
conventional O
word O
type O
vectors O
, O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
fastText O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
. O

In O
all O
three O
tasks O
, O
the O
decontextualized O
vectors O
substantially O
improve O
over O
fastText O
and O
GloVe B-MethodName
vectors O
, O
and O
perform O
nearly O
on O
par O
with O
contextualVector O
DEU O
SPA O
FRA O
ITA O
POR O
SWE O
fastText O
31.6 O
54.8 O
56.7 O
50.2 O
55.5 O
43.9 O
ELMos B-MethodName
Layer O
0 O
19.7 O
41.5 O
41.1 O
36.9 O
44.6 O
27.5 O
Layer O
1 B-MetricValue
24.4 O
46.4 O
47.6 O
44.2 O
48.3 O
36.3 O
Layer O
2 O
19.9 O
40.5 O
41.9 O
38.1 O
42.5 O
30.9 O
Rosita O
Layer O
0 O
37.9 O
56.6 O
58.2 O
57.5 O
56.6 O
50.6 O
Layer O
1 O
40.3 O
56.3 O
57.2 O
58.1 O
56.5 O
53.7 O
Layer O
2 O
38.8 O
51.1 O
52.7 O
53.6 O
50.7 O
50.8 O
Table O
6 O
: O
Crosslingual O
alignment O
results O
( O
precision B-MetricName
at O
1 O
) O
from O
decontextual O
probe O
. O

Layers O
0 O
, O
1 O
, O
and O
2 O
denote O
the O
character O
CNN O
, O
ﬁrst O
LSTM B-MethodName
, O
and O
second O
LSTM B-MethodName
layers O
in O
the O
language O
models O
respectively O
. O

ELMo B-MethodName
. O

The O
closest O
target O
vector O
is O
found O
using O
the O
cross O
- O
domain O
similarity O
local O
scaling O
( O
CSLS O
) O
measure O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
, O
which O
is O
designed O
to O
remedy O
the O
hubness O
problem O
( O
where O
a O
few O
“ O
hub O
” O
points O
are O
nearest B-MethodName
neighbors I-MethodName
to O
many O
other O
points O
each O
) O
in O
word O
translation O
by O
normalizing O
the O
cosine O
similarity O
according O
to O
the O
degree O
of O
hubness O
. O

We O
see O
that O
the O
ﬁrstLSTM B-MethodName
layer O
generally O
achieves O
the O
best O
crosslingual O
alignment O
both O
in O
ELMos B-MethodName
and O
Rosita O
. O

This O
ﬁnding O
mirrors O
recent O
studies O
on O
layerwise O
transferability O
; O
representations O
from O
the O
ﬁrst O
LSTM B-MethodName
layer O
in O
a O
language O
model O
are O
most O
transferable O
across O
a O
range O
of O
tasks O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O

Our O
decontextual O
probe O
demonstrates O
that O
the O
ﬁrst O
LSTM B-MethodName
layer O
learns O
the O
most O
generalizable O
representations O
not O
only O
across O
tasks O
but O
also O
across O
languages O
. O

In O
all O
six O
languages O
, O
Rosita O
( O
joint O
LM O
training O
approach O
) O
outperforms O
ELMos B-MethodName
( O
retroﬁtting O
approach O
) O
and O
the O
fastText O
vectors O
. O

3.2 O
Integrating O
Boundary O
Information O
In O
order O
to O
integrate O
boundary O
information O
into O
the O
model O
, O
we O
take O
advantage O
of O
how O
recurrent O
neural B-MethodName
networks I-MethodName
compute O
their O
output O
. O

In O
this O
work O
, O
we O
use O
GRUs O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
, O
but O
our O
methodology O
is O
applicable O
to O
any O
other O
type O
of O
recurrent O
cell O
such O
as O
vanilla O
RNNs O
or O
LSTMs B-MethodName
. O

A O
similar O
approach O
to O
ours O
was O
proposed O
by O
Chen O
et O
al O
. O
( O
2019 O
) O
in O
an O
Audio O
- O
Word2Vec B-MethodName
experiment O
, O
where O
instead O
of O
being O
given O
gold O
segment O
boundaries O
, O
a O
classiﬁer O
outputs O
a O
probability O
that O
a O
given O
frame O
constitutes O
a O
segment O
boundary O
. O

Unigram O
embeddings O
are O
initialized O
from O
GloVe B-MethodName
vectors O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
and O
we O
use O
5 O
M O
hash O
buckets O
for O
out O
- O
of O
- O
vocabulary O
unigrams O
and O
bigrams(Ganchev O
and O
Dredze O
, O
2008 O
) O
. O

2.For O
each O
mention O
, O
retrieve O
the O
most O
similar O
10 O
entities O
( O
i.e. O
, O
its O
nearest B-MethodName
neighbors I-MethodName
) O
. O

They O
do O
this O
by O
pre O
- O
training O
a O
single O
LSTM B-MethodName
that O
predicts O
among O
248k O
mentions O
, O
and O
then O
the O
parameters O
of O
this O
model O
are O
used O
to O
warm O
start O
each O
of O
the O
523k O
mention O
- O
speciﬁc O
models O
. O

Third O
, O
since O
our O
model O
produces O
encodings O
for O
all O
5.7 O
M O
entities O
, O
we O
can O
retrieve O
nearest B-MethodName
neighbors I-MethodName
for O
anyEntity O
Nearest O
neighbors O
Jorge O
Costa O
Jos´e O
Alberto O
Costa O
, O
Eduardo O
Costa O
, O
Peter O
Shilton O
, O
Rui O
Costa O
, O
Nuno O
Gomes O
, O
Ricardo O
Costa O
( O
Portuguese O
footballer O
) O
, O
Andr O
´ O
e O
Gomes O
, O
Bruno O
Ribeiro O
, O
Diego O
Costa O
Costa O
CruisesMSC O
Cruises O
, O
P&O O
Cruises O
, O
Princess O
Cruises O
, O
Island O
Cruises O
, O
AIDA O
Cruises O
, O
Silversea O
Cruises O
, O
Carnival O
Corporation O
& O
plc O
, O
Costa O
Concordia O
, O
Celebrity O
Cruises O
Arctic O
sea O
ice O
declineArctic O
ice O
pack O
, O
Measurement O
of O
sea O
ice O
, O
Arctic O
geoengineering O
, O
Arctic O
sea O
ice O
ecology O
and O
history O
, O
Climate O
Change O
Science O
Program O
, O
Abrupt O
climate O
change O
, O
Sea O
ice O
thickness O
, O
Antarctic O
sea O
ice O
, O
Marine O
ice O
sheet O
instability O
Pink O
Floyd O
Led O
Zeppelin O
, O
The O
Who O
, O
Duran O
Duran O
, O
Syd O
Barrett O
, O
The O
Velvet O
Underground O
, O
Eddie O
Floyd O
, O
The O
Beatles O
, O
The O
Australian O
Pink O
Floyd O
Show O
, O
Roger O
Waters O
Table O
4 O
: O
Nearest O
neighbors O
retrieved O
by O
DEER O
for O
a O
sample O
of O
entities O
. O

The O
nearest B-MethodName
neighbors I-MethodName
for O
Jorge O
Costa O
, O
our O
running O
example O
, O
include O
a O
variety O
of O
retired O
Portuguese O
football O
players O
, O
many O
of O
whom O
have O
Costa O
in O
their O
names O
. O

While O
our O
bag O
- O
of O
- O
ngrams O
encoders O
provided O
a O
strong O
proof O
of O
concept O
, O
we O
can O
almost O
certainly O
improve O
results O
with O
more O
sophisticated O
encoders O
, O
using O
a O
BERT B-MethodName
architecture O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
for O
example O
. O

Our O
experiments O
show O
that O
our O
model O
signiﬁcantly O
outperforms O
the O
traditional O
Word2Vec B-MethodName
continuous O
bag O
- O
of O
- O
words O
and O
skip O
- O
gram O
models O
, O
demonstrating O
the O
effectiveness O
of O
the O
character O
embeddings O
we O
introduce O
. O

Currently O
, O
the O
most O
commonly O
used O
word O
embedding O
models O
such O
as O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013a O
, O
b O
) O
and O
Glove O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
represent O
characters O
using O
the O
embeddings O
corresponding O
to O
the O
tokens O
used O
to O
name O
them O
. O

Most O
of O
the O
existing O
story O
understanding O
work O
feeds O
the O
model O
with O
the O
vector O
representations O
of O
names O
based O
on O
a O
global O
model O
such O
as O
Word2Vec B-MethodName
or O
Glove O
, O
which O
hinders O
the O
ability O
of O
these O
models O
to O
understand O
dialogue O
( O
Tapaswi O
et O
al O
. O
, O
2016 O
; O
Na O
et O
al O
. O
, O
2017 O
; O
Lei O
et O
al O
. O
, O
2018 O
) O
. O

3.1 O
Setup O
Our O
architecture O
builds O
on O
a O
pretrained O
embedding O
model O
generated O
by O
standard O
Word2Vec B-MethodName
models O
( O
Mikolov O
et O
al O
. O
, O
2013a O
, O
b O
) O
or O
pre O
- O
trained O
contextualized O
word O
representations O
from O
neural O
language O
models O
( O
ELMo B-MethodName
) O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

3.2 O
Architecture O
We O
propose O
two O
post O
- O
training O
schemes O
, O
which O
we O
refer O
to O
as O
Character O
Embedding O
( O
SG O
) O
and O
Charac O
- O
ter O
Embedding O
( O
CBOW B-MethodName
) O
. O

We O
propose O
our O
post O
- O
training O
objectives O
as O
following O
: O
L=1 O
NX O
si2SX O
wi2C(si)X O
 swjsw O
log(p(wijsi+j)))(1 O
) O
L=1 O
NX O
si2SX O
wi2C(si)(log(p(sijwi)+ O
X O
 swjsw;j6=0log(p(sijsi+j)))(2 O
) O
Our O
Character O
Embedding O
( O
SG O
) O
model O
maximizes O
the O
objective O
on O
Equation O
1 O
, O
while O
Character O
Embedding O
( O
CBOW B-MethodName
) O
maximizes O
the O
objective O
on O
Equation O
2 O
, O
where O
N O
indicates O
the O
number O
of O
training O
examples O
and O
swindicates O
the O
size O
of O
the O
speaker O
window O
( O
speaker O
window O
of O
size O
one O
means B-MetricName
we O
consider O
speakers O
of O
one O
preceding O
turn O
and O
one O
succeeding O
turn O
) O
. O

Given O
a O
dialogue O
turn O
, O
we O
encode O
it O
using O
ELMo B-MethodName
’s O
pre O
- O
trained O
BiLSTM B-MethodName
model O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
to O
generate O
a O
sequence O
of O
contextualized O
vectors O
for O
words O
. O

We O
refer O
to O
this O
model O
as O
Character O
Embedding O
( O
ELMo B-MethodName
) O
. O

Left O
: O
Character O
Embedding(CBOW B-MethodName
) O
, O
Right O
: O
Character O
Embedding(SG O
) O
. O

Word2Vec B-MethodName
( O
CBOW B-MethodName
) O
model O
. O

We O
use O
the O
traditional O
Word2Vec B-MethodName
architecture O
to O
train O
a O
word O
embedding O
space O
based O
on O
the O
continuous O
bag O
- O
ofwords O
approach O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
. O

Given O
a O
sequence O
of O
words O
D O
, O
the O
context O
words O
that O
exist O
in O
a O
deﬁned O
window O
size O
are O
considered O
as O
input O
to O
the O
network O
and O
the O
objective O
is O
to O
predict O
the O
target O
word O
by O
maximizing O
the O
average B-MetricName
long O
probability O
: O
L=1 O
jDjX O
wi2DlogP(wijC(wi O
) O
) O
( O
3 B-MetricValue
) O
Word2Vec B-MethodName
( O
SG O
) O
model O
. O

We O
use O
the O
skip O
- O
gram O
architecture O
of O
Word2Vec B-MethodName
with O
negative O
sampling O
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
. O

We O
represent O
each O
character O
as O
the O
mean B-MetricName
- O
pooling O
of O
a O
300 B-MetricValue
- O
dimension O
pretrained O
Word2Vec B-MethodName
representation O
of O
all O
the O
words O
that O
this O
character O
has O
uttered O
through O
the O
entire O
dialogue O
. O

ELMo B-MethodName
( O
Mean O
- O
Pooling O
) O
. O

We O
use O
pre O
- O
trained O
contextualized O
word O
representations O
from O
neural O
language O
models O
( O
ELMo B-MethodName
) O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
to O
generate O
character O
names O
representations O
based O
on O
the O
sentences O
that O
include O
their O
names.2To O
generate O
these O
representations O
, O
we O
feed O
the O
pretrained O
ELMo B-MethodName
model O
with O
a O
Glove O
representation O
2We O
also O
tried O
training O
ELMo B-MethodName
from O
scratch O
on O
our O
data O
but O
the O
pre O
- O
trained O
model O
produces O
better O
results.for O
the O
words O
and O
ELMo B-MethodName
augments O
their O
representation O
with O
the O
hidden O
states O
of O
its O
two O
layers O
bi O
- O
directional O
LSTM B-MethodName
to O
represent O
the O
words O
with O
respect O
to O
their O
context O
. O

5.2 O
Experimental O
Setting O
To O
have O
these O
models O
trained O
on O
in O
- O
domain O
data O
, O
we O
use O
GenSim O
( O
ˇReh˚uˇrek O
and O
Sojka O
, O
2010 O
) O
to O
train O
the O
different O
architectures O
of O
Word2Vec B-MethodName
on O
the O
almost O
600 O
K O
sentences O
/ O
4 O
M O
words O
of O
subtitles O
and O
Shakespeare O
plays O
. O

For O
Character O
Embedding O
( O
CBOW B-MethodName
) O
, O
we O
use O
a O
context O
window O
of O
size O
two O
. O

We O
use O
a O
speaker O
window O
of O
size O
one O
for O
both O
the O
Character O
Embedding O
( O
CBOW B-MethodName
) O
and O
the O
Character O
Embedding O
( O
SG O
) O
. O

For O
each O
model O
, O
given O
a O
pair O
of O
characters O
we O
compute O
the O
cosine O
similarity O
score B-MetricName
between O
the O
embeddings O
of O
these O
two O
characters O
, O
deﬁned O
as O
: O
similarity O
( O
CCC1;CCC2 O
) O
= O
CCC1CCC2 O
jjCCC1jjjjCCC2jj(5 O
) O
and O
compute O
the O
similarity O
score B-MetricName
between O
two O
characters O
in O
the O
embedding O
space O
similar O
to O
( O
Col O
- O
Movie O
Character O
Methods O
Closest O
Second O
closest O
Third O
Closest O
The O
Devil O
’s O
AdvocateAlice O
LomaxGround O
Truth O
Kevin O
Lomax O
John O
Milton O
Mary O
Lomax O
Interaction O
Frequency O
Kevin O
Lomax O
Pam O
Garrety O
John O
Milton O
TF O
- O
IDF O
Mary O
Lomax O
John O
Milton O
Don O
King O
Character O
Average B-MetricName
BOW O
John O
Milton O
Kevin O
Lomax O
Barbara O
Word2Vec B-MethodName
( O
CBOW B-MethodName
) O
Lloyd O
Gettys O
Judge O
Poe O
Alexander O
Cullen O
Word2Vec B-MethodName
( O
SG O
) O
Alfonse O
D’amato O
Lloyd O
Gettys O
Judge O
Poe O
ELMo B-MethodName
( O
Mean O
- O
Pooling O
) O
Kevin O
Lomax O
Mary O
Lomax O
Alexander O
Cullen O
Character O
Embedding(CBOW B-MethodName
) O
Kevin O
Lomax O
Judge O
Poe O
Mary O
Lomax O
Character O
Embedding(SG O
) O
Kevin O
Lomax O
John O
Milton O
Mary O
Lomax O
Character O
Embedding(ELMo B-MethodName
) O
Kevin O
Lomax O
Pam O
Garrety O
Mary O
Lomax O
Table O
3 B-MetricValue
: O
Example O
of O
character O
relatedness O
task O
. O

The O
list O
of O
the O
nearest O
characters O
of O
a O
given O
character O
C O
are O
all O
the O
other O
characters O
from O
the O
same O
movie O
sorted O
in O
descending O
order O
by O
their O
similarity O
score B-MetricName
with O
respect O
to O
C. O
Pearson O
Coeff O
Interaction O
Frequency O
0.3632 B-MetricValue
TF O
- O
IDF O
0.3129 O
Doc2Vec O
0.1771 O
Word2Vec B-MethodName
( O
CBOW B-MethodName
) O
0.2081 O
Word2Vec B-MethodName
( O
SG O
) O
0.1989 O
Character O
BOW O
0.2256 O
ELMo B-MethodName
( O
Mean O
- O
Pooling O
) O
0.3212 O
Character O
Embedding(CBOW B-MethodName
) O
0.4644 O
Character O
Embedding(SG O
) O
0.4933 O
Character O
Embedding(ELMo B-MethodName
) O
0.3475 O
Table O
4 O
: O
Comparison O
between O
the O
average B-MetricName
Pearson O
correlation B-MetricName
coefﬁcient O
scores B-MetricName
of O
the O
different O
models O
against O
average B-MetricName
human O
relatedness O
scores B-MetricName
. O

On O
the O
other O
hand O
the O
characters O
suggested O
by O
both O
Word2Vec B-MethodName
CBOW B-MethodName
and O
SG O
models O
did O
notinteract O
with O
Alice O
through O
the O
whole O
movie O
. O

These O
results O
show O
that O
our O
character O
embedding O
model O
consistently O
outperforms O
the O
traditional O
Word2Vec B-MethodName
baseline O
models O
and O
reﬂect O
the O
robustness O
of O
our O
model O
in O
generating O
better O
character O
embeddings O
. O

For O
each O
of O
these O
tasks O
, O
we O
train O
a O
logistic B-MethodName
regression I-MethodName
classiﬁer O
using O
the O
Scikit O
- O
learn O
library O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
. O

These O
classiﬁers O
take O
a O
pair O
of O
character O
embeddings O
as O
a O
concatenation O
of O
their O
vectors O
and O
predict O
theirFine O
- O
grained O
Relation O
Coarse O
- O
grained O
Relation O
Sentiment O
P O
R O
F O
P O
R O
F O
P O
R O
F O
Interaction O
Frequency O
0.04 O
0.16 O
0.06 O
0.30 O
0.44 O
0.33 O
0.33 O
0.58 O
0.42 O
TF O
- O
IDF O
0.11 O
0.12 O
0.10 O
0.39 O
0.42 O
0.40 O
0.43 O
0.53 O
0.40 O
Character O
Average B-MetricName
BOW O
0.08 B-MetricValue
0.16 O
0.05 O
0.33 O
0.43 O
0.28 O
0.28 O
0.53 O
0.37 O
Word2Vec B-MethodName
( O
CBOW B-MethodName
) O
0.11 O
0.13 O
0.12 O
0.37 O
0.38 O
0.38 O
0.39 O
0.40 O
0.39 O
Word2Vec B-MethodName
( O
SG O
) O
0.09 O
0.12 O
0.10 O
0.37 O
0.37 O
0.37 O
0.41 O
0.43 O
0.42 O
Doc2Vec O
0.12 O
0.12 O
0.12 O
0.40 O
0.40 O
0.40 O
0.42 O
0.42 O
0.42 O
ELMo B-MethodName
( O
Mean O
- O
Pooling O
) O
0.14 O
0.18 O
0.14 O
0.39 O
0.41 O
0.40 O
0.44 O
0.50 O
0.46 O
Character O
Embedding(CBOW B-MethodName
) O
0.11 O
0.14 O
0.12 O
0.43 O
0.44 O
0.43 O
0.44 O
0.47 O
0.44 O
Character O
Embedding(SG O
) O
0.11 O
0.17 O
0.12 O
0.43 O
0.46 O
0.42 O
0.40 O
0.51 O
0.42 O
Character O
Embedding O
( O
ELMo B-MethodName
) O
0.18 O
0.19 O
0.19 O
0.48 O
0.48 O
0.48 O
0.48 O
0.48 O
0.48 O
Table O
5 O
: O
Comparison O
between O
the O
average B-MetricName
of O
the O
precision B-MetricName
, O
recall B-MetricName
and O
macro O
- O
weighted B-HyperparameterName
f O
- O
score B-MetricName
of O
the O
baselines O
and O
our O
character O
embedding O
model O
on O
both O
ﬁne O
- O
grained O
, O
coarse O
- O
grained O
character O
relation O
and O
sentiment O
classiﬁcation O
. O
. O
Fine- O
Coarse- O
SentiPlay O
Char O
1 B-MetricValue
Char O
2 O
Methods O
grained O
grained O
ment O
The O
Two O
Gentlemen O
of O
VeronaJulia O
ProteusGround O
Truth O
lovers O
social O
positive O
Interaction O
Frequency O
lovers O
social O
positive O
TF O
- O
IDF O
servant O
social O
negative O
Character O
Average B-MetricName
BOW O
friend O
social O
positive O
Word2Vec B-MethodName
( O
CBOW B-MethodName
) O
servant O
familial O
negative O
Word2Vec B-MethodName
( O
SG O
) O
servant O
familial O
positive O
ELMo B-MethodName
( O
Mean O
- O
Pooling O
) O
friend O
social O
positive O
Character O
Embedding(CBOW B-MethodName
) O
lovers O
social O
negative O
Character O
Embedding(SG O
) O
lovers O
social O
positive O
Character O
Embedding(ELMo B-MethodName
) O
lovers O
social O
positive O
Table O
6 B-MetricValue
: O
Example O
of O
classiﬁcation O
task O
on O
Shakespeare O
’s O
play O
, O
using O
different O
baselines O
and O
our O
character O
representation O
methods O
. O

Table O
5 O
shows O
the O
classiﬁcation O
average B-MetricName
precision B-MetricName
, O
recall B-MetricName
and O
weighted B-HyperparameterName
F O
- O
score B-MetricName
obtained O
by O
training O
the O
logistic B-MethodName
regression I-MethodName
classiﬁers O
using O
the O
character O
embeddings O
produced O
by O
the O
different O
models O
. O

TVQA O
( O
Lei O
et O
al O
. O
, O
2018 O
) O
is O
a O
challenging O
dataset O
that O
includes O
152.5 O
K O
multipleAccuracy O
Q+S O
Q+S+V O
MS O
( O
Glove O
) O
( O
Lei O
et O
al O
. O
, O
2018 O
) O
0.6515 O
0.6770 O
MS O
( O
Glove O
w/o O
names O
) O
0.6177 O
0.6467 O
MS O
( O
CharEmbedding(CBOW B-MethodName
) O
) O
0.6590 O
0.6852 O
MS O
( O
CharEmbedding(SG O
) O
) O
0.6554 O
0.6884 O
Table O
7 O
: O
Comparison O
on O
the O
TVQA O
validation O
dataset O
using O
the O
MS O
method O
with O
Glove O
and O
Glove O
ﬁne O
- O
tuned O
using O
our O
proposed O
character O
embedding O
method O
. O

Our O
experiments O
show O
that O
our O
model O
signiﬁcantly O
outperforms O
the O
traditional O
Word2Vec B-MethodName
continuous O
bag O
- O
of O
- O
words O
and O
skip O
- O
gram O
models O
, O
thus O
demonstrating O
the O
effectiveness O
of O
the O
character O
embeddings O
we O
introduced O
. O

On O
the O
other O
hand O
, O
the O
system O
of O
J O
¨ager O
et O
al O
. O
( O
2017 O
) O
is O
more O
complex O
; O
it O
uses O
similarity O
as O
measured O
by O
other O
methods O
such O
as O
those O
below O
as O
a O
feature O
source O
for O
classiﬁcation O
of O
cognacy O
using O
Support O
Vector O
Machines O
( O
SVMs B-MethodName
) O
. O

There O
are O
various O
approaches O
offering O
recognition O
frameworks O
utilizing O
deep O
neural B-MethodName
networks I-MethodName
, O
reinforcement O
learning O
or O
recurrent O
neural B-MethodName
networks I-MethodName
. O

We O
train O
Logistic B-MethodName
Regression I-MethodName
classiﬁer O
using O
the O
‘ O
lbfgs O
’ O
solver O
and O
L2 O
penalty.4.4 O
Suggested O
Train O
- O
Test O
Splits O
As O
stated O
in O
Table O
2 O
, O
each O
subset O
has O
5signers O
, O
which O
were O
assigned O
an O
approximately O
equal O
number O
of O
videos O
. O

For O
this O
reason O
we O
trained O
two O
baseline O
models O
: O
a O
logistic B-MethodName
regression I-MethodName
model O
using O
only O
manual O
features O
and O
with O
non O
- O
manual O
features O
as O
an O
input O
, O
anda O
R(2 O
+ O
1)D O
model O
on O
full O
frames O
as O
an O
input O
. O

We O
use O
GRU O
since O
it O
is O
a O
lightweight B-HyperparameterName
and O
more O
computationally O
efﬁcient O
version O
of O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
networks O
that O
preserves O
a O
comparable O
performance O
without O
using O
a O
memory O
unit O
( O
Chung O
et O
al O
. O
, O
2014 O
) O
. O

3.2 O
Deﬁning O
text O
prompts O
Starting O
from O
a O
seed O
of O
50 O
short O
sentences O
, O
similar O
to O
what O
is O
presented O
in O
Figure O
1(d O
) O
, O
we O
expanded O
them O
using O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
which O
stands O
for O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
. O

We O
simply O
masked O
the O
different O
subject O
, O
verb O
and O
object O
words O
in O
our O
initial O
utterances O
and O
let O
BERT B-MethodName
predict O
the O
masked O
words O
. O

1 O
Introduction O
Recently O
, O
approaches O
based O
on O
neural B-MethodName
networks I-MethodName
which O
embed O
words O
into O
low O
- O
dimensional O
vector O
spaces O
from O
text O
corpora O
( O
i.e. O
word O
embeddings O
) O
have O
become O
increasingly O
popular O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

Moreover O
, O
our O
model O
provides O
three O
additional O
key O
features O
: O
( O
1 O
) O
both O
word O
and O
sense O
embeddings O
are O
represented O
in O
the O
same O
vector O
space O
, O
( O
2 O
) O
it O
is O
ﬂexible O
, O
as O
it O
can O
be O
applied O
to O
different O
predictive O
models O
, O
and O
( O
3 O
) O
it O
is O
scalable O
for O
very O
large O
semantic O
networks O
and O
text O
corpora.2 O
Related O
work O
Embedding O
words O
from O
large O
corpora O
into O
a O
lowdimensional O
vector O
space O
has O
been O
a O
popular O
task O
since O
the O
appearance O
of O
the O
probabilistic O
feedforward O
neural O
network O
language O
model O
( O
Bengio O
et O
al O
. O
, O
2003 O
) O
and O
later O
developments O
such O
as O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

While O
we O
will O
focus O
on O
the O
Continuous O
Bag O
Of O
Words O
( O
CBOW B-MethodName
) O
architecture O
of O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
our O
extension O
can O
easily O
be O
applied O
similarly O
to O
Skip O
- O
Gram O
, O
or O
to O
other O
predictive O
approaches O
based O
on O
neural B-MethodName
networks I-MethodName
. O

The O
CBOW B-MethodName
architecture O
is O
based O
on O
the O
feedforward O
neural O
network O
language O
model O
( O
Bengio O
et O
al O
. O
, O
2003 O
) O
and O
aims O
at O
predicting O
the O
current O
word O
using O
its O
surrounding O
context O
. O

In O
contrast O
to O
the O
original O
CBOW B-MethodName
architecture O
, O
where O
the O
training O
criterion O
is O
to O
correctly O
classify O
wt O
, O
our O
approach O
aims O
to O
predict O
the O
word O
wtand O
its O
setStof O
associated O
senses O
. O

There O
is O
a O
single O
output O
layer O
with O
the O
size O
of O
the O
word O
vocabulary O
as O
in O
the O
original O
CBOW B-MethodName
model O
. O

This O
conﬁguration O
, O
coupled O
with O
the O
only O
- O
words O
output O
layer O
conﬁguration O
, O
corresponds O
exactly O
to O
the O
default O
CBOW B-MethodName
architecture O
of O
word2vec O
with O
the O
only O
addition O
of O
the O
update O
step O
for O
senses O
. O
Only O
senses O
. O

For O
evaluation O
purposes O
, O
we O
use O
the O
CBOW B-MethodName
model O
of O
word2vec O
with O
standard O
hyperparameters O
: O
the O
dimensionality O
of O
the O
vectors O
is O
set O
to O
300 O
and O
the O
window O
size O
to O
8 O
, O
and O
hierarchical O
softmax O
is O
used O
for O
normalization O
. O

In O
fact O
, O
both O
pairs O
appear O
as O
coordinate O
synsets O
in O
WordNet B-DatasetName
. O
Accuracy O
F O
- O
Measure O
SW2V O
87.8 O
63.9 O
SensEmbed O
82.7 O
40.3 O
NASARI O
87.0 O
62.5 O
Multi O
- O
SVM B-MethodName
85.5 O
Mono O
- O
SVM B-MethodName
83.5 O
Baseline O
17.5 O
29.8 O
Table O
4 O
: O
Accuracy O
and O
F O
- O
Measure O
percentages O
of O
different O
systems O
on O
the O
SemEval O
Wikipedia O
sense O
clustering O
dataset O
. O

For O
comparison O
, O
we O
also O
include O
the O
supervised O
approach O
of O
Dandala O
et O
al O
. O
( O
2013 O
) O
based O
on O
a O
multi O
- O
feature O
Support O
Vector O
Machine O
classiﬁer O
trained O
on O
an O
automaticallylabeled O
dataset O
of O
the O
English O
Wikipedia O
( O
MonoSVM B-MethodName
) O
and O
Wikipedia O
in O
four O
different O
languages O
( O
Multi O
- O
SVM B-MethodName
) O
. O

In O
this O
paper O
, O
inspired O
by O
previous O
methods O
designed O
for O
monolingual O
settings O
, O
we O
investigate O
two O
approaches O
( O
i.e. O
, O
joint O
mapping O
and O
mixture O
mapping O
) O
based O
on O
a O
pre O
- O
trained O
multilingual O
model O
BERT B-MethodName
for O
addressing O
the O
out O
- O
of O
- O
vocabulary O
( O
OOV O
) O
problem O
on O
a O
variety O
of O
tasks O
, O
including O
part O
- O
of O
- O
speech O
tagging O
, O
named O
entity O
recognition O
, O
machine O
translation O
quality O
estimation O
, O
and O
machine O
reading O
comprehension O
. O

Recently O
, O
ﬁne O
- O
tuning O
a O
pre O
- O
trained O
deep O
language O
model O
, O
such O
as O
Generative O
Pre O
- O
Training O
( O
GPT B-MethodName
) O
( O
Radford O
et O
al O
. O
, O
2018 O
) O
and O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
( O
BERT B-MethodName
) O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
has O
achieved O
remarkable O
success O
on O
various O
downstream B-TaskName
natural O
language O
processing O
tasks O
. O

Instead O
of O
pre O
- O
training O
many O
monolingual O
models O
like O
the O
existing O
English O
GPT B-MethodName
, O
English O
BERT B-MethodName
, O
and O
Chinese O
BERT B-MethodName
, O
a O
more O
natural O
choice O
is O
to O
develop O
a O
powerful O
multilingual O
model O
such O
as O
the O
multilingual O
BERT B-MethodName
. O

Due O
to O
the O
expensive O
computation O
of O
softmax O
( O
Yang O
et O
al O
. O
, O
2017 O
) O
and O
data O
imbalance O
across O
different O
languages O
, O
the O
vocabulary O
size O
for O
each O
language O
in O
a O
multilingual O
model O
is O
relatively O
small O
compared O
to O
the O
monolingual O
BERT B-MethodName
/ O
GPT B-MethodName
models O
, O
especially O
for O
lowresource O
languages O
. O

Even O
for O
a O
high O
- O
resource O
language O
like O
Chinese O
, O
its O
vocabulary O
size O
10k O
in O
the O
multilingual O
BERT B-MethodName
is O
only O
half O
the O
size O
of O
that O
in O
the O
Chinese O
BERT B-MethodName
. O

For O
ex O
- O
ample O
, O
in O
the O
POS B-TaskName
tagging O
problem O
( O
Table O
2 O
) O
, O
11 O
out O
of O
16 O
languages O
have O
signiﬁcant O
OOV O
issues O
( O
OOV O
rate5 O
% O
) O
when O
using O
multilingual O
BERT B-MethodName
. O

2 O
Approach O
We O
use O
the O
multilingual O
BERT B-MethodName
as O
the O
pre O
- O
trained O
model O
. O

Note O
that O
these O
approaches O
are O
not O
restricted O
to O
BERT B-MethodName
but O
also O
applicable O
to O
other O
similar O
models O
. O

2.1 O
Pre O
- O
Trained O
BERT B-MethodName
Compared O
to O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
) O
and O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
uses O
a O
bidirectional O
transformer O
, O
whereas O
GPT B-MethodName
pre O
- O
trains O
a O
left O
- O
to O
- O
right O
transformer O
( O
Liu O
et O
al O
. O
, O
2018 O
) O
; O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
independently O
trains O
left O
- O
to O
- O
right O
and O
right O
- O
to O
- O
left O
LSTMs B-MethodName
( O
Peters O
et O
al O
. O
, O
2017 O
) O
to O
generate O
representations O
as O
additional O
features O
for O
end O
tasks O
. O

We O
show O
how O
to O
adapt O
BERT B-MethodName
to O
different O
downstream B-TaskName
tasks I-TaskName
in O
Figure O
1 O
( O
left O
) O
. O

2.2 O
Vocabulary O
Expansion O
Devlin O
et O
al O
. O
( O
2018 O
) O
pre O
- O
train O
the O
multilingual O
BERT B-MethodName
on O
Wikipedia O
in O
102 O
languages O
, O
with O
a O
shared O
vocabulary O
that O
contains O
110k O
subwords O
calculated O
from O
the O
WordPiece O
model O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
. O

If O
we O
ignore O
the O
shared O
subwords O
between O
languages O
, O
on O
average B-MetricName
, O
each O
language O
has O
a1:1k O
vocabulary O
, O
which O
is O
signiﬁcantly O
smaller O
than O
that O
of O
a O
monolingual O
pre O
- O
trained O
model O
such O
as O
GPT B-MethodName
( O
40k O
) O
. O

Formally O
, O
suppose O
we O
have O
an O
embedding O
Ebert O
extracted O
from O
the O
( O
non O
- O
contextualized O
) O
embedding O
layer O
in O
the O
multilingual O
BERT B-MethodName
( O
i.e. O
, O
the O
ﬁrst O
layer O
of O
BERT B-MethodName
) O
. O

Mixture O
Mapping O
Following O
the O
work O
of O
Gu O
et O
al O
. O
( O
2018 O
) O
where O
they O
use O
English O
as O
“ O
universal O
tokens O
” O
and O
map O
all O
other O
languages O
to O
Englishto O
obtain O
the O
subword O
embeddings O
, O
we O
represent O
each O
subword O
in O
E0 O
l(described O
in O
joint O
mapping O
) O
as O
a O
mixture O
of O
English O
subwords O
where O
those O
English O
subwords O
are O
already O
in O
the O
BERT B-MethodName
vocab O
Vbert O
. O

Speciﬁcally O
, O
for O
each O
w2Vl O
, O
we O
obtain O
its O
embedding O
e(w)in O
the O
BERT B-MethodName
embedding O
space O
Ebertas O
follows O
. O

In O
all O
our O
experiments O
, O
we O
set O
the O
number O
of O
nearest B-MethodName
neighbors I-MethodName
in O
CSLS O
to O
10 O
. O

We O
add O
all O
unseen O
subwords O
from O
50k O
vocabulary O
to O
BERT B-MethodName
. O

For O
example O
, O
in O
BERT B-MethodName
, O
the O
sentence O
“ O
Je O
sens O
qu O
’ O
entre O
c O
¸a O
et O
les O
ﬁlms O
de O
m O
´ O
edecins O
et O
scientiﬁques O
” O
is O
represented O
as O
“ O
je O
sens O
qu O
# O
# O
’ O
entre O
[ O
UNK O
] O
et O
les O
ﬁlms O
de O
[ O
UNK O
] O
et O
scientiﬁques O
” O
, O
where O
qu’is O
an O
OOV O
word O
since O
it O
can O
only O
be O
represented O
by O
two O
subword O
units O
: O
quand O
# O
# O
’ O
, O
but O
it O
is O
not O
OOV O
at O
subword O
level O
; O
c O
¸aandm´edecins O
can O
not O
be O
represented O
by O
any O
single O
word O
or O
combination O
of O
subword O
units O
, O
and O
thus O
they O
are O
OOV O
at O
both O
word O
and O
subword O
level O
. O

We O
use O
the O
multilingual O
BERT B-MethodName
with O
default O
parameters O
in O
all O
our O
experiments O
, O
except O
that O
we O
tune O
the O
batch B-HyperparameterName
size I-HyperparameterName
and O
training O
epochs B-HyperparameterName
. O

Considering O
the O
differences O
between O
BERT B-MethodName
and O
fastText O
: O
e.g. O
, O
the O
objectives O
, O
the O
way O
to O
differentiate O
between O
different O
subwords O
, O
and O
the O
much O
deeper O
architecture O
of O
BERT B-MethodName
, O
it O
is O
very O
unlikely O
that O
the O
( O
non O
- O
contextualized O
) O
BERT B-MethodName
embedding O
and O
fastText O
embedding O
reside O
in O
the O
same O
geometric O
space O
. O

Besides O
, O
due O
to O
that O
BERT B-MethodName
has O
a O
relatively O
smaller O
vocabulary O
for O
each O
language O
, O
when O
we O
map O
a O
pre O
- O
trained O
vector O
to O
its O
portion O
in O
BERT B-MethodName
indirectly O
as O
previous O
methods O
, O
the O
supervision O
signal O
is O
relatively O
weak O
, O
especially O
for O
low O
- O
resource O
languages O
. O

In O
our O
experiment O
, O
we O
ﬁnd O
that O
the O
accuracy B-MetricName
of O
the O
mapping O
from O
our O
pre O
- O
trained O
English O
embedding O
to O
multilingual O
BERT B-MethodName
embedding O
( O
English O
portion O
) O
is O
lower O
than O
30 B-MetricValue
% O
. O

In O
contrast O
, O
the O
accuracy B-MetricName
of O
the O
mapping O
between O
two O
regular O
English O
embeddings O
that O
are O
pre O
- O
trained O
using O
similar O
methods O
( O
e.g. O
, O
CBOW B-MethodName
or O
SkipGram B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 B-MetricValue
) O
) O
could O
be O
above O
95 O
% O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
. O

In O
Table O
1 O
, O
the O
ﬁrst O
two O
rows O
are O
results O
obtained O
by O
mapping O
our O
pre O
- O
trained O
English O
embedding O
( O
using O
fastText O
) O
to O
the O
( O
non O
- O
contextualized O
) O
BERT B-MethodName
embedding O
. O

In O
this O
new O
uniﬁed O
space O
, O
we O
align O
words O
with O
CSLS O
metric O
, O
and O
for O
each O
subword O
that O
appears O
in O
English O
Wikipedia O
, O
we O
seek O
its O
closest O
neighbor O
in O
the O
BERT B-MethodName
vocabulary O
. O

Ideally O
, O
each O
word O
should O
ﬁnd O
itself O
if O
it O
exists O
in O
the O
BERT B-MethodName
vocabulary O
. O

For O
example O
, O
although O
“ O
however O
” O
exists O
in O
the O
BERT B-MethodName
vocabulary O
, O
independent O
mapping O
fails O
to O
ﬁnd O
it O
as O
its O
own O
closest O
neighbor O
. O

Furthermore O
, O
our O
POS B-TaskName
tagging O
experiments O
( O
Section O
3.3 O
) O
further O
show O
that O
joint O
mapping O
MJ O
does O
not O
improve O
( O
or O
even O
hurt O
) O
the O
performance O
of O
the O
multilingual O
BERT B-MethodName
. O

Therefore O
, O
we O
use O
mixture O
mapping O
MMto O
address O
and O
discuss O
OOV O
issues O
in O
the O
remaining O
sections O
. O
BTS|BiLSTM}FREQ}BERT B-MethodName
BERT B-MethodName
oov O
BERT B-MethodName
oovR O
BERT B-MethodName
oovMJ O
OOV O
wOOV O
sw O
ar O
- O
98.23 O
90.06 O
53.34 O
56.70 O
56.57 O
56.23 O
89.8 O
70.6 O
bg O
97.84 O
98.23 O
90.06 O
98.70 O
98.22 O
94.41 O
97.21 O
45.7 O
1.2 O
da O
95.52 O
96.16 O
96.35 O
97.16 O
96.53 O
94.15 O
94.85 O
38.9 O
2.8 O
de O
92.87 O
93.51 O
93.38 O
93.58 O
93.81 O
91.77 O
93.12 O
43.2 O
5.6 O
es O
95.80 O
95.67 O
95.74 O
96.04 O
96.92 O
95.10 O
95.77 O
29.4 O
6.0 O
fa O
96.82 O
97.60 O
97.49 O
95.62 O
94.90 O
94.35 O
95.82 O
35.6 O
6.5 O
ﬁ O
95.48 O
95.74 O
95.85 O
87.72 O
93.35 O
84.75 O
89.39 O
64.9 O
10.4 O
fr O
95.75 O
96.20 O
96.11 O
95.17 O
96.59 O
94.84 O
95.24 O
33.9 O
10.3 O
hr O
- O
96.27 O
96.82 O
95.03 O
96.49 O
89.87 O
93.48 O
49.5 O
8.3 O
it O
97.56 O
97.90 O
97.95 O
98.22 O
98.00 O
97.63 O
97.85 O
30.3 O
2.3 O
nl O
- O
92.82 O
93.30 O
93.89 O
92.89 O
91.30 O
91.71 O
35.5 O
0.3 O
no O
- O
98.06 O
98.03 O
97.25 O
95.98 O
94.21 O
95.83 O
38.7 O
4.4 O
pl O
- O
97.63 O
97.62 O
91.62 O
95.95 O
87.50 O
92.56 O
56.5 O
13.6 O
pt O
- O
97.94 O
97.90 O
96.66 O
97.63 O
95.93 O
96.90 O
34.0 O
8.3 O
sl O
- O
96.97 O
96.84 O
95.02 O
96.91 O
89.55 O
93.97 O
49.2 O
7.8 O
sv O
95.57 O
96.60 B-MetricValue
96.69 O
91.23 O
96.66 O
90.45 O
91.92 O
48.2 O
17.7 O
average B-MetricName
- O
96.60 O
95.64 O
92.27 O
93.60 O
90.15 O
92.20 O
45.2 O
11.0 O
Table O
2 O
: O
POS B-TaskName
tagging O
accuracy B-MetricName
( O
% O
) O
on O
the O
Universal O
Dependencies O
v1.2 O
dataset O
. O

BERT B-MethodName
oov O
: O
BERT B-MethodName
with O
method O
MM O
. O

BERT B-MethodName
oovR O
: O
BERT B-MethodName
with O
randomly O
picked O
embedding O
from O
BERT B-MethodName
. O

BERT B-MethodName
oovMJ O
: O
BERT B-MethodName
with O
method O
MJ O
. O

Approach O
Precision B-MetricName
Recall B-MetricName
F1 B-MetricName
score B-MetricName
DomainMask O
( O
Peng O
and O
Dredze O
, O
2017a O
) O
60.8 B-MetricValue
44.9 O
51.7 O
Linear O
Projection O
( O
Peng O
and O
Dredze O
, O
2017a O
) O
67.2 O
41.2 O
51.1 O
Updates O
( O
Peng O
and O
Dredze O
, O
2017b O
) O
- O
- O
56.1 O
Updates O
( O
Peng O
and O
Dredze O
, O
2017b O
) O
- O
- O
59.0 O
BERT B-MethodName
56.6 O
61.7 O
59.0 O
BERT B-MethodName
oov O
60.2 O
62.8 O
61.4 O
BERT B-MethodName
zh O
63.4 O
70.8 O
66.9 O
Table O
3 O
: O
Performance O
of O
various O
models O
on O
the O
test O
set O
of O
Weibo O
NER B-TaskName
. O

BERT B-MethodName
zh O
: O
Chinese O
BERT B-MethodName
pre O
- O
trained O
over O
Chinese O
Wikipedia O
. O

We O
use O
the O
original O
multilingual O
BERT B-MethodName
( O
without O
using O
CRF O
( O
Lafferty O
et O
al O
. O
, O
2001 O
) O
on O
top O
of O
it O
for O
sequence O
labeling O
) O
to O
tune O
hyperparameters O
on O
the O
dev O
set O
and O
use O
the O
ﬁxed O
hyperparameters O
for O
the O
expanded O
multilingual O
model O
. O

Chinese O
NER B-TaskName
: O
We O
are O
also O
interested O
in O
investigating O
the O
performance O
gap O
between O
the O
expanded O
multilingual O
model O
and O
a O
monolingual O
BERT B-MethodName
that O
is O
pre O
- O
trained O
on O
a O
large O
- O
scale O
monolingual O
corpus O
. O

Currently O
, O
pre O
- O
trained O
monolingual O
BERT B-MethodName
models O
are O
available O
in O
English O
and O
Chinese O
. O

As O
English O
has O
been O
used O
as O
the O
interlingua O
, O
we O
compare O
the O
expanded O
multilingual O
BERT B-MethodName
and O
the O
Chinese O
BERT B-MethodName
on O
a O
Chinese O
NER B-TaskName
task O
, O
evaluated O
on O
the O
Weibo O
NER B-TaskName
dataset O
constructed O
from O
social O
media O
by O
Peng O
and O
Dredze O
( O
2015 O
) O
. O

As O
shown O
in O
Table O
3 O
, O
the O
expanded O
model O
outperforms O
the O
multilingual O
BERT B-MethodName
on O
the O
Weibo O
NER B-TaskName
dataset O
. O

Compared O
to O
the O
Chinese O
BERT B-MethodName
( O
66:9 O
% O
) O
, O
there O
still O
exists O
a O
noticeable O
performance O
gap O
. O

As O
BERT B-MethodName
uses O
the O
language O
model O
loss O
function O
for O
pre O
- O
training O
, O
the O
pre O
- O
trained O
Chinese O
BERT B-MethodName
could O
better O
capture O
the O
language O
- O
speciﬁc O
information O
comapred O
to O
the O
multilingual O
BERT B-MethodName
. O

3.4 O
Code O
- O
Mixed O
Sequence O
Labeling O
Tasks O
As O
the O
multilingual O
BERT B-MethodName
is O
pre O
- O
trained O
over O
102 O
languages O
, O
it O
should O
be O
able O
to O
handle O
code O
- O
mixed O
texts O
. O

en O
- O
es O
ar O
- O
eg O
Model O
Prec O
Rec O
F1 B-MetricName
Prec O
Rec O
F1 B-MetricName
FAIR|- O
- O
62.4 B-MetricValue
- O
- O
71.6 O
IIT|- O
- O
63.8 O
- O
- O
FAIR}- O
- O
67.7 O
- O
- O
81.4 O
BERT B-MethodName
72.7 O
63.6 O
67.8 O
73.8 O
75.6 O
74.7 O
BERT B-MethodName
oov O
74.2 O
60.9 O
66.9 O
76.9 O
77.8 O
77.3 O
Table O
4 O
: O
Accuracy O
( O
% O
) O
on O
the O
code O
- O
switch O
challenge O
. O

On O
all O
three O
categories O
, O
the O
expanded O
model O
consistently O
outperforms O
the O
original O
multilingual O
BERT B-MethodName
( O
Table O
5)2 O
. O

3http://kingline.speechocean.com O
. O
Words O
in O
MT O
Gaps O
in O
MT O
Words O
in O
SRC O
Model O
F1 B-MetricName
- O
BAD O
F1 B-MetricName
- O
OK O
F1 B-MetricName
- O
multi O
F1 B-MetricName
- O
BAD O
F1 B-MetricName
- O
OK O
F1 B-MetricName
- O
multi O
F1 B-MetricName
- O
BAD O
F1 B-MetricName
- O
OK O
F1 B-MetricName
- O
multi O
Fan O
et O
al O
. O
( O
2018 B-MetricValue
) O
0.68 O
0.92 O
0.62 O
- O
- O
- O
- O
- O
Fan O
et O
al O
. O
( O
2018 O
) O
0.66 O
0.92 O
0.61 O
0.51 O
0.98 O
0.50 O
- O
- O
SHEF O
- O
PT|0.51 O
0.85 O
0.43 O
0.29 O
0.96 O
0.28 O
0.42 O
0.80 O
0.34 O
BERT B-MethodName
0.58 O
0.91 O
0.53 O
0.47 O
0.98 O
0.46 O
0.48 O
0.90 O
0.43 O
BERT B-MethodName
oov O
0.60 O
0.91 O
0.55 O
0.50 O
0.98 O
0.49 O
0.49 O
0.90 O
0.44 O
Table O
5 O
: O
WMT18 O
Quality O
Estimation O
Task O
2 O
for O
the O
en O
! O
de O
SMT O
dataset.| O
: O
result O
from O
Specia O
et O
al O
. O
( O
2018 O
) O
. O

In O
this O
experiment O
, O
for O
each O
BERT B-MethodName
model O
, O
we O
follow O
its O
default O
hyperparameters O
. O

As O
shown O
in O
Table O
6 O
, O
the O
expanded O
model O
improves O
the O
multilingual O
BERT B-MethodName
( O
38:1 O
% O
) O
by O
1:2%in O
accuracy B-MetricName
. O

Accuracy O
Model O
Development O
Test O
BERT B-MethodName
en O
38.2 O
37.3 O
BERT B-MethodName
38.7 O
38.1 O
BERT B-MethodName
oov O
39.4 O
39.3 O
BERT B-MethodName
zh O
40.0 O
45.0 O
Table O
6 O
: O
Accuracy O
( O
% O
) O
of O
models O
on O
the O
code O
- O
mixed O
reading O
comprehension O
dataset O
. O

BERT B-MethodName
en O
: O
pre O
- O
trained O
English O
BERT B-MethodName
. O

BERT B-MethodName
zh O
: O
pre O
- O
trained O
Chinese O
BERT B-MethodName
. O

In O
the O
original O
BERT B-MethodName
, O
the O
accuracy B-MetricName
of O
OOV O
words O
is O
much O
lower O
than O
that O
of O
non O
- O
OOV O
words O
, O
and O
we O
signiﬁcantly O
boost O
the O
accuracy B-MetricName
of O
OOV O
words O
with O
the O
expanded O
BERT B-MethodName
. O

Currently O
, O
BERT B-MethodName
BERT B-MethodName
oov O
Lang O
non O
- O
OOV O
OOV O
non O
- O
OOV O
OOV O
ﬁ O
98.1 B-MetricValue
81.3 O
98.5 O
90.2 O
fr O
97.0 O
90.2 O
97.2 O
95.6 O
hr O
97.8 O
91.9 O
97.7 O
94.5 O
pl O
98.8 O
84.6 O
99.0 O
93.2 O
pt O
98.8 O
91.5 O
98.6 O
94.8 O
sl O
98.6 O
91.6 O
98.7 O
95.1 O
sv O
97.4 O
82.9 O
98.2 O
94.8 O
average B-MetricName
98.1 O
87.7 O
98.3 O
94.0 O
Table O
7 O
: O
POS B-TaskName
tagging O
accuracy B-MetricName
( O
% O
) O
for O
OOV O
tokens O
and O
non O
- O
OOV O
tokens O
on O
the O
Universal O
Dependencies O
v1.2 O
dataset O
, O
where O
the O
OOV O
/ O
non O
- O
OOV O
are O
deﬁned O
at O
word O
level O
with O
the O
original O
BERT B-MethodName
vocabulary O
. O

Adding O
too O
many O
subwords O
may O
prevent O
the O
model O
from O
utilizing O
the O
information O
from O
pretrained O
subword O
embedding O
in O
BERT B-MethodName
, O
especially O
when O
there O
is O
a O
low O
word O
- O
level O
overlap O
between O
the O
training O
and O
test O
set O
. O

In O
such O
case O
, O
the O
BERT B-MethodName
embedding O
already O
contains O
sufﬁcient O
information O
, O
and O
therefore O
adding O
additional O
subwords O
may O
hurt O
the O
performance O
. O

On O
the O
other O
hand O
, O
for O
a O
distant O
language O
such O
as O
Polish O
( O
Slavic O
family O
) O
, O
which O
shares O
some O
subwords O
with O
English O
( O
Germanic O
family O
) O
, O
adding O
subwords O
to O
BERT B-MethodName
brings O
performance O
improvements O
. O

In O
contrast O
, O
we O
focus O
on O
dealing O
with O
the O
OOV O
issue O
at O
subword O
level O
in O
the O
context O
of O
pre O
- O
trained O
BERT B-MethodName
model O
. O

The O
part O
- O
of O
- O
speech O
tagger O
is O
a O
time O
- O
distributed O
afﬁne O
classiﬁer O
over O
tokens O
in O
a O
sentence O
, O
where O
tokens O
are O
ﬁrst O
embedded O
with O
a O
word O
encoder O
which O
sums O
together O
a O
learned O
token O
embedding O
, O
a O
pre O
- O
trained O
token O
embedding O
and O
a O
token O
embedding O
encoded O
from O
the O
sequence O
of O
its O
characters O
using O
unidirectional O
LSTM B-MethodName
. O

After O
that O
bidirectional O
LSTM B-MethodName
reads O
the O
sequence O
of O
embedded O
tokens O
in O
a O
sentence O
to O
create O
a O
context O
- O
aware O
token O
representations O
. O

Tokens O
are O
ﬁrst O
embedded O
with O
a O
word O
encoder O
which O
sums O
together O
a O
learned O
token O
embedding O
, O
a O
pretrained O
token O
embedding O
and O
a O
token O
embedding O
encoded O
from O
the O
sequence O
of O
its O
characters O
using O
unidirectional O
LSTM B-MethodName
. O

After O
that O
bidirectional O
LSTM B-MethodName
reads O
the O
sequence O
of O
embedded O
tokens O
in O
a O
sentence O
to O
create O
a O
context O
- O
aware O
token O
representations O
. O

We O
use O
a O
deep O
attentional O
encoder O
- O
decoder O
network O
with O
2 O
layered O
bidirectional O
LSTM B-MethodName
encoder O
for O
reading O
the O
sequence O
of O
input O
characters O
+ O
morphological O
tags O
and O
producing O
a O
sequence O
of O
encoded O
vectors O
. O

Our O
decoder O
is O
a O
2 O
layered O
unidirectional O
LSTM B-MethodName
with O
input O
feeding O
attention O
for O
generating O
the O
sequence O
of O
output O
characters O
based O
on O
the O
encoded O
representations O
. O

AGMmodel O
consists O
of O
encoder O
and O
decoder O
, O
where O
an O
encoder O
is O
just O
a O
bidirectional O
LSTM B-MethodName
. O

Each O
element O
of O
the O
input O
sequence O
contains O
a O
0 O
- O
1 O
encoding O
of O
a O
current O
letter O
and O
two O
LSTMs B-MethodName
traverse O
this O
sequence O
in O
opposite O
directions O
. O

Here O
the O
step O
symbol O
denotes O
pointer O
shift O
, O
for O
precise O
algorithm O
of O
transformation O
see O
( O
Aharoni O
and O
Goldberg O
, O
2017 O
): O
The O
decoder O
is O
one O
- O
directional O
LSTM B-MethodName
. O

We O
choose O
the O
simplest O
possible O
architecture O
of O
the O
language O
model O
, O
namely O
, O
on O
each O
step O
it O
takes O
a O
concatenation O
of O
dprevious O
symbol O
embeddingsui= O
[ O
gi−d, O
... O
,gi−1]and O
applies O
an O
LSTMcell B-MethodName
to O
obtain O
a O
vector O
viand O
update O
LSTM B-MethodName
hidden O
statehi.viis O
propagated O
through O
a O
two O
- O
layer O
perceptron O
to O
predict O
the O
next O
output O
symbol O
analogously O
to O
the O
output O
layer O
of O
the O
baseline O
model O
: O
/hatwideui= O
max O
( O
WLM O
pui+bLM O
p,0 O
) O
, O
pLM O
i O
= O
softmax O
( O
WLM O
o O
/ O
hatwideui+bLM O
o O
) O
, O
yi= O
argmaxkpLM O
ik O
The O
model O
is O
trained O
to O
predict O
next O
output O
symbol O
separately O
from O
the O
basic O
model O
. O

In O
principle O
, O
one O
can O
use O
more O
complex O
neural O
architectures O
, O
for O
example O
, O
a O
multilayer O
LSTM B-MethodName
or O
apply O
attention O
mechanism O
. O

The O
language O
model O
used O
history O
of O
5symbols O
and O
contained O
64units O
in O
LSTM B-MethodName
layers O
. O

It O
shows O
that O
memory O
containing O
last O
output O
symbols O
plays O
the O
role O
of O
a O
language O
model O
for O
local O
dependencies O
and O
the O
memory O
of O
LSTM B-MethodName
encoder O
– O
for O
global O
and O
often O
there O
is O
no O
need O
to O
duplicate O
them O
. O

Doing O
a O
quick O
error O
analysis O
, O
we O
found O
that O
in O
low O
setting O
neural B-MethodName
networks I-MethodName
often O
are O
not O
able O
to O
discriminate O
between O
diﬀerent O
forms O
, O
predicting O
a O
correct O
variant O
for O
another O
tense O
or O
person O
. O

The O
relative O
success O
of O
paradigmbased O
approach O
in O
low O
- O
resource O
setting O
implies O
that O
neural B-MethodName
networks I-MethodName
lack O
control O
mechanism O
provided O
by O
abstract O
paradigms O
. O

Therefore O
the O
combination O
of O
neural B-MethodName
networks I-MethodName
with O
ﬁnite O
state O
techniques O
seems O
a O
perspective O
direction O
of O
study O
. O

Also O
following O
an O
approach O
using O
neural B-MethodName
networks I-MethodName
, O
is O
the O
SECTOR O
algorithm O
( O
Arnold O
et O
al O
. O
, O
2019 O
) O
, O
which O
uses O
a O
topic O
embedding O
trained O
based O
on O
utterance O
topic O
classiﬁcation O
. O

Following O
the O
network O
architecture O
from O
( O
Koshorek O
et O
al O
. O
, O
2018 O
) O
, O
two O
stacked O
LSTM B-MethodName
layers O
are O
used O
to O
decode O
word O
embedding O
representation O
of O
utterances O
. O

Our O
model O
uses O
bidirectional O
LSTMs B-MethodName
to O
learn O
feature O
representations O
shared O
for O
both O
POS B-TaskName
tagging O
and O
dependency O
parsing O
tasks O
, O
thus O
handling O
the O
feature O
- O
engineering O
problem O
. O

Keywords O
: O
Neural O
network O
, O
POS B-TaskName
tagging O
, O
Dependency O
parsing O
, O
Bidirectional O
LSTM B-MethodName
, O
Universal O
Dependencies O
, O
Multilingual O
parsing O
. O

Our O
model O
learns O
latent O
feature O
representations O
shared O
for O
both O
POS B-TaskName
tagging O
and O
dependency O
parsing O
tasks O
by O
using O
BiLSTM B-MethodName
— O
the O
bidirectional O
LSTM B-MethodName
( O
Schuster O
and O
Paliwal O
, O
1997 O
; O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

We O
learn O
shared O
latent O
feature O
vectors O
representing O
word O
tokens O
in O
an O
input O
sentence O
by O
using O
BiLSTMs B-MethodName
. O

BiLSTM B-MethodName
- O
based O
latent O
feature O
representations O
: O
Given O
an O
input O
sentence O
sconsisting O
of O
nword O
tokensw1,w2, O
... O
,w O
n O
, O
we O
represent O
each O
word O
wiin O
sby O
an O
embedding O
e(• O
) O
wi O
. O

So O
, O
we O
also O
use O
a O
sequence O
BiLSTM B-MethodName
( O
BiLSTM B-MethodName
seq O
) O
to O
compute O
a O
character O
- O
based O
vector O
representation O
for O
each O
word O
wiins O
. O

For O
a O
word O
type O
wconsisting O
ofkcharactersw O
= O
c1c2 O
... O
ck O
, O
the O
input O
to O
the O
sequence O
BiLSTM B-MethodName
consists O
of O
kcharacter O
embeddings O
c1 O
: O
kin O
which O
each O
embedding O
vector O
cj O
represents O
the O
jthcharactercjinw O
; O
and O
the O
output O
is O
the O
character O
- O
based O
embedding O
e(∗ O
) O
wof O
the O
word O
typew O
, O
computed O
as O
: O
e(∗ O
) O
w= O
BiLSTM B-MethodName
seq(c1 O
: O
k O
) O
For O
theithwordwiin O
the O
input O
sentence O
s O
, O
we O
create O
an O
input O
vector O
eiwhich O
is O
a O
concatenation O
( O
◦ O
) O
of O
the O
corresponding O
word O
embedding O
and O
character O
- O
based O
embedding O
vectors O
: O
ei O
= O
e(• O
) O
wi O
◦ O
e(∗ O
) O
wiThen O
, O
we O
feed O
the O
sequence O
of O
input O
vectorse1 O
: O
nwith O
an O
additional O
index O
icorresponding O
to O
a O
context O
position O
into O
another O
BiLSTM B-MethodName
( O
BiLSTM B-MethodName
ctx O
) O
, O
resulting O
in O
shared O
feature O
vectors O
virepresenting O
the O
ithwordswiin O
the O
sentence O
s O
: O
vi= O
BiLSTM B-MethodName
ctx(e1 O
: O
n O
, O
i O
) O
POS B-TaskName
tagging O
: O
Using O
shared O
BiLSTM B-MethodName
- O
based O
latent O
feature O
vector O
representations O
, O
then O
we O
follow O
a O
common O
approach O
to O
compute O
the O
cross O
- O
entropy B-MetricName
objective O
lossLPOS(ˆt B-TaskName
, O
t O
) O
, O
in O
which O
ˆtandtare O
the O
sequence O
of O
predicted O
POS B-TaskName
tags O
and O
sequence O
of O
gold O
POS B-TaskName
tags O
of O
words O
in O
the O
input O
sentence O
s O
, O
respectively O
( O
Goldberg O
, O
2016 B-MetricValue
; O
Plank O
et O
al O
. O
, O
2016 O
) O
. O

Following O
Kiperwasser O
and O
Goldberg O
( O
2016b O
) O
, O
we O
score B-MetricName
an O
arc O
by O
using O
a O
MLP O
with O
one O
- O
node O
output O
layer O
( O
MLP O
arc O
) O
on O
top O
of O
the O
BiLSTM B-MethodName
ctx O
: O
score B-MetricName
arc(h O
, O
m O
) O
= O
MLP O
arc(vh O
◦ O
vm O
) O
where O
vhandvmare O
the O
shared O
BiLSTM B-MethodName
- O
based O
feature O
vectors O
representing O
the O
hthandmthwords O
ins O
, O
respectively O
. O

We O
use O
another O
MLP O
on O
top O
of O
theBiLSTM B-MethodName
ctxfor O
predicting O
relation O
type O
of O
an O
head O
- O
modiﬁer O
arc O
. O

The O
model O
parameters O
, O
including O
word O
embeddings O
, O
character O
embeddings O
, O
two O
BiLSTMs B-MethodName
and O
two O
MLPs O
, O
are O
learned O
to O
minimize O
the O
sum O
of O
the O
losses O
. O

Discussion O
: O
Prior O
neural O
network O
- O
based O
joint O
models O
for O
POS B-TaskName
tagging O
and O
dependency O
parsing O
are O
feed O
- O
forward O
network- O
and O
transition O
- O
based O
approaches O
( O
Alberti O
et O
al O
. O
, O
2015 O
; O
Zhang O
and O
Weiss O
, O
2016 O
) O
, O
while O
our O
model O
is O
a O
BiLSTM- B-MethodName
and O
graphbased O
method O
. O

Here O
, O
the O
tagging O
component O
can O
be O
viewed O
as O
a O
simpliﬁed O
version O
without O
the O
additional O
auxiliary O
loss O
for O
rare O
words O
of O
the O
BiLSTM B-MethodName
- O
based O
POS B-TaskName
tagging O
model O
proposed O
by O
Plank O
et O
al O
. O
( O
2016 O
) O
. O

For O
consistency O
, O
we O
use O
19 B-MetricValue
languages O
as O
in O
Zhang O
and O
Weiss O
( O
2016).Methodar O
bg O
da O
de•en O
es O
eu•fa O
ﬁ•fr O
hi O
i O
d O
it O
iw O
nl O
no O
pl•pt O
sl•A O
VG O
10.3 O
12.3 O
15.6 O
11.9 O
9.1 O
7.3 O
17.8 O
8.2 O
24.4 O
5.7 O
4.6 O
13.8 O
5.7 O
10.9 O
18.8 O
11.2 O
23.1 O
10.0 O
19.9 O
12.7 O
PART O
- O
OF O
- O
SPEECH O
TAGGING O
UDPipe O
98.7 O
97.8 O
95.8 O
90.7 O
94.5 O
95.0 O
93.1 O
96.9 O
94.9 O
95.9 O
95.8 O
93.6 O
97.2 O
94.8 O
89.2 O
97.2 O
96.0 O
97.4 O
95.6 O
95.3 O
TnT O
[ O
⊕ O
] O
97.8 O
96.8 O
94.3 O
92.6 O
92.7 O
94.6 O
93.4 O
96.0 O
93.6 O
94.5 O
94.5 O
93.2 O
96.2 O
93.7 O
88.5 O
96.3 O
95.6 O
96.3 O
94.9 O
94.5 O
CRF O
[ O
⊕ O
] O
97.6 O
96.4 O
93.8 O
91.4 O
93.4 O
94.2 O
91.6 O
95.7 O
90.3 O
95.1 O
96.0 O
93.0 O
96.4 O
93.6 O
90.0 O
96.2 O
94.0 O
96.3 O
94.8 O
94.2 O
BiLSTM B-MethodName
- O
aux O
98.9 O
98.0 O
96.2 O
92.6 O
94.5 O
95.1 O
94.7 O
97.2 O
94.9 O
95.8 O
96.2 O
93.1 O
97.6 O
95.8 O
93.3 O
97.6 O
96.4 O
97.5 O
97.6 O
95.9 O
Stack O
- O
prop O
- O
- O
- O
- O
- O
- O
- O
- O
- O
- O
- O
- O
- O
- O
- O
- O
- O
- O
- O
95.4 O
OurjPTDP O
98.8 O
97.4 O
95.8 O
92.7 O
94.7 O
95.9 O
93.7 O
96.8 O
94.6 O
96.0 O
96.4 O
93.1 O
97.5 O
95.5 O
91.4 O
97.4 O
96.3 O
97.5 O
97.1 O
95.7 O
/triangleinv O
- O
Chars O
3.1 O
2.4 O
3.9 O
2.3 O
1.6 O
0.8 O
4.3 O
0.8 O
5.4 O
1.1 O
0.3 O
3.7 O
1.4 O
1.6 O
6.6 O
2.7 O
4.7 O
3.1 O
5.7 O
2.9 O
DEPENDENCY O
PARSING O
UDPipe O
76.0 O
84.7 O
74.8 O
71.8 O
80.2 O
79.7 O
69.7 O
79.7 O
76.3 O
77.8 O
87.5 O
73.9 O
85.7 O
77.1 O
71.3 O
84.5 O
79.4 O
81.3 O
80.2 O
78.5 O
B’15 O
[ O
* O
] O
75.6 O
83.1 O
69.6 O
72.4 O
77.9 O
78.5 O
67.5 O
74.7 O
73.2 O
77.4 O
85.9 O
72.3 O
84.1 O
73.1 O
69.5 O
82.4 O
78.0 O
79.9 O
80.1 O
76.6 O
Pipeline O
Ptag[*]73.7 O
83.6 O
72.0 O
73.0 O
79.3 O
79.5 O
63.0 O
78.0 O
66.9 O
78.5 O
87.8 O
73.5 O
84.2 O
75.4 O
70.3 O
83.6 O
73.4 O
79.5 O
79.4 O
76.6 O
RBGParser O
[ O
* O
] O
75.8 O
83.6 O
73.9 O
73.5 O
79.9 O
79.6 O
68.0 O
78.5 O
65.4 O
78.9 O
87.7 O
74.2 O
84.7 O
77.6 O
72.4 O
83.9 O
75.4 O
81.3 O
80.7 O
77.6 O
Stack O
- O
prop O
77.0 O
84.3 O
73.8 O
74.2 O
80.7 O
80.7 O
70.1 O
78.5 O
74.5 O
80.0 O
88.9 O
74.1 O
85.8 O
77.5 O
73.6 O
84.7 O
79.2 O
80.4 O
81.8 O
78.9 O
OurjPTDP O
79.0 O
83.9 O
75.8 O
75.8 O
82.0 O
82.4 O
73.2 O
81.5 O
75.0 O
80.0 O
87.3 O
75.7 O
86.4 O
79.2 O
66.8 O
84.9 O
82.5 O
79.3 O
81.7 O
79.6 O
/triangleinv O
- O
Chars O
3.8 O
4.1 O
4.5 O
3.6 O
1.4 O
2.3 O
12.0 O
1.1 O
11.1 O
0.2 O
0.3 O
4.1 O
1.9 O
1.9 O
5.4 O
2.3 O
10.6 O
3.4 O
9.2 O
4.4 O
Table O
1 O
: O
Universal O
POS B-TaskName
tagging O
accuracies B-MetricName
and O
LAS O
scores B-MetricName
computed O
on O
all O
tokens O
( O
including O
punctuation O
) O
on O
test O
sets O
for O
19 O
languages O
in O
UD O
v1.2 O
. O

BiLSTM B-MethodName
- O
aux O
refers O
to O
the O
state O
- O
of O
- O
the O
- O
art O
( O
SOTA O
) O
BiLSTMbased B-MethodName
POS B-TaskName
tagging O
model O
with O
an O
additional O
auxiliary O
loss O
for O
rare O
words O
( O
Plank O
et O
al O
. O
, O
2016 O
) O
. O

B’15 O
denotes O
the O
character O
- O
based O
stack O
LSTM B-MethodName
model O
for O
transition O
- O
based O
dependency O
parsing O
( O
Ballesteros O
et O
al O
. O
, O
2015 O
) O
. O

We O
ﬁnd O
that O
the O
highest O
mixed O
accuracy B-MetricName
on O
the O
English O
develop3https://github.com/clab/dynetment O
set O
is O
when O
using O
64 B-MetricValue
- O
dimensional O
character O
embeddings O
, O
128 O
- O
dimensional O
word O
embeddings O
, O
128 O
- O
dimensional O
BiLSTM B-MethodName
states O
, O
2 O
BiLSTM B-MethodName
layers O
and O
100 O
hidden B-HyperparameterName
nodes I-HyperparameterName
in O
MLPs O
with O
one O
hidden B-HyperparameterName
layer.4We I-HyperparameterName
then O
apply O
those O
hyper O
- O
parameters O
to O
all O
18 B-HyperparameterValue
remaining O
languages O
. O

Regarding O
POS B-TaskName
tagging O
, O
our O
joint O
model O
jPTDP O
generally O
obtains O
similar O
POS B-TaskName
tagging O
accuracies B-MetricName
to O
the O
BiLSTM B-MethodName
- O
aux O
model O
( O
Plank O
et O
al O
. O
, O
4On O
English O
, O
carried O
out O
on O
a O
computer O
with O
2.2 B-MetricValue
GHz O
Core O
i7 O
processor O
, O
jPTDP O
took O
6 O
hours O
for O
training O
with O
these O
hyper O
- O
parameters O
, O
and O
then O
obtained O
a O
joint O
tagging O
and O
parsing O
speed O
of O
700 O
words O
/ O
second.2016 O
) O
. O

There O
are O
slightly O
higher O
tagging O
results O
obtained O
by O
BiLSTM B-MethodName
- O
aux O
when O
utilizing O
pre O
- O
trained O
word O
embeddings O
for O
initialization B-HyperparameterName
, O
as O
presented O
in O
Plank O
et O
al O
. O
( O
2016 B-HyperparameterValue
) O
. O

5 O
Conclusion O
In O
this O
paper O
, O
we O
describe O
our O
novel O
model O
for O
joint O
POS B-TaskName
tagging O
and O
graph O
- O
based O
dependency O
parsing O
, O
using O
bidirectional O
LSTM B-MethodName
- O
based O
feature O
representations O
. O

Results O
from O
the O
2016 O
SIGMORPHON O
Shared O
Task O
on O
Morphological O
Reinﬂection O
( O
Cotterell O
et O
al O
. O
, O
2016 O
) O
indicate O
that O
models O
based O
on O
recurrent O
neural B-MethodName
networks I-MethodName
can O
deliver O
high O
accuracies B-MetricName
for O
reinﬂection O
. O

It O
incorporates O
two O
encoder O
LSTMs B-MethodName
, O
which O
operate O
on O
embeddings O
of O
input O
characters O
and O
morphological O
features O
. O

This O
results O
in O
two O
sequences O
of O
state O
vectors O
, O
which O
are O
translated O
into O
a O
sequence O
of O
output O
characters O
by O
a O
decoder O
LSTM B-MethodName
with O
an O
attention O
mechanism O
. O

These O
embeddings O
are O
then O
encoded O
into O
forward O
state O
vectors O
fiand O
backward O
state O
vectorsbiby O
a O
bidirectional O
LSTM B-MethodName
( O
a O
combination O
of O
a O
forward O
and O
backward O
LSTM B-MethodName
) O
. O

Each O
forward O
and O
backward O
state O
pair O
ei= O
( O
f O
i O
, O
bi)is O
used O
as O
the O
bidirectional O
LSTM B-MethodName
state O
at O
position O
i. O
Subsequently O
, O
a O
decoder O
LSTM B-MethodName
generates O
a O
sequence O
of O
embeddings O
which O
is O
then O
transformed O
into O
output O
characters O
by O
a O
softmax O
layer O
. O

For O
the O
encoders O
and O
the O
decoder O
, O
we O
use O
2layer O
LSTMs B-MethodName
( O
Hermans O
and O
Schrauwen O
, O
2013 O
) O
with O
peephole O
connections O
( O
Gers O
and O
Schmidhuber O
, O
2000 O
) O
and O
coupled O
input O
and O
forget O
gates O
( O
Greff O
et O
al O
. O
, O
2015 O
) O
. O

Our O
system O
is O
implemented O
using O
the O
Dynet O
toolkit O
( O
Neubig O
et O
al O
. O
, O
2017)4and O
our O
code O
is O
freely O
available.5 O
There O
are O
three O
hyper O
- O
parameters O
in O
our O
system O
: O
the O
character O
embedding O
dimension O
, O
the O
size O
of O
the O
hidden B-HyperparameterName
layer I-HyperparameterName
of O
the O
LSTM B-MethodName
models O
and O
the O
size O
of O
the O
hidden B-HyperparameterName
layer I-HyperparameterName
of O
the O
attention O
network O
. O

This O
seems O
to O
adversely O
impact O
accuracy B-MetricName
even O
though O
the O
Encoder O
LSTM B-MethodName
does O
not O
treat O
morphological O
feature O
sets O
as O
atomic O
units O
( O
for O
example O
” O
V;PRS;PCP O
” O
) O
but O
instead O
splits O
them O
into O
separate O
symbols O
( O
” O
V O
” O
, O
” O
PRS O
” O
, O
” O
PCP O
” O
) O
. O

Our O
model O
performs O
better O
than O
previous O
typedriven O
models O
and O
is O
competitive O
with O
state O
of O
the O
art O
representation O
learning O
methods O
such O
as O
BERT B-MethodName
and O
neural O
sentence O
encoders O
. O

In O
the O
verb O
and O
sentence O
similarity O
and O
verb O
disambiguation O
datasets O
, O
our O
model O
outperforms O
all O
previous O
type O
- O
driven O
models O
, O
and O
in O
most O
cases O
it O
also O
outperforms O
InferSent O
and O
Universal O
Sentence O
encoders O
, O
as O
well O
as O
pre O
- O
trained O
ELMo B-MethodName
and O
BERT B-MethodName
embeddings O
. O

However O
, O
it O
does O
not O
outperform O
BERT B-MethodName
embeddings O
ﬁne O
- O
tuned O
on O
NLI O
data O
. O

Sentence O
embeddings O
are O
either O
learnt O
by O
mixing O
word O
embeddings O
e.g. O
the O
additive O
models O
of O
( O
Mitchell O
and O
Lapata O
, O
2010 O
; O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
or O
as O
a O
whole O
, O
e.g. O
the O
supervised O
InferSent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
and O
Universal O
Sentence O
Encoder O
( O
Cer O
et O
al O
. O
, O
2018 O
) O
, O
and O
the O
unsupervised O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
models O
. O

TreeRNNs O
( O
Socher O
et O
al O
. O
, O
2013 O
) O
, O
Tree O
- O
LSTMs B-MethodName
( O
Tai O
et O
al O
. O
, O
2015 O
) O
, O
and O
Lifted O
Matrix O
Space O
model O
( O
Chung O
et O
al O
. O
, O
2018 O
) O
, O
do O
use O
the O
constituency O
tree O
of O
a O
sentence O
as O
a O
guide O
, O
but O
to O
learn O
a O
semantic O
function O
composition O
rather O
than O
different O
types O
of O
representations O
for O
words O
. O

On O
the O
other O
hand O
, O
formal O
distributional O
models O
, O
e.g. O
the O
categorial O
framework O
of O
Coecke O
et O
al O
. O
( O
2010 O
, O
2013 O
) O
, O
the O
linear B-MethodName
regression I-MethodName
approach O
of O
Baroni O
et O
al O
. O
( O
2014 O
) O
, O
and O
the O
Combinatory O
Categorial O
Grammar O
( O
CCG O
) O
tensor O
contraction O
model O
of O
Maillard O
et O
al O
. O
( O
2014 O
) O
, O
directly O
take O
the O
grammatical O
types O
of O
words O
into O
account O
, O
but O
fail O
to O
scale O
up O
to O
sentences O
of O
any O
length O
and O
complexity O
, O
and O
do O
notperform O
as O
well O
as O
their O
neural O
embedding O
counterparts O
. O

2 O
Multilinear O
Skipgram O
Embeddings O
The O
skipgram O
model O
with O
negative O
sampling O
generates O
word O
embeddings O
by O
optimising O
a O
logistic B-MethodName
regression I-MethodName
objective O
in O
which O
target O
vectors O
have O
high O
inner O
product O
with O
context O
vectors O
for O
positive O
contexts O
, O
and O
low O
inner O
product O
with O
negative O
ones O
. O

We O
moreover O
compare O
to O
state O
- O
of O
- O
the O
- O
art O
contextualised O
encoders O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

For O
ELMo B-MethodName
, O
we O
use O
a O
pre O
- O
trained O
model O
and O
apply O
mean B-MetricName
pooling6 O
. O

For O
BERT B-MethodName
, O
we O
take O
the O
implementation O
of O
Reimers O
and O
Gurevych O
( O
2019)7 O
, O
as O
it O
implements O
both O
the O
original O
pre O
- O
trained O
BERT B-MethodName
models O
and O
ﬁne O
- O
tuned O
sentence O
embedding O
models O
. O

These O
compute O
a O
representation O
for O
each O
sentence O
of O
the O
dataset O
by O
composing O
the O
representations O
of O
the O
words O
of O
that O
sentence O
, O
rather O
than O
by O
only O
working O
with O
individual O
word O
representations O
, O
as O
was O
done O
in O
the O
previousML08 O
ML10 O
GS11 O
KS13a O
KS13b O
eVojs O
= O
sjo O
( O
1)0.19 O
0.55 O
0.54 O
0.37 O
0.75 O
IS O
0.18 O
0.63 O
0.30 O
0.17 O
0.78 O
USE O
0.04 O
0.33 O
0.09 O
0.21 O
0.54 O
ELMo B-MethodName
0.17 O
0.54 O
0.11 O
0.24 O
0.73 O
BERTp B-MethodName
0.19 O
0.34 O
0.24 O
0.32 O
0.61 O
BERTf0.32 B-MethodName
0.74 O
0.61 O
0.32 O
0.82 O
Human O
0.66 O
0.71 O
0.74 O
0.58 O
0.75 O
Table O
8 O
: O
Spearman O
scores B-MetricName
on O
compositional O
tasks O
, O
for O
our O
proposed O
unary O
map O
verb O
representation O
versus O
state O
of O
the O
art O
sentence O
embedding O
methods O
. O

4.2.2 O
Elliptical O
Phrase O
and O
SICK O
Datasets O
The O
results O
in O
Tables O
9 O
show O
that O
our O
proposed O
verb O
unary O
map O
representations O
achieve O
competitive O
results O
compared O
to O
the O
additive O
baseline O
, O
and O
pre O
- O
trained O
BERT B-MethodName
embeddings O
, O
on O
the O
ELLDIS O
and O
ELLSIM O
tasks O
and O
on O
( O
a O
subset O
of O
) O
the O
SICK O
relatedness O
task O
. O

What O
is O
more O
, O
they O
clearly O
outperform O
the O
analytic O
tensors O
and O
in O
ellipsis O
datasets O
; O
they O
also O
improve O
the O
state O
of O
the O
art O
of O
ELLDIS O
, O
whichAdd O
Kron O
Rel O
eVojs O
= O
s O
jo O
( O
1)IS O
USE O
BERT B-MethodName
pBERT B-MethodName
f O
0.31 O
0.30 O
0.37 O
0.56 O
0.34 O
0.27 O
0.36 O
0.65 O
0.67 O
0.52 O
0.65 O
0.76 O
0.80 O
0.68 O
0.67 O
0.79 O
0.71 O
0.58 O
0.44 O
0.70 O
0.74 O
0.76 O
0.70 O
0.76 O
Table O
9 O
: O
Spearman O
scores B-MetricName
on O
the O
ELLDIS O
( O
top O
) O
, O
ELLSIM O
( O
middle O
) O
, O
and O
SICK O
relatedness O
( O
bottom O
) O
tasks O
. O

However O
, O
they O
are O
surpassed O
by O
ﬁne O
- O
tuned O
BERT B-MethodName
sentence O
embeddings O
and O
sentence O
encoders O
, O
that O
achieve O
the O
highest O
. O

4.3 O
Comparison O
with O
Sentence O
Embeddings O
We O
compare O
our O
model O
with O
the O
InferSent O
encoder O
and O
the O
Universal O
Sentence O
Encoder O
, O
and O
with O
ELMo B-MethodName
and O
BERT B-MethodName
encodings O
in O
Table O
8 O
. O

For O
the O
BERT B-MethodName
embeddings O
we O
observe O
an O
interesting O
pattern O
: O
our O
proposed O
method O
outperforms O
any O
pre O
- O
trained O
BERT B-MethodName
model O
, O
but O
after O
ﬁne O
- O
tuning O
on O
NLI O
datasets O
, O
the O
BERT B-MethodName
models O
score B-MetricName
the O
highest O
on O
all O
datasets O
but O
KS2013 O
. O

Although O
fully O
analysing O
the O
syntactic O
awareness O
of O
BERT B-MethodName
is O
beyond O
the O
scope O
of O
this O
paper O
, O
it O
seems O
that O
both O
explicitly O
modelling O
syntax O
in O
the O
embeddings O
as O
our O
method O
does O
, O
and O
ﬁne O
- O
tuning O
BERT B-MethodName
embeddings O
are O
viable O
strategies O
. O

They O
also O
outperformed O
sentence O
encoders O
and O
pre O
- O
trained O
BERT B-MethodName
embeddings O
. O

When O
moving O
to O
datasets O
of O
longer O
sentences O
, O
e.g. O
sentences O
with O
elliptical O
phrases O
and O
the O
SICK O
relatedness O
, O
some O
sentence O
encoders O
and O
ﬁne O
- O
tuned O
BERT B-MethodName
representations O
were O
superior O
. O

Word O
embedding O
approaches O
like O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
or O
Glove O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
are O
powerful O
tools O
for O
the O
semantic O
analysis O
of O
natural O
language O
. O

2.1.2 O
Realizations O
of O
Word O
Embedding O
Models O
In O
this O
paper O
, O
we O
work O
with O
a O
well O
- O
known O
embedding O
model O
, O
Mikolov O
et O
al O
. O
’s O
Word2Vec B-MethodName
model O
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
. O

Word2Vec B-MethodName
models O
use O
a O
neural O
- O
network O
based O
learning O
algorithm O
. O

They O
learn O
by O
maximizing O
the O
probability O
of O
predicting O
the O
word O
given O
the O
context O
( O
Continuous O
Bag O
of O
Words O
model O
, O
CBOW B-MethodName
) O
( O
Mikolov O
et O
al O
. O
, O
2013c O
, O
b O
) O
. O

Note O
that O
the O
results O
can O
be O
transferred O
to O
the O
skip O
- O
gram O
learning O
algorithm O
of O
Word2Vec B-MethodName
as O
well O
as O
to O
Glove O
( O
Pennington O
et O
al O
. O
, O
2014 O
; O
Levy O
and O
Goldberg O
, O
2014 O
) O
. O

We O
use O
a O
bidirectional O
long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
, O
which O
, O
produces O
hidden O
state O
htin O
each O
step O
t. O
The O
decoder O
is O
a O
unidirectional O
LSTM B-MethodName
receiving O
the O
word O
embedding O
of O
the O
previous O
word O
. O

LM O
In O
this O
work O
, O
we O
focus O
on O
sentence O
generation O
, O
so O
we O
evaluate O
our O
data O
with O
the O
same O
twolayer O
LSTM B-MethodName
LM O
for O
comparison O
. O

It O
is O
trained O
using O
a O
two O
- O
layer O
LSTM B-MethodName
with O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
200 B-HyperparameterValue
and O
unrolled O
for O
35 O
steps O
. O

The O
embedding O
size O
is O
equal O
to O
the O
LSTM B-MethodName
hidden B-HyperparameterName
size I-HyperparameterName
for O
weight B-HyperparameterName
tying O
( O
Press O
and O
Wolf O
, O
2017 B-HyperparameterValue
) O
. O

Then O
, O
Adel O
et O
al O
. O
( O
2013a)extended O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
by O
adding O
POS B-TaskName
information O
to O
the O
input O
layer O
and O
a O
factorized O
output O
layer O
with O
a O
language O
identiﬁer O
. O

Our O
second O
baseline O
is O
R O
EG O
, O
a O
supervised O
linear B-MethodName
regression I-MethodName
model O
( O
Tran O
et O
al O
. O
, O
2013b O
; O
Wang O
et O
al O
. O
, O
2015 O
) O
. O

6 O
Related O
work O
This O
work O
is O
situated O
in O
a O
rich O
body O
of O
computational O
research O
that O
attempts O
to O
establish O
the O
boundaries O
of O
what O
distributed O
semantic O
representations O
and O
neural B-MethodName
networks I-MethodName
can O
learn O
. O

In O
our O
experiments O
we O
use O
SVM B-MethodName
- O
based O
ranking O
and O
bi O
- O
LSTMCRF B-MethodName
sequence O
labeling O
. O

The O
ﬁrst O
poses O
segmentation O
as O
a O
ranking O
problem O
, O
where O
we O
use O
an O
SVM B-MethodName
ranker O
. O

The O
second O
poses O
the O
problem O
as O
a O
sequence O
labeling O
problem O
, O
where O
we O
use O
a O
bidirectional O
Long O
Short O
- O
Term O
Memory O
( O
bi O
- O
LSTM B-MethodName
) O
Recurrent O
Neural O
Network O
( O
RNN O
) O
that O
is O
coupled O
with O
Conditional O
Random O
Fields O
( O
CRF O
) O
sequence O
labeler O
. O

The O
ﬁrst O
uses O
SVM B-MethodName
- O
based O
ranking O
( O
SVMRank)2to B-MethodName
rank O
different O
possible O
seg2https://www.cs.cornell.edu/people/tj/ O
svm_light O
/ O
svm_rank.htmlmentations O
for O
a O
word O
using O
a O
variety O
of O
features O
. O

The O
second O
uses O
bi O
- O
LSTM B-MethodName
- O
CRF O
, O
which O
performs O
character O
- O
based O
sequence O
- O
to O
- O
sequence O
mapping O
to O
predict O
word O
segmentation O
. O

5.1 O
SVMRankApproach B-MethodName
We O
used O
the O
SVM B-MethodName
- O
based O
ranking O
approach O
proposed O
by O
Abdelali O
et O
al O
. O
( O
2016 O
) O
, O
in O
which O
they O
used O
SVM B-MethodName
based O
ranking O
to O
ascertain O
the O
best O
segmentation O
for O
Modern O
Standard O
Arabic O
( O
MSA O
) O
, O
which O
they O
show O
to O
be O
fast O
and O
of O
high O
accuracy B-MetricName
. O

SVMRankwould B-MethodName
attempt O
to O
rank O
the O
correct O
segmentation O
higher O
than O
all O
others O
. O

To O
train O
SVMRank B-MethodName
, O
we O
use O
the O
following O
features O
: O
•Conditional O
probability O
that O
a O
leading O
character O
sequence O
is O
a O
preﬁx O
. O

3http://sourceforge.net/projects/ O
aracomlex O
/ O
The O
segmentations O
with O
their O
corresponding O
features O
are O
then O
passed O
to O
the O
SVM B-MethodName
ranker O
( O
Joachims O
, O
2006 O
) O
for O
training O
. O

Our O
SVMRankuses B-MethodName
a O
linear O
kernel O
and O
a O
trade O
- O
off O
parameter O
between O
training O
error O
and O
margin O
of O
100 O
. O

5.2 O
Bi O
- O
LSTM B-MethodName
- O
CRF O
Approach O
In O
this O
subsection O
we O
describe O
the O
different O
components O
of O
our O
Arabic O
segmentation O
bi O
- O
LSTMCRF B-MethodName
based O
model O
, O
shown O
in O
Figure O
2 O
. O

It O
is O
a O
slight O
variant O
of O
the O
bi O
- O
LSTM B-MethodName
- O
CRF O
architecture O
ﬁrst O
proposed O
by O
Huang O
et O
al O
. O
( O
2015 O
) O
, O
Lample O
et O
al O
. O
( O
2016 O
) O
, O
and O
Ma O
and O
Hovy O
( O
2016 O
) O
5.2.1 O
Recurrent O
Neural O
Networks O
A O
recurrent O
neural B-MethodName
network I-MethodName
( O
RNN O
) O
together O
with O
its O
variants O
, O
i.e. O
LSTM B-MethodName
, O
bi O
- O
LSTM B-MethodName
, O
GRU O
, O
belong O
to O
a O
family O
of O
powerful O
neural B-MethodName
networks I-MethodName
that O
are O
well O
suited O
for O
modeling O
sequential O
data O
. O

LSTMs B-MethodName
LSTMs B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
are O
variants O
of O
the O
RNNs O
that O
can O
efﬁciently O
overcome O
difﬁculties O
with O
training O
and O
efﬁciently O
cope O
with O
long O
distance O
dependencies O
. O

Bi O
- O
LSTMs B-MethodName
Another O
extension O
to O
the O
single O
LSTM B-MethodName
networks O
are O
the O
bi O
- O
LSTMs B-MethodName
( O
Schuster O
and O
Paliwal O
, O
1997 O
) O
. O

CRF O
In O
many O
sequence O
labeling O
tasks O
biLSTMs B-MethodName
achieve O
very O
competitive O
results O
against O
traditional O
models O
, O
still O
when O
they O
are O
used O
for O
some O
speciﬁc O
sequence O
classiﬁcation O
tasks O
, O
such O
as O
segmentation O
and O
named O
entity O
detection O
, O
where O
there O
is O
a O
strict O
dependence O
between O
the O
output O
labels O
, O
they O
fail O
to O
generalize O
perfectly O
. O

During O
the O
training O
phase O
of O
the O
bi O
- O
LSTM B-MethodName
networks O
, O
the O
resulting O
probability O
distribution O
of O
each O
time O
step O
is O
independent O
from O
each O
other O
. O

To O
overcome O
this O
independence O
assumptions O
imposed O
by O
the O
bi O
- O
LSTM B-MethodName
and O
to O
exploit O
this O
kind O
of O
labeling O
constraints O
in O
our O
Arabic O
segmentation O
system O
, O
we O
model O
label O
sequence O
logic O
jointly O
using O
Conditional O
Random O
Fields O
( O
CRF O
) O
( O
Lafferty O
et O
al O
. O
, O
2001 O
) O
5.2.2 O
DA O
segmentation O
Model O
The O
concept O
we O
followed O
in O
bi O
- O
LSTM B-MethodName
- O
CRF O
sequence O
labeling O
is O
that O
segmentation O
is O
a O
one O
- O
to O
- O
one O
mapping O
at O
the O
character O
level O
where O
each O
character O
is O
annotated O
as O
either O
beginning O
a O
segment O
( O
B O
) O
, O
continues O
a O
previous O
segment O
( O
M O
) O
, O
ends O
a O
segment O
( O
E O
) O
, O
or O
is O
a O
segment O
by O
itself O
( O
S O
) O
. O

At O
the O
hidden B-HyperparameterName
layer I-HyperparameterName
, O
the O
output O
from O
the O
character O
embeddings O
is O
used O
as O
the O
input O
to O
the O
biLSTM B-MethodName
layer O
to O
obtain O
ﬁxed O
- O
dimensional O
representations O
of O
characters O
. O

At O
the O
output O
layer O
, O
a O
CRF O
is O
applied O
on O
the O
top O
of O
bi O
- O
LSTM B-MethodName
to O
jointly O
decode O
labels O
for O
the O
whole O
input O
characters O
. O

Using O
both O
algorithms O
( O
SVM B-MethodName
and O
LSTM B-MethodName
) O
the O
results O
show O
a O
general O
trend O
where O
EGY O
segmentation O
yields O
better O
results O
from O
the O
LEV O
model O
than O
from O
the O
GLF O
’s O
. O

In O
the O
case O
with O
no O
lookup O
, O
LSTM B-MethodName
fairs O
better O
than O
SVM B-MethodName
when O
training O
and O
testing O
is O
done O
on O
the O
same O
dialect O
. O

This O
may O
indicate O
that O
the O
SVM B-MethodName
- O
ranker O
has O
better O
cross O
- O
dialect O
generalization O
than O
the O
biLSTM B-MethodName
- O
CRF O
sequence O
labeler O
. O

When O
lookup O
is O
used O
, O
SVM B-MethodName
yields O
better O
results O
across O
the O
board O
except O
in O
three O
cases O
, O
namely O
when O
training O
and O
testing O
on O
Egyptian O
with O
DA+MSA O
lookup O
, O
when O
training O
with O
Egyptian O
and O
testing O
on O
MGR O
, O
and O
when O
training O
with O
GLF O
and O
testing O
on O
MGR O
with O
DA+MSA O
lookup O
. O

Lastly O
, O
the O
best O
SVM B-MethodName
crossdialect O
results O
with O
lookup O
consistently O
beat O
the O
Farasa O
MSA O
baseline O
often O
by O
several O
percentage O
points O
for O
every O
dialect O
. O

The O
same O
is O
true O
for O
LSTM B-MethodName
when O
training O
with O
relatively O
related O
dialects O
( O
EGY O
, O
LEV O
, O
and O
GLF O
) O
, O
but O
the O
performance O
decreases O
when O
training O
or O
testing O
using O
MGR O
. O

Using O
SVM B-MethodName
, O
the O
combined O
model O
drops O
by O
0.3 O
% O
to O
1.3 O
% O
compared O
to O
exclusively O
using O
matching O
dialectal O
training O
data O
. O

We O
also O
conducted O
another O
SVM B-MethodName
experiment O
in O
which O
we O
use O
the O
joint O
model O
in O
conjunction O
with O
a O
dialect O
identiﬁcation O
oracle O
to O
restrict O
possible O
afﬁxes O
only O
to O
those O
that O
are O
possible O
for O
that O
dialect O
( O
last O
two O
rowTest O
Set O
Farasa O
85.7 O
82.6 O
82.9 O
82.6 O
Training O
EGY O
LEV O
GLF O
MGR O
SVM B-MethodName
LSTM B-MethodName
SVM B-MethodName
LSTM B-MethodName
SVM B-MethodName
LSTM B-MethodName
SVM B-MethodName
LSTM B-MethodName
with O
no O
lookup O
EGY O
91.0 O
93.8 O
87.7 O
87.1 O
86.5 O
85.8 O
81.3 O
82.5 O
LEV O
85.2 O
85.5 O
87.8 O
91.0 O
85.5 O
85.7 O
83.42 O
80.0 O
GLF O
85.7 O
85.0 O
86.4 O
86.9 O
87.7 O
89.4 O
82.6 O
81.6 O
MGR O
85.0 O
78.6 O
85.7 O
78.8 O
84.5 O
78.4 O
84.7 O
87.1 O
with O
DA O
lookup O
EGY O
94.5 O
94.2 O
89.2 O
87.6 O
87.5 O
86.5 O
81.5 O
82.8 O
LEV O
89.7 O
85.9 O
92.9 O
91.8 O
89.6 O
86.3 O
83.5 O
80.4 O
GLF O
89.7 O
85.5 O
89.2 O
87.5 O
92.8 O
90.8 O
83.0 O
82.4 O
MGR O
88.6 O
78.9 O
86.9 O
78.8 O
87.3 O
79.0 O
90.5 O
88.5 O
with O
DA+MSA O
lookup O
EGY O
94.6 O
95.0 O
90.5 O
89.2 O
88.8 O
88.3 O
83.5 O
89.2 O
LEV O
90.1 O
87.5 O
93.3 O
93.0 O
89.7 O
87.8 O
84.3 O
82.4 O
GLF O
90.3 O
87.3 O
89.6 O
88.6 O
93.1 O
91.9 O
84.1 O
84.8 O
MGR O
88.6 O
81.2 O
88.1 O
80.3 O
88.1 O
80.7 O
91.2 O
90.1 O
Table O
7 O
: O
Cross O
dialect O
results O
. O

Test O
Set O
Lookup O
EGY O
LEV O
GLF O
MGR O
SVM B-MethodName
LSTM B-MethodName
SVM B-MethodName
LSTM B-MethodName
SVM B-MethodName
LSTM B-MethodName
SVM B-MethodName
LSTM B-MethodName
No O
lookup O
91.4 O
94.1 O
89.8 O
92.4 O
88.8 O
91.7 O
83.82 O
89.1 O
DA O
94.1 O
94.8 O
92.8 O
93.3 O
91.8 O
92.6 O
89.6 O
90.7 O
DA+MSA O
94.3 O
95.3 O
93.0 O
93.9 O
92.2 O
93.1 O
90.0 O
91.4 O
Joint O
with O
restricted O
afﬁxes O
DA O
94.5 O
- O
92.8 O
- O
91.9 O
- O
89.7 O
DA+MSA O
94.8 O
- O
93.0 O
- O
92.4 O
- O
90.3 O
Table O
8 O
: O
Joint O
model O
results O
. O

Conversely O
, O
the O
bi O
- O
LSTM B-MethodName
- O
CRF O
joint O
model O
with O
DA+MSA O
lookup O
beats O
every O
other O
experimental O
setup O
that O
we O
tested O
, O
leading O
to O
the O
best O
segmentation O
results O
for O
all O
dialects O
, O
without O
doing O
dialect O
identiﬁcation O
. O

This O
may O
indicate O
that O
bi O
- O
LSTM B-MethodName
- O
CRF O
beneﬁted O
from O
cross O
- O
dialect O
data O
in O
improving O
segmentation O
for O
individual O
dialects O
. O

Our O
results O
show O
that O
a O
single O
joint O
segmentation O
model O
, O
based O
on O
bi O
- O
LSTM B-MethodName
- O
CRF O
, O
can O
be O
developed O
for O
a O
group O
of O
dialects O
and O
this O
model O
yields O
results O
that O
are O
comparable O
to O
, O
or O
even O
superior O
to O
, O
the O
performance O
of O
single O
dialect O
- O
speciﬁc O
models O
. O

First O
, O
we O
use O
Provo O
to O
test O
the O
psychometric O
performance O
of O
three O
state O
- O
of O
- O
the O
- O
art O
Transformerbased O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
LMs O
— O
XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
, O
Transformer O
- O
XL O
( O
Dai O
et O
al O
. O
, O
2019 O
) O
, O
andGPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
— O
alongside O
a O
smaller O
2 O
- O
layer O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
trained O
on O
wikitext-103 O
( O
Merity O
et O
al O
. O
, O
2016 O
) O
, O
and O
a O
5 O
- O
gram O
LM O
baseline O
( O
Stolcke O
, O
2002 O
) O
. O

We O
ﬁnd O
that O
, O
while O
the O
Transformer O
models O
achieve O
the O
lowest O
perplexity B-MetricName
on O
Provo O
and O
the O
best O
ﬁt O
to O
the O
cloze O
data O
, O
the O
LSTM B-MethodName
model O
provides O
the O
best O
account O
of O
reading O
times O
in O
terms O
of O
raw O
correlation B-MetricName
. O

We O
apply O
this O
method O
to O
the O
LSTM B-MethodName
model O
and O
show O
substantial O
improvement O
in O
reading O
time O
prediction O
and O
word O
frequency O
estimation O
, O
in O
addition O
to O
generalization O
to O
held O
- O
out O
human O
cloze O
data O
. O

2.LSTM B-MethodName
: O
A O
standard O
2 O
- O
layer O
LSTM B-MethodName
RNN O
implemented O
in O
PyTorch O
( O
Paszke O
et O
al O
. O
, O
2017 O
) O
, O
used O
here O
with O
256hidden O
units O
and O
word O
embedding O
size O
of O
256 O
, O
and O
trained O
on O
the O
wikitext-103 O
corpus O
( O
Merity O
et O
al O
. O
, O
2016 O
) O
viaa O
next O
- O
word O
prediction O
task O
( O
40epochs B-HyperparameterName
, O
batch B-HyperparameterName
size= I-HyperparameterName
40 B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
= O
20 B-HyperparameterValue
) O
. O

3.GPT-2 B-MethodName
: O
A O
Transformer O
- O
based O
LM O
trained O
on O
the O
WebText O
corpus O
( O
Radford O
et O
al O
. O
, O
2019 O
) O
. O

4.Transformer O
- O
XL O
( O
TXL O
; O
Dai O
et O
al O
. O
, O
2019 O
): O
A O
Transformer O
- O
based O
LM O
with O
a O
segment O
level O
recurrence O
mechanism O
and O
relative O
positional O
embeddings O
trained O
on O
wikitext-103 O
. O

5.XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
): O
A O
Transformerbased O
LM O
trained O
with O
a O
permutation O
language O
modeling O
objective O
as O
well O
as O
a O
segment O
level O
recurrence O
mechanism O
and O
relative O
positional O
embeddings O
. O

These O
Transformer O
models O
use O
subword O
tokens O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
; O
we O
deﬁned O
word O
probabilities O
for O
these O
models O
as O
the O
joint O
probability O
of O
the O
subword O
tokens O
comprising O
the O
word O
given O
the O
context O
. O
ModelhDii O
h O
 O
ii O
h O
SiiFintrFbasegazefreq O
Cloze O
NA O
NA O
3:992:60 O
198:10 O
30:90 O
0:36 0:43 O
GPT-2 B-MethodName
2:301:57 0:570:004 O
6:115:00 O
252:70 O
46:11 O
0:40 0:46 O
XLNet B-MethodName
2:391:68 0:580:005 O
6:395:70 O
260:50 O
46:08 O
0:41 0:48 O
TXL O
3:271:92 0:470:005 O
8:095:50 O
238:30 O
30:54 O
0:39 0:50 O
LSTM B-MethodName
3:741:86 0:390:006 O
8:584:90 O
361:20 O
41:47 O
0:47 0:63 O
5 O
- O
gram O
3:891:84 0:200:007 O
12:487:00 O
161:00 O
16:72 O
0:31 0:41 O
Table O
1 O
: O
Evaluation O
of O
LMs O
on O
Provo O
reveals O
a O
dissociation O
between O
performance O
on O
next O
- O
word O
prediction O
and O
psychometric O
measures O
that O
reﬂect O
human O
language O
processing O
. O

Speciﬁcally O
, O
the O
LSTM B-MethodName
model O
, O
which O
does O
not O
perform O
as O
well O
as O
the O
Transformer O
- O
based O
LMs O
in O
next O
- O
word O
prediction O
on O
Provo O
, O
as O
reﬂected O
in O
its O
higher O
hSii O
, O
exhibits O
superior O
ability O
in O
predicting O
reading O
times O
, O
as O
measured O
in O
gazeandFintr O
. O

We O
note O
that O
when O
predicting O
reading O
times O
not O
only O
from O
the O
model O
’s O
surprisal O
values O
, O
but O
also O
using O
the O
baseline O
predictors O
( O
word O
frequency O
and O
do O
not O
include O
cloze O
probabilities O
of O
zero O
( O
which O
would O
yield O
inﬁnite O
surprisal).length O
) O
, O
the O
LSTM B-MethodName
model O
no O
longer O
outperforms O
the O
Transformer O
- O
based O
models O
( O
Table O
1 O
, O
Fbase O
) O
. O

Nonetheless O
, O
it O
is O
striking O
that O
the O
LSTM B-MethodName
model O
, O
which O
is O
much O
smaller O
than O
the O
Transformer O
- O
based O
models O
and O
was O
trained O
on O
much O
less O
data O
, O
achieves O
the O
best O
performance O
in O
predicting O
reading O
times O
without O
the O
baseline O
predictors O
. O

To O
evaluate O
the O
utility O
of O
the O
human O
cloze O
data O
, O
we O
vary O
the O
values O
of O
 O
from O
 O
= O
0 O
, O
which O
corresponds O
to O
pure O
next O
- O
word O
prediction O
driven O
ﬁnetuning O
, O
to O
 O
= O
1 O
, O
which O
corresponds O
to O
pure O
clozeprediction O
based O
ﬁne O
- O
tuning.4.3 O
Cloze O
- O
Distilled O
LSTM B-MethodName
To O
begin O
to O
evaluate O
the O
CD O
paradigm O
, O
we O
apply O
it O
to O
the O
LSTM B-MethodName
from O
Section O
3 O
by O
ﬁne O
- O
tuning O
this O
model O
using O
the O
CD O
objective O
over O
Provo O
. O

We O
refer O
to O
the O
resultant O
model O
as O
cloze O
- O
distilled O
LSTM B-MethodName
( O
CDLSTM B-MethodName
) O
. O

These O
mainly O
include O
signiﬁcant O
improvement O
over O
the O
standard O
LSTM B-MethodName
baseline O
in O
predicting O
human O
reading O
times O
and O
cloze O
distributions O
( O
Figure O
2 O
) O
. O

This O
can O
be O
seen O
in O
Figure O
2 O
, O
which O
shows O
the O
statistical O
comparison O
of O
the O
CD O
- O
LSTM B-MethodName
for O
varying O
levels O
of O
 O
. O

We O
add O
another O
model O
comparison O
designed O
to O
isolate O
the O
ability O
of O
CD O
- O
LSTM B-MethodName
to O
predict O
reading O
times O
above O
the O
standard O
LSTM B-MethodName
( O
Figure O
2a O
) O
. O

Speciﬁcally O
, O
we O
enter O
CD O
- O
LSTM B-MethodName
’s O
surprisals O
into O
an O
LME O
along O
with O
baseline O
predictors O
and O
surprisals O
from O
the O
standard O
LSTM B-MethodName
and O
compute O
the O
F O
- O
test O
statistic O
against O
a O
LME O
with O
CD O
- O
LSTM B-MethodName
surprisal O
ablated O
out O
. O

Dashed O
lines O
in O
panels O
( O
b)-(f O
) O
show O
the O
performance O
of O
the O
LSTM B-MethodName
model O
. O

( O
a O
) O
LME O
based O
on O
CD O
- O
LSTM B-MethodName
’s O
surprisals O
outperforms O
the O
LME O
based O
on O
the O
LSTM B-MethodName
’s O
surprisals O
for O
most O
values O
of O
 O
( O
not O
signiﬁcant O
for O
 O
< O
0:65 O
) O
. O

( O
b O
) O
LME O
based O
on O
CD O
- O
LSTM B-MethodName
’s O
surprisals O
outperforms O
the O
null O
( O
intercept O
only O
) O
model O
, O
and O
this O
performance O
generally O
improves O
with O
 O
. O

( O
c O
) O
LME O
based O
on O
CD O
- O
LSTM B-MethodName
’s O
surprisals O
with O
the O
baseline O
factors O
( O
word O
frequency O
and O
length O
) O
outperforms O
the O
baseline O
- O
only O
LME O
for O
several O
values O
of O
 O
. O

( O
d O
) O
Pearson O
’s O
correlation B-MetricName
between O
CD O
- O
LSTM B-MethodName
’s O
surprisals O
and O
reading O
times O
. O

( O
e O
) O
Pearson O
’s O
correlation B-MetricName
between O
CD O
- O
LSTM B-MethodName
’s O
surprisals O
and O
word O
frequencies O
. O

( O
f O
) O
Kendall O
’s O
 O
correlation B-MetricName
between O
CD O
- O
LSTM B-MethodName
’s O
surprisals O
and O
human O
cloze O
surprisals O
. O

Correlation B-MetricName
with O
reading O
time O
and O
CD O
- O
LSTM B-MethodName
’s O
surprisal O
also O
steadily O
increases O
with O
 B-MetricValue
( O
Figure O
2d O
) O
. O

We O
note O
, O
the O
standard O
deviation O
inhSiifor O
our O
LSTM B-MethodName
over O
Provo O
was O
1.86 O
bits O
( O
Table O
1 O
) O
. O

Dashed O
lines O
show O
LSTM B-MethodName
performance O
before O
ﬁne O
- O
tuning O
. O

4.4.4 O
Frequency O
We O
also O
note O
that O
as O
 O
increases O
, O
the O
CD O
- O
LSTM B-MethodName
next O
- O
word O
predictions O
exhibit O
increased O
correlation B-MetricName
with O
frequency O
( O
Figure O
2e O
) O
, O
suggesting O
that O
cloze O
distilled O
LMs O
may O
learn O
to O
better O
predict O
frequent O
words O
. O

Furthermore O
, O
we O
use O
simple O
LSTMs B-MethodName
to O
perform O
a O
detailed O
analysis O
of O
Cloze O
Distillation O
with O
dense O
sampling O
in O
 O
and O
thorough O
cross O
- O
validation O
. O

We O
test O
the O
performance O
of O
a O
bidirectional O
LSTM B-MethodName
with O
convolutional O
features O
in O
distinguishing O
people O
with O
PD O
from O
agematched O
controls O
typing O
in O
English O
and O
Spanish O
, O
both O
in O
clinics O
and O
online.1 O
1 O
Introduction O
Parkinson O
’s O
disease O
is O
a O
neurodegenerative O
disease O
that O
affects O
approximately O
1%of O
people O
over O
the O
age O
of O
60 O
( O
De O
Lau O
and O
Breteler O
, O
2006 O
) O
. O

We O
describe O
a O
method O
for O
using O
a O
convolutional O
neural O
network O
( O
CNN O
) O
long O
short O
term O
memory O
( O
LSTM B-MethodName
) O
network O
to O
distinguish O
people O
with O
PD O
from O
age O
- O
matched O
people O
with O
no O
diagnosis O
. O

We O
provide O
evidence O
that O
adding O
character O
information O
to O
a O
CNN O
- O
LSTM B-MethodName
that O
contains O
only O
timing O
information O
improves O
performance O
across O
datasets.2 O
Related O
work O
A O
simple O
motor O
test O
that O
we O
might O
consider O
a O
precursor O
to O
the O
use O
of O
typing O
is O
the O
alternating O
ﬁnger O
tapping O
test O
( O
Burns O
and O
DeJong O
, O
1960 O
) O
. O

Convolutional O
neural B-MethodName
networks I-MethodName
( O
CNN O
) O
employ O
layers O
with O
convolving O
ﬁlters O
( O
Kim O
, O
2014 O
) O
which O
are O
applied O
to O
local O
features O
( O
derived O
in O
our O
case O
from O
the O
above O
information O
list O
of O
textual O
information).Diverging O
from O
their O
approach O
, O
we O
use O
a O
smaller O
number O
of O
convolutional O
layers O
followed O
by O
a O
bidirectional O
long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
layer O
( O
Schuster O
and O
Paliwal O
, O
1997 O
) O
. O

We O
also O
evaluate O
using O
a O
continuous O
vector O
representation O
of O
characters O
, O
which O
is O
an O
adaptation O
of O
the O
commonlyused O
continuous O
bag O
- O
of O
- O
words O
( O
CBOW B-MethodName
) O
embedding O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
. O

While O
for O
word O
embeddings O
the O
CBOW B-MethodName
algorithm O
learns O
the O
representation O
by O
predicting O
words O
from O
the O
surrounding O
context O
, O
our O
character O
level O
adaptation O
utilises O
the O
same O
algorithm O
but O
for O
the O
task O
of O
predicting O
characters O
from O
their O
context O
. O

From O
left O
to O
right O
, O
we O
see O
the O
elements O
of O
the O
NLM O
including O
convolutional O
layers O
, O
a O
bi O
- O
directional O
LSTM B-MethodName
layer O
and O
ﬁnally O
a O
fully O
connected O
output O
layer O
with O
Softmax O
activation B-HyperparameterName
. O

Finally O
a O
sentence O
is O
represented O
as O
xn O
1 O
: O
K O
= O
x1x2xkxK O
( O
1 O
) O
whereis O
the O
concatenation O
operator O
, O
where O
n O
indexes O
each O
sentence O
, O
kindexes O
each O
character O
within O
each O
sentence O
and O
xis O
the O
character O
identity O
encoding O
vector O
( O
one O
- O
hot O
or O
CBOW B-MethodName
) O
appended O
with O
the O
timing O
features O
associated O
with O
the O
key O
press O
of O
that O
character O
keypress O
. O

For O
participant O
classiﬁcation O
we O
aggregate O
the O
sentence O
classiﬁcation O
probabilities O
using O
logistic B-MethodName
regression I-MethodName
with O
leave O
- O
one O
- O
out O
cross O
validation O
and O
employ O
bootstrapping O
to O
report O
mean B-MetricName
and O
standard O
deviation O
. O

The O
AUCs B-MetricName
for O
logistic B-MethodName
regressions I-MethodName
usingthese O
features O
as O
predictors O
can O
be O
seen O
in O
table O
3 O
. O

Table O
3 O
: O
Results O
from O
logistic B-MethodName
regression I-MethodName
models O
with O
median O
and O
interquartile O
range O
for O
interkey O
intervals O
as O
features O
. O

The O
model O
appears O
to O
pick O
up O
on O
the O
dampening O
of O
surprisal O
- O
related O
IKI O
spikes O
for O
PwPD O
relative O
to O
controls.7 O
Conclusion O
In O
this O
paper O
we O
have O
provided O
evidence O
that O
natural O
language O
processing O
techniques O
and O
in O
particular O
CNN O
- O
LSTM B-MethodName
networks O
can O
identify O
markers O
of O
Parkinson O
’s O
disease O
in O
logged O
typing O
behaviour O
. O

In O
this O
work O
we O
ﬁrst O
remove O
this O
disease O
sign O
from O
the O
data O
and O
then O
use O
a O
CNN O
- O
LSTM B-MethodName
to O
pick O
up O
on O
more O
subtle O
changes O
in O
performance O
. O

We O
propose O
a O
uniﬁed O
parsing O
model O
using O
biafﬁne O
attention(Dozat O
and O
Manning O
, O
2017 O
) O
, O
consisting O
of O
1 O
) O
a O
BERT B-MethodName
- O
BiLSTM B-MethodName
encoder O
and O
2 O
) O
a O
biafﬁne O
attention O
decoder O
. O

First O
, O
the O
BERT B-MethodName
- O
BiLSTM B-MethodName
for O
sentence O
encoder O
uses O
BERT B-MethodName
to O
compose O
a O
sentence O
’s O
wordpieces O
into O
word O
- O
level O
embeddings O
and O
subsequently O
applies O
BiLSTM B-MethodName
to O
word O
- O
level O
representations O
. O

Our O
system O
consists O
of O
two O
main O
components O
: O
1.BERT B-MethodName
- O
BiLSTM B-MethodName
sentence O
encoder O
( O
shared O
across O
frameworks O
): O
Given O
a O
sentence O
, O
the O
BERT B-MethodName
encoder O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
encodes O
to O
its O
wordpieces O
and O
the O
encoded O
word O
piece O
- O
level O
represenations O
are O
composed O
into O
word O
- O
level O
embeddings O
based O
on O
BiLSTM B-MethodName
. O

Another O
BiLSTM B-MethodName
layer O
is O
then O
applied O
to O
the O
resulting O
word O
- O
level O
embeddings O
to O
create O
the O
ﬁnal O
sentence O
representations O
. O

We O
refer O
to O
this O
neural O
layer O
for O
encoding O
sentences O
as O
the O
BERT B-MethodName
- O
BiLSTM B-MethodName
sentence O
encoder O
. O

For O
multitask O
learning O
, O
the O
BERT B-MethodName
- O
BiLSTM B-MethodName
sentence O
encoder O
is O
shared O
across O
all O
target O
frameworks O
. O

2.Biafﬁne O
attention O
decoder O
( O
frameworkspeciﬁc O
): O
Role O
- O
dependent O
representationsfor O
each O
word O
are O
ﬁrst O
induced O
from O
the O
sentence O
- O
level O
embeddings O
of O
the O
BERTBiLSTM B-MethodName
encoder O
using O
simple O
feed O
- O
forward O
layers O
. O

2.BiLSTM B-MethodName
neural O
models O
for O
node O
property O
prediction O
: O
In O
addition O
to O
predicting O
the O
existence O
and O
labels O
of O
an O
edge O
, O
the O
system O
is O
required O
to O
predict O
node O
properties O
( O
for O
DM O
and O
PSD O
) O
. O

To O
handle O
node O
properties O
, O
we O
further O
develop O
property O
- O
speciﬁc O
BiLSTMbased B-MethodName
neural O
models.2These O
propertyspeciﬁc O
neural O
components O
are O
designed O
in O
a O
framework O
- O
speciﬁc O
manner O
and O
are O
not O
shared O
across O
frameworks O
. O

We O
prepared O
a O
BiLSTM B-MethodName
neural O
model O
for O
predicting O
the O
frame O
information O
of O
a O
node O
only O
, O
whereas O
we O
used O
the O
companion O
data O
of O
MRP O
2019 O
to O
predict O
POS B-TaskName
tags O
. O

The O
neural O
architecture O
consists O
of O
two O
components O
: O
1 O
) O
the O
BERT B-MethodName
- O
BiLSTM B-MethodName
encoder O
and O
2 O
) O
the O
biafﬁne O
attention O
decoder O
. O

1 O
) O
In O
BERTBiLSTM B-MethodName
encoder O
, O
an O
input O
sentence O
is O
fed O
to O
a O
word O
representation O
layer O
using O
BERT B-MethodName
, O
resulting O
in O
a O
sequence O
of O
word O
embedding O
vectors O
, O
which O
are O
then O
given O
to O
the O
BiLSTM B-MethodName
layer O
to O
produce O
a O
sentence O
representation O
. O

2.1 O
Encoder O
: O
BERT B-MethodName
- O
BiLSTM B-MethodName
2.1.1 O
Word O
representation O
layer O
using O
BERT B-MethodName
The O
word O
representation O
using O
BERT B-MethodName
uses O
BiLSTM B-MethodName
for O
composing O
to O
word O
- O
level O
embeddings O
from O
wordpiece O
- O
level O
embeddings O
, O
similar O
to O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
, O
which O
used O
the O
average B-MetricName
pooling O
for O
composition O
. O

To O
obtain O
the O
word O
representation O
xiforxi O
, O
we O
use O
BERT B-MethodName
from O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
as O
shown O
in O
Figure O
2 O
. O

pieces O
and O
they O
are O
fed O
to O
the O
BERT B-MethodName
encoder O
. O

The O
resulting O
output O
from O
BERT B-MethodName
, O
which O
consists O
of O
the O
word O
pieces O
in O
the O
i O
- O
th O
word O
are O
aggregated O
using O
BiLSTM B-MethodName
, O
producing O
wbert O
i O
, O
named O
BERT B-MethodName
wordlevel O
embedding O
.3 O
The O
BERT B-MethodName
word O
- O
level O
embedding O
is O
further O
combined O
with O
the O
pretrained O
GloVe B-MethodName
word O
embedding O
of O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
part O
- O
ofspeech O
( O
POS B-TaskName
) O
tag O
embedding O
to O
produce O
the O
ﬁnal O
word O
representation O
, O
as O
follows O
: O
xi O
= O
h O
wbert O
i;eglove O
i;ePOS B-TaskName
ii O
where O
eglove O
i O
andePOS B-TaskName
i O
denote O
the O
pretrained O
GloVe B-MethodName
word O
embedding O
and O
the O
POS B-TaskName
tag O
embedding O
for O
the O
i O
- O
th O
word O
, O
respectively O
. O

2.1.2 O
BiLSTM B-MethodName
sentence O
encoding O
layer O
Once O
word O
representations O
are O
obtained O
, O
we O
further O
apply O
BiLSTM B-MethodName
to O
x1xnto O
obtain O
the O
following O
initial O
hidden O
representation O
of O
the O
i O
- O
th O
word O
: O
ri O
= O
BiLSTM B-MethodName
i(x1xn O
) O
whereBiLSTM B-MethodName
irefers O
to O
the O
i O
- O
th O
hidden O
representation O
obtained O
by O
applying O
BiLSTM B-MethodName
to O
a O
given O
sequence O
. O

2.2 O
Decoder O
: O
Biafﬁne O
attention O
To O
formulate O
a O
decoder O
using O
biafﬁne O
attention O
, O
let O
BiAff O
( O
x;y)be O
a O
biafﬁne O
function O
using O
the O
notations O
of O
( O
Dozat O
and O
Manning O
, O
2018 O
) O
and O
( O
Socher O
3This O
aggregation O
is O
similar O
to O
the O
BiLSTM B-MethodName
- O
based O
composition O
in O
( O
Ballesteros O
et O
al O
. O
, O
2015 O
; O
Na O
et O
al O
. O
, O
2018 O
) O
which O
uses O
characters O
as O
subtokens O
, O
whereas O
our O
aggregation O
uses O
word O
pieces O
as O
subtokens.et O
al O
. O
, O
2013 O
) O
as O
follows O
: O
BiAff O
m(x;y O
) O
= O
xTU[1 O
: O
m]y+Vx O
y O
+ O
b O
where O
U[1 O
: O
k]2Rddmis O
a O
tensor O
, O
xTU[1 O
: O
m]y O
produces O
vector O
r2Rk O
, O
V2Rmdis O
a O
matrix O
andb2Rmis O
a O
vector O
for O
the O
bias O
term O
. O

To O
formulate O
the O
multi O
- O
level O
biafﬁne O
attention O
, O
we O
ﬁrst O
apply O
deep O
BiLSTM B-MethodName
encoder O
of O
L O
- O
levels O
to O
a O
list O
of O
word O
embeddings O
x1;;xnas O
follows O
. O

ri;0 O
= O
xi O
ri;l O
= O
BiLSTM B-MethodName
i(r1;l 1rn;l 1 O
) O
where O
ri;lis O
the O
hidden O
representation O
of O
the O
BiLSTM B-MethodName
at O
thel O
- O
th O
layer O
. O

2.4 O
Property O
prediction O
based O
on O
BiLSTM B-MethodName
To O
predict O
frame O
information O
, O
which O
is O
one O
of O
the O
node O
properties O
in O
DM O
and O
PSD O
, O
we O
use O
a O
simple O
BiLSTM B-MethodName
architecture O
with O
a O
single O
output O
layer O
that O
generates O
a O
node O
property O
for O
each O
word.6 O
Different O
from O
the O
biafﬁne O
attention O
model O
, O
the O
property O
predictor O
does O
not O
use O
BERT B-MethodName
but O
a O
simple O
word O
representation O
that O
consists O
of O
the O
pretrained O
GloVe B-MethodName
and O
the O
POS B-TaskName
tag O
embedding O
as O
follows O
: O
x(prop O
) O
i O
= O
h O
eglove O
i;ePOS B-TaskName
ii O
For O
encoding O
a O
sentence O
, O
another O
BiLSTM B-MethodName
is O
then O
applied O
to O
the O
sequence O
of O
word O
representations O
, O
as O
follows O
: O
r(prop O
) O
i O
= O
BiLSTM(prop B-MethodName
) O
i O
( O
x1xn O
) O
6Here O
, O
words O
( O
or O
tokens O
) O
correspond O
to O
nodes O
in O
a O
semantic O
graph O
. O
The O
output O
layer O
uses O
the O
following O
simple O
afﬁne O
transformation O
: O
s(prop O
) O
i O
= O
FFN(prop) O
r(prop O
) O
i O
( O
4 O
) O
The O
loss O
function O
uses O
the O
cross O
entropy B-MetricName
, O
which O
is O
formulated O
given O
a O
single O
training O
sentence O
as O
follows O
: O
L(prop)=X O
ilogsoftmax O
g(i) O
s(prop O
) O
i O
( O
5 B-MetricValue
) O
whereg(i)is O
the O
gold O
property O
value O
of O
the O
i O
- O
th O
word O
andsoftmax O
kis O
the O
function O
of O
k O
- O
th O
element O
of O
softmax O
values.7 O
3 O
Training O
3.1 O
Preprocessing O
We O
use O
word O
tokens O
and O
their O
POS B-TaskName
tags O
in O
the O
companion O
dataset O
provided O
by O
MRP O
2019 O
. O

Given O
this O
setting O
, O
the O
values O
of O
g(i)are O
mostly O
NULL O
in O
the O
frame O
property O
of O
PSD.GloVe B-MethodName
source O
840B O
dim O
300 O
BERT B-MethodName
layer O
source O
BERT B-MethodName
- O
Base O
- O
cased O
dim O
784 O
Word O
embedding O
layer O
: O
BiLSTM B-MethodName
hidden B-HyperparameterName
size I-HyperparameterName
384 B-HyperparameterValue
num O
layers O
1 O
Sentence O
encoder O
: O
BiLSTM B-MethodName
hidden B-HyperparameterName
size I-HyperparameterName
600 B-HyperparameterValue
num O
layers O
3 O
( O
Multi O
- O
level O
) O
Biafﬁne O
decoder O
hidden B-HyperparameterName
size I-HyperparameterName
600 O
Property O
predictor O
BiLSTM B-MethodName
hidden B-HyperparameterName
size I-HyperparameterName
600 O
BiLSTM B-MethodName
num O
layers O
3 O
output O
vocab O
size(DM O
) O
474 O
output O
vocab O
size(PSD O
) O
5474 O
Adam O
optimizer B-HyperparameterName
learning B-HyperparameterName
rate I-HyperparameterName
0.001 B-HyperparameterValue
weight B-HyperparameterName
decay O
rate O
3e-9 O
Adam O
 B-HyperparameterValue
1 O
0.0 O
Adam O
 O
2 O
0.95 O
BERT B-MethodName
Adam O
optimizer B-HyperparameterName
learning B-HyperparameterName
rate I-HyperparameterName
2e-5 O
weight B-HyperparameterName
decay O
rate O
0.01 B-HyperparameterValue
Adam O
 O
1 O
0.9 O
Adam O
 O
2 O
0.999 O
Loss O
for O
multi O
- O
task O
learning O
of O
Eq O
. O

( O
6 O
) O
1 O
0.025 O
2 O
0.975 O
3 O
1.0 O
batch B-HyperparameterName
size I-HyperparameterName
16 B-HyperparameterValue
Table O
1 O
: O
Hyper O
- O
parameter O
settings O
3.3 O
Multi O
- O
task O
learning O
across O
frameworks O
To O
enable O
multi O
- O
task O
learning O
across O
frameworks O
, O
we O
share O
the O
BERT B-MethodName
- O
BiLSTM B-MethodName
encoder O
as O
a O
common O
neural O
component O
across O
three O
frameworks O
and O
use O
framework O
- O
speciﬁc O
models O
for O
the O
biafﬁne O
attention O
decoder O
. O

4.1 O
Experimental O
results O
We O
evaluated O
the O
following O
four O
biafﬁne O
attention O
methods O
: O
1.Biafﬁne O
: O
This O
model O
is O
the O
baseline O
biafﬁne O
attention O
model O
based O
on O
the O
BiLSTM B-MethodName
sentence O
encoder O
without O
using O
BERT B-MethodName
. O

2.BERT+Biafﬁne B-MethodName
: O
This O
model O
uses O
the O
BERTBiLSTM B-MethodName
encoder O
of O
Section O
2.1 O
and O
the O
biafﬁne O
attention O
model O
of O
Section O
2.2 O
. O

3.BERT+Multi B-MethodName
- O
level O
Biafﬁne O
: O
This O
model O
uses O
BERT B-MethodName
- O
BiLSTM B-MethodName
encoder O
of O
Section O
2.1 O
and O
uses O
the O
multi O
- O
level O
attention O
method O
of O
Section O
2.3 O
. O

4.BERT+Biafﬁne+MTL B-MethodName
: O
This O
model O
is O
the O
same O
as O
BERT+Biafﬁne B-MethodName
but O
uses O
the O
multitask O
learning O
across O
frameworks O
described O
in O
Section O
3.3 O
. O

BERT+Biafﬁne B-MethodName
performs O
better O
than O
Biafﬁne O
, O
in O
particular O
, O
obtaining O
the O
increases O
of O
about O
5 O
% O
for O
UF O
and O
LF O
on O
the O
UCCA O
framework O
. O

However O
, O
BERT+Multi B-MethodName
- O
level O
Biafﬁne O
does O
not O
achieve O
anyfurther O
improvements O
with O
respect O
to O
Biafﬁne O
, O
often O
yielding O
weak O
performances O
similar O
to O
that O
of O
the O
BERT B-MethodName
- O
Biafﬁne O
model O
on O
the O
PSD O
and O
UCCA O
frameworks O
. O

BERT+Biafﬁne+MTL B-MethodName
only O
achieves O
small O
improvements O
on O
UCCA O
framework O
whereas O
no O
improvements O
on O
DM O
and O
PSD O
frameworks O
can O
be O
observed O
. O

A O
statistically O
insigniﬁcant O
improvement O
for O
multi O
- O
task O
learning O
in O
BERT+Biafﬁne+MTL B-MethodName
was O
similarly O
reported O
in O
the O
results O
of O
SHARED1 O
in O
( O
Peng O
et O
al O
. O
, O
2017 O
) O
. O

5 O
Ofﬁcial O
Results O
Given O
the O
preliminary O
results O
, O
we O
chose O
the O
basic O
biafﬁne O
model O
“ O
BERT+Biafﬁne B-MethodName
” O
of O
Table O
3 O
for O
the O
ﬁnal O
submission O
to O
MRP O
2019 O
. O

The O
ofﬁcial O
results O
using O
BERT+Biafﬁne B-MethodName
are O
summarized O
in O
Tables O
4 O
and O
5 O
, O
which O
compare O
the O
results O
of O
ERG O
( O
Oepen O
and O
Flickinger O
, O
2019 O
) O
and O
TUPA O
( O
Hershcovich O
and O
Arviv O
, O
2019 O
) O
which O
were O
provided O
by O
the O
task O
organizer O
. O

lower O
on O
DM O
framework O
, O
about O
3.4 O
8Our O
system O
ranked O
ﬁfth O
for O
framework O
- O
speciﬁc O
LF O
on O
DM O
and O
PSD O
, O
ranked O
eighth O
on O
UCCA O
, O
ﬁrst O
for O
frameworkspeciﬁc O
UF O
using O
the O
100 O
- O
sentence O
LPPS O
sub O
- O
set O
, O
and O
second O
for O
LF O
on O
the O
PSD O
framework.methodDM O
PSD O
UCCA O
Top O
UF O
LF O
Top O
UF O
LF O
Top O
UF O
LF O
Biafﬁne O
93.67 O
92.08 O
90.86 O
95.97 O
90.50 O
78.21 O
72.60 O
69.67 O
65.17 O
BERT+Biafﬁne B-MethodName
95.06 O
93.85 O
93.00 O
96.89 O
92.30 O
80.24 O
77.09 O
74.85 O
70.15 O
BERT+Multi B-MethodName
- O
level O
Biafﬁne O
95.09 O
93.86 O
93.02 O
96.76 O
91.95 O
79.76 O
78.12 O
74.42 O
69.81 O
BERT+Biafﬁne+MTL B-MethodName
N O
/ O
A O
93.66 O
92.73 O
N O
/ O
A O
92.13 O
79.63 O
N O
/ O
A O
75.40 O
70.59 O
Table O
3 O
: O
Unofﬁcial O
results O
of O
Top O
, O
UF O
, O
and O
LF O
metrics O
on O
the O
three O
frameworks O
( O
DM O
, O
PSD O
, O
and O
UCCA O
) O
, O
comparing O
variants O
of O
biafﬁne O
attention O
models O
. O

For O
sequential O
classiﬁcation O
, O
we O
utilize O
a O
multi O
- O
layer O
BiLSTM B-MethodName
network O
. O

A O
multi O
- O
layer O
BiLSTM B-MethodName
is O
utilized O
to O
encode O
tokens O
and O
another O
two O
softmax O
layers O
to O
predict O
concept O
- O
related O
labels O
: O
One O
for O
lexicalized O
concepts O
and O
the O
other O
for O
the O
rest O
. O

We O
also O
use O
recently O
widely O
- O
used O
contextualized O
word O
representation O
models O
, O
including O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

This O
strategy O
matches O
a O
recent O
research O
interest O
in O
graph O
neural B-MethodName
networks I-MethodName
( O
Li O
et O
al O
. O
, O
2015 O
; O
Veliˇckovi O
´ O
c O
et O
al O
. O
, O
2017 O
; O
Defferrard O
et O
al O
. O
, O
2016 O
; O
Chen O
et O
al O
. O
, O
2018a O
; O
Song O
et O
al O
. O
, O
2018 O
) O
, O
one O
goal O
of O
which O
is O
to O
associate O
vectors O
to O
graph O
nodes O
. O

Such O
vectors O
can O
be O
more O
easily O
to O
be O
integrated O
to O
neural B-MethodName
networks I-MethodName
for O
various O
purposes O
. O

We O
employ O
a O
Graph O
- O
based O
LSTM B-MethodName
( O
Song O
et O
al O
. O
, O
2018 O
) O
to O
encode O
anEDSgraph O
and O
a O
multi O
- O
layer O
feed O
- O
forward O
network O
to O
determine O
whether O
a O
node O
is O
top O
. O

These O
models O
use O
different O
initial O
random B-HyperparameterName
seeds I-HyperparameterName
, O
different O
pretraining O
methods O
( O
ELMo B-MethodName
or O
BERT B-MethodName
) O
or O
different O
encoder O
architectures O
( O
Transformer O
or O
BiLSTM B-MethodName
) O
. O

For O
this O
reason O
, O
we O
re O
- O
used O
most O
of O
the O
training O
algorithms O
implemented O
for O
the O
BIST O
- O
parser O
since O
these O
have O
proven O
to O
be O
effective O
when O
dealing O
with O
sequential O
information O
even O
for O
long O
sentences O
, O
thanks O
to O
bidirectional O
LSTM B-MethodName
feature O
representations O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
) O
. O

( O
1 O
) O
Embedding O
Layer O
: O
vectorized O
features O
that O
are O
feeding O
into O
Bidirectional O
LSTM B-MethodName
. O

( O
2 O
) O
Bidirectional O
- O
LSTM B-MethodName
: O
train O
representation O
of O
each O
token O
as O
vector O
values O
based O
on O
bidirectional O
LSTM B-MethodName
neural O
network O
. O

( O
3 O
) O
Multi O
- O
Layer O
Perceptron O
: O
build O
candidate O
of O
parse O
trees O
based O
on O
trained(changed O
) O
features O
by O
bidirectional O
LSTM B-MethodName
layer O
, O
and O
then O
calculate O
probabilistic O
scores B-MetricName
for O
each O
of O
candidates O
. O

Basically O
the O
Graph O
- O
based O
BIST O
- O
parser O
uses O
bidirectional O
Long O
Short O
Term O
Memory O
( O
LSTM B-MethodName
) O
feature O
representations O
thanks O
to O
two O
neural O
network O
layers O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
) O
. O

In O
order O
to O
select O
the O
best O
relation O
and O
head O
for O
each O
tokens O
in O
a O
sentence O
, O
Kiperwasser O
and O
Goldberglink O
the O
output O
of O
the O
bidirectional O
LSTM B-MethodName
with O
the O
Multi O
- O
Layer O
Perceptron O
( O
MLP O
) O
thanks O
to O
one O
neural O
layer O
. O

The O
model O
consists O
of O
1 O
) O
a O
recurrent O
neural O
network O
( O
RNN O
) O
to O
learn O
scoring O
functions O
for O
pair O
- O
wise O
relations O
, O
and O
2 O
) O
a O
structured O
support O
vector O
machine O
( O
SSVM B-MethodName
) O
to O
make O
joint O
predictions O
. O

The O
neural O
network O
automatically O
learns O
representations O
that O
account O
for O
long O
- O
term O
contexts O
to O
provide O
robust O
features O
for O
the O
structured O
model O
, O
while O
the O
SSVM B-MethodName
incorporates O
domain O
knowledge O
such O
as O
transitive O
closure O
of O
temporal O
relations O
as O
constraints O
to O
make O
better O
globally O
consistent O
decisions O
. O

Speciﬁcally O
, O
we O
adapt O
the O
structured O
support O
vector O
machine O
( O
SSVM B-MethodName
) O
( O
Finley O
and O
Joachims O
, O
2008 O
) O
to O
incorporate O
linguistic O
constraints O
and O
domain O
knowledge O
for O
making O
joint O
predictions O
on O
events O
temporal O
relations O
. O

Furthermore O
, O
we O
augment O
this O
framework O
with O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
to O
learn O
long O
- O
term O
contexts O
. O

Despite O
the O
recent O
success O
of O
employing O
neural B-MethodName
network I-MethodName
models O
for O
event O
temporal O
relation O
extraction O
( O
Tourille O
et O
al O
. O
, O
2017a O
; O
Cheng O
and O
Miyao O
, O
2017 O
; O
Meng O
et O
al O
. O
, O
2017 O
; O
Meng O
and O
Rumshisky O
, O
2018 O
) O
, O
these O
systems O
make O
pairwise O
predictions O
, O
and O
do O
not O
take O
advantage O
of O
problem O
structures O
. O
We O
develop O
a O
joint O
end O
- O
to O
- O
end O
training O
scheme O
that O
enables O
the O
feedback O
from O
global O
structure O
to O
directly O
guide O
neural B-MethodName
networks I-MethodName
to O
learn O
representations O
, O
and O
hence O
allows O
our O
deep O
structured O
model O
to O
combine O
the O
beneﬁts O
of O
both O
data O
- O
driven O
learning O
and O
knowledge O
exploitation O
. O

To O
summarize O
, O
our O
main O
contributions O
are O
: O
We O
propose O
a O
deep O
SSVM B-MethodName
model O
for O
event O
temporal O
relation O
extraction O
. O

The O
input O
representations O
consist O
of O
BERT B-MethodName
representations O
( O
vw;k O
) O
and O
POS B-TaskName
tag O
embeddings O
( O
vp;k O
) O
. O

They O
are O
concatenated O
to O
pass O
through O
BiLSTM B-MethodName
layers O
and O
classiﬁcation O
layers O
to O
get O
pairwise O
local O
scores B-MetricName
. O

Incompatible O
local O
pairwise O
prediction O
( O
denoted O
by O
red O
lines O
) O
is O
corrected O
by O
the O
SSVM B-MethodName
layer O
. O

The O
intuition O
behind O
the O
SSVM B-MethodName
loss O
is O
that O
it O
requires O
the O
score B-MetricName
of O
gold O
output O
structure O
ynto O
be O
greater O
than O
the O
score B-MetricName
of O
the O
best O
output O
structure O
under O
the O
current O
model O
^ynwith O
a O
margin O
(yn;^yn)1 O
, O
or O
else O
there O
will O
be O
some O
loss O
. O

The O
major O
difference O
between O
our O
deep O
SSVM B-MethodName
and O
the O
traditional O
SSVM B-MethodName
model O
is O
the O
scoring O
function O
. O

Traditional O
SSVM B-MethodName
uses O
a O
linear O
function O
over O
hand O
- O
crafted O
features O
to O
compute O
the O
scores B-MetricName
, O
whereas O
we O
propose O
to O
use O
a O
RNN O
for O
estimation O
. O

As O
shown O
in O
Figure O
2 O
, O
the O
input O
layer O
consists O
of O
word O
representations O
and O
part O
- O
of1Note O
that O
if O
the O
best O
prediction O
is O
the O
same O
as O
the O
gold O
structure O
, O
the O
margin O
is O
zero.speech O
( O
POS B-TaskName
) O
tag O
embeddings O
of O
each O
token O
in O
the O
input O
sentence O
, O
denoted O
as O
vw;kandvp;krespectively.2The O
word O
representations O
are O
obtained O
via O
pre O
- O
trained O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018)3model O
and O
are O
ﬁxed O
throughout O
training O
, O
while O
the O
POS B-TaskName
tag O
embeddings O
are O
tuned O
. O

The O
word O
and O
POS B-TaskName
tag O
embeddings O
are O
concatenated O
to O
represent O
an O
input O
token O
, O
and O
then O
fed O
into O
a O
Bi O
- O
LSTM B-MethodName
layer O
to O
get O
contextualized O
representations O
. O

3.4 O
Learning O
We O
develop O
a O
two O
- O
state O
learning O
approach O
to O
optimize O
the O
neural O
SSVM B-MethodName
. O

5We O
experiment O
with O
optimizing O
SSVM B-MethodName
loss O
directly O
, O
but O
model O
performance O
degrades O
signiﬁcantly O
. O

7It O
is O
noted O
that O
if O
symmetric O
constraint O
is O
applied O
, O
scores B-MetricName
for O
testing O
on O
augmented O
or O
unaugmented O
set O
are O
equal O
. O
TB O
- O
Dense O
MATRES O
TCR O
Local O
Model O
hidsize O
60 O
40 O
30 O
dropout B-HyperparameterName
0.5 B-HyperparameterValue
0.7 O
0.5 O
BiLSTM B-MethodName
layers O
1 O
2 O
1 O
learning B-HyperparameterName
rate I-HyperparameterName
0.002 B-HyperparameterValue
0.002 O
0.002 O
Structured O
Learning O
learning B-HyperparameterName
rate I-HyperparameterName
0.05 B-HyperparameterValue
0.08 O
0.08 O
decay O
0.7 O
0.7 O
0.9 O
Table O
2 O
: O
Best O
hyper O
- O
parameters O
Start O
- O
point O
temporal O
scheme O
is O
adopted O
when O
outsourcing O
the O
annotation O
task O
, O
which O
contributes O
to O
the O
performance O
improvement O
of O
machine O
learning O
models O
built O
on O
this O
dataset O
. O

We O
use O
pre O
- O
trained O
BERT B-MethodName
embedding O
with O
768 O
dimensions O
as O
the O
input O
word O
representations O
and O
one O
- O
layer O
MLP O
as O
the O
classiﬁcation O
layer O
. O

In O
this O
section O
, O
we O
perform O
a O
though O
ablation O
study O
to O
understand O
the O
importance O
of O
structured O
constraints O
, O
linguistic O
features O
, O
and O
the O
BERT B-MethodName
representations O
. O

To O
better O
understand O
the O
beneﬁts O
of O
the O
symmetry O
constraints O
, O
we O
study O
both O
the O
contribution O
of O
explicitly O
applying O
symmetry O
constraint O
in O
our O
SSVM B-MethodName
as O
well O
as O
its O
implicit O
impact O
in O
data O
augmentation O
. O

These O
features O
are O
concatenated O
with O
the O
Bi O
- O
LSTM B-MethodName
hidden O
states O
before O
the O
linear O
layer O
( O
i.e. O
fi O
, O
bi O
, O
fj O
, O
bjin O
Figure O
2 O
) O
. O

6.3 O
Effect O
of O
BERT B-MethodName
representations O
In O
this O
section O
, O
we O
explore O
the O
impact O
of O
contextualized O
BERT B-MethodName
representations O
under O
our O
deep O
SSVM B-MethodName
framework O
. O

We O
replace O
BERT B-MethodName
representations O
with O
the O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
word O
embeddings O
. O

Table O
9 O
shows O
the O
F1 B-MetricName
scores B-MetricName
of O
our O
local O
model O
and O
global O
model O
using O
BERT B-MethodName
and O
GloVe11respectively B-MethodName
. O

BERT B-MethodName
improves O
the O
performance O
with O
a O
signiﬁcant O
margin O
. O

Besides O
, O
even O
without O
BERT B-MethodName
representations O
, O
our O
RNN O
- O
based O
local O
model O
and O
the O
deep O
structured O
global O
model O
11For O
GloVe B-MethodName
model O
, O
additional O
linguistic O
features O
are O
used.previous O
local- O
global- O
local- O
globalSOTA O
Glove O
Glove O
BERT B-MethodName
BERT B-MethodName
TB O
- O
Dense O
57.0 O
56.6 O
57.0 O
62.6 O
63.2 O
MATRES O
69.0 O
71.8 O
75.6 O
80.3 O
81.7 O
TCR O
71.1 O
73.5 O
76.5 O
79.7 O
80.9 O
Table O
9 O
: O
Ablation O
over O
word O
representation O
: O
BERT B-MethodName
vs O
GloVe B-MethodName
. O

Although O
BERT B-MethodName
representation O
largely O
contributes O
to O
the O
performance O
boost O
, O
our O
proposed O
framework O
remains O
strong O
and O
outperforms O
current O
SOTA O
approaches O
when O
GloVe B-MethodName
is O
used O
. O

7 O
Conclusion O
In O
this O
paper O
, O
we O
propose O
a O
novel O
deep O
structured O
model O
based O
on O
SSVM B-MethodName
that O
combines O
the O
beneﬁts O
of O
structured O
models O
’ O
ability O
to O
encode O
structure O
knowledge O
, O
and O
data O
- O
driven O
deep O
neural O
architectures O
’ O
ability O
to O
learn O
long O
- O
range O
features O
. O

We O
did O
not O
observe O
a O
signiﬁcant O
difference O
between O
the O
models O
, O
and O
present O
results O
for O
Support O
Vector O
Machine O
( O
SVM B-MethodName
) O
model O
( O
Cortes O
and O
Vapnik O
, O
1995 O
) O
that O
have O
shownthe O
highest O
performance O
on O
a O
validation O
set O
. O

This O
is O
the O
feature O
vector O
which O
is O
fed O
as O
an O
input O
to O
the O
SVM B-MethodName
classiﬁer O
, O
built O
for O
each O
entity O
separately O
. O

We O
experimentwith O
several O
deep O
models O
, O
including O
Bi O
- O
LSTM B-MethodName
and O
Bi O
- O
GRU O
unit O
: O
each O
textual O
description O
is O
translated O
to O
sequence O
of O
pre O
- O
trained O
embeddings O
. O

That O
sequence O
is O
fed O
into O
a O
Bi O
- O
Directional O
Long O
Short O
Term O
Memory O
( O
Bi O
- O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Schuster O
and O
Paliwal O
, O
1997 O
) O
or O
Bi O
- O
GRU O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
, O
and O
based O
on O
the O
ﬁnal O
cell O
state O
, O
we O
perform O
a O
binary O
prediction O
whether O
the O
given O
entity O
is O
implicitly O
mentioned O
or O
not O
. O

Among O
the O
one O
- O
vs O
- O
all O
possible O
methods O
( O
Section O
4 O
) O
, O
the O
most O
successful O
method O
, O
in O
terms O
of O
macro O
metric O
, O
is O
the O
bag O
- O
of O
- O
words O
& O
SVM B-MethodName
model O
( O
section O
4.1 O
) O
. O

This O
does O
not O
allow O
deep O
- O
learning O
models O
to O
train O
well O
, O
and O
therefore O
the O
macro O
score B-MetricName
of O
SVM B-MethodName
methods O
tends O
to O
be O
higher O
. O

The O
reason O
the O
SVM B-MethodName
with O
BOW O
performs O
better O
than O
the O
more O
semantic O
embeddings O
( O
Section O
4.2–4.4 O
) O
with O
SVM B-MethodName
might O
also O
be O
due O
to O
the O
low O
amount O
of O
training O
examples O
that O
cause O
the O
contribution O
of O
semantics O
to O
be O
limited O
for O
the O
LEE O
task O
in O
this O
dataset O
. O

The O
results O
are O
shown O
for O
the O
two O
best O
performing O
classiﬁers O
( O
bag O
- O
of O
- O
words O
embedding O
with O
SVM B-MethodName
classi O
- O
Model O
Precision B-MetricName
Recall B-MetricName
F1 B-MetricName
Bag O
- O
of O
- O
Words O
( O
TF O
- O
IDF O
) O
& O
SVM B-MethodName
( O
Section O
4.1 B-MetricValue
) O
0.803 O
0.873 O
0.837 O
Weighted O
- O
average B-MetricName
Embedding O
& O
SVM B-MethodName
( O
Section O
4.2 B-MetricValue
) O
0.770 O
0.746 O
0.758 O
Element O
- O
wise O
Document O
Embedding O
& O
SVM B-MethodName
( O
Section O
4.3 O
) O
0.817 O
0.817 O
0.817 O
Combined O
Document O
Embedding O
& O
SVM B-MethodName
( O
Section O
4.4 O
) O
0.823 O
0.810 O
0.816 O
Pre O
- O
Trained O
PubMed O
Word O
Embedding O
& O
Bi O
- O
LSTM B-MethodName
( O
Section O
4.5 O
) O
0.888 O
0.867 O
0.877 O
Pre O
- O
Trained O
PubMed O
Word O
Embedding O
& O
Bi O
- O
GRU O
( O
Section O
4.5 O
) O
0.899 O
0.836 O
0.866 O
Multitask O
- O
Embedding O
based O
Bi O
- O
GRU O
( O
Section O
5.1 O
) O
0.869 O
0.883 O
0.876 O
Multitask O
- O
Embedding O
based O
Bi O
- O
LSTM B-MethodName
( O
Section O
5.1 O
) O
0.869 O
0.883 O
0.876 O
Multitask O
with O
Grouping O
- O
Embedding O
based O
Bi O
- O
LSTM B-MethodName
( O
Section O
5.2 O
) O
0.909 O
0.859 O
0.884 O
Multitask O
with O
Grouping O
- O
Embedding O
based O
Bi O
- O
GRU O
( O
Section O
5.2 O
) O
0.914 O
0.828 O
0.869 O
Table O
2 O
: O
Extraction O
of O
ATP O
as O
a O
latent O
entity O
. O

Entity O
Bag O
- O
of O
- O
Words O
AUC B-MetricName
Grouped O
- O
MTL O
AUC B-MetricName
ATP O
0.906 B-MetricValue
0.938 O
ADP O
0.910 O
0.965 O
H2O O
0.864 O
0.928 O
PI O
0.872 O
0.937 O
H+ O
0.924 O
0.889 O
O2 O
0.904 O
0.928 O
NADPH O
0.917 O
0.998 O
NADP+ O
0.918 O
0.972 O
COA O
- O
SH O
0.960 O
0.998 O
Table O
3 O
: O
AUC B-MetricName
scores B-MetricName
for O
bag O
- O
of O
- O
words O
vectors O
& O
SVM B-MethodName
baseline O
performance O
compared O
to O
the O
multi O
- O
task O
learner O
with O
task O
- O
grouping O
. O

F1 B-MetricName
Bag O
- O
of O
- O
Words O
( O
TF O
- O
IDF O
) O
& O
SVM B-MethodName
( O
Section O
4.1 B-MetricValue
) O
0.784 O
0.746 O
0.795 O
0.785 O
0.750 O
0.754 O
Weighted O
- O
average B-MetricName
Embedding O
& O
SVM B-MethodName
( O
Section O
4.2 B-MetricValue
) O
0.682 O
0.649 O
0.665 O
0.636 O
0.565 O
0.580 O
Element O
- O
wise O
Document O
Embedding O
& O
SVM B-MethodName
( O
Section O
4.3 O
) O
0.740 O
0.728 O
0.734 O
0.696 O
0.647 O
0.648 O
Combined O
Document O
Embedding O
& O
SVM B-MethodName
( O
Section O
4.4 O
) O
0.743 O
0.729 O
0.736 O
0.707 O
0.651 O
0.656 O
Pre O
- O
Trained O
PubMed O
Word O
Embedding O
& O
Bi O
- O
LSTM B-MethodName
( O
Section O
4.5 O
) O
0.817 O
0.773 O
0.794 O
0.707 O
0.645 O
0.664 O
Pre O
- O
Trained O
PubMed O
Word O
Embedding O
& O
Bi O
- O
GRU O
( O
Section O
4.5 O
) O
0.798 O
0.817 O
0.808 O
0.707 O
0.690 O
0.687 O
Multitask O
- O
Embedding O
based O
Bi O
- O
GRU O
( O
Section O
5.1 O
) O
0.798 O
0.820 O
0.809 O
0.671 O
0.664 O
0.662 O
Multitask O
& O
Task O
- O
Grouping O
- O
Embedding O
based O
Bi O
- O
GRU O
( O
Section O
5.2 O
) O
0.822 O
0.849 O
0.835 O
0.809 O
0.839 O
0.811 O
Table O
4 O
: O
Results O
of O
multiple O
latent O
entities O
extraction O
of O
top O
40 O
frequent O
entities O
. O

For O
the O
encoder O
of O
our O
model O
, O
we O
encode O
the O
input O
sequence O
into O
context O
representations O
using O
BERT B-MethodName
. O

In O
the O
second O
stage O
, O
we O
mask O
each O
word O
of O
the O
draft O
sequence O
and O
feed O
it O
to O
BERT B-MethodName
, O
then O
by O
combining O
the O
input O
sequence O
and O
the O
draft O
representation O
generated O
by O
BERT B-MethodName
, O
we O
use O
a O
Transformer O
- O
based O
decoder O
to O
predict O
the O
reﬁned O
word O
for O
each O
masked O
position O
. O

To O
the O
best O
of O
our O
knowledge O
, O
our O
approach O
is O
the O
ﬁrst O
method O
which O
applies O
the O
BERT B-MethodName
into O
text O
generation O
tasks O
. O

Recently O
, O
BERT B-MethodName
has O
been O
successfully O
used O
in O
various O
natural O
language O
processing O
tasks O
, O
such O
as O
textual B-TaskName
entailment I-TaskName
, O
name O
entity O
recognition O
and O
machine O
reading O
comprehensions O
. O

In O
this O
paper O
, O
we O
present O
a O
novel O
natural O
language O
generation O
model O
based O
on O
pre O
- O
trained O
language O
models O
( O
we O
use O
BERT B-MethodName
in O
this O
work O
) O
. O

As O
far O
as O
we O
know O
, O
this O
is O
the O
ﬁrst O
work O
to O
extend O
BERT B-MethodName
to O
the O
sequence O
generation O
task O
. O

To O
address O
the O
above O
issues O
of O
previous O
abstractive O
methods O
, O
in O
our O
model O
, O
we O
design O
a O
two O
- O
stage O
decoding O
process O
to O
make O
good O
use O
of O
BERT B-MethodName
’s O
context O
modeling O
ability O
. O

We O
propose O
a O
natural O
language O
generationmodel O
based O
on O
BERT B-MethodName
, O
making O
good O
use O
of O
the O
pre O
- O
trained O
language O
model O
in O
the O
encoder O
and O
decoder O
process O
, O
and O
the O
model O
can O
be O
trained O
endto O
- O
end O
without O
handcrafted O
features O
. O

2.2 O
Bi O
- O
Directional O
Pre O
- O
Trained O
Context O
Encoders O
Recently O
, O
context O
encoders O
such O
as O
ELMo B-MethodName
, O
GPT B-MethodName
, O
and O
BERT B-MethodName
have O
been O
widely O
used O
in O
many O
NLP O
tasks O
. O

Since O
our O
method O
is O
based O
on O
BERT B-MethodName
, O
we O
illustrate O
the O
process O
brieﬂy O
here O
. O

BERT B-MethodName
consists O
of O
several O
layers O
. O

The O
issue O
is O
that O
while O
using O
a O
pre O
- O
trained O
context O
encoder O
like O
BERT B-MethodName
, O
they O
model O
token O
- O
level O
representations O
by O
conditioning O
on O
both O
direction O
context O
. O

Based O
on O
the O
sequence O
- O
to O
- O
sequence O
framework O
built O
on O
top O
of O
BERT B-MethodName
, O
we O
ﬁrst O
design O
a O
reﬁne O
decoder O
at O
word O
- O
level O
to O
tackle O
the O
two O
problems O
described O
in O
the O
above O
section O
. O

3.2.1 O
Encoder O
We O
simply O
use O
BERT B-MethodName
as O
the O
encoder O
. O

H O
= O
BERT B-MethodName
( O
x1;:::;x O
m O
) O
( O
5 O
) O
3.2.2 O
Summary O
Draft O
Decoder O
In O
the O
draft O
decoder O
, O
we O
ﬁrst O
introduce O
BERT B-MethodName
’s O
word O
embedding O
matrix O
to O
map O
the O
previous O
summary O
draft O
outputs O
fy1;:::;y O
t 1ginto O
embeddings O
vectorsfq1;:::;q O
t 1gat O
t O
- O
th O
time O
step O
. O

Note O
that O
as O
the O
input O
sequence O
of O
the O
decoder O
is O
not O
complete O
, O
we O
do O
not O
use O
the O
BERT B-MethodName
network O
to O
predict O
the O
context O
vectors O
here O
. O
Then O
we O
introduce O
an O
Nlayer O
Transformer O
decoder O
to O
learn O
the O
conditional O
probability O
P(AjH O
) O
. O

However O
a O
decoder O
with O
this O
structure O
is O
not O
sufﬁcient O
enough O
: O
if O
we O
use O
the O
BERT B-MethodName
network O
in O
this O
decoder O
, O
then O
during O
training O
and O
inference O
, O
in O
- O
complete O
context(part O
of O
sentence O
) O
is O
fed O
into O
the O
BERT B-MethodName
module O
, O
and O
although O
we O
can O
ﬁnetune O
BERT B-MethodName
’s O
parameters O
, O
the O
input O
distribution O
is O
quite O
different O
from O
the O
pre O
- O
train O
process O
, O
and O
thus O
harms O
the O
quality O
of O
generated O
context O
representations O
. O

If O
we O
just O
use O
the O
embedding O
matrix O
here O
, O
it O
will O
be O
more O
difﬁcult O
for O
the O
decoder O
with O
fresh O
parameters O
to O
learn O
to O
model O
representations O
as O
well O
as O
vocabulary O
probabilities O
, O
from O
a O
relative O
small O
corpus O
compared O
to O
BERT B-MethodName
’s O
huge O
pre O
- O
training O
corpus O
. O

In O
a O
word O
, O
the O
decoder O
can O
not O
utilize O
BERT B-MethodName
’s O
ability O
to O
generate O
high O
quality O
context O
vectors O
, O
which O
will O
also O
harm O
performance O
. O

The O
ﬁnal O
probability O
is O
calculated O
as O
follow O
: O
Pt(w O
) O
= O
( O
1 gt)Pvocab O
t(w O
) O
+ O
gtX O
i O
: O
wi O
= O
w O
 O
i O
t(11 O
) O
3.3 O
Summary O
Reﬁne O
Process O
The O
main O
reason O
to O
introduce O
the O
reﬁne O
process O
is O
to O
enhance O
the O
decoder O
using O
BERT B-MethodName
’s O
contextualized O
representations O
, O
so O
we O
do O
not O
modify O
the O
encoder O
and O
reuse O
it O
during O
this O
process O
. O

As O
Figure O
1 O
Stage O
2 O
shows O
, O
it O
ﬁrst O
masks O
each O
word O
in O
the O
summary O
draft O
one O
by O
one O
, O
then O
feeds O
the O
draft O
to O
BERT B-MethodName
to O
generatecontext O
vectors O
. O

Lrefine O
= O
jyjX O
t=1 logP(yt O
= O
y O
tja6 O
= O
t;H)(12 O
) O
From O
the O
view O
of O
BERT B-MethodName
or O
other O
contextualized O
embeddings O
, O
the O
reﬁne O
decoding O
process O
provides O
a O
more O
complete O
input O
sequence O
which O
is O
consistent O
with O
their O
pre O
- O
training O
processes O
. O

We O
design O
the O
word O
- O
level O
reﬁne O
decoder O
because O
this O
process O
is O
similar O
to O
the O
cloze O
task O
in O
BERT B-MethodName
’s O
pre O
- O
train O
process O
, O
therefore O
by O
using O
the O
ability O
of O
the O
contextual O
language O
model O
the O
decoder O
can O
generate O
more O
ﬂuent O
and O
natural O
sequences O
. O

4 O
Experiment O
4.1 O
Settings O
In O
this O
work O
, O
all O
of O
our O
models O
are O
built O
on O
BERT B-MethodName
BASE O
, O
although O
another O
larger O
pre O
- O
trained O
model O
with O
better O
performance O
( O
BERT B-MethodName
LARGE O
) O
has O
published O
but O
it O
costs O
too O
much O
time O
and O
GPU O
memory O
. O

We O
use O
WordPiece O
embeddings O
with O
a O
30,000 O
vocabulary O
which O
is O
the O
same O
as O
BERT B-MethodName
. O

We O
set O
the O
layer O
of O
transformer O
decoders O
to O
12(8 O
on O
NYT50 B-MethodName
) O
, O
and O
set O
the O
attention O
heads O
number O
to O
12(8 O
on O
NYT50 B-MethodName
) O
, O
set O
fully O
- O
connected O
sub O
- O
layer O
hidden B-HyperparameterName
size I-HyperparameterName
to O
3072 B-HyperparameterValue
. O

During O
training O
, O
we O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
36 B-HyperparameterValue
, O
and O
train O
for O
4 O
epochs(8 B-HyperparameterName
epochs B-HyperparameterName
for O
NYT50 B-MethodName
since O
it O
has O
many O
fewer O
training O
samples O
) O
, O
after O
training O
the O
best O
model O
are O
selected O
from O
last O
10 B-HyperparameterValue
models O
based O
on O
development O
set O
performance O
. O

ing O
samples O
and O
3,452 O
test O
samples O
and O
is O
called O
NYT50 B-MethodName
since O
all O
summaries O
are O
longer O
than O
50 O
words O
. O

After O
tokenizing O
, O
the O
average B-MetricName
article O
length O
and O
summary O
length O
of O
CNN O
/ O
Daily O
Mail O
are O
691 B-MetricValue
and O
51 O
, O
and O
NYT50 B-MethodName
’s O
average B-MetricName
article O
length O
and O
summary O
length O
are O
1152 B-MetricValue
and O
75 O
. O

We O
truncate O
the O
article O
length O
to O
512 O
, O
and O
the O
summary O
length O
to O
100 O
in O
our O
experiment(max O
summary O
length O
is O
set O
to O
150 O
on O
NYT50 B-MethodName
as O
its O
average B-MetricName
golden O
summary O
length O
is O
longer O
) O
. O

On O
NYT50 B-MethodName
, O
following O
( O
Paulus O
et O
al O
. O
, O
2018 O
) O
we O
evaluate O
limited O
length O
ROUGE B-MetricName
recall B-MetricName
score(limit B-MetricName
the O
generated O
summary O
length O
to O
the O
ground O
truth O
length O
) O
. O

We O
split O
NYT50 B-MethodName
summaries O
into O
sentences O
by O
semicolons O
to O
calculate O
the O
ROUGE B-MetricName
scores B-MetricName
. O

One O
- O
Stage O
: O
A O
sequence O
- O
to O
- O
sequence O
model O
with O
copy O
mechanism O
based O
on O
BERT B-MethodName
; O
Two O
- O
Stage O
: O
Adding O
the O
reﬁne O
decoder O
to O
the O
One O
- O
Stage O
model O
; O
Two O
- O
Stage O
3https://1drv.ms/u/s!AvPUdqbfQJ503Qffg8HZzV98iosq+ O
RL O
: O
Full O
model O
with O
reﬁne O
process O
cooperated O
with O
RL O
objective O
. O

4.3 O
Additional O
Results O
on O
NYT50 B-MethodName
Table O
2 O
reports O
experiment O
results O
on O
the O
NYT50 B-MethodName
corpus O
. O

Since O
the O
short O
summary O
samples O
are O
ﬁltered O
, O
NYT50 B-MethodName
has O
average B-MetricName
longer O
summaries O
than O
CNN O
/ O
Daily O
Mail O
. O

( O
Li O
et O
al O
. O
, O
2018 O
) O
extend O
seq2seq O
model O
with O
an O
information O
selection O
network O
to O
generate O
more O
informative O
summaries O
. O
Model O
R-1 O
R-2 O
First O
sentences O
28.60 O
17.30 O
Firstkwords O
35.70 O
21.60 O
Full O
( O
Durrett O
et O
al O
. O
, O
2016 O
) O
42.20 O
24.90 O
ML+RL+intra O
- O
attn O
( O
Paulus O
et O
al O
. O
, O
2018 O
) O
42.94 O
26.02 O
Two O
- O
Stage O
+ O
RL O
( O
Ours O
) O
45.33 O
26.53 O
Table O
2 O
: O
Limited O
length O
ROUGE B-MetricName
recall B-MetricName
results O
on O
the O
NYT50 B-MethodName
test O
set O
. O

More O
recently O
, O
pre O
- O
trained O
language O
models O
( O
ELMo B-MethodName
, O
GPT B-MethodName
and O
BERT B-MethodName
) O
, O
have O
also O
achieved O
great O
success O
on O
several O
NLP O
problems O
such O
as O
textual B-TaskName
entailment I-TaskName
, O
semantic O
similarity O
, O
reading O
comprehension O
, O
and O
question B-TaskName
answering I-TaskName
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Radford O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

In O
this O
work O
, O
we O
use O
BERT(which B-MethodName
is O
a O
pretrained O
language O
model O
using O
large O
scale O
unlabeled O
data O
) O
on O
the O
encoder O
and O
decoder O
of O
a O
seq2seq O
model O
, O
and O
by O
designing O
a O
two O
stage O
decoding O
structure O
we O
build O
a O
competitive O
model O
for O
abstractive O
text O
summarization O
. O

Our O
model O
utilize O
BERT B-MethodName
on O
both O
encoder O
and O
decoder O
sides O
, O
and O
introduce O
reinforce O
objective O
in O
learning O
process O
. O

Our O
models O
are O
built O
on O
top O
of O
pretrained O
BERT B-MethodName
parameters O
. O

2 O
Related O
Work O
Our O
work O
draws O
on O
prior O
research O
into O
the O
relationship O
between O
language O
and O
assessments O
of O
guilt O
, O
as O
well O
as O
work O
seeking O
to O
jointly O
model O
text O
- O
level O
and O
token O
- O
level O
annotations O
using O
neural B-MethodName
networks I-MethodName
. O

2.3 O
Span O
- O
Level O
Supervision O
BERT B-MethodName
models O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
deﬁne O
an O
output O
representation O
for O
every O
token O
- O
level O
input O
( O
see O
also O
Vaswani O
et O
al O
. O
2017 O
) O
. O

All O
of O
them O
begin O
with O
BERT B-MethodName
. O

We O
build O
regression O
models O
on O
top O
of O
these O
parameters O
using O
just O
the O
CLS O
token O
, O
which O
is O
the O
initial O
token O
in O
all O
BERT B-MethodName
input O
sequences O
and O
is O
often O
taken O
to O
provide O
an O
aggregate O
sequence O
representation O
, O
as O
well O
as O
mean B-MetricName
- O
pooling O
over O
all O
the O
ﬁnal O
output O
states O
, O
and O
we O
additionally O
deﬁne O
extensions O
for O
predicting O
token O
- O
level O
highlighting O
. O

4.1 O
Guilt O
Ratings O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
is O
a O
Transformer O
- O
based O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
that O
is O
usually O
trained O
jointly O
to O
do O
masked O
language O
modeling O
and O
next O
sentence O
prediction O
. O

BERT B-MethodName
maps O
these O
inputs O
to O
a O
sequence O
of O
output O
representations O
[ O
h0;:::;hn O
] O
. O

The O
individual O
regression O
models O
are O
trained O
using O
a O
mean B-MetricName
squared O
error O
( O
MSE O
) O
loss O
: O
Jr(r O
) O
= O
1 B-MetricValue
mmX O
i=11 O
2kHr(xi) yr O
ik2(1 O
) O
Here O
, O
mis O
the O
number O
of O
examples O
, O
rrepresents O
all O
the O
parameters O
of O
BERT B-MethodName
plus O
our O
new O
taskspeciﬁc O
parameters O
Wrandbr O
, O
yr O
iis O
the O
true O
label O
for O
example O
xi O
, O
andHr(xi)is O
the O
prediction O
of O
the O
model O
for O
example O
xi O
. O

4.2 O
Genre O
Pretraining O
BERT B-MethodName
was O
trained O
on O
the O
BookCorpus O
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
and O
Wikipedia O
. O

We O
use O
the O
output O
representation O
of O
each O
token O
from O
BERT B-MethodName
and O
apply O
a O
linear B-MethodName
regression I-MethodName
similar O
to O
( O
1 O
): O
Jt(t O
) O
= O
1 O
n1 O
mnX O
i=11 O
2kHt(xij) yt O
ijk2(2 O
) O
Here O
, O
mis O
the O
number O
of O
examples O
, O
nis O
the O
number O
of O
tokens O
, O
and O
xijandyijstand O
for O
the O
jth O
token O
in O
examplei O
, O
with O
corresponding O
token O
label O
yt O
ij.t O
denotes O
all O
the O
BERT B-MethodName
parameters O
plus O
token O
- O
level O
regression O
parameters O
Wtandbt O
, O
andHt(xij)is O
the O
prediction O
of O
the O
model O
for O
xij O
. O

Our O
problem O
formulation O
might O
be O
taken O
to O
more O
naturally O
suggest O
a O
logistic B-MethodName
regression I-MethodName
. O

However O
, O
we O
opted O
for O
a O
linear B-MethodName
regression I-MethodName
objective O
instead O
, O
in O
the O
hopes O
that O
this O
would O
better O
capture O
not O
just O
the O
probability O
that O
a O
token O
is O
important O
, O
but O
also O
how O
important O
these O
tokens O
are O
. O

The O
linear B-MethodName
regression I-MethodName
performed O
better O
in O
our O
evaluations O
, O
though O
the O
improvements O
over O
the O
logistic O
were O
modest.dev O
test O
BERT B-MethodName
- O
based O
2.224 O
2.223 O
Genre O
Pretrained O
0.884 O
0.887 O
Table O
1 O
: O
Losses O
with O
and O
without O
genre O
pretraining O
. O

5.1 O
Methods O
We O
use O
the O
BERT B-MethodName
- O
base O
uncased O
parameters O
for O
all O
of O
our O
experiments O
. O

Indeed O
, O
we O
fail O
to O
ﬁnd O
evidence O
that O
BERT B-MethodName
with O
the O
CLS O
token O
improves O
performance O
over O
the O
baseline O
( O
p= O
0:440forReader O
perception O
; O
p= O
0:996 O
forAuthor O
belief O
) O
. O

Recent O
gradient O
- O
based O
methods O
for O
assessing O
feature O
importance O
in O
models O
like O
BERT B-MethodName
( O
Sundararajan O
et O
al O
. O
, O
2017 O
; O
Shrikumar O
et O
al O
. O
, O
2017 O
) O
can O
help O
us O
answer O
this O
question O
. O

We O
also O
showed O
that O
SuspectGuilt O
can O
be O
used O
to O
train O
predictive O
models O
on O
top O
of O
BERT B-MethodName
parameters O
, O
and O
that O
these O
models O
are O
improved O
by O
genre O
- O
speciﬁc O
pretraining O
and O
supervision O
derived O
from O
token O
- O
level O
highlighting O
. O

Another O
example O
is O
a O
construction O
with O
a O
locative O
preposition O
and O
a O
deﬁnite O
noun O
phrase O
, O
e.g. O
, O
hid O
a O
parcel O
under O
the O
bed O
, O
whose O
DRS O
contains O
the O
following O
fragment O
: O
b2REF O
e1 O
b2 O
Location O
e1 O
x3 O
b2hide O
" O
v.01 O
" O
e1 O
b2 O
SZP O
x2 O
x3 O
b2REF O
x1 O
b3 O
REF O
x2 O
b2parcel O
" O
n.01 O
" O
x1 O
b3 O
bed O
" O
n.01 O
" O
x2 O
b2Patient O
e1 O
x1 O
where O
the O
binary O
relation O
SZP(spatial O
above O
) O
is O
inDRS O
parsersBB O
A O
< O
aaB(C O
( O
A O
< O
aaBC O
A O
< O
aaBC O
A O
— O
BCI O
DRS O
DRGs O
chLSTM#64.6 B-MethodName
79.6 O
74.3 O
77.7 O
77.9 O
78.2 O
Boxer#78.2 O
89.5 O
86.8 O
87.5 O
87.6 O
87.7 O
chLSTM"84.3 B-MethodName
92.3 O
88.4 O
90.9 O
90.9 O
91.1 O
Boxer"87.2 O
94.2 O
92.3 O
92.9 O
92.9 O
93.0 O
Table O
3 O
: O
Macro O
F O
- O
scores B-MetricName
of O
the O
models O
when O
their O
output O
is O
treated O
as O
DRS O
or O
DRG O
. O

Two O
of O
the O
parsers O
are O
end O
- O
to O
- O
end O
character O
- O
based O
LSTM B-MethodName
models O
from O
van O
Noord O
et O
al O
. O
( O
2018b O
): O
one O
is O
their O
best O
model O
( O
chLSTM B-MethodName
" O
) O
while O
another O
one O
is O
trained O
on O
fewer O
data O
on O
purpose O
to O
have O
mediocre O
performance O
( O
chLSTM B-MethodName
# O
) O
. O

Another O
two O
parsers O
are O
based O
on O
the O
semanticBB O
A O
< O
aBC O
A O
< O
aC(B O
( O
A O
< O
aB(C O
A O
< O
aBC O
A O
— O
BC O
A O
— O
aBC O
A O
< O
aB(C O
A O
< O
aBC O
A O
— O
BC O
A O
— O
aBC O
A O
— O
BCI O
A O
— O
aBCI O
DRS O
parser O
8C O
" O
chLSTM#13 B-MethodName
11 O
Boxer#7 O
9 O
chLSTM"4 B-MethodName
3 O
Boxer"7 O
3 O
BBenc O
Cl O
- O
B!-Anchar_lstm_gold_only O
amateur_boxer O
tacl O
pro_boxer27.2 O
30.4 O
28.0 O
12.5 O
15.0 O
18.1 O
15.0 O
9.3 O
10.3 O
12.8 O
10.3 O
8.1 O
7.0 O
9.2 O
10.3 O
9.7 O
3.6 O
4.8 O
6.7 O
4.8 O
2.0 O
2.2 O
3.5 O
2.2 O
1.0 O
0.1 O
8.3 O
9.8 O
13.1 O
4.3 O
5.2 O
6.6 O
5.2 O
3.0 O
3.2 O
4.4 O
3.2 O
2.2 O
1.6 O
4.4 O
5.1 O
4.8 O
1.3 O
2.7 O
3.3 O
2.7 O
1.0 O
1.0 O
1.8 O
1.0 O
0.9 O
0.3NonExact_matches O
102030 O
BBenc O
Co O
- O
Bo O
- O
An O
C!-B!-An O
Cl O
- O
B!-An O
Cl O
- O
Bo O
- O
An O
Cl O
- O
Bo O
- O
Ab O
Cl O
- O
Bo O
- O
Abn O
Cor O
- O
B!-An O
Cor O
- O
Bo O
- O
An O
Cor O
- O
Bo O
- O
Ab O
Cor O
- O
Bo O
- O
Abn O
Cor O
- O
Bo O
- O
Ab O
- O
Ia1 O
Cor O
- O
Bo O
- O
Abn O
- O
Ia1char_lstm_gold_only O
amateur_boxer O
tacl O
pro_boxer27.2 O
30.4 O
28.0 O
12.5 O
15.0 O
18.1 O
15.0 O
9.3 O
10.3 O
12.8 O
10.3 O
8.1 O
7.0 O
9.2 O
10.3 O
9.7 O
3.6 O
4.8 O
6.7 O
4.8 O
2.0 O
2.2 O
3.5 O
2.2 O
1.0 O
0.1 O
8.3 O
9.8 O
13.1 O
4.3 O
5.2 O
6.6 O
5.2 O
3.0 O
3.2 O
4.4 O
3.2 O
2.2 O
1.6 O
4.4 O
5.1 O
4.8 O
1.3 O
2.7 O
3.3 O
2.7 O
1.0 O
1.0 O
1.8 O
1.0 O
0.9 O
0.3NonExact_matches O
102030 O
Table O
4 O
: O
The O
percentage O
of O
approximate O
( O
i.e. O
, O
non O
- O
exact O
) O
matches O
w.r.t O
. O

Given O
that O
models O
are O
sorted O
according O
to O
their O
performance O
in O
ascending O
order O
from O
top O
to O
bottom O
, O
the O
table O
shows O
that O
for O
relatively O
distinct O
graphs O
it O
can O
be O
di O
cult O
to O
guarantee O
the O
MCES O
solution.12But O
things O
are O
not O
so O
straightforward O
as O
chLSTM"outperforms B-MethodName
Boxer#but O
ﬁnding O
MCES O
for O
Boxer#is O
easier O
for O
10 O
encodings O
out O
of O
13 O
. O

For O
instance O
, O
non O
- O
exact O
( O
i.e. O
, O
approximate O
) O
MCES O
was O
found O
for O
237 O
DRG O
pairs O
out O
of O
872 O
for O
chLSTM#and B-MethodName
BBencoding O
. O

The O
starting O
point O
for O
this O
model O
is O
the O
transitionbased O
parser O
described O
in O
Kiperwasser O
and O
Goldberg O
( O
2016b O
) O
, O
which O
relies O
on O
a O
BiLSTM B-MethodName
to O
learninformative O
features O
of O
words O
in O
context O
and O
a O
feed O
- O
forward O
network O
for O
predicting O
the O
next O
parsing O
transition O
. O

However O
, O
if O
there O
are O
more O
than O
200 O
unique O
multi O
- O
word O
tokens O
contained O
in O
the O
training O
data O
, O
we O
employ O
an O
attention O
- O
based O
encoder O
- O
decoder O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
) O
equipped O
with O
shared O
long O
- O
short O
term O
memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
as O
the O
basic O
recurrent O
cell O
. O

Character O
embedding O
size O
50 O
GRU O
/ O
LSTM B-MethodName
state O
size O
200 O
Optimizer O
Adagrad O
Initial O
learning B-HyperparameterName
rate I-HyperparameterName
( O
main O
) O
0.1 B-HyperparameterValue
Decay O
rate O
0.05 O
Gradient O
Clipping O
5.0 O
Initial O
learning B-HyperparameterName
rate I-HyperparameterName
( O
encoder O
- O
decoder O
) O
0.3 B-HyperparameterValue
Dropout O
rate O
0.5 O
Batch O
size O
10 O
Table O
1 O
: O
Hyper O
- O
parameters O
for O
segmentation O
. O

The O
weights B-HyperparameterName
of O
the O
neural B-MethodName
networks I-MethodName
, O
including O
the O
embeddings O
, O
are O
initialized O
using O
the O
scheme O
introduced O
in O
Glorot O
and O
Bengio O
( O
2010 O
) O
. O

In O
this O
case O
, O
the O
classiﬁer O
is O
a O
multi O
- O
layer O
perceptron O
( O
MLP O
) O
and O
 O
( O
)is O
a O
concatenation O
of O
BiLSTM B-MethodName
vectors O
on O
top O
of O
the O
stack O
and O
the O
beginning O
of O
the O
buffer O
. O

The O
main O
modiﬁcation O
of O
the O
parser O
for O
the O
shared O
task O
concerns O
the O
construction O
of O
the O
BiLSTM B-MethodName
vectors O
, O
where O
we O
remove O
the O
reliance O
on O
part O
- O
of O
- O
speech O
tags O
and O
instead O
add O
characterbased O
representations O
. O

We O
construct O
a O
character O
vector O
che(wi O
) O
for O
eachwiby O
running O
a O
BiLSTM B-MethodName
over O
the O
characterschj(1jm O
) O
ofwi O
: O
che(wi O
) O
= O
BILSTM(ch1 B-MethodName
: O
m O
) O
As O
in O
the O
original O
parser O
, O
we O
also O
concatenate O
these O
vectors O
with O
pretrained O
word O
embeddings O
pe(wi O
) O
. O

Finally O
, O
each O
input O
element O
is O
represented O
by O
a O
BiLSTM B-MethodName
vector O
, O
vi O
: O
vi O
= O
BILSTM(x1 B-MethodName
: O
n;i O
) O
For O
each O
conﬁguration O
c O
, O
the O
feature O
extractor O
concatenates O
the O
BiLSTM B-MethodName
representations O
of O
core O
elements O
from O
the O
stack O
and O
buffer O
. O

Both O
the O
embeddings O
and O
the O
BiLSTMs B-MethodName
are O
trained O
together O
with O
the O
model O
. O

With O
the O
aim O
of O
training O
a O
multilingual O
parser O
, O
we O
additionally O
created O
a O
variant O
of O
the O
parserInternal O
word O
embedding O
dimension O
100 O
Pre O
- O
trained O
word O
embedding O
dimension O
50 O
Character O
embedding O
dimension O
12 O
Character O
BI O
- O
LSTM B-MethodName
Dimensions O
100 O
Hidden O
units O
in O
MLP O
100 O
BI O
- O
LSTM B-MethodName
Layers O
2 O
BI O
- O
LSTM B-MethodName
Dimensions O
( O
hidden O
/ O
output O
) O
200 O
/ O
200 O
 O
( O
for O
word O
dropout B-HyperparameterName
) O
0.25 B-HyperparameterValue
Character O
dropout B-HyperparameterName
0.33 B-HyperparameterValue
pagg(for O
exploration O
training O
) O
0.1 O
Table O
2 O
: O
Hyper O
- O
parameter O
values O
for O
parsing O
. O

Each O
vector O
is O
a O
BiLSTM B-MethodName
encoding O
of O
the O
word O
. O

Character O
vectors O
are O
obtained O
using O
a O
BiLSTM B-MethodName
over O
the O
characters O
of O
the O
word O
. O

The O
ﬁgure O
depicts O
a O
single O
- O
layer O
BiLSTM B-MethodName
, O
while O
in O
practice O
we O
use O
two O
layers O
. O

We O
did O
no O
hyper O
- O
parameter O
tuning O
for O
the O
parser O
component O
but O
instead O
mostly O
used O
the O
values O
that O
had O
been O
found O
to O
work O
well O
in O
Kiperwasser O
and O
Goldberg O
( O
2016b O
) O
, O
except O
for O
the O
BiLSTM B-MethodName
hidden B-HyperparameterName
layer I-HyperparameterName
which O
we O
increased O
as O
we O
had O
increased O
the O
dimensions O
of O
the O
output O
layer O
by O
using O
pseudoprojective O
parsing O
. O

Our O
system O
implements O
a O
new O
joint O
transition O
- O
based O
parser O
, O
based O
on O
the O
Stack O
- O
LSTM B-MethodName
framework O
and O
the O
ArcStandard O
algorithm O
, O
that O
handles O
tokenization O
, O
part O
- O
of O
- O
speech O
tagging O
, O
morphological O
tagging O
and O
dependency O
parsing O
in O
one O
single O
model O
. O

We O
also O
present O
a O
new O
sentence O
segmentation O
neural O
architecture O
based O
on O
StackLSTMs B-MethodName
that O
was O
the O
4th O
best O
overall O
. O

Our O
starting O
point O
is O
the O
Stack O
- O
LSTM3parser B-MethodName
( O
Dyer O
et O
al O
. O
, O
2015 O
; O
Ballesteros O
et O
al O
. O
, O
2017 O
) O
with O
character O
- O
based O
word O
representations O
( O
Ballesteros O
et O
al O
. O
, O
2015 O
) O
, O
which O
we O
extend O
to O
handle O
tokenization O
, O
POS B-TaskName
tagging O
and O
morphological O
tagging O
. O

For O
the O
rest O
of O
languages O
, O
we O
produce O
parses O
from O
raw O
text O
that O
may O
be O
in O
documents O
( O
and O
thus O
we O
need O
to O
find O
the O
sentence O
markers O
within O
those O
documents O
) O
; O
for O
some O
of O
the O
treebanks O
we O
adapted O
Ballesteros O
and O
Wanner O
( O
2016 O
) O
punctuation O
prediction O
system O
( O
which O
is O
also O
based O
in O
the O
StackLSTM B-MethodName
framework O
) O
to O
predict O
sentence O
markers O
. O

Given O
that O
the O
text O
to O
be O
segmented O
into O
sentences O
3We O
use O
the O
dynamic O
neural O
network O
library O
Dynet O
http://dynet.io/ O
- O
( O
Neubig O
et O
al O
. O
, O
2017 O
) O
to O
implement O
our O
parser.can O
be O
of O
a O
significant O
length O
, O
we O
implemented O
a O
sliding O
- O
window O
extension O
of O
the O
punctuation O
prediction O
system O
where O
the O
Stack O
- O
LSTM B-MethodName
is O
reinitialized O
and O
primed O
when O
the O
window O
is O
advanced O
( O
see O
Section O
3 O
for O
details O
) O
. O

2.1 O
Stack O
- O
LSTM B-MethodName
Parser O
Our O
base O
model O
is O
the O
Stack O
- O
LSTM B-MethodName
parser O
( O
Dyer O
et O
al O
. O
, O
2015 O
; O
Ballesteros O
et O
al O
. O
, O
2017 O
) O
with O
character O
- O
based O
word O
representations O
( O
Ballesteros O
et O
al O
. O
, O
2015 O
) O
. O

This O
parser O
implements O
the O
ArcStandard O
with O
SWAP O
parsing O
algorithm O
( O
Nivre O
, O
2004 O
, O
2009 O
) O
and O
it O
uses O
Stack O
- O
LSTMs B-MethodName
to O
model O
three O
data O
structures O
: O
a O
buffer O
B O
initialized O
with O
the O
sequence O
of O
words O
to O
be O
parsed O
, O
a O
stack O
S O
containing O
partially O
built O
parses O
, O
and O
a O
list O
A O
of O
actions O
previously O
taken O
by O
the O
parser O
. O

We O
use O
Ballesteros O
et O
al O
. O
( O
2015 O
) O
version O
of O
the O
parser O
which O
means B-MetricName
that O
we O
compute O
characterbased O
word O
vectors O
using O
bidirectional O
LSTMs B-MethodName
( O
Graves O
and O
Schmidhuber O
, O
2005 O
) O
; O
but O
, O
in O
addition O
, O
we O
also O
add O
pretrained O
word O
embeddings O
for O
all O
languages O
. O

Word O
embeddings O
: O
Parser O
state O
representation O
is O
composed O
by O
three O
Stack O
- O
LSTM B-MethodName
’s O
: O
stack O
, O
buffer O
, O
actions O
, O
as O
in O
( O
Ballesteros O
et O
al O
. O
, O
2017 O
) O
. O

2.3 O
Cross O
- O
Lingual O
Parser O
We O
adapted O
cross O
- O
lingual O
architecture O
of O
Ammar O
et O
al O
. O
( O
2016 O
) O
( O
also O
based O
in O
the O
Stack O
- O
LSTM B-MethodName
parser O
) O
in O
our O
joint O
model O
presented O
in O
Section O
2.2 O
to O
handle O
low O
- O
resource O
and O
zero O
- O
shot O
languages O
. O

This O
architecture O
enables O
effective O
training O
of O
the O
StackLSTM B-MethodName
parser O
on O
multilingual O
training O
data O
. O

This O
method O
is O
essentially O
a O
character O
- O
level O
language O
model O
, O
where O
a O
2 O
- O
layered O
LSTM B-MethodName
predicts O
next O
character O
at O
each O
time O
step O
given O
previous O
character O
inputs O
. O

Table O
1 O
shows O
the O
nearest B-MethodName
neighbors I-MethodName
of O
a O
variety O
of O
languages O
based O
on O
the O
language O
vectors O
from O
each O
model O
. O

2.4 O
Sentence O
- O
based O
Ensemble O
and O
MST O
Ensemble O
2.4.1 O
Graph O
- O
based O
ensemble O
We O
adapt O
Sagae O
and O
Lavie O
( O
2006 O
) O
ensemble O
method O
to O
our O
Stack O
- O
LSTM B-MethodName
only O
models O
( O
see O
Section O
2.1 O
) O
to O
obtain O
the O
final O
parses O
of O
Chinese O
, O
Japanese O
, O
Hebrew O
, O
Hungarian O
, O
Turkish O
and O
Czech O
. O

Kuncoro O
et O
al O
. O
( O
2016 O
) O
already O
tried O
an O
ensemble O
of O
several O
Stack O
- O
LSTM B-MethodName
parser O
models O
achieving O
state O
- O
of O
- O
the O
- O
art O
in O
English O
, O
German O
and O
Chinese O
, O
which O
motivated O
us O
to O
improve O
the O
results O
of O
our O
greedy O
decoding O
method.6 O
5Bilingual O
dictionaries O
used O
for O
multilingual O
mapping O
of O
word O
embeddings O
, O
https://people.uta.fi/ O
km56049 O
/ O
same O
/ O
svocab.html O
https://github.com/apertium/apertium-kmr-eng O
https://github.com/apertium/apertium-fao-nor O
6Kuncoro O
et O
al O
. O
( O
2016 O
) O
developed O
ensemble O
distillation O
into O
a O
single O
model O
which O
we O
did O
not O
attempt O
to O
try O
for O
the O
Shared O
Task O
but O
we O
leave O
for O
future O
developments.2.4.2 O
Model O
Rescoring O
: O
Sentence O
Level O
Ensemble O
For O
all O
of O
the O
languages O
and O
treebank O
combinations O
except O
for O
Chinese O
, O
Japanese O
, O
Hebrew O
, O
Hungarian O
, O
Turkish O
and O
Czech O
, O
we O
apply O
a O
sentence O
- O
level O
ensemble O
technique O
to O
obtain O
the O
final O
parses O
. O

This O
model O
is O
derived O
from O
the O
StackLSTM B-MethodName
parser O
introduced O
in O
Section O
2.1 O
and O
it O
uses O
the O
same O
architecture O
( O
including O
a O
stack O
, O
a O
buffer O
and O
a O
stack O
containing O
the O
transitions O
already O
taken O
) O
but O
it O
is O
restricted O
to O
two O
distinct O
transitions O
, O
either O
SHIFT O
or O
BREAK O
( O
which O
adds O
a O
sentence O
marker O
between O
two O
tokens O
) O
. O

Each O
window O
is O
treated O
as O
a O
training O
unit O
, O
where O
the O
loss O
is O
computed O
, O
the O
optimizer B-HyperparameterName
is O
invoked O
and O
the O
stack O
LSTM B-MethodName
state O
is O
reset O
. O

We O
use O
two O
different O
approaches O
to O
provide O
left O
and O
right O
context O
to O
the O
stack O
LSTM B-MethodName
. O

To O
provide O
left O
context O
, O
we O
snapshot O
the O
stack O
and O
action O
buffer O
after O
the O
last O
prediction O
in O
the O
window O
, O
we O
slide O
the O
window O
to O
the O
right O
by O
W Owords O
, O
we O
reset O
the O
LSTM B-MethodName
state O
, O
and O
we O
prime O
the O
input O
buffer O
with O
the O
Lwords O
to O
the O
left O
of O
the O
new O
window O
, O
the O
action O
buffer O
with O
the O
most O
recent O
L O
actions O
, O
and O
the O
stack O
with O
the O
Ltopmost O
entries O
from O
the O
snapshot O
. O

4 O
Models O
4.1 O
Stack O
- O
LSTM B-MethodName
For O
6 O
treebanks O
( O
cs_pdt O
, O
he_htb O
, O
ja_gsd O
, O
hu_szeged O
, O
tr_imst O
, O
zh_gsd O
) O
, O
we O
trained O
20 O
baseline O
Stack O
- O
LSTM B-MethodName
models O
for O
parsing O
( O
utilizing O
UDPipe O
pre O
- O
processing O
for O
sentence O
segmentation O
, O
tokenization O
and O
UPOS B-TaskName
tagging O
) O
per O
treebank O
. O

Independent O
LSTM B-MethodName
models O
are O
trained O
on O
each O
treebank O
for O
labeling O
. O

We O
utilized O
diverse O
set O
of O
word O
embeddings O
for O
Stack O
- O
LSTM B-MethodName
and O
graph O
- O
based O
models O
: O
cs_pdt O
, O
he_htb O
and O
tr_imst O
( O
CoNLL2017 O
embedding O
with O
dimension O
100 O
) O
, O
ja_gsd O
( O
in O
- O
house O
cross O
- O
lingual O
embeddings O
with O
dimension O
300 O
) O
, O
hu_szeged O
and O
zh_gsd O
( O
Facebook O
embeddings O
with O
dimension O
300 O
) O
. O

In O
addition O
, O
we O
also O
used O
the O
same O
Stack O
- O
LSTM B-MethodName
framework O
for O
sentence O
segmentation O
achieving O
good O
results O
. O

But O
neural B-MethodName
networks I-MethodName
require O
a O
lot O
of O
training O
data O
to O
work O
. O

Characters O
of O
the O
lemma O
cialong O
with O
the O
additional O
start O
and O
stop O
characters O
are O
fed O
one O
by O
one O
into O
a O
bidirectional O
LSTM B-MethodName
encoder O
producing O
a O
sequence O
of O
hidden O
states O
hli O
. O

Similarly O
, O
using O
a O
separate O
bidirectional O
LSTM B-MethodName
encoder O
, O
the O
tags O
tgi O
are O
encoded O
and O
another O
sequence O
of O
hidden O
states O
htgiis O
obtained O
. O

We O
use O
a O
unidirectional O
LSTM B-MethodName
as O
the O
decoder O
. O

Single O
layer O
LSTMs B-MethodName
were O
used O
as O
encoders O
and O
decoders O
to O
reduce O
number O
of O
parameters O
. O

Optimal O
size O
of O
embeddings O
and O
the O
number O
of O
hidden B-HyperparameterName
units I-HyperparameterName
in O
LSTMs B-MethodName
were O
determined O
based O
on O
the O
performance O
of O
the O
model O
on O
a O
subset O
of O
languages O
in O
development O
set O
. O

The O
values O
for O
hyperparameters O
p O
, O
e1,e2 O
, O
embedding O
size O
and O
hidden B-HyperparameterName
units I-HyperparameterName
of O
LSTM B-MethodName
are O
given O
in O
Table O
1 B-HyperparameterValue
. O

Asthe O
number O
of O
tags O
vary O
for O
each O
example O
, O
using O
LSTM B-MethodName
to O
encode O
them O
seemed O
apt O
. O

Even O
though O
our O
approach O
was O
completely O
based O
on O
neural B-MethodName
networks I-MethodName
, O
our O
system O
works O
very O
well O
for O
low O
resource O
setting O
. O

Applying O
the O
Sinkhorn O
operator O
, O
we O
obtain O
SA= O
Sinkhorn O
( O
~SA O
) O
, O
which O
, O
in O
our O
setup O
, O
will O
be O
modeled O
as O
a O
continuous O
approximation O
of O
the O
underlying O
permutation O
matrix O
A.3.3 O
Implementation O
Encoder O
- O
Decoder O
We O
ﬁrst O
encode O
sentences O
using O
BERTje B-MethodName
( O
de O
Vries O
et O
al O
. O
, O
2019 O
) O
, O
a O
pretrained O
BERT B-MethodName
- O
Base O
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
localized O
for O
Dutch O
. O

This O
effectively O
converts O
it O
into O
a O
bi O
- O
modal O
encoder O
which O
operates O
on O
two O
input O
sequences O
of O
different O
length O
and O
dimensionality O
, O
namely O
the O
BERT B-MethodName
output O
and O
the O
sequence O
of O
proof O
frame O
symbols O
, O
and O
constructs O
contextualized O
representations O
of O
the O
latter O
as O
informed O
by O
the O
former O
. O

Implementation O
details O
and O
hyper O
- O
parameter O
tables O
, O
an O
illustration O
of O
the O
full O
architecture O
, O
dataset O
statistics O
and O
example O
parses O
are O
provided O
in O
Appendix O
A.5 O
4.1 O
Training O
We O
train O
our O
architecture O
end O
- O
to O
- O
end O
, O
including O
all O
BERT B-MethodName
parameters O
apart O
from O
the O
embedding O
layer O
, O
using O
AdamW O
( O
Loshchilov O
and O
Hutter O
, O
2018 O
) O
. O

Most O
similar O
to O
our O
work O
, O
( O
Zheng O
et O
al O
. O
, O
2019 O
) O
propose O
the O
use O
of O
a O
guiding O
object O
and O
produces O
a O
caption O
by O
using O
a O
forward O
and O
backward O
LSTM B-MethodName
to O
separately O
generate O
the O
caption O
text O
before O
and O
after O
the O
guiding O
object O
. O

The O
ﬁrst O
uses O
a O
simple O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
for O
lowsized O
dataset O
, O
while O
it O
uses O
an O
LSTMbased B-MethodName
encoder O
- O
decoder O
based O
model O
for O
the O
medium O
and O
high O
sized O
datasets O
. O

The O
second O
uses O
a O
simple O
Gated O
Recurrent O
Unit O
( O
GRU O
) O
for O
low O
- O
sized O
data O
, O
while O
it O
uses O
a O
combination O
of O
simple O
LSTMs B-MethodName
, O
simple O
GRUs O
, O
stacked O
GRUs O
and O
encoderdecoder O
models O
, O
depending O
on O
the O
language O
, O
for O
medium O
- O
sized O
data O
. O

Akin O
to O
machine O
translation O
systems O
, O
this O
system O
uses O
an O
encoder O
- O
decoder O
LSTM B-MethodName
model O
as O
proposed O
by O
Hochreiter O
and O
Schmidhuber O
( O
1997 O
) O
. O

The O
encoder O
is O
a O
bidirectional O
LSTM B-MethodName
, O
while O
the O
decoder O
LSTM B-MethodName
feeds O
into O
a O
softmax O
layer O
for O
every O
character O
position O
in O
the O
target O
string O
. O

3.1 O
First O
Submission O
3.1.1 O
Low O
- O
sized O
Dataset O
For O
training O
the O
model O
on O
the O
low O
- O
sized O
dataset O
, O
we O
did O
not O
use O
any O
encoder O
and O
we O
used O
a O
simple O
LSTM B-MethodName
with O
a O
single O
layer O
as O
the O
recurrent O
unit O
( O
Figure O
1 O
) O
. O

3.1.2 O
Medium O
- O
sized O
Dataset O
For O
training O
the O
model O
on O
the O
medium O
- O
sized O
dataset O
, O
we O
used O
a O
bidirectional O
LSTM B-MethodName
as O
the O
encoder O
and O
a O
simple O
LSTM B-MethodName
with O
a O
single O
layer O
as O
the O
decoder O
( O
Figure O
2 O
) O
. O

3.1.3 O
High O
- O
sized O
Dataset O
For O
training O
the O
model O
on O
the O
high O
- O
sized O
dataset O
, O
we O
used O
a O
bidirectional O
LSTM B-MethodName
as O
the O
encoder O
and O
a O
simple O
LSTM B-MethodName
with O
a O
single O
layer O
as O
the O
decoder O
( O
Figure O
2 O
) O
. O

Four O
different O
kinds O
of O
conﬁgurations O
were O
used O
: O
1 O
) O
Bidirectional O
LSTM B-MethodName
as O
the O
encoder O
and O
a O
simple O
LSTM B-MethodName
with O
a O
single O
layer O
as O
the O
decoder O
( O
Figure O
2 O
) O
2 O
) O
Bidirectional O
GRU O
as O
the O
encoder O
and O
a O
simple O
GRU O
with O
a O
single O
layer O
as O
the O
decoder O
( O
Figure O
4 O
) O
3 O
) O
No O
encoder O
and O
a O
simple O
GRU O
with O
a O
single O
layer O
as O
the O
recurrent O
unit O
( O
Figure O
3 O
) O
4 O
) O
Bidirectional O
GRU O
as O
the O
encoder O
and O
a O
deep O
GRU O
( O
two O
GRUs O
stacked O
one O
above O
the O
other O
) O
as O
the O
decoder O
( O
Figure O
5 O
) O
The O
speciﬁc O
conﬁguration O
used O
for O
each O
language O
has O
been O
listed O
in O
Table O
1 O
. O

4.2.1 O
Early O
Stop O
Patience O
We O
observed O
that O
for O
low O
- O
sized O
datasets O
, O
both O
the O
models O
( O
LSTM B-MethodName
as O
well O
as O
GRU O
based O
) O
required O
that O
at O
least O
10 O
epochs B-HyperparameterName
be O
run O
before O
early O
stop O
, O
every O
time O
no O
progress O
is O
detected O
on O
the O
validation O
set O
. O

For O
low O
- O
sized O
datasets O
, O
in O
almost O
all O
cases O
, O
using O
a O
GRU O
gave O
better O
results O
than O
using O
an O
LSTM B-MethodName
. O

On O
an O
average B-MetricName
, O
the O
accuracy B-MetricName
increased O
by O
2.33 B-MetricValue
% O
when O
shifting O
from O
LSTM B-MethodName
to O
GRU O
as O
the O
choice O
of O
recurrent O
unit O
. O

In O
the O
case O
of O
medium O
- O
sized O
datasets O
, O
8 O
out O
of O
52 O
languages O
performed O
better O
with O
an O
LSTM B-MethodName
than O
a O
GRU O
, O
while O
the O
rest O
showed O
better O
performance O
with O
a O
GRU O
. O

4.2.5 O
Stacking O
Recurrent O
Units O
Deeper O
models O
( O
more O
than O
one O
layer O
of O
LSTM B-MethodName
/ O
GRU O
) O
resulted O
in O
drastic O
accuracy B-MetricName
drops O
for O
low O
- O
sized O
datasets O
. O

One O
is O
that O
different O
conﬁgurations O
of O
deep O
neural B-MethodName
networks I-MethodName
work O
well O
for O
different O
languages O
. O

Specifically O
, O
they O
evaluated O
the O
ability O
of O
an O
ELMo B-MethodName
language O
model O
to O
predict O
brain O
responses O
of O
multiple O
1https://github.com/DS3Lab/cognivalfMRI O
datasets O
. O

hidden B-HyperparameterName
layer I-HyperparameterName
units O
Glove O
50 B-HyperparameterValue
[ O
30 O
, O
26 O
, O
20 O
, O
5 O
] O
Glove O
100 O
[ O
50 O
, O
30 O
] O
Glove O
200 O
[ O
100 O
, O
50 O
] O
Glove O
300 O
[ O
150 O
, O
50 O
] O
Word2vec O
300 O
[ O
150 O
, O
50 O
] O
WordNet2vec B-DatasetName
850 O
[ O
400 O
, O
200 O
] O
FastText O
300 O
[ O
150 O
, O
50 O
] O
ELMo B-MethodName
1024 O
[ O
600 O
, O
200 O
] O
BERT B-MethodName
768 O
[ O
400 O
, O
200 O
] O
BERT B-MethodName
1024 O
[ O
600 O
, O
200 O
] O
Table O
1 O
: O
Overview O
of O
word O
embeddings O
evaluated O
with O
CogniVal O
. O

•ELMo B-MethodName
models O
both O
complex O
characteristics O
of O
word O
use O
( O
i.e. O
syntax O
and O
semantics O
) O
, O
and O
how O
these O
uses O
vary O
across O
linguistic O
contexts O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

•BERT B-MethodName
embeddings O
are O
contextual O
, O
bidirectional O
word O
representations O
, O
based O
on O
the O
idea O
that O
ﬁne O
- O
tuning O
a O
pre O
- O
trained O
language O
model O
can O
help O
the O
model O
achieve O
better O
results O
in O
the O
downstream B-TaskName
tasks I-TaskName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

For O
the O
regression O
models O
, O
we O
train O
neural B-MethodName
networks I-MethodName
with O
kinput O
dimensions O
, O
one O
dense O
hidden B-HyperparameterName
layer I-HyperparameterName
of O
nnodes O
using O
ReLU O
activation B-HyperparameterName
and O
an O
output O
layer O
of O
mnodes O
using O
linear O
activation B-HyperparameterName
. O

BERT B-MethodName
, O
ELMo B-MethodName
and O
FastText O
embeddings O
achieve O
the O
bestprediction O
results O
. O

This O
is O
exempliﬁed O
byFigure5 O
, O
which O
shows O
the O
20 O
best O
and O
worst O
pre O
- O
dicted O
electrodes O
of O
the O
ZuCo O
data O
for O
the O
BERTembeddings B-MethodName
of O
1024 O
dimensions O
as O
well O
as O
aggre O
- O
gated O
over O
all O
cognitive O
data O
sources O
. O

( O
2019 O
) O
for O
BERT B-MethodName
, O
from O
Mikolov O
et O
al O
. O
( O
2018 O
) O
for O
FastText O
, O
and O
from O
Peters O
et O
al O
. O
( O
2018 O
) O
for O
ELMo B-MethodName
. O

The O
NER B-TaskName
results O
are O
from O
the O
same O
source O
for O
ELMo B-MethodName
and O
BERT B-MethodName
, O
for O
Glove-50 O
from O
Pennington O
et O
al O
. O
( O
2014 O
) O
and O
for O
Glove-200 O
from O
Ghannay O
et O
al O
. O
( O
2016 O
) O
. O

HIT O
- O
SCIR O
at O
MRP O
2019:.R O
at O
MRP O
2019 O
: O
A O
Uniﬁed O
Pipeline O
for O
Meaning O
Representation O
Parsing O
via O
Efﬁcient O
Training O
and O
Effective O
Encoding O
Wanxiang O
Che O
, O
Longxu O
Dou O
, O
Yang O
Xu O
, O
Yuxuan O
Wang O
, O
Yijia O
Liu O
, O
Ting O
Liu O
Research O
Center O
for O
Social O
Computing O
and O
Information O
Retrieval O
Harbin O
Institute O
of O
Technology O
, O
China O
fcar O
, O
lxdou O
, O
yxu O
, O
yxwang O
, O
yjliu O
, O
tliu O
g@ir.hit.edu.cn O
A O
Training O
Details O
A.1 O
Fine O
- O
tuning O
BERT B-MethodName
with O
Parser O
We O
ﬁnd O
it O
beneﬁcial O
to O
warm O
up O
learning B-HyperparameterName
rate I-HyperparameterName
at O
beginning O
of O
training O
progress O
and O
cool O
down O
after O
. O

Gradual O
unfreezing O
is O
also O
used O
during O
training O
so O
in O
the O
ﬁrst O
few(1 O
5 O
) O
epochs B-HyperparameterName
BERT B-MethodName
parameters O
are O
frozen O
. O

A.2 O
Hyperparameters O
AMR O
model O
adopts O
the O
BERT B-MethodName
- O
large O
cased O
( O
whole O
word O
masking O
) O
pre O
- O
trained O
weights B-HyperparameterName
while O
other O
models O
adopt O
the O
BERT B-MethodName
- O
base O
cased O
pre O
- O
trained O
weights B-HyperparameterName
. O

In O
preliminary O
experiment O
on O
split O
dataset O
, O
we O
did O
not O
get O
the O
obvious O
improvement O
using O
BERT B-MethodName
- O
large O
in O
DM O
, O
PSD O
, O
AMR O
and O
UCCA O
, O
thus O
we O
use O
BERT B-MethodName
- O
base O
simply O
. O

b O
) O
Concept O
Node O
We O
limit O
the O
number O
of O
generatedHYPERPARAMETER O
VALUE O
Hidden O
dimension O
200 O
Action O
dimension O
50 O
Optimizer O
Adam O
 O
1 O
; O
 O
2 O
0.9 O
, O
0.99 O
Dropout O
0.5 O
Layer O
dropout B-HyperparameterName
0.2 B-HyperparameterValue
Recurrent O
dropout B-HyperparameterName
0.2 O
Input O
dropout B-HyperparameterName
0.2 O
Batch O
size O
16 O
Epochs O
50 O
Base O
learning B-HyperparameterName
rate I-HyperparameterName
110 3 O
BERT B-MethodName
learning B-HyperparameterName
rate I-HyperparameterName
510 5 O
Gradient O
clipping O
5.0 B-HyperparameterValue
Gradient O
norm O
5.0 O
Learning O
rate O
scheduler O
slanted O
triangular O
Gradual O
Unfreezing O
True O
Cut O
Frac O
0.1 O
Ratio O
32 O
Table O
1 O
: O
A O
summary O
of O
model O
hyperparameters O
. O

We O
ﬁnd O
that O
humans O
’ O
choice O
of O
telicity O
interpretation O
is O
reliably O
inﬂuenced O
by O
theoretically O
- O
motivated O
cues O
, O
transformer O
models O
( O
BERT B-MethodName
and O
RoBERTa B-MethodName
) O
are O
inﬂuenced O
by O
some O
( O
though O
not O
all O
) O
of O
the O
cues O
, O
and O
transformer O
models O
often O
rely O
more O
heavily O
on O
temporal O
units O
than O
humans O
do O
. O

1 O
Introduction O
Large O
pretrained O
- O
language O
models O
( O
ELMo B-MethodName
: O
Peters O
et O
al O
. O
, O
2018a O
, O
BERT B-MethodName
: O
Devlin O
et O
al O
. O
, O
2019 O
, O
RoBERTa B-MethodName
Liu O
et O
al O
. O
, O
2019 O
, O
etc O
. O
) O
keep O
achieving O
new O
states O
of O
the O
art O
in O
a O
variety O
of O
NLP O
tasks O
, O
leading O
to O
a O
growing O
interest O
in O
exploring O
what O
has O
been O
acquired O
by O
the O
pretraining O
objectives O
. O

We O
tested O
our O
surveys O
on O
BERT B-MethodName
uncased O
( O
base O
and O
large O
) O
and O
RoBERTa B-MethodName
( O
base O
and O
large O
) O
models O
. O

Secondly O
, O
we O
used O
a O
soft O
ranking O
measure O
: O
we O
ask O
whether O
humans O
or O
transformers O
showed O
a O
theoretically O
- O
motivated O
tendency O
( O
i.e. O
, O
signiﬁcantly O
greater O
preference O
for O
“ O
for O
” O
in O
items O
expected O
to O
5https://www.prolific.co/ O
6For O
the O
English O
set O
, O
where O
each O
transformer O
produces O
only O
a O
few O
scores B-MetricName
per O
category O
, O
we O
plot O
the O
model O
’s O
mean B-MetricName
prediction O
as O
a O
letter O
, O
overlaid O
on O
the O
human O
violin O
plot.have O
an O
atelic O
preference O
than O
in O
items O
expected O
to O
have O
a O
telic O
preference O
) O
by O
ﬁtting O
Bayesian O
mixed O
effect O
logistic B-MethodName
regression I-MethodName
models7to O
predict O
telicity O
preference O
from O
linguistic O
cues O
and O
temporal O
unit O
features O
. O

8https://github.com/burchill/zplyr/ O
blob O
/ O
master O
/ O
R O
/ O
sliding_contrast O
. O
Rsecond O
vs O
hour O
hour O
vs O
week O
week O
vs O
year O
Subject O
Coef O
P O
- O
value O
Coef O
P O
- O
value O
Coef O
P O
- O
value O
Human O
0.15 O
1:4510 90.03 O
0.237 O
0.11 O
2:5310 5 O
BERT B-MethodName
- O
base O
0.31 O
< O
2:010 160.34 O
< O
2:010 160.09 O
5:5310 4 O
BERT B-MethodName
- O
large O
0.25 O
< O
2:010 160.06 O
3:0610 30.18 O
< O
2:010 16 O
RoBERTa B-MethodName
- O
base O
0.39 O
< O
2:010 160.11 O
1:6510 80.10 O
2:3110 7 O
RoBERTa B-MethodName
- O
large O
0.28 O
< O
2:010 160.13 O
4:5810 90.06 O
0.0138 O
Table O
2 O
: O
Inﬂuence O
of O
temporal O
units O
on O
telicity O
preference O
among O
human O
subjects O
and O
transformers O
: O
results O
of O
the O
soft O
ranking O
measure O
described O
in O
section O
3.3 O
. O

Soft O
ranking O
measure O
Ablation O
NP O
- O
non O
vs O
NP O
- O
a O
NP O
- O
a O
vs O
NP O
- O
num O
 NP O
quantity O
 time O
unit O
Subject O
Coef O
P O
- O
value O
Coef O
P O
- O
value O
Resid O
P O
- O
value O
Resid O
P O
- O
value O
Human O
-0.45 O
7:8010 9 O
- O
0.44 O
2:0010 7119.78 O
< O
2:210 1671.741 O
1:8010 15 O
BERT B-MethodName
- O
base O
-0.87 O
< O
2:010 16 O
- O
2.37 O
< O
2:010 162226.7 O
< O
2:210 161756.4 O
< O
2:210 16 O
BERT B-MethodName
- O
large O
-0.56 O
< O
2:010 16 O
- O
4.78 O
6:2310 133168.2 O
< O
2:210 16682.22 O
< O
2:210 16 O
RoBERTa B-MethodName
- O
base O
0.13 O
5:0810 4 O
- O
2.03 O
< O
2:010 161938.1 O
< O
2:210 16774.18 O
< O
2:210 16 O
RoBERTa B-MethodName
- O
large O
-0.65 O
< O
2:010 16 O
- O
1.83 O
< O
2:010 162328.3 O
< O
2:210 16667.72 O
< O
2:210 16 O
Table O
3 O
: O
Inﬂuence O
of O
noun O
phrase O
quantity O
on O
telicity O
preference O
among O
human O
subjects O
and O
transformers O
in O
the O
novel O
word O
set O
: O
results O
of O
the O
soft O
ranking O
measure O
and O
ablation O
described O
in O
section O
3.3 O
. O

Transformers O
partially O
follow O
the O
predictions O
in O
the O
novel O
word O
set O
, O
consistently O
passing O
our O
strict O
decision O
measure O
for O
cardinal O
number O
quantized O
NPs O
( O
p O
< O
2:210 16 O
for O
all O
four O
models O
) O
and O
non O
- O
quantized O
NPs O
( O
BERTbase B-MethodName
: O
p O
< O
2:210 16 O
; O
BERT B-MethodName
- O
large O
: O
p O
< O
2:210 16 O
; O
RoBERTa B-MethodName
- O
base O
: O
p= O
4:8910 10 O
; O
RoBERTa B-MethodName
- O
large O
: O
p<2:210 16 O
) O
, O
but O
preferring O
an O
atelic O
interpretation O
with O
‘ O
a O
’ O
quantized O
NPs O
, O
where O
preference O
for O
‘ O
for O
’ O
was O
higher O
than O
50 O
% O
( O
BERT B-MethodName
- O
base O
: O
p=9:8110 14 O
; O
BERT B-MethodName
- O
large O
: O
p O
< O
2:210 16 O
; O
RoBERTa B-MethodName
- O
base O
: O
p O
< O
2:210 16 O
; O
RoBERTa B-MethodName
- O
large O
: O
p=1:8310 11 O
) O
. O

Soft O
ranking O
measure O
Ablation O
base O
vs O
resultative O
 base O
/ O
resultative O
 time O
unit O
Subject O
Coef O
P O
- O
value O
Resid O
P O
- O
value O
Resid O
P O
- O
value O
Human O
-1.69 O
< O
2:010 16235.17 O
< O
2:210 1663.35 O
1:3010 13 O
BERT B-MethodName
- O
base O
0.46 O
0.0094 O
6.17 O
0.01299 O
87.96 O
< O
2:210 16 O
BERT B-MethodName
- O
large O
-1.45 O
1:1610 550.6 O
1:1310 1297.51 O
< O
2:210 16 O
RoBERTa B-MethodName
- O
base O
-1.51 O
< O
210 16195.63 O
< O
2:210 16250.60 O
< O
2:210 16 O
RoBERTa B-MethodName
- O
large O
-1.49 O
0.0361 O
13.62 O
0.0002235 O
60.00 O
5:8710 13 O
Table O
4 O
: O
Inﬂuence O
of O
resultative O
construction O
on O
telicity O
preference O
among O
human O
subjects O
and O
other O
transformers O
in O
the O
novel O
word O
set O
: O
results O
of O
the O
soft O
ranking O
measure O
and O
ablation O
described O
in O
section O
3.3 O
. O
strict O
decision O
measure O
: O
preference O
for O
‘ O
for O
’ O
with O
resultatives O
was O
lower O
than O
50 O
% O
( O
p= O
2:9810 9 O
) O
. O

RoBERTa B-MethodName
- O
base O
is O
similar O
to O
humans O
( O
p=0.1997 O
) O
. O

All O
other O
transformers O
strongly O
prefer O
‘ O
for O
’ O
, O
the O
opposite O
of O
the O
theoretical O
prediction O
( O
BERT B-MethodName
- O
base O
: O
p= O
8:3310 12 O
; O
BERT B-MethodName
- O
large O
: O
p=1:7210 11 O
, O
RoBERTa B-MethodName
- O
large O
: O
p= O
9:8210 12 O
) O
. O

Humans O
and O
transformers O
except O
BERT B-MethodName
- O
base O
do O
loosely O
follow O
the O
theoretical O
predictions O
under O
our O
soft O
ranking O
measure O
: O
Table O
4 O
shows O
that O
they O
have O
a O
lower O
preference O
for O
‘ O
for O
’ O
in O
a O
resultative O
context O
than O
in O
the O
base O
context O
( O
the O
coefﬁcients O
of O
the O
mixed O
effects O
models O
are O
negative O
) O
. O

Soft O
ranking O
measure O
Ablation O
atelic O
context O
vs O
telic O
context O
 context O
 time O
unit O
Subject O
Coef O
P O
- O
value O
Resid O
P O
- O
value O
Resid O
P O
- O
value O
Human O
-2.87 O
< O
2:010 16155.46 O
< O
2:210 167.74 O
0.05168 O
BERT B-MethodName
- O
base O
2.23 O
0.0169 O
9.85 O
0.001694 O
286.99 O
< O
2:210 16 O
BERT B-MethodName
- O
large O
-2.63 O
9:4110 735.72 O
< O
2:2810 9249.19 O
< O
2:210 16 O
RoBERTa B-MethodName
- O
base O
-2.98 O
1:1510 853.75 O
< O
2:2710 13213.5 O
< O
2:210 16 O
RoBERTa B-MethodName
- O
large O
-4.04 O
7:0410 1086.19 O
< O
2:210 16210.55 O
< O
2:210 16 O
Table O
5 O
: O
Inﬂuence O
of O
context O
information O
on O
telicity O
preference O
among O
human O
subjects O
and O
other O
transformers O
: O
results O
of O
the O
soft O
ranking O
and O
ablation O
measures O
described O
in O
section O
3.3 O
. O

RoBERTa B-MethodName
- O
large O
: O
p= O
1:9310 9 O
; O
BERT B-MethodName
- O
large O
: O
p=0.241 O
; O
RoBERTa B-MethodName
- O
base O
: O
p=0.203 O
) O
. O

Transformers O
except O
BERT B-MethodName
- O
base O
do O
loosely O
follow O
the O
theoretical O
predictions O
under O
our O
soft O
ranking O
measure O
: O
Table O
5 O
shows O
that O
they O
have O
a O
higher O
preference O
for O
‘ O
for O
’ O
in O
atelic O
contexts O
than O
in O
telic O
contexts O
. O

Due O
to O
the O
lack O
of O
speaker O
information O
in O
the O
OpenSubtitles O
corpus O
, O
before O
extracting O
the O
dialogs O
, O
we O
followed O
the O
same O
procedure O
proposed O
by O
Lison O
and O
Meena O
( O
2016 O
) O
and O
built O
an O
SVM B-MethodName
classiﬁer O
to O
determine O
whether O
two O
consecutive O
lines O
in O
one O
subtitle O
ﬁle O
are O
actually O
spoken O
by O
the O
same O
character O
and O
should O
be O
in O
the O
same O
dialog O
turn O
. O

To O
build O
the O
emotion O
classiﬁer O
, O
we O
followed O
Welivita O
and O
Pu O
( O
2020 O
) O
and O
ﬁne O
- O
tuned O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
on O
the O
situation O
sentences O
from O
the O
EmpatheticDialogues O
( O
Rashkin O
et O
al O
. O
, O
2019 O
) O
training O
set O
( O
labeled O
with O
32 O
ﬁne O
- O
grained O
emotions O
) O
, O
and O
7 O
K O
listener O
utterances O
labeled O
with O
8 O
empathetic O
intents O
( O
questioning O
, O
agreeing O
, O
acknowledging O
, O
sympathizing O
, O
encouraging O
, O
consoling O
, O
suggesting O
, O
and O
wishing O
) O
plus O
one O
neutral O
category O
( O
all O
other O
not O
mentioned O
intents O
) O
. O

We O
use O
the O
RoBERTa B-MethodName
tokenizer O
to O
tokenize O
the O
utterances O
u1 O
; O
u2 O
; O
: O
: O
: O
; O
u O
min O
the O
input O
dialog O
contextX O
, O
and O
concatenate O
them O
by O
two O
special O
tokens:<s O
> O
and</s O
> O
, O
as O
shown O
in O
the O
ﬁgure O
. O

For O
this O
metric O
, O
we O
use O
Sentence O
- O
BERT B-MethodName
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
to O
obtain O
an O
embedding O
for O
the O
generated O
response O
as O
well O
as O
the O
groundtruth O
, O
and O
then O
calculate O
the O
cosine O
similarity O
between O
the O
two O
embeddings O
. O

While O
neural B-MethodName
networks I-MethodName
with O
attention O
mechanisms O
have O
achieved O
superior O
performance O
on O
many O
natural O
language O
processing O
tasks O
, O
it O
remains O
unclear O
to O
which O
extent O
learned O
attention O
resembles O
human O
visual O
attention O
. O

We O
compare O
state O
of O
the O
art O
networks O
based O
on O
long O
shortterm O
memory O
( O
LSTM B-MethodName
) O
, O
convolutional O
neural O
models O
( O
CNN O
) O
and O
XLNet B-MethodName
Transformer O
architectures O
. O

We O
ﬁnd O
that O
higher O
similarity O
to O
human O
attention O
and O
performance O
signiﬁcantly O
correlates O
to O
the O
LSTM B-MethodName
and O
CNN O
models O
. O

However O
, O
we O
show O
this O
relationship O
does O
not O
hold O
true O
for O
the O
XLNet B-MethodName
models O
– O
despite O
the O
fact O
that O
the O
XLNet B-MethodName
performs O
best O
on O
this O
challenging O
task O
. O

Attention O
mechanisms O
in O
neural B-MethodName
networks I-MethodName
have O
been O
inspired O
by O
human O
visual O
attention O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
; O
Hassabis O
et O
al O
. O
, O
2017 O
) O
. O

As O
neural B-MethodName
attention I-MethodName
allows O
us O
to O
“ O
peek O
” O
inside O
neural B-MethodName
networks I-MethodName
, O
it O
can O
help O
us O
to O
better O
understand O
how O
models O
make O
predictions O
( O
see O
Figure O
1 O
) O
. O

Third O
, O
we O
interpret O
the O
relationship O
between O
human O
attention O
and O
three O
state O
of O
the O
art O
systems O
based O
on O
CNN O
, O
LSTM B-MethodName
, O
and O
XLNet B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Yang O
et O
al O
. O
, O
2019 O
) O
using O
Kullback O
- O
Leibler O
divergence O
( O
Kullback O
and O
Leibler O
, O
1951 O
) O
. O

The O
main O
ﬁndings O
of O
our O
work O
are O
two O
- O
fold O
: O
First O
, O
we O
show O
that O
there O
is O
a O
statistically O
significant O
correlation B-MetricName
between O
the O
CNNs O
and O
LSTMs B-MethodName
model O
performances O
and O
similarity O
to O
human O
attention O
. O

Second O
, O
we O
show O
that O
the O
behavior O
of O
LSTM B-MethodName
models O
is O
signiﬁcantly O
more O
similar O
to O
humans O
than O
the O
XLNet B-MethodName
ones O
even O
though O
the O
latter O
perform O
best O
on O
the O
MovieQA O
dataset O
. O

In O
Transformer O
networks O
, O
the O
main O
differences O
to O
previous O
attentive O
models O
are O
that O
these O
networks O
are O
purely O
based O
on O
attention O
where O
LSTM B-MethodName
or O
GRU O
units O
are O
not O
used O
, O
and O
attention O
is O
applied O
via O
selfattention O
and O
multi O
- O
headed O
attention O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
without O
any O
order O
constraint O
. O

2.4 O
Neural O
Interpretability O
In O
order O
to O
further O
understand O
the O
behavior O
of O
neural B-MethodName
networks I-MethodName
, O
research O
in O
neural O
interpretability O
has O
grown O
dramatically O
in O
the O
recent O
years O
( O
Lipton O
, O
2018 O
; O
Gilpin O
et O
al O
. O
, O
2018 O
; O
Hooker O
et O
al O
. O
, O
2019 O
) O
. O

Data O
collection O
Our O
dataset O
is O
based O
on O
two O
studies O
: O
in O
Study O
1 O
we O
randomly O
selected O
a O
set O
of O
16 O
documents O
on O
which O
the O
majority O
of O
both O
LSTMs B-MethodName
and O
CNNs O
models O
failed O
to O
correctly O
answer O
the O
1The O
dataset O
is O
available O
at O
https://perceptualui O
. O

Study O
2 O
We O
conducted O
a O
follow O
up O
study O
in O
which O
we O
took O
only O
the O
plots O
for O
which O
the O
majority O
of O
CNN O
and O
LSTM B-MethodName
models O
predicted O
correctly O
. O

4 O
Neural O
Models O
4.1 O
Two O
Staged O
Attention O
Models O
We O
re O
- O
implement O
both O
the O
CNN O
and O
LSTM B-MethodName
QA O
ensemble O
models O
with O
two O
staged O
attention O
from O
Blohm O
et O
al O
. O
( O
2018 O
) O
that O
provides O
state O
of O
the O
art O
results O
on O
the O
MovieQA O
dataset O
( O
Tapaswi O
et O
al O
. O
, O
2016 O
) O
. O

The O
two O
variations O
of O
this O
model O
with O
CNN O
and O
LSTM B-MethodName
models O
provided O
state O
of O
the O
art O
results O
on O
the O
MovieQA O
dataset O
with O
an O
average B-MetricName
of O
84.5 B-MetricValue
% O
on O
the O
validation O
set O
and O
an O
average B-MetricName
of O
85 B-MetricValue
% O
on O
the O
test O
set O
. O

This O
motivates O
why O
we O
used O
the O
difﬁcult O
and O
easy O
documents O
for O
the O
CNN O
and O
LSTM B-MethodName
models O
( O
Blohm O
et O
al O
. O
, O
2018 O
) O
, O
as O
they O
are O
the O
only O
paper O
to O
date O
which O
both O
obtain O
SOTA O
results O
and O
offered O
qualitative O
analysis O
on O
the O
gap O
between O
human O
and O
model O
performance O
. O

4.2 O
XLNet B-MethodName
Models O
We O
used O
the O
pre O
- O
trained O
XLNet B-MethodName
model O
and O
ﬁnetuned O
it O
for O
the O
QA O
task O
( O
Tapaswi O
et O
al O
. O
, O
2016 O
; O
Yang O
et O
al O
. O
, O
2019 O
) O
. O

We O
opted O
for O
XLNet B-MethodName
given O
that O
it O
is O
a O
recent O
Transformer O
network O
for O
language O
understanding O
that O
outperformed O
BERT B-MethodName
and O
other O
largescale O
pre O
- O
trained O
language O
models O
on O
a O
variety O
ofNLP O
tasks O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
. O

XLNet B-MethodName
is O
based O
on O
an O
auto O
- O
regressive O
approach O
in O
which O
the O
model O
uses O
observations O
from O
previous O
time O
steps O
in O
order O
to O
predict O
the O
weight B-HyperparameterName
for O
the O
next O
time O
step O
. O

Advancing O
from O
the O
traditional O
auto O
- O
regressive O
approach O
, O
such O
as O
a O
Bidirectional O
LSTM B-MethodName
, O
the O
authors O
also O
combine O
their O
network O
with O
an O
auto O
- O
encoding O
approach O
seen O
with O
the O
BERT B-MethodName
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

By O
combining O
both O
approaches O
, O
XLNet B-MethodName
introduces O
permutations O
on O
both O
sides O
. O

During O
ﬁne O
- O
tuning O
, O
however O
, O
the O
model O
is O
essentially O
the O
Transformer O
- O
XL O
( O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Dai O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2019 O
) O
. O

The O
auto O
- O
regressive O
language O
model O
estimates O
the O
joint O
probability O
over O
the O
input O
elements O
( O
in O
XLNet B-MethodName
this O
xis O
language O
agnostic O
, O
i.e O
it O
is O
a O
subtoken O
) O
. O

Note O
, O
the O
permutation O
language O
model O
is O
the O
component O
which O
helps O
XLNet B-MethodName
capture O
longer O
dependencies O
between O
elements O
in O
a O
given O
input O
sequence O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
. O

In O
our O
method O
, O
we O
ﬁne O
- O
tune O
the O
XLNet B-MethodName
with O
24 O
attention O
layers O
and O
16 O
attention O
heads O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
. O

The O
ﬁne O
- O
tuned O
XLNet B-MethodName
outperforms O
all O
other O
results O
on O
the O
validation O
set O
, O
obtaining O
the O
new O
highest O
accuracy B-MetricName
of O
91%.5 O
Analysis O
Method O
5.1 B-MetricValue
Human O
Gaze O
- O
Attention O
Extraction O
We O
obtain O
token O
level O
gaze O
counts O
( O
frequency O
counts O
) O
by O
mapping O
the O
x;ycoordinates O
to O
bounding O
boxes O
set O
around O
each O
word O
of O
the O
stimuli O
. O

5.2 O
Extracting O
LSTM B-MethodName
and O
CNN O
Word O
Level O
Attention O
The O
sentence O
level O
attention O
for O
the O
CNN O
and O
LSTM B-MethodName
models O
have O
very O
low O
entropy B-MetricName
, O
where O
essentially O
almost O
all O
of O
the O
attention O
is O
distributed O
to O
one O
sentence O
and O
the O
rest O
of O
the O
sentence O
attention O
weights B-HyperparameterName
are O
almost O
zero O
. O

This O
is O
a O
property O
of O
the O
two O
- O
staged O
attention O
, O
which O
XLNet B-MethodName
does O
not O
have O
. O

Figure O
7a O
and O
7b O
in O
the O
Appendix O
show O
the O
word O
level O
attention O
distribution O
of O
CNN O
and O
LSTM B-MethodName
models O
. O

5.3 O
Extracting O
XLNet B-MethodName
Word O
Level O
Attention O
We O
extracted O
the O
attention O
weights B-HyperparameterName
from O
the O
nine O
best O
XLNet B-MethodName
models O
by O
leveraging O
the O
output O
of O
the O
last O
attention O
layer O
. O

We O
did O
so O
because O
in O
Transformers O
, O
attention O
computations O
happen O
simultaneously O
, O
while O
for O
LSTMs B-MethodName
and O
CNNs O
they O
happen O
last O
. O

In O
order O
to O
compare O
XLNet B-MethodName
to O
the O
LSTM B-MethodName
and O
CNN O
models O
, O
we O
therefore O
only O
take O
the O
ﬁnal O
output O
of O
the O
self O
- O
attention O
layer O
. O

With O
p O
- O
values O
below O
0.01 O
, O
we O
can O
reject O
the O
null O
hypothesis O
and O
thus O
accept O
that O
there O
is O
a O
statistically O
signiﬁcant O
correlation B-MetricName
between O
divergence O
and O
accuracy.6 B-MetricName
Analysis O
Results O
6.1 B-MetricValue
Models O
vs. O
Humans O
In O
order O
to O
explore O
the O
relationship O
of O
model O
performance O
and O
similarity O
between O
model O
attention O
and O
human O
visual O
attention O
, O
we O
plot O
in O
Figures O
3a O
and O
3b O
the O
nine O
best O
LSTM B-MethodName
and O
XLNet B-MethodName
models O
performances O
for O
each O
document O
, O
sorted O
by O
the O
sum O
of O
divergence O
scores B-MetricName
and O
number O
of O
correct O
models O
. O

Similar O
comparison O
between O
CNN O
and O
XLNet B-MethodName
models O
can O
be O
found O
in O
the O
Appendix O
, O
Figure O
4a O
and O
4b O
. O

Moreover O
, O
our O
plots O
show O
a O
correlation B-MetricName
between O
attentive O
LSTM B-MethodName
and O
CNN O
model O
performance O
and O
similarity O
to O
human O
visual O
attention O
. O

Nine O
Best O
Val O
Accuracy O
Spearman O
p O
 value O
LSTM B-MethodName
84.37 O
% O
-0.73<0.001 O
CNN O
82.58 O
% O
-0.72<0.001 O
XLNet B-MethodName
91.00 O
% O
-0.16 O
0:381 O
Table O
2 O
: O
Spearman O
’s O
rank O
correlation B-MetricName
coefﬁcients O
between O
the O
number O
of O
models O
which O
correctly O
answered O
a O
given O
question O
on O
each O
document O
and O
the O
KL O
divergence O
between O
models O
and O
human O
visual O
attention O
. O

As O
observed O
in O
Figure O
3a O
( O
and O
Figure O
4a O
in O
the O
Appendix O
) O
, O
there O
are O
two O
statistically O
signiﬁcant O
negative O
correla-0.0150.0200.0250.030KL O
Divergence O
lstm O
xlnet123456789 O
# O
Correct O
( O
a O
) O
LSTM B-MethodName
versus O
humans O
— O
KL O
divergence O
and O
number O
of O
correct O
models O
per O
document O
. O

0.0150.0200.0250.030KL O
Divergence O
lstm O
xlnet123456789 O
# O
Correct O
Documents0.0100.0150.0200.0250.030KL O
Divergence O
lstm O
xlnet O
0123456789 O
# O
Correct O
( O
b O
) O
XLNet B-MethodName
versus O
humans O
— O
KL O
divergence O
and O
number O
of O
correct O
models O
per O
document O
. O

We O
plot O
performance O
of O
LSTM B-MethodName
( O
cf O
. O

Figure O
3a O
) O
and O
XLNet B-MethodName
( O
cf O
. O

In O
Figure O
3a O
, O
the O
larger O
blue O
dots O
show O
the O
LSTM B-MethodName
divergence O
score B-MetricName
for O
each O
document O
, O
while O
the O
smaller O
orange O
dots O
show O
the O
divergence O
score B-MetricName
of O
XLNet B-MethodName
models O
. O

Vice O
- O
Versa O
, O
in O
Figure O
3b O
, O
the O
larger O
orange O
dots O
show O
the O
XLNet B-MethodName
score B-MetricName
for O
each O
document O
, O
while O
the O
smaller O
blue O
dots O
show O
the O
divergence O
score B-MetricName
of O
the O
LSTM B-MethodName
models O
. O

tions O
from O
the O
attentive O
LSTM B-MethodName
( O
-0.73 O
) O
and O
CNN O
( O
-0.72 O
) O
models O
. O

These O
correlation B-MetricName
scores B-MetricName
indicate O
that O
for O
either O
LSTM B-MethodName
or O
CNN O
, O
as O
the O
number O
of O
models O
that O
correctly O
answered O
a O
question O
related O
to O
a O
document O
increases O
, O
the O
KL O
divergence O
of O
these O
model O
types O
to O
human O
visual O
attention O
decreases O
. O

We O
conclude O
that O
there O
is O
a O
correlation B-MetricName
between O
task O
performance O
and O
similarity O
between O
neural O
attention O
when O
leveraging O
LSTM B-MethodName
or O
CNN O
and O
human O
visual O
attention O
distributions O
. O

However O
in O
contrast O
, O
behavior O
from O
XLNet B-MethodName
models O
show O
weak O
negative O
correlation B-MetricName
of O
-0.16 O
and O
p O
= O
0.381 B-MetricValue
( O
cf O
. O

Most O
XLNet B-MethodName
models O
correctly O
answer O
the O
questions O
, O
although O
the O
KL O
divergence O
increases O
( O
cf O
. O

All O
the O
nine O
XLNet B-MethodName
models O
always O
provide O
correct O
answers O
. O

One O
potential O
reason O
could O
be O
that O
we O
chose O
documents O
that O
are O
difﬁcult O
to O
answer O
based O
on O
an O
analysis O
of O
CNN O
and O
LSTM B-MethodName
models O
. O

6.2 O
Models O
vs. O
Models O
In O
Table O
3 O
, O
we O
perform O
a O
pairwise O
comparison O
of O
the O
average B-MetricName
KL O
divergence O
for O
the O
three O
neural O
models O
using O
a O
linear B-MethodName
regression I-MethodName
model O
with O
Tukey O
’s O
alpha O
adjustment O
method O
( O
Sinclair O
et O
al O
. O
, O
2013 B-MetricValue
) O
. O

Interestingly O
, O
there O
is O
a O
statistically O
signiﬁcant O
difference O
between O
the O
KL O
divergence O
of O
LSTMscompared B-MethodName
to O
XLNets B-MethodName
( O
 O
= O
 0:003;p O
< O
0:01 O
) O
. O

Even O
though O
the O
performance O
of O
the O
XLNets B-MethodName
are O
better O
with O
respect O
to O
accuracy B-MetricName
, O
LSTMs B-MethodName
are O
signiﬁcantly O
more O
similar O
to O
human O
visual O
attention O
. O

Our O
ﬁndings O
show O
that O
CNNs O
and O
LSTMs B-MethodName
have O
a O
statistically O
signiﬁcant O
correlation B-MetricName
between O
similarity O
to O
human O
visual O
attention O
distributions O
and O
system O
performance O
. O

Interestingly O
, O
the O
same O
is O
not O
true O
for O
XLNets B-MethodName
. O

Moreover O
, O
the O
attention O
weights B-HyperparameterName
of O
the O
LSTMs B-MethodName
are O
signiﬁcantly O
different O
compared O
to O
the O
XLNets B-MethodName
. O

Error O
t O
- O
value O
p O
- O
value O
LSTM B-MethodName
0.018 O
LSTM B-MethodName
vs. O
XLNet B-MethodName
-0.003 O
0.001 O
-2.835 O
< O
0.01 O
CNN O
0.020 O
LSTM B-MethodName
vs. O
CNN O
-0.001 O
0.001 O
-1.098 O
0:27 O
XLNet B-MethodName
0.022 O
CNN O
vs. O
XLNet B-MethodName
-0.001 O
0.001 O
-1.736 O
0:17 O
Table O
3 O
: O
Pairwise O
comparison O
of O
the O
average B-MetricName
KL O
divergence O
for O
the O
three O
models O
. O

Here O
we O
show O
the O
comparison O
of O
each O
model O
against O
each O
other O
( O
LSTM B-MethodName
vs. O
CNN O
, O
LSTM B-MethodName
vs. O
XLNET O
, O
and O
CNN O
vs. O
XLNet B-MethodName
) O
. O

We O
compare O
the O
models O
to O
show O
if O
the O
differences O
in O
attention O
distributions O
between O
models O
is O
of O
statistical O
signiﬁcance O
; O
the O
signiﬁcantly O
different O
model O
type O
( O
LSTM B-MethodName
) O
can O
be O
seen O
in O
bold O
, O
where O
p O
- O
value O
< O
0.01 O
. O

It O
would O
be O
interesting O
to O
investigate O
whether O
the O
observed O
increase O
in O
performance O
but O
lack O
of O
similarity O
to O
humans O
in O
the O
XLNet B-MethodName
models O
is O
because O
they O
are O
pre O
- O
trained O
on O
large O
external O
corpora O
or O
whether O
this O
is O
due O
to O
inherent O
properties O
in O
architecture O
, O
when O
compared O
to O
other O
pre O
- O
trained O
models O
( O
such O
as O
BERT B-MethodName
) O
. O

For O
example O
, O
the O
winning O
technique O
of O
HIT O
- O
SCIR O
( O
Che O
et O
al O
. O
, O
2019 O
) O
at O
MRP O
2019 O
used O
a O
transitionbased O
parser O
based O
on O
a O
BERT B-MethodName
encoder O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

SUDA O
- O
Alibaba O
( O
Zhang O
et O
al O
. O
, O
2019c O
) O
proposed O
a O
graph O
- O
based O
approach O
with O
BERT B-MethodName
. O

2.2 O
Comparison O
with O
Other O
Systems O
Like O
Zhang O
et O
al O
. O
( O
2019a O
) O
, O
we O
model O
a O
context O
- O
free O
language O
instead O
of O
a O
sequence O
of O
transition O
actions O
, O
and O
parser O
states O
can O
be O
regarded O
as O
being O
implicitly O
materialized O
inside O
BERT B-MethodName
’s O
memory O
. O

In O
addition O
, O
they O
provided O
LSTMs B-MethodName
whereas O
we O
provide O
Transformers O
that O
can O
draw O
attentions O
from O
both O
past O
node O
and O
edge O
representations O
in O
the O
decoder O
. O

Since O
neural B-MethodName
networks I-MethodName
may O
produce O
ill O
- O
formatted O
PGN O
, O
the O
left O
- O
to O
- O
right O
decoding O
ﬁnds O
as O
many O
edges O
as O
possible.3If O
there O
is O
an O
ill O
- O
formatted O
action O
such O
that O
a O
PGN O
sequence O
terminates O
with O
a O
non O
- O
empty O
stack O
, O
we O
generate O
additional O
edges O
according O
to O
the O
stack O
state O
. O

Given O
an O
input O
text O
, O
our O
parser O
encodes O
the O
tokens O
by O
a O
pre O
- O
trained O
language O
model O
( O
PLM O
) O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

At O
the O
i O
- O
th O
decode O
step O
, O
our O
4We O
omit O
reentrancy O
and O
property O
outputs O
for O
simplicity O
. O
G O
 O
L O
 O
L O
L O
G O
 O
L O
G O
 O
E O
 O
G O
 O
L O
 O
L O
L O
G O
 O
R O
 O
G O
 O
E O
take O
 O
-10 O
 O
it O
 O
long O
 O
-03 O
 O
< O
bos O
> O
 O
[ O
EON O
] O
 O
[ O
EON O
] O
 O
[ O
EOD O
] O
 O
[ O
EON O
] O
 O
[ O
EON O
] O
 O
ARG0 O
 O
ARG1 O
Input O
tokensBERT B-MethodName
encoder O
Mode O
embed O
Label O
embed O
Edge O
embed O
Graph O
embedMode O
classifierLabel O
classifier O
Edge O
classifier O
Graph O
classifier O
Generate O
a O
token O
mask O
for O
each O
classifierReentrancy O
classifier O
Biaffine O
scoring O
[ O
CLS O
] O
 O
It O
did O
n'ttake O
long O
. O

[ O
SEP O
] O
0 O
 O
0 O
 O
0 O
 O
0 O
 O
0 O
 O
0AS O
 O
AE O
 O
L O
 O
L O
L O
L O
_ O
 O
may O
 O
_ O
 O
v O
Transformer O
decoderanchor O
classifier O
Biaffine O
scoringBiaffine O
scoring O
BERT B-MethodName
encoderanchor O
classifier O
AS O
: O
anchor O
start O
AE O
: O
anchor O
endG O
 O
AS O
 O
AE O
 O
L O
 O
L O
L O
_ O
 O
may O
 O
_ O
< O
bos O
> O
Input O
tokensMode O
classifierLabel O
classifier O
[ O
CLS O
] O
Jackets O
 O
may O
 O
be O
 O
sold O
 O
next O
 O
. O

Implementation O
: O
We O
utilize O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
models O
asTeam O
Mean O
EDS O
PTG O
UCCA O
AMR O
DRG O
Hitachi O
( O
ours O
) O
.8642 O
/1 O
.9356 O
/1 O
.8873 O
/1 O
.7507 O
/2 O
.8154 O
/1 O
.9319 O
/2 O
´ O
UFAL O
( O
Samuel O
and O
Straka O
, O
2020 O
) O
.8639 O
/1 O
.9273 O
/2 O
.8844 O
/2 O
.7640 O
/1 O
.8023 O
/2 O
.9416 O
/1 O
HIT O
- O
SCIR O
( O
Dou O
et O
al O
. O
, O
2020 O
) O
.8106 O
/3 O
.8740 O
/3 O
.8426 O
/3 O
.7476 O
/3 O
.6980 O
/3 O
.8907 O
/3 O
HUJI O
- O
KU O
( O
Arviv O
et O
al O
. O
, O
2020 O
) O
.6429 O
/4 O
.7968 O
/5 O
.5376 O
/4 O
.7291 O
/4 O
.5236 O
/5 O
.6275 O
/5 O
ISCAS O
.4813 O
/5 O
.8586 O
/4 O
.1799 O
/6 O
.0599 O
/6 O
.6148 O
/4 O
.6935 O
/4 O
TJU O
- O
BLCU O
.3016 O
/6 O
.4904 O
/6 O
.2149 O
/5 O
.1041 O
/5 O
.2996 O
/6 O
.3991 O
/6 O
JBNU O
( O
Na O
and O
Min O
, O
2020 O
) O
.1323 O
/- O
- O
- O
- O
.6613 O
/- O
Table O
3 O
: O
Ofﬁcial O
MRP O
results O
for O
the O
cross O
- O
framework O
track O
( O
shown O
as O
score B-MetricName
/rank O
) O
. O

RoBERTa B-MethodName
large O
model O
is O
used O
for O
the O
crossframework O
track O
because O
we O
found O
in O
our O
preliminary O
experiments O
that O
this O
model O
generally O
performs O
better O
. O

In O
the O
cross O
- O
lingual O
track O
, O
we O
utilize O
multi O
- O
lingual O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
except O
for O
Chinese O
AMR O
. O

Chinese O
RoBERTa B-MethodName
( O
Cui O
et O
al O
. O
, O
2020 O
) O
is O
used O
for O
the O
Chinese O
AMR O
graphs O
because O
the O
model O
is O
carefully O
tuned O
for O
Chinese O
. O

The O
only O
exception O
is O
Czech O
PTG O
because O
node O
label O
tokens O
in O
Czech O
PTG O
graphs O
include O
accents O
that O
are O
removed O
from O
the O
vocabulary O
of O
multilingual O
BERT B-MethodName
. O

First O
, O
we O
concatenated O
the O
cross O
- O
framework O
( O
CF O
) O
( O
e.g. O
, O
English O
DRG O
) O
and O
cross O
- O
lingual O
( O
CL O
) O
7This O
was O
done O
to O
save O
the O
computation O
time O
. O
EDS O
PTG O
UCCA O
AMR O
DRG O
roberta O
- O
large O
.9101 O
.8779 O
.7692 O
.7776 O
.9080 O
bert O
- O
base O
- O
cased O
.8964 O
.8589 O
.7473 O
.7634 O
.8675 O
Table O
5 O
: O
Comparison O
of O
MRP O
all O
- O
F O
scores B-MetricName
between O
BERT B-MethodName
base O
and O
RoBERTa B-MethodName
large O
versions O
. O

Then O
, O
we O
applied O
pre O
- O
training O
on O
the O
concatenated O
data O
for O
each O
framework O
with O
multi O
- O
lingual O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

RoBERTa B-MethodName
large O
models O
were O
better O
than O
BERT B-MethodName
small O
models O
, O
showing O
improvements O
ranging O
from O
one O
to O
four O
points O
. O

We O
used O
BERT B-MethodName
base O
in O
this O
run O
. O

While O
they O
tried O
different O
types O
of O
LMs O
, O
best O
results O
were O
obtained O
for O
neural B-MethodName
models I-MethodName
, O
namely O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
. O

We O
calculate O
the O
probability O
of O
a O
sentence O
with O
a O
long O
- O
short O
term O
memory O
( O
LSTM B-MethodName
, O
Hochreiter O
and O
Schmidhuber O
( O
1997 O
) O
) O
LM O
, O
i.e. O
, O
a O
special O
type O
of O
RNN O
LM O
, O
which O
has O
been O
trained O
on O
a O
large O
corpus O
. O

More O
details O
on O
LSTM B-MethodName
LMs O
2Note O
that O
the O
sentence O
log O
- O
probability O
which O
is O
normalized O
by O
sentence O
length O
corresponds O
to O
the O
negative O
crossentropy B-MetricName
. O
ILP O
NAMAS O
SEQ2SEQ O
T3 O
ﬂuency O
2.22 O
1.30 O
1.51 O
1.40 O
Table O
2 O
: O
Average B-MetricName
ﬂuency O
ratings O
for O
each O
compression O
system O
in O
the O
dataset O
by O
Toutanova O
et O
al O
. O
( O
2016 B-MetricValue
) O
. O

4.2 O
LM O
Hyperparameters O
and O
Training O
We O
train O
our O
LSTM B-MethodName
LMs O
on O
the O
English O
Gigaword O
corpus O
( O
Parker O
et O
al O
. O
, O
2011 O
) O
, O
which O
consists O
of O
news O
data O
. O

Namely O
we O
train O
a O
support O
vector O
machine O
( O
SVM B-MethodName
) O
( O
Cortes O
and O
Vapnik O
, O
1995 O
) O
to O
predict O
POS B-TaskName
labels O
using O
word O
embeddings O
as O
features O
. O

After O
training O
the O
network O
on O
each O
feature O
set O
, O
we O
calculate O
rd20 O
of O
the O
hidden O
states O
of O
these O
networks O
, O
and O
use O
these O
distances O
as O
a O
predictor O
in O
a O
linear B-MethodName
regression I-MethodName
experiment O
2.1 O
Representation O
Distance O
20 O
Representation O
Distance O
20 O
( O
rd20 O
) O
is O
a O
measure O
that O
does O
not O
assume O
a O
particular O
representational O
format O
, O
and O
thus O
applies O
to O
any O
kind O
of O
vector O
representation O
. O

It O
is O
therefore O
suitable O
for O
inspecting O
both O
external O
phenomena O
, O
i.e. O
featurized O
string O
representations O
, O
and O
internal O
representations O
, O
e.g. O
weight B-HyperparameterName
matrices O
of O
neural B-MethodName
networks I-MethodName
. O

All O
score B-MetricName
predictors O
for O
each O
model O
but O
the O
N O
model O
show O
positive O
effect O
of O
score B-MetricName
on O
RT O
, O
indicating O
that O
words O
in O
denser O
neighborhoods O
, O
i.e. O
words O
with O
a O
lower O
average B-MetricName
distance O
to O
nearest B-MethodName
neighbors I-MethodName
, O
have O
shorter O
Reaction O
Times O
. O

This O
opens O
up O
new O
avenues O
for O
research O
, O
and O
allows O
us O
to O
quantitatively O
determine O
the O
effect O
of O
neighborhood O
density O
in O
neural B-MethodName
networks I-MethodName
on O
behavioral O
measures O
. O

Hierarchical O
neural B-MethodName
networks I-MethodName
are O
often O
used O
to O
model O
inherent O
structures O
within O
dialogues O
. O

1 O
Introduction O
Modeling O
a O
probability O
distribution O
over O
word O
sequences O
is O
a O
core O
topic O
in O
natural O
language O
processing O
, O
with O
language O
modeling O
being O
a O
ﬂagship O
problem O
and O
mostly O
tackled O
via O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
( O
Mikolov O
and O
Zweig O
, O
2012 O
; O
Melis O
et O
al O
. O
, O
2017 O
; O
Merity O
et O
al O
. O
, O
2018 O
) O
. O

Our O
method O
performs O
on O
par O
with O
the O
state O
of O
the O
art O
performance O
on O
Natural O
Language O
Context O
to O
Command O
task O
and O
performs O
better O
than O
ﬁne O
- O
tuned O
T5 B-MethodName
and O
Seq2Seq O
models O
. O

We O
apply O
our O
method O
to O
NLC2CMD O
challenge O
( O
Agarwal O
et O
al O
. O
, O
2021 O
) O
dataset O
( O
Lin O
et O
al O
. O
, O
2018 O
) O
and O
compare O
the O
results O
with O
baselines O
like O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
, O
Seq2Seq O
and O
the O
winning O
solution O
to O
NLC2CMD O
challenge O
( O
Agarwal O
et O
al O
. O
, O
2021 O
) O
. O

Our O
method O
performs O
better O
than O
T5 B-MethodName
and O
Seq2Seq O
while O
providing O
explanations O
for O
the O
predictions O
. O

•T5 B-MethodName
: O
T5 B-MethodName
is O
a O
transformer O
based O
model O
proposed O
by O
Raffel O
et O
al O
. O
( O
2019 O
) O
. O

We O
ﬁne O
tune O
T5 B-MethodName
on O
our O
dataset O
. O

The O
T5 B-MethodName
- O
small O
model O
by O
huggingface O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
performed O
the O
best O
among O
T5 B-MethodName
- O
small O
, O
T5 B-MethodName
- O
large O
and O
T5 B-MethodName
- O
base O
. O

We O
report O
the O
results O
for O
T5 B-MethodName
- O
small O
. O

It O
is O
an O
LSTM B-MethodName
based O
encoder O
- O
decoder O
model O
that O
uses O
the O
attention O
mechanism O
to O
dynamically O
generate O
context O
for O
decoding O
. O

For O
Seq2Seq O
, O
we O
used O
two O
256 O
dimensional O
bidirectional O
LSTM B-MethodName
layers O
for O
the O
encoder O
and O
two O
256 O
dimensional O
LSTM B-MethodName
layers O
for O
the O
decoder O
with O
attention O
in O
between O
encoder O
and O
decoder O
. O

For O
T5 B-MethodName
, O
we O
ﬁne O
tuned O
the O
T5 B-MethodName
- O
small O
model O
on O
our O
dataset O
. O

Our O
method O
performs O
comparable O
to O
the O
state O
of O
the O
art O
solution O
( O
Magnum O
) O
and O
better O
than O
other O
baselines O
like O
T5 B-MethodName
and O
Seq2Seq O
. O

Moreover O
, O
T5 B-MethodName
is O
trained O
on O
massive O
amounts O
of O
parallel O
data O
, O
and O
our O
model O
trained O
on O
only O
2,600 O
examples O
and O
116 O
utility O
descriptions O
surpasses O
T5 B-MethodName
. O

Here O
both O
of O
these O
n O
- O
gramsModel O
Test O
score B-MetricName
Seq2Seq O
( O
Bahdanau O
et O
al O
. O
, O
2015 B-MetricValue
) O
0:5940:032 O
T5 B-MethodName
( O
Raffel O
et O
al O
. O
, O
2019 O
) O
0:6390:027 O
Magnum O
( O
Agarwal O
et O
al O
. O
, O
2021 O
) O
0:6850:027 O
Proposed O
Method O
- O
Parent O
Attention O
0:5030:154 O
Proposed O
Method O
- O
Tree O
Coordinates O
0:5970:052 O
Proposed O
Method O
- O
Guidance O
Module O
0:6300:072 O
Proposed O
Method O
0:6510:017 O
Table O
1 O
: O
NLC2CMD O
Competition O
metric O
on O
test O
set O
. O

Jacovi O
et O
al O
. O
( O
2018 O
) O
present O
an O
analysis O
of O
convolutional O
neural B-MethodName
networks I-MethodName
for O
text O
. O

Inspired O
by O
the O
human O
visual O
system O
, O
neural B-MethodName
machine I-MethodName
attention O
allows O
neural B-MethodName
networks I-MethodName
to O
selectively O
focus O
on O
particular O
parts O
of O
the O
input O
, O
resulting O
in O
signiﬁcant O
improvements O
in O
performance O
and O
interpretability O
( O
Correia O
and O
Colombini O
, O
2021 O
) O
. O

We O
binned O
VQAv2 O
val O
pairs O
accordingly O
by O
training O
a O
LSTM B-MethodName
- O
based O
classiﬁer O
on O
1.6 O
M O
TDIUC O
and O
145 O
K O
VQAv2 O
train+val O
samples O
which O
we O
labelled O
using O
regular O
expressions O
. O

Ordinal O
Logistic B-MethodName
Regression I-MethodName
. O

answer O
correctly O
, O
we O
performed O
an O
Ordinal O
Linear B-MethodName
Regression I-MethodName
( O
OLR O
) O
. O

Given O
that O
the O
dependant O
variables O
are O
ranked O
we O
opted O
for O
using O
ordered O
logistic B-MethodName
regression I-MethodName
to O
predict O
for O
each O
accuracy B-MetricName
bin.5 O
Results O
5.1 B-MetricValue
Human O
and O
Neural O
Attention O
Relationship O
– O
Averaged B-MetricName
Over O
Documents O
Table O
2 O
shows O
the O
overall O
accuracy B-MetricName
scores B-MetricName
of O
the O
ﬁve O
models O
on O
the O
VQAv2 O
validation O
set O
when O
trained O
only O
on O
the O
training O
partition O
. O

5.2 O
Ordinal O
Logistic B-MethodName
Regression I-MethodName
By O
averaging O
evaluation O
metrics O
( O
correlation B-MetricName
and O
JSD O
) O
across O
documents O
, O
we O
obscure O
the O
impact O
that O
similarity O
has O
on O
each O
document O
with O
respect O
to O
accuracy B-MetricName
. O

The O
Ordinal O
Logistic B-MethodName
Regression I-MethodName
model O
results O
uncover O
the O
importance O
of O
the O
text O
and O
image O
correlation B-MetricName
scores B-MetricName
as O
predictors O
on O
per O
document O
accuracy B-MetricName
. O

Such O
an O
observation O
was O
also O
reported O
in O
previous O
work O
which O
compared O
the O
XLNet B-MethodName
transformer O
to O
human O
attention O
( O
Sood O
et O
al O
. O
, O
2020a O
) O
. O

Analysis O
from O
the O
Ordinal O
Logistic B-MethodName
Regression I-MethodName
model O
shows O
, O
for O
the O
ﬁrst O
time O
, O
that O
correlation B-MetricName
to O
human O
text O
attention O
is O
a O
signiﬁcant O
predictor O
across O
all O
VQA O
model O
types O
, O
where O
dissimilarity O
between O
human O
and O
neural O
text O
attention O
decreases O
the O
likelihood O
of O
the O
models O
ability O
to O
predict O
the O
answer O
correctly O
. O

We O
then O
evaluate O
three O
models O
for O
generating O
responses O
: O
a O
syntax O
- O
aware O
rulebased O
system O
, O
a O
seq2seq O
LSTM B-MethodName
neural O
models O
with O
attention O
( O
S2SA O
) O
, O
and O
the O
same O
neural O
model O
augmented O
with O
a O
copy O
mechanism O
( O
S2SA+C O
) O
. O

All O
models O
use O
concatenated O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
GloVE O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
embedding O
for O
the O
input O
embeddings O
. O

Our O
baseline O
neural O
model O
is O
a O
bidirectional O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
with O
attention O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O

Formally O
, O
the O
encoder O
outputs O
a O
set O
of O
hidden O
states O
for O
each O
token O
given O
byfh1 O
; O
: O
: O
: O
; O
hng O
= O
LSTMfm(x1 B-MethodName
; O
: O
: O
: O
; O
x O
n)gfor O
word O
embeddings O
m(x O
) O
. O

The O
decoder O
is O
also O
an O
LSTM B-MethodName
with O
hidden O
state O
initialized O
to O
the O
sum O
of O
the O
ﬁnal O
hidden O
states O
of O
the O
forward O
and O
backward O
LSTMs B-MethodName
contained O
in O
the O
bidirectional O
LSTM B-MethodName
encoder O
. O

All O
LSTMs B-MethodName
are O
bidirectional O
with O
a O
single O
layer O
. O

Both O
sequence O
to O
sequence O
models O
for O
the O
IDONTKNOW O
task O
use O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
524within O
the O
LSTM B-MethodName
, O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
100for O
the O
attention O
layer O
, O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
638for O
the O
copy O
layer O
, O
and O
a O
dropout B-HyperparameterName
value O
of O
0.1 B-HyperparameterValue
. O

Both O
sequence O
to O
sequence O
models O
for O
the O
EMOTIVE B-MetricName
task O
use O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
600within O
the O
LSTM B-MethodName
, O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
200for O
the O
attention O
layer O
, O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
650for O
the O
copy O
layer O
, O
and O
a O
dropout B-HyperparameterName
value O
of O
0.1 B-HyperparameterValue
. O

The O
distractors O
are O
chosen O
from O
the O
nearest B-MethodName
neighbors I-MethodName
of O
the O
prompt O
using O
an O
averagedModel B-MetricName
App O
. O

3.2 O
Transition O
Classiﬁer O
To O
predict O
the O
next O
transition O
at O
each O
step O
, O
TUPA O
uses O
a O
BiLSTM B-MethodName
with O
feature O
embeddings O
as O
inputs O
, O
followed O
by O
an O
MLP O
and O
a O
softmax O
layer O
for O
classiﬁcation O
. O

Parser O
state O
S O
madeB O
to O
feel O
very O
wel O
... O
G O
Wensubj O
wereauxhead O
Classiﬁer O
BiLSTM B-MethodName
Embeddings O
We O
were O
welcome O
. O

Vector O
representation O
for O
the O
input O
tokens O
is O
computed O
by O
two O
layers O
of O
bidirectional O
LSTMs B-MethodName
. O

20 O
MLP O
layers O
2 O
MLP O
dimensions O
50 O
BiLSTM B-MethodName
layers O
2 O
BiLSTM B-MethodName
dimensions O
500 O
Table O
2 O
: O
Hyperparameter O
settings O
. O

4.1 O
Hyperparameters O
We O
use O
dropout B-HyperparameterName
( O
Srivastava O
et O
al O
. O
, O
2014 B-HyperparameterValue
) O
between O
MLP O
layers O
, O
and O
recurrent O
dropout B-HyperparameterName
( O
Gal O
and O
Ghahramani O
, O
2016 B-HyperparameterValue
) O
between O
BiLSTM B-MethodName
layers O
, O
both O
withp= O
0:4 O
. O

Moreover O
, O
all O
teams O
except O
for O
SURUG O
, O
which O
employed O
a O
convolutional O
neural O
network O
, O
made O
use O
of O
some O
form O
of O
gated O
recurrent O
network O
— O
either O
a O
gated O
recurrent O
network O
( O
GRU O
) O
( O
Chung O
et O
al O
. O
, O
2014 O
) O
or O
long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

However O
, O
different O
with O
previous O
work O
, O
we O
use O
8 O
- O
layers O
with O
8 O
heads O
transformer O
encoder O
, O
which O
shows O
better O
performance O
than O
LSTM B-MethodName
in O
Kitaev O
and O
Klein O
( O
2018 O
) O
. O

For O
AMR O
, O
DM O
, O
and O
PSD O
, O
they O
all O
use O
one O
layer O
Bi O
- O
directional O
LSTM B-MethodName
for O
input O
sentence O
encoder O
, O
and O
two O
layers O
Bi O
- O
directional O
LSTM B-MethodName
for O
head O
or O
dependent O
node O
encoder O
in O
the O
bi O
- O
afﬁne O
classiﬁer O
. O

For O
the O
latent O
AMR O
model O
, O
to O
model O
the O
posterior O
alignment O
, O
we O
use O
another O
Bi O
- O
LSTM B-MethodName
for O
node O
sequence O
encoding O
. O

When O
adding O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
into O
our O
model O
, O
it O
can O
further O
improve O
almost O
3 O
points O
on O
it O
. O

Moreover O
, O
we O
believe O
that O
multitask O
learning O
and O
pre O
- O
trained O
deep O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
may O
also O
boost O
the O
performance O
of O
our O
paser O
in O
future O
. O

4.4.2 O
Error O
Analysis O
on O
Phrasal O
- O
Anchoring O
According O
to O
Table O
7 O
, O
our O
model O
with O
ELMo B-MethodName
works O
slightly O
better O
than O
the O
top O
1 O
model O
on O
anchors O
prediction O
. O

However O
, O
when O
predicting O
the O
edge O
and O
edgedata O
tops O
anchors O
edge O
attr O
all O
TUPA O
singleall O
78.73 O
69.17 O
16.96 O
15.18 O
27.56 O
lpps O
86.03 O
76.26 O
28.32 O
24.00 O
40.06 O
TUPA O
multiall O
84.92 O
65.74 O
12.99 O
9.07 O
23.65 O
lpps O
88.89 O
77.76 O
26.45 O
18.32 O
41.04 O
( O
Che O
et O
al O
. O
, O
2019)all O
1.00 O
95.36 O
72.66 O
61.98 O
81.67 O
lpps O
1.00 O
96.99 O
73.08 O
48.37 O
82.61 O
Ours(*5)all O
98.85 O
94.92 O
60.17 O
0.00 O
74.00 O
lpps O
96.00 O
96.75 O
60.20 O
0.00 O
75.17 O
Ours O
+ O
ELMoall B-MethodName
99.38 O
95.70 O
64.88 O
0.00 O
76.94 O
lpps O
98.00 O
96.84 O
66.63 O
0.00 O
78.77 O
Table O
7 O
: O
Our O
UCCA O
parser O
in O
post O
- O
evaluation O
ranked O
5th O
according O
to O
the O
original O
ofﬁcial O
evaluation O
results O
. O

BistParser O
uses O
a O
bidirectional O
LSTM B-MethodName
neural O
network O
. O

Finally O
we O
would O
like O
to O
thank O
our O
colleague O
Ghislain O
Putois O
for O
help O
on O
all O
aspects O
on O
neural B-MethodName
networks I-MethodName
. O

The O
system O
consists O
of O
jointly O
trained O
tagger O
, O
lemmatizer O
, O
and O
dependency O
parser O
which O
are O
based O
on O
features O
extracted O
by O
a O
biLSTM B-MethodName
network O
. O

The O
concatenation O
of O
these O
embeddings O
is O
fed O
to O
a O
bidirectional O
long O
short O
- O
term O
memory O
network O
( O
biLSTM B-MethodName
, O
Graves O
and O
Schmidhuber O
, O
2005 O
; O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
which O
ex2https://github.com/360er0/COMBOtracts O
the O
ﬁnal O
features O
( O
see O
Section O
2.1 O
) O
. O

The O
tagger O
takes O
extracted O
features O
and O
predicts O
universal O
part O
- O
of O
- O
speech O
tags O
, O
language O
- O
speciﬁc O
tags O
and O
morphological O
features O
using O
three O
separate O
fully O
connected O
neural B-MethodName
networks I-MethodName
with O
one O
hidden B-HyperparameterName
layer I-HyperparameterName
( O
see O
Section O
2.2 B-HyperparameterValue
) O
. O

The O
lemmatizer O
uses O
a O
dilated O
CNN O
to O
predict O
lemmas O
based O
on O
characters O
of O
corresponding O
words O
and O
features O
previously O
extracted O
by O
a O
biLSTM B-MethodName
encoder O
( O
see O
Section O
2.3 O
) O
. O

These O
representations O
are O
output O
by O
two O
single O
fully O
connected O
layers O
which O
take O
feature O
vectors O
extracted O
by O
a O
biLSTM B-MethodName
encoder O
as O
input O
. O

We O
decide O
to O
use O
the O
dilated O
CNN O
instead O
of O
commonly O
used O
biLSTM B-MethodName
encoder O
to O
speed O
up O
the O
training O
of O
the O
system O
. O
First O
, O
each O
word O
is O
transformed O
to O
a O
sequence O
of O
the O
trainable O
character O
embeddings O
. O

2.1.3 O
Sentence O
Level O
biLSTM B-MethodName
Both O
word O
representations O
are O
concatenated O
together O
and O
fed O
into O
the O
sentence O
level O
biLSTM B-MethodName
network O
. O

The O
tagger O
takes O
the O
features O
extracted O
by O
the O
biLSTM B-MethodName
as O
input O
and O
predicts O
a O
universal O
part O
- O
of O
- O
speech O
tag O
and O
a O
language O
- O
speciﬁc O
tag O
for O
each O
word O
. O

First O
, O
features O
extracted O
by O
the O
biLSTM B-MethodName
encoder O
are O
used O
, O
however O
their O
dimensionality O
is O
reduced O
with O
a O
single O
fully O
connected O
layer O
. O

The O
ﬁnal O
input O
to O
the O
lemmatizer O
is O
a O
sequence O
of O
character O
embeddings O
concatenated O
with O
the O
reduced O
version O
of O
features O
extracted O
by O
the O
biLSTM B-MethodName
encoder O
. O

2.4 O
Parser O
2.4.1 O
Arc O
Prediction O
Two O
single O
fully O
connected O
layers O
transform O
features O
extracted O
by O
the O
biLSTM B-MethodName
encoder O
into O
head O
and O
dependent O
vector O
representations O
. O

Feature O
Extraction O
Two O
biLSTM B-MethodName
layers O
with O
512 O
hidden B-HyperparameterName
units I-HyperparameterName
are O
used O
to O
extract O
the O
ﬁnal O
features O
. O

The O
input O
characters O
, O
represented O
as O
the O
embeddings O
with O
256 O
dimensions O
, O
are O
concatenated O
with O
the O
features O
extracted O
with O
the O
biLSTM B-MethodName
encoder O
and O
reduced O
to O
32 O
dimensions O
with O
a O
single O
fully O
connected O
layer O
. O

3.2 O
Regularization O
We O
apply O
both O
Gaussian O
Dropout O
( O
with O
the O
dropout B-HyperparameterName
rate I-HyperparameterName
of O
0.25 B-HyperparameterValue
) O
and O
Gaussian O
Noise(with O
the O
standard O
deviation O
on O
0.2 O
) O
to O
the O
ﬁnal O
word O
embedding5and O
after O
processing O
each O
biLSTM B-MethodName
layer O
. O

The O
biLSTM B-MethodName
layers O
use O
both O
the O
standard O
and O
recurrent O
dropout B-HyperparameterName
with O
the O
rate O
of O
0.25 B-HyperparameterValue
. O

Moreover O
, O
the O
biLSTM B-MethodName
and O
convolutional O
layers O
use O
L2 O
regularization O
with O
the O
rate O
of O
110 6and O
the O
trainable O
embeddings O
use O
L2 O
regularization O
with O
the O
rate O
of O
110 5 O
. O

We O
include O
Glove O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
a O
purely O
distributional O
model O
similar O
to O
word2vec O
; O
two O
embeddings O
making O
use O
of O
external O
semantic O
knowledge O
: O
dict2vec O
( O
Tissier O
et O
al O
. O
, O
2017 O
) O
and O
ConceptNet O
Numberbatch O
( O
Speer O
et O
al O
. O
, O
2017 O
) O
; O
and O
the O
static O
token O
embeddings O
of O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
. O

Within O
broad O
relation O
types O
, O
we O
see O
similar O
results O
across O
embeddings O
, O
with O
a O
few O
exceptions O
( O
the O
static O
GPT-2 B-MethodName
embeddings O
are O
poor O
throughout O
, O
but O
much O
better O
than O
the O
others O
on O
derivational O
morphology O
) O
. O

Here O
, O
in O
line O
with O
that O
tradition O
, O
we O
explore O
how O
recurrent O
neural B-MethodName
networks I-MethodName
acquire O
the O
complex O
German O
plural O
system O
and O
reﬂect O
upon O
how O
their O
strategy O
compares O
to O
human O
generalisation O
and O
rule O
- O
based O
models O
of O
this O
system O
. O

In O
that O
vein O
, O
we O
present O
a O
detailed O
examination O
of O
how O
recurrent O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
process O
the O
complex O
German O
plural O
system O
which O
– O
contrary O
to O
the O
classical O
example O
of O
English O
past O
tense O
– O
features O
generalisation O
of O
multiple O
classes O
( O
Mar O
- O
cus O
et O
al O
. O
, O
1995 O
; O
McCurdy O
et O
al O
. O
, O
2020 O
; O
Belth O
et O
al O
. O
, O
2021 O
) O
. O

The O
encoder O
and O
decoder O
consist O
of O
two O
- O
layer O
unidirectional O
LSTMs B-MethodName
, O
a O
hidden O
dimensionality O
of O
128 O
, O
character O
embeddings O
of O
size O
128 O
and O
a O
dropout B-HyperparameterName
of O
0.1 B-HyperparameterValue
between O
layers O
. O

Rule O
- O
based O
comparison O
Belth O
et O
al O
. O
( O
2021 O
) O
propose O
a O
cognitive O
model O
of O
morphological O
learning O
which O
uses O
recursive O
application O
of O
the O
frequency O
threshold B-MetricName
deﬁned O
by O
the O
Tolerance O
Principle O
( O
Yang O
, O
2016 B-MetricValue
) O
to O
identify O
productive O
rules O
, O
resulting O
in O
a O
decision B-MethodName
tree I-MethodName
. O

The O
model O
checks O
at O
each O
node O
whether O
to O
keep O
traversing O
the O
tree O
, O
apply O
a O
learnt O
rule O
, O
or O
match O
the O
input O
form O
to O
a O
stored O
exception O
. O
For O
German O
plural O
inﬂection O
, O
their O
model O
relies O
upon O
grammatical O
gender O
and O
the O
last O
few O
characters O
of O
the O
input O
word O
as O
features O
in O
the O
decision B-MethodName
tree I-MethodName
. O

The O
RNNs O
outperform O
the O
rule O
- O
based O
model O
of O
Belth O
et O
al O
. O
( O
2021 O
) O
on O
unseen O
data.6Figure O
2a O
shows O
the O
RNNs O
’ O
training O
curve O
per O
plural O
class O
, O
with O
all O
classes O
undergoing O
rapid O
5We O
make O
examples O
of O
these O
decision B-MethodName
trees I-MethodName
available O
here O
. O

The O
hidden O
state O
and O
memory O
cell O
state O
from O
the O
ultimate O
time O
step O
of O
the O
encoder O
form O
the O
initialisation O
of O
the O
decoding O
LSTM B-MethodName
. O

Morphological O
inﬂection O
in O
neural B-MethodName
networks I-MethodName
Recently O
, O
others O
have O
explored O
the O
potential O
linguistic O
and O
cognitive O
implications O
of O
morphological O
generalisation O
in O
neural B-MethodName
networks I-MethodName
. O

In O
this O
discussion O
, O
neural B-MethodName
networks I-MethodName
are O
traditionally O
considered O
as O
an O
alternative O
to O
the O
explicit O
representation O
of O
rules O
. O

This O
raises O
the O
question O
of O
what O
kind O
of O
solution O
is O
implemented O
by O
neural B-MethodName
networks I-MethodName
to O
process O
language O
in O
seemingly O
rule O
- O
governed O
domains O
, O
how O
these O
solutions O
relate O
to O
rule O
- O
based O
models O
, O
and O
what O
it O
teaches O
us O
about O
human O
processing O
of O
inﬂectional O
morphology O
. O

Future O
work O
could O
address O
the O
broader O
questions O
raised O
by O
these O
ﬁndings O
, O
such O
as O
what O
constitutes O
a O
rule O
given O
overlap O
in O
strategy O
between O
neural B-MethodName
and I-MethodName
rule O
- O
based O
models O
, O
and O
how O
a O
mechanistic O
understanding O
of O
how O
neural B-MethodName
networks I-MethodName
approach O
seemingly O
rule O
- O
governed O
domains O
might O
contribute O
to O
understanding O
how O
such O
generalisation O
is O
instantiated O
in O
the O
human O
brain O
. O

We O
apply O
this O
method O
to O
study O
how O
BERT B-MethodName
models O
of O
different O
sizes O
process O
relative O
clauses O
( O
RCs O
) O
. O

We O
ﬁnd O
that O
BERT B-MethodName
variants O
use O
RC O
boundary O
information O
during O
word O
prediction O
in O
a O
manner O
that O
is O
consistent O
with O
the O
rules O
of O
English O
grammar O
; O
this O
RC O
boundary O
information O
generalizes O
to O
a O
considerable O
extent O
across O
different O
RC O
types O
, O
suggesting O
that O
BERT B-MethodName
represents O
RCs O
as O
an O
abstract O
linguistic O
category O
. O

We O
report O
experiments O
applying O
this O
logic O
to O
BERT B-MethodName
variants O
of O
different O
sizes O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Turc O
et O
al O
. O
, O
2019 O
) O
. O

We O
found O
that O
while O
all O
layers O
of O
the O
BERT B-MethodName
variants O
encoded O
information O
about O
RC O
boundaries O
, O
only O
the O
information O
encoded O
in O
the O
middle O
layers O
was O
used O
in O
a O
manner O
consistent O
with O
the O
grammar O
of O
English O
. O

For O
BERT B-MethodName
- O
base O
, O
we O
also O
found O
that O
counterfactual O
representations O
learned O
solely O
from O
one O
type O
of O
RC O
inﬂuenced O
the O
model O
’s O
predictions O
in O
sentences O
containing O
other O
RC O
types O
, O
suggesting O
that O
this O
model O
encodes O
information O
about O
RC O
boundaries O
in O
an O
abstract O
manner O
that O
generalizes O
across O
different O
RC O
types O
. O

Going O
beyond O
our O
case O
study O
of O
RC O
representations O
in O
BERT B-MethodName
variants O
, O
we O
hope O
that O
future O
work O
can O
apply O
this O
method O
to O
test O
linguistically O
motivated O
hypotheses O
about O
a O
wide O
range O
of O
structures O
, O
tasks O
and O
models O
. O

4.1 O
Models O
We O
use O
BERT B-MethodName
- O
base O
( O
12 O
layers,768 O
hidden B-HyperparameterName
units I-HyperparameterName
) O
and O
BERT B-MethodName
- O
large O
( O
24 B-HyperparameterValue
layers O
, O
1024 O
hidden B-HyperparameterName
units I-HyperparameterName
) O
( O
Devlin O
et O
al O
. O
, O
2019 B-HyperparameterValue
) O
, O
as O
well O
as O
the O
smaller O
BERT B-MethodName
models O
released O
by O
Turc O
et O
al O
. O
( O
2019 O
): O
BERT B-MethodName
- O
medium O
( O
8 O
layers512 O
hidden B-HyperparameterName
units I-HyperparameterName
) O
, O
BERT B-MethodName
- O
small O
( O
4 B-HyperparameterValue
layers O
, O
512 O
hidden B-HyperparameterName
units I-HyperparameterName
) O
, O
BERT B-MethodName
- O
mini O
( O
4 O
256 O
) O
, O
and O
BERT B-MethodName
- O
tiny O
( O
2128 O
) O
. O

Identifying O
and O
Altering O
RC O
Subspaces O
To O
identify O
RC O
subspaces O
, O
we O
used O
INLP O
with O
SVM B-MethodName
classiﬁers O
as O
implemented O
in O
scikit O
- O
learn O
. O

6In O
particular O
, O
running O
INLP O
for O
768 O
iterations O
— O
the O
dimensionality O
of O
BERT B-MethodName
representations O
— O
yields O
the O
original O
space O
, O
which O
is O
exhaustive O
but O
not O
useful O
in O
distilling O
RC O
information.4.3 O
Measuring O
the O
Effect O
of O
the O
Intervention O
on O
Agreement O
Accuracy O
Dataset O
We O
measure O
the O
models O
’ O
agreement O
prediction O
accuracy B-MetricName
using O
a O
subset O
of O
the O
Marvin O
and O
Linzen O
( O
2018 B-MetricValue
) O
dataset O
in O
which O
the O
subject O
is O
modiﬁed O
by O
an O
RC O
. O

In O
each O
sentence O
, O
we O
masked O
the O
copula O
, O
started O
the O
forward O
pass O
, O
performed O
the O
intervention O
on O
the O
representation O
of O
the O
masked O
copula O
in O
the O
layer O
of O
interest O
, O
and O
continued O
with O
the O
forward O
pass O
to O
obtain O
BERT B-MethodName
’s O
distribution O
over O
the O
vocabulary O
for O
the O
masked O
token O
. O

6 O
Results O
Counterfactual O
Intervention O
in O
the O
Middle O
Layers O
of O
BERT B-MethodName
- O
base O
Modulates O
Agreement O
Error O
Rate O
in O
RC O
Sentences O
with O
Attractors O
. O

Interventions O
using O
counterfactual O
representations O
generated O
from O
the O
middle O
layers O
of O
the O
BERT B-MethodName
- O
base O
( O
5–8 O
out O
of O
12 O
) O
resulted O
in O
changes O
in O
the O
probability O
of O
error O
which O
partially O
aligned O
with O
Prediction O
1 O
( O
Figure O
3a O
) O
. O

This O
suggests O
that O
while O
BERT B-MethodName
’s O
representation O
of O
RC O
boundaries O
is O
partly O
shared O
across O
different O
RC O
types O
, O
there O
are O
also O
structure O
- O
speciﬁc O
RC O
boundary O
representations O
. O

This O
suggests O
that O
the O
change O
in O
probability O
of O
error O
that O
resulted O
from O
intervening O
with O
RC O
subspaces O
was O
not O
merely O
a O
by O
- O
product O
of O
intervening O
on O
a O
large O
enough O
subspace O
of O
BERT B-MethodName
’s O
original O
representation O
space O
. O

Intervening O
on O
the O
Middle O
Layers O
of O
Other O
BERT B-MethodName
Variants O
Yielded O
Qualitatively O
Similar O
Results O
. O

We O
repeated O
the O
experiments O
on O
BERTlarge B-MethodName
and O
four O
smaller O
versions O
of O
BERT B-MethodName
, O
trained O
on O
the O
same O
amount O
of O
data O
as O
the O
BERT B-MethodName
- O
base O
model O
( O
Turc O
et O
al O
. O
, O
2019 O
) O
. O

As O
with O
BERT B-MethodName
- O
base O
, O
intervening O
on O
the O
middle O
layers O
of O
BERT B-MethodName
- O
large O
( O
12–17 O
out O
of O
24 O
) O
with O
the O
RC O
subspaces O
— O
but O
not O
the O
random O
subspaces O
— O
resulted O
in O
predicted O
changes O
in O
the O
probability O
of O
error O
. O

Compared O
to O
BERT B-MethodName
- O
base O
, O
the O
smaller O
models O
showed O
a O
greater O
change O
in O
the O
probability O
of O
error O
as O
a O
result O
of O
intervention O
with O
counterfactuals O
generated O
from O
random O
subspaces O
. O

However O
, O
when O
the O
counterfactual O
representations O
were O
generated O
from O
particular O
layers—4 O
and O
5 O
( O
out O
of O
8) O
in O
BERT B-MethodName
- O
medium O
, O
3 O
( O
out O
of O
4 O
) O
in O
BERTmini B-MethodName
and O
2 O
( O
out O
of O
4 O
) O
in O
BERT B-MethodName
- O
small O
— O
the O
change O
in O
error O
probability O
aligned O
with O
Prediction O
1 O
over O
and O
above O
the O
changes O
from O
intervening O
with O
random O
subspaces O
. O

No O
such O
layer O
was O
observed O
for O
BERT B-MethodName
- O
tiny O
, O
which O
has O
only O
2 O
layers O
( O
see O
Figure O
4 O
) O
. O

As O
a O
case O
study O
, O
we O
applied O
this O
method O
to O
study O
whether O
altering O
the O
information O
encoded O
about O
RC O
boundaries O
in O
the O
contextual O
representations O
of O
masked O
verbs O
in O
different O
BERT B-MethodName
variants O
inﬂuences O
the O
verb O
’s O
number O
inﬂection O
in O
a O
manner O
that O
is O
consistent O
with O
the O
grammar O
of O
English O
. O

We O
found O
that O
while O
all O
layers O
of O
the O
BERT B-MethodName
variants O
encoded O
information O
about O
RC O
boundaries O
, O
only O
the O
information O
in O
the O
middle O
layers O
inﬂuenced O
the O
masked O
verb O
’s O
number O
inﬂection O
as O
predicted O
by O
English O
grammar O
. O

We O
also O
found O
that O
in O
BERTbase B-MethodName
, O
counterfactual O
representations O
based O
on O
subspaces O
that O
were O
learned O
from O
sentences O
with O
one O
type O
of O
RC O
inﬂuenced O
the O
number O
inﬂection O
of O
the O
masked O
verb O
in O
sentences O
with O
other O
types O
of O
RCs O
; O
this O
suggests O
that O
the O
model O
encodes O
information O
about O
RC O
boundaries O
in O
an O
abstract O
manner O
that O
generalizes O
across O
the O
different O
RC O
types O
. O

Caveat O
: O
Linear O
Analysis O
of O
a O
Non O
- O
linear O
Network O
AlterRep O
interventions O
are O
based O
on O
concept O
subspaces O
identiﬁed O
using O
linear O
classiﬁers O
, O
but O
most O
neural B-MethodName
networks I-MethodName
components O
, O
including O
BERT B-MethodName
layers O
, O
are O
non O
- O
linear O
. O

Applying O
this O
method O
to O
BERT B-MethodName
, O
we O
found O
that O
the O
model O
uses O
information O
about O
RC O
boundaries O
that O
is O
encoded O
in O
its O
word O
representations O
when O
inﬂecting O
the O
number O
of O
masked O
verb O
in O
a O
manner O
consistent O
with O
the O
grammar O
of O
English O
. O

Andor O
et O
al O
. O
( O
2016 O
) O
also O
created O
a O
transition O
- O
based O
dependency O
parser O
based O
on O
neural B-MethodName
networks I-MethodName
and O
word O
embeddings O
after O
the O
Chen O
and O
Manning O
( O
2014 O
) O
work O
. O

They O
proposed O
to O
use O
global O
instead O
of O
local O
model O
normalizations O
to O
overcome O
label O
bias O
problem O
with O
feed O
forward O
neural B-MethodName
networks I-MethodName
. O

CCG O
category O
( O
S[dcl O
] O
\NP)/(S[b O
] O
\NP O
) O
Without O
subcategories O
( O
S\NP)/(S O
\NP O
) O
Without O
directions O
( O
S O
- O
NP)-(S O
- O
NP O
) O
Table O
1 O
: O
Category O
simpliﬁcation O
for O
supertags O
A O
logistic B-MethodName
regression I-MethodName
classiﬁer O
is O
trained O
to O
label O
the O
tokens O
in O
training O
and O
development O
sets O
using O
Scikit O
- O
Learn O
( O
Pedregosa O
et O
al O
. O
, O
2011 O
) O
. O

For O
word O
embedding O
ﬁle O
, O
GloVe B-MethodName
50 O
dimensional O
data O
is O
used O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

Other O
than O
these O
, O
we O
will O
experiment O
over O
neural B-MethodName
networks I-MethodName
integrated O
into O
graphbased O
dependency O
parsers O
. O

Such O
adversarial O
examples O
have O
been O
derived O
from O
the O
MultiNLI O
( O
MNLI B-MethodName
) O
dataset O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
a O
large O
English O
dataset O
with O
NLI O
examples O
from O
multiple O
genres O
. O

Naik O
et O
al O
. O
( O
2018 O
) O
automatically O
build O
such O
1The O
datasets O
are O
available O
at O
https://github.com/ O
mahartmann O
/ O
negationminpairs O
.adversarial O
examples O
by O
adding O
a O
tautology O
( O
and O
false O
is O
not O
true O
) O
to O
the O
end O
of O
every O
hypothesis O
in O
the O
MNLI B-MethodName
data O
, O
leading O
to O
a O
drop O
in O
model O
accuracies B-MetricName
caused O
by O
a O
large O
amount O
of O
false O
positives O
( O
FPs O
) O
for O
the O
neutral O
class O
, O
rather O
than O
an O
expected O
increase O
in O
FPs O
for O
the O
contradiction O
class O
. O

On O
their O
challenge O
dataset O
for O
non O
- O
entailed O
subsequences O
based O
on O
the O
MNLI B-MethodName
data O
, O
McCoy O
and O
Linzen O
( O
2019 O
) O
ﬁnd O
that O
models O
exploit O
a O
mismatch O
between O
negation O
cues O
in O
premise O
and O
hypothesis O
to O
predict O
non O
- O
entailment O
, O
and O
that O
removing O
unimportant O
negation O
cues O
decreases O
model O
accuracy B-MetricName
to O
almost O
0 B-MetricValue
. O

Kim O
et O
al O
. O
( O
2019 O
) O
build O
an O
NLI O
dataset O
based O
on O
MNLI B-MethodName
premise O
- O
hypothesis O
pairs O
that O
contain O
antonyms O
. O

Hossain O
et O
al O
. O
( O
2020b O
) O
create O
a O
dataset O
containing O
challenging O
negations O
by O
adding O
the O
syntactic O
negation O
cue O
notto O
the O
main O
verb O
of O
the O
premise O
and/or O
the O
hypothesis O
of O
MNLI B-MethodName
training O
examples O
. O

They O
ﬁnd O
that O
their O
newly O
created O
examples O
are O
more O
challenging O
for O
the O
model O
than O
the O
original O
negated O
examples O
. O
Authors O
Task O
Base O
Dataset O
Data O
Creation O
Langs O
Ettinger O
( O
2020 O
) O
MLM O
( O
cloze O
) O
Psycholinguistic O
stimuli O
- O
en O
Kassner O
and O
Schütze O
( O
2020 O
) O
MLM O
( O
cloze O
) O
- O
Template O
ﬁlling O
en O
Naik O
et O
al O
. O
( O
2018 O
) O
NLI O
MNLI B-MethodName
Adding O
tautology O
en O
Kim O
et O
al O
. O
( O
2019 O
) O
NLI O
MNLI B-MethodName
Inserting O
/ O
removing O
negation O
, O
swapping O
antonyms O
en O
Hossain O
et O
al O
. O
( O
2020b O
) O
NLI O
MNLI B-MethodName
Inserting O
negation O
en O
Richardson O
et O
al O
. O
( O
2020 O
) O
NLI O
- O
Template O
ﬁlling O
en O
Geiger O
et O
al O
. O
( O
2020 O
) O
NLI O
SNLI O
Replacing O
single O
words O
en O
Ours O
NLI O
XNLI B-MetricName
Removing O
negation O
to O
build O
minimal O
pairs O
en O
, O
bg O
, O
de O
, O
fr O
, O
zh O
Table O
2 B-MetricValue
: O
Our O
approach O
( O
bottom O
line O
) O
in O
comparison O
to O
related O
work O
on O
diagnostic O
datasets O
involving O
negation O
. O

Geiger O
et O
al O
. O
( O
2020 O
) O
investigate O
if O
models O
can O
learn O
interactions O
between O
lexical O
entailment O
and O
negation O
, O
in O
particular O
the O
algorithm O
behind O
downward O
monotonicity O
( O
e.g. O
, O
dance O
entails O
move O
, O
and O
not O
move O
entails O
not O
dance O
) O
, O
and O
ﬁnd O
that O
models O
can O
not O
solve O
the O
task O
when O
ﬁne O
- O
tuned O
on O
MNLI B-MethodName
, O
but O
when O
ﬁne O
- O
tuned O
on O
the O
challenge O
dataset O
. O

Again O
, O
models O
ﬁne O
- O
tuned O
on O
the O
standard O
MNLI B-MethodName
data O
perform O
poorly O
, O
but O
improve O
when O
ﬁne O
- O
tuned O
on O
the O
target O
dataset O
. O

The O
English O
part O
was O
collected O
following O
the O
same O
setup O
as O
for O
the O
MNLI B-MethodName
dataset O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
and O
the O
non O
- O
English O
parts O
are O
manual O
translations O
of O
the O
English O
data O
. O

We O
map O
their O
modiﬁed O
examples O
back O
to O
their O
original O
counterparts O
, O
which O
come O
from O
the O
English O
training O
split O
of O
the O
MNLI B-MethodName
dataset O
. O

en O
fr O
de O
bg O
zh O
original O
0.36 O
0.54 O
0.39 O
0.45 O
0.42 O
modiﬁed O
0.37 O
0.54 O
0.40 O
0.45 O
0.43 O
Table O
5 O
: O
Probability O
score B-MetricName
for O
original O
and O
modiﬁed O
data O
measured O
with O
mBERT B-MethodName
. O

In O
order O
to O
rule O
this O
out O
as O
a O
possible O
factor O
for O
performance O
drop O
on O
the O
modiﬁed O
examples O
, O
we O
compare O
the O
probability O
mBERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
assigns O
to O
utterances O
in O
the O
original O
data O
( O
M  O
) O
and O
in O
the O
newly O
created O
data O
( O
M O
O O
) O
. O

5 O
Probing O
Negation O
- O
Awareness O
We O
now O
use O
our O
new O
multilingual O
benchmark O
to O
probe O
the O
negation O
awareness O
of O
a O
multilingual O
language O
model O
ﬁne O
- O
tuned O
for O
the O
NLI O
task O
. O
Multilingual O
language O
model O
for O
NLI O
We O
ﬁnetune O
the O
cased O
version O
of O
mBERT B-MethodName
on O
the O
English O
MNLI B-MethodName
training O
data O
using O
the O
standard O
sequence O
classiﬁcation O
approach O
for O
sentence O
pairs O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
follow O
Hossain O
et O
al O
. O
( O
2020b O
) O
in O
training O
the O
model O
for O
3 O
epochs B-HyperparameterName
, O
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
and O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
2e-5 O
. O

NLP O
- O
Cube O
: O
End O
- O
to O
- O
end O
raw O
text O
processing O
with O
neural B-MethodName
networks I-MethodName
. O

Based O
entirely O
on O
recurrent O
neural B-MethodName
networks I-MethodName
, O
written O
in O
Python O
, O
this O
ready O
- O
to O
- O
use O
open O
source O
system O
is O
freely O
available O
on O
GitHub1 O
. O

Written O
in O
Python O
, O
it O
is O
based O
entirely O
on O
recurrent O
neural B-MethodName
networks I-MethodName
built O
in O
DyNET O
( O
Neubig O
et O
al O
. O
, O
2017 O
) O
. O

We O
then O
concatenate O
the O
ﬁnal O
outputs O
from O
the O
second O
layer O
( O
top O
) O
forward O
and O
backward O
LSTM B-MethodName
with O
an O
attention O
vector O
( O
totaling O
400 O
values O
: O
100 O
from O
last O
fwd O
. O

Note O
, O
that O
we O
use O
fandb O
for O
the O
internal O
states O
of O
the O
LSTM B-MethodName
cells O
and O
that O
the O
missing O
superscript O
means B-MetricName
the O
variables O
refer O
to O
the O
output O
of O
the O
LSTM B-MethodName
cells O
. O

For O
every O
symbol O
( O
si O
) O
in O
the O
input O
text O
, O
the O
decision O
for O
tokenization O
or O
sentence O
splitting O
( O
after O
si O
) O
is O
generated O
using O
a O
softmax O
layer O
that O
takes O
as O
input O
4 O
distinct O
vectors O
( O
ﬁnal O
output O
states O
) O
of O
: O
1.Forward O
Network O
: O
A O
unidirectional O
LSTM B-MethodName
that O
sees O
the O
input O
symbol O
by O
symbol O
in O
natural O
order O
; O
2.Peek O
Network O
: O
A O
unidirectional O
LSTM B-MethodName
, O
that O
peeks O
at O
a O
limited O
window O
of O
symbols6 O
in O
front O
of O
the O
current O
symbol O
- O
the O
input O
is O
fed O
to O
the O
network O
in O
reverse O
order O
; O
3.Language O
Model O
( O
LM O
) O
Network O
: O
A O
unidirectional O
LSTM B-MethodName
that O
takes O
as O
input O
external O
word O
embeddings O
for O
previously O
genera6We O
set O
the O
value O
to O
5 O
based O
on O
empirical O
observationsted O
words O
; O
it O
updates O
only O
when O
a O
new O
word O
is O
predicted O
by O
the O
network O
; O
4.Partial O
Word O
Embeddings O
( O
PWE O
) O
Network O
: O
It O
is O
often O
the O
case O
that O
we O
are O
able O
to O
generate O
valid O
( O
known O
) O
words O
made O
up O
of O
symbols O
from O
the O
previously O
tokenized O
word O
up O
to O
the O
current O
symbol O
. O

The O
model O
is O
composed O
of O
a O
bidirectional O
LSTM B-MethodName
encoder O
and O
an O
unidirectional O
LSTM B-MethodName
decoder O
. O

2.4 O
Tagging O
Tagging O
is O
achieved O
using O
a O
two O
- O
layer O
bidirectional O
LSTM B-MethodName
( O
same O
size O
for O
all O
languages O
) O
. O

For O
regularization O
, O
we O
use O
an O
auxiliary O
layer O
of O
softmax O
functions O
( O
Szegedy O
et O
al O
. O
, O
2015 O
) O
, O
located O
after O
the O
ﬁrst O
bidirectional O
LSTM B-MethodName
layer O
. O

2.5 O
Parsing O
Our O
parser O
is O
inspired O
by O
Kiperwasser O
and O
Goldberg O
( O
2016 O
) O
and O
Dozat O
et O
al O
. O
( O
2017 O
) O
, O
in O
the O
sense O
that O
we O
use O
multiple O
stacked O
bidirectional O
LSTM B-MethodName
layers O
and O
project O
4 O
specialized O
representations O
for O
each O
word O
in O
a O
sentence O
, O
which O
are O
later O
aggregated O
in O
a O
multilayer O
perceptron O
in O
order O
to O
produce O
arc O
and O
label O
probabilities O
. O

Our O
parser O
architecture O
( O
Figure O
3 O
) O
is O
composed O
of O
5 O
layers O
of O
bidirectional O
LSTMs B-MethodName
( O
sized O
300 O
, O
300 O
, O
200 O
, O
200 O
, O
200 O
) O
. O

Our O
models O
are O
implemented O
using O
DyNET O
( O
Neubig O
et O
al O
. O
, O
2017 O
) O
, O
which O
is O
a O
framework O
for O
neural B-MethodName
networks I-MethodName
with O
dynamic O
computation O
graph O
. O

Whenever O
more O
than O
one O
aux O
9For O
the O
tokenizer O
we O
even O
drop O
entire O
LSTM B-MethodName
- O
outputs O
that O
represent O
the O
input O
of O
the O
ﬁnal O
Softmax O
layer O
- O
but O
we O
still O
infer O
loss O
via O
the O
auxiliary O
softmaxessoftmax O
layers O
are O
used O
, O
the O
weighed O
value O
is O
equally O
divided O
between O
the O
losses O
( O
i.e. O
if O
we O
use O
two O
auxiliary O
loss O
layers O
, O
each O
will O
infer O
a O
loss O
that O
is O
scaled O
with O
the O
value O
0.1 O
, O
not O
0.2 O
) O
. O

Except O
the O
input O
sizes O
( O
like O
the O
300 O
- O
to-100 O
linear O
transform O
in O
the O
tokenizer O
) O
, O
all O
other O
LSTM B-MethodName
sizes O
are O
conﬁgurable O
through O
the O
automatically O
generated O
conﬁg O
ﬁle O
for O
each O
task O
. O

Part O
of O
NLP O
- O
Cube O
, O
we O
have O
a O
Named O
Entity O
Recognition O
( O
NER B-TaskName
) O
system12that O
employs O
GraphBased O
- O
Decoding O
( O
GBD O
) O
over O
a O
hybrid O
network O
architecture O
composed O
of O
bidirectional O
LSTMs B-MethodName
for O
word O
- O
level O
encoding O
, O
which O
had O
great O
results13 O
. O

We O
ﬁnd O
that O
both O
generative O
models O
improve O
parsing O
performance O
over O
a O
discriminative O
baseline O
, O
but O
, O
in O
contrast O
to O
RNNGs O
, O
they O
are O
signiﬁcantly O
less O
effective O
than O
non O
- O
syntactic O
LSTM B-MethodName
language O
models O
. O

However O
, O
in O
our O
limited O
, O
ﬁnite O
, O
and O
imperfect O
world O
, O
these O
two O
models O
will O
impose O
different O
biases O
on O
the O
learner O
: O
in O
one O
order O
, O
relevant O
conditioning O
information O
may O
be O
more O
local O
( O
which O
could O
mean B-MetricName
the O
neural B-MethodName
networks I-MethodName
have O
an O
easier O
time O
learning O
to O
exploit O
the O
relevant O
information O
rather O
than O
becom1We O
release O
code O
for O
these O
two O
models O
, O
which O
can O
be O
found O
at O
https://github.com/armatthews/ O
dependency O
- O
lm O
. O

Second O
, O
we O
ﬁnd O
that O
both O
dependency O
models O
are O
less O
effective O
as O
language O
models O
than O
phrase O
structure O
RNNGs O
or O
than O
standard O
LSTM B-MethodName
language O
models O
. O

Both O
models O
both O
are O
parameterized O
with O
recursively O
structured O
neural B-MethodName
networks I-MethodName
that O
have O
access O
to O
the O
complete O
history O
of O
generation O
events O
. O

At O
each O
decision O
point O
the O
model O
conditions O
on O
the O
output O
of O
an O
LSTM B-MethodName
over O
the O
partially O
completed O
constituents O
on O
the O
stack O
, O
beginning O
with O
the O
root O
and O
ending O
with O
the O
top O
- O
most O
constituent O
. O

In O
this O
way O
it O
is O
though O
the O
root O
node O
has O
already O
generated O
its O
STOP O
-L O
, O
though O
this O
step O
is O
not O
explicitly O
modelledAlgorithm O
1 O
Top O
- O
Down O
Tree O
Generation O
1 O
: O
procedure O
EMBED B-MetricName
TREE(node O
) O
2 O
: O
state O
= O
lstm_initial_state O
3 O
: O
forchild O
innode O
do O
4 O
: O
ifchild O
is O
terminal O
then O
5 O
: O
state O
: O
add(WordEmbs O
[ O
child O
] O
) O
6 O
: O
else O
7 O
: O
state O
: O
add(EMBED B-MetricName
TREE(child O
) O
) O
8 O
: O
return O
state O
9 O
: O
procedure O
PICKNEXTACTION O
( O
stack O
) O
10 O
: O
h O
= O
MLP O
action(EmbedTree O
( O
stack O
) O
) O
11 O
: O
actionsoftmax O
( O
h O
) O
12 O
: O
return O
action O
13 O
: O
procedure O
PICKWORD(stack O
) O
14 O
: O
h O
= O
MLP O
word(EmbedTree O
( O
stack O
) O
) O
15 O
: O
wordsoftmax O
( O
h O
) O
16 O
: O
return O
word O
17 O
: O
procedure O
GENERATE B-TaskName
NODE(stack O
) O
18 O
: O
action O
= O
PICKNEXTACTION O
( O
stack O
) O
19 O
: O
ifaction O
= O
= O
GENthen O
20 O
: O
word O
= O
PICKWORD(STACK O
) O
21 O
: O
stack O
.push(new O
Node O
( O
word O
) O
) O
22 O
: O
else O
if O
action O
= O
= O
STOP O
-Lthen O
23 O
: O
stack O
: O
back():add_child O
( O
STOP O
-L O
) O
24 O
: O
else O
if O
action O
= O
= O
STOP O
-Rthen O
25 O
: O
stack O
: O
back():add_child O
( O
STOP O
-R O
) O
26 O
: O
child_emb O
= O
stack O
: O
pop O
( O
) O
27 O
: O
stack O
: O
back():add_child O
( O
child O
_ O
emb O
) O
To O
embed O
each O
subtree O
on O
the O
stack O
we O
use O
another O
LSTM B-MethodName
. O

We O
then O
additionally O
add O
a O
gated O
residual O
connection O
from O
the O
head O
word O
to O
ﬁnal O
subtree O
representation O
to O
allow O
salient O
information O
of O
the O
head O
word O
to O
be O
captured O
without O
needing O
to O
pass O
through O
an O
arbitrary O
number O
of O
LSTM B-MethodName
steps O
( O
Figure O
4 O
) O
. O

The O
LSTM B-MethodName
proceeds O
from O
the O
root O
of O
the O
tree O
down O
to O
the O
most O
recent O
open O
node O
. O

Each O
item O
in O
the O
LSTM B-MethodName
is O
an O
embedding O
of O
a O
word O
and O
its O
already O
generated O
descendants O
. O

At O
each O
time O
step O
the O
model O
conditions O
on O
the O
state O
of O
the O
stack O
using O
an O
LSTM B-MethodName
running O
over O
entries O
from O
oldest O
to O
newest O
. O

A O
subtree O
is O
embedded O
using O
an O
LSTM B-MethodName
over O
its O
child O
subtrees O
( O
solid O
lines O
) O
with O
a O
gated O
residual O
connection O
from O
the O
root O
word O
to O
the O
ﬁnal O
embedding O
( O
dotted O
lines O
) O
. O

parser O
of O
Dyer O
et O
al O
. O
( O
2015 O
) O
, O
a O
discriminative O
neural O
stack O
- O
LSTM B-MethodName
- O
based O
bottom O
- O
up O
parser O
, O
as O
our O
proposal O
distribution O
q(x;y)and O
compute O
the O
approximate O
marginal O
using O
N= O
1000 O
samples O
per O
sentence O
: O
p(x)1 O
NPN O
i=1p(x;y O
) O
q(x;y O
) O
. O

3.2 O
Baseline O
Models O
On O
the O
language O
modeling O
task O
we O
compare O
against O
a O
standard O
LSTM B-MethodName
- O
based O
language O
model O
baseline O
( O
Mikolov O
et O
al O
. O
, O
2010 O
) O
, O
using O
1024dimensional O
2 O
- O
layer O
LSTM B-MethodName
cells O
, O
and O
optimized O
using O
Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
. O

For O
the O
parsing O
task O
we O
compare O
against O
the O
discriminative O
parser O
of O
Dyer O
et O
al O
. O
( O
2015 O
) O
, O
a O
bottom O
- O
up O
transition O
- O
based O
parser O
that O
uses O
stackLSTMs B-MethodName
, O
as O
well O
as O
the O
overall O
top O
system O
( O
Dozat O
et O
al O
. O
, O
2017 O
) O
from O
the O
2017 O
CoNLL O
shared O
task O
on O
multilingual O
dependency O
parsing O
( O
Zeman O
et O
al O
. O
, O
2017 O
) O
. O

At O
each O
timestep O
, O
we O
use O
the O
output O
of O
an O
LSTM B-MethodName
over O
the O
stack O
to O
choose O
the O
next O
action O
, O
which O
is O
then O
executed O
to O
produce O
a O
new O
stack O
state O
. O

3.3 O
Hyperparameters O
All O
models O
use O
two O
- O
layer O
1024 O
- O
unit O
LSTMs B-MethodName
and O
1024 O
- O
dimensional O
word O
/ O
action O
embeddings O
. O

Dozat O
et O
al O
. O
( O
2017 O
) O
train O
a O
discriminative O
neural O
parser O
which O
uses O
a O
BiLSTM B-MethodName
to O
generate O
hidden O
representations O
of O
each O
word O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
) O
. O

More O
recently O
Dyer O
et O
al O
. O
( O
2015 O
) O
created O
a O
similar O
model O
based O
on O
stack O
LSTMs B-MethodName
. O

1 O
Introduction O
approaches O
based O
on O
human O
- O
designed O
features O
 O
( O
Lazaridou O
et O
al O
. O
, O
2013 O
; O
Zhang O
et O
al O
. O
, O
2015a O
) O
and O
 O
neural B-MethodName
networks I-MethodName
based O
on O
deep O
architectures O
( O
Lai O
 O
et O
al O
. O
, O
2015 O
; O
Yang O
et O
al O
. O
, O
2016 O
) O
. O

Some O
researchers O
incorporated O
knowledge O
 O
bases O
into O
traditional O
approaches O
( O
Feng O
et O
al O
. O
, O
2013 O
; O
 O
Wang O
et O
al O
. O
, O
2014 O
) O
or O
neural B-MethodName
networks I-MethodName
( O
Wang O
et O
al O
. O
, O
 O
2017 O
) O
to O
overcome O
these O
challenges O
. O

Specifically O
, O
it O
firstly O
uses O
 O
a O
bi O
- O
directional O
long O
short O
- O
term O
memory O
model O
 O
( O
BiLSTM B-MethodName
) O
to O
learn O
word O
representations O
and O
 O
capture O
local O
context O
in O
text O
. O

Based O
on O
documents O
’ O
hierarchical O
structure O
, O
it O
 O
performed O
attention O
mechanisms O
on O
word O
- O
level O
 O
and O
sentence O
- O
level O
representations O
extracted O
by O
 O
BiLSTMs B-MethodName
. O

3.1 O
Overall O
Architecture O
of O
the O
Model O
 O
Figure O
1 O
presents O
the O
CGCNN O
structure O
, O
 O
composing O
of O
five O
major O
components O
: O
( O
1 O
) O
a O
word O
 O
encoder O
layer O
based O
on O
BiLSTM B-MethodName
to O
learn O
word O
 O
representations O
in O
each O
short O
text O
, O
( O
2 O
) O
a O
clustering O
 O
layer O
that O
calculates O
words O
’ O
distributions O
and O
 O
performs O
a O
linear O
transformation O
to O
get O
clusterdependent O
text O
representations O
, O
( O
3 O
) O
a O
cluster O
- O
gated O
 O
convolutional O
layer O
that O
integrates O
cluster O
centers O
 O
into O
a O
gated O
CNN O
for O
further O
controlling O
clusterrelated O
feature O
flows O
, O
( O
4 O
) O
a O
max O
- O
pooling O
layer O
to O
 O
select O
most O
important O
features O
and O
concatenate O
 O
them O
as O
the O
final O
text O
features O
, O
and O
( O
5 O
) O
a O
fully O
 O
connected O
layer O
with O
softmax O
function O
for O
 O
classification O
. O

To O
capture O
the O
local O
context O
in O
text O
, O
we O
employ O
a O
 O
BiLSTM B-MethodName
to O
derive O
the O
forward O
representation O
𝑓௧ O
 O
and O
backward O
representation O
𝑏௧ O
. O

It O
can O
reduce O
the O
 O
role O
of O
words O
unrelated O
with O
the O
cluster O
, O
and O
 O
ensures O
the O
sum O
of O
all O
cluster O
- O
dependent O
word O
the O
network O
, O
which O
have O
been O
proven O
effective O
in O
 O
LSTM B-MethodName
and O
CNN O
( O
Dauphin O
et O
al O
. O
, O
2016 O
) O
. O

We O
also O
conduct O
 O
experiments O
with O
the O
model O
directly O
using O
word O
 O
embeddings O
instead O
of O
BiLSTM B-MethodName
, O
representing O
as O
 O
CGCNN O
* O
. O

CNN O
- O
LSTM B-MethodName
( O
Zhou O
et O
al O
. O
, O
2015 O
) O
. O

This O
method O
 O
uses O
a O
multi O
- O
channel O
convolutional O
layer O
to O
extract O
 O
higher O
- O
level O
phrase O
features O
, O
and O
employs O
a O
 O
BiLSTM B-MethodName
to O
capture O
their O
sequences O
for O
 O
classification O
. O

AttBiLSTM B-MethodName
( O
Lin O
et O
al O
. O
, O
2017 O
) O
. O

It O
uses O
a O
 O
BiLSTM B-MethodName
to O
explore O
the O
sequences O
of O
texts O
, O
and O
 O
develops O
a O
self O
- O
attention O
mechanism O
to O
get O
 O
sentence O
- O
level O
representations O
. O

For O
the O
multiple O
- O
channel O
convolutional O
 O
architecture O
of O
CNN O
and O
CNN O
- O
LSTM B-MethodName
, O
the O
filter O
 O
sizes O
are O
3 O
, O
4 O
and O
5 O
, O
as O
Kim O
( O
2014 O
) O
’s O
default O
 O
settings O
. O

For O
the O
hidden O
vectors O
of O
BiLSTM B-MethodName
in O
 O
these O
methods O
, O
we O
also O
set O
their O
dimensions O
to O
200 O
. O

The O
CNN O
- O
LSTM B-MethodName
 O
outperforms O
the O
other O
baseline O
methods O
on O
AG O
 O
News O
, O
Amazon O
Review O
and O
Yahoo O
! O

As O
compared O
with O
CNN O
- O
LSTM B-MethodName
, O
 O
CGCNN O
has O
about O
1.5 O
% O
performance O
 O
improvements O
on O
Amazon O
Review O
and O
Yahoo O
! O

The O
AttBiLSTM B-MethodName
method O
has O
poor O
 O
performance O
. O

The O
CNN O
- O
LSTM B-MethodName
method O
uses O
CNN O
and O
 O
BiLSTM B-MethodName
to O
capture O
phrase O
features O
and O
their O
 O
sequences O
, O
outperforms O
CNN O
and O
CNNM O
on O
AG O
 O
News O
, O
Amazon B-DatasetName
Reviews I-DatasetName
and O
Yahoo O
! O

It O
employs O
a O
 O
BiLSTM B-MethodName
to O
learn O
word O
representations O
for O
local O
 O
contexts O
in O
short O
texts O
. O

Our O
model O
is O
modiﬁed O
fromTUPA O
( O
Transition O
- O
based O
UCCA O
Parser O
) O
( O
Hershcovich O
et O
al O
. O
, O
2017 O
, O
2018 O
) O
in O
terms O
of O
neural B-MethodName
networks I-MethodName
, O
which O
is O
powerful O
in O
a O
lot O
of O
NLP O
tasks O
( O
Cai O
and O
Zhao O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2016 O
; O
Qin O
et O
al O
. O
, O
2016 O
; O
Vaswani O
et O
al O
. O
, O
2017 O
; O
Cai O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2017 O
; O
Qin O
et O
al O
. O
, O
2017 O
; O
Bai O
and O
Zhao O
, O
2018 O
; O
He O
et O
al O
. O
, O
2018 O
; O
Cai O
et O
al O
. O
, O
2018 O
; O
Zhang O
and O
Zhao O
, O
2018 O
; O
Zhang O
et O
al O
. O
, O
2018a O
, O
b O
; O
Zhu O
et O
al O
. O
, O
2018 O
; O
Huang O
and O
Zhao O
, O
2018 O
; O
Li O
et O
al O
. O
, O
2018c O
; O
Wu O
et O
al O
. O
, O
2018 O
; O
Zhang O
et O
al O
. O
, O
2019 O
; O
Xiao O
et O
al O
. O
, O
2019 O
) O
. O

The O
model O
uses O
a O
bi O
- O
directional O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
to O
encode O
the O
sentence O
and O
a O
multi O
- O
layer O
perceptron O
( O
MLP O
) O
with O
a O
softmax O
layer O
for O
classiﬁcation O
. O

Following O
Hershcovich O
et O
al O
. O
( O
2018 O
) O
, O
in O
the O
model O
, O
we O
have O
shared O
embedding O
components O
and O
a O
shared O
LSTM B-MethodName
module O
, O
and O
for O
each O
framework O
, O
we O
have O
a O
task O
- O
speciﬁed O
LSTM B-MethodName
module O
and O
a O
corresponding O
classiﬁer O
. O

For O
each O
framework O
, O
the O
outputs O
of O
shared O
LSTM B-MethodName
and O
task O
- O
speciﬁed O
LSTM B-MethodName
are O
concatenated O
and O
fed O
into O
the O
task O
- O
speciﬁed O
classiﬁer O
for O
action O
prediction O
. O

For O
the O
word O
embeddings O
, O
we O
use O
the O
pre O
- O
trained O
GloVe B-MethodName
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
the O
pre O
- O
trained O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

These O
embeddings O
together O
with O
token O
embeddings O
and O
BERT B-MethodName
outputsare O
concatenated O
and O
sent O
to O
the O
BiLSTMs B-MethodName
as O
input O
. O

The O
ﬁnal O
hidden O
state O
vectors O
of O
shared O
and O
speciﬁc O
BiLSTMs B-MethodName
and O
the O
feature O
vector O
of O
the O
state O
are O
concatenated O
and O
fed O
as O
input O
to O
the O
action O
classiﬁers O
. O

The O
classiﬁer O
is O
an O
MLP O
and O
the O
input O
is O
the O
concatenated O
output O
vectors O
of O
each O
token O
from O
shared O
and O
speciﬁc O
BiLSTM B-MethodName
since O
the O
nodes O
are O
one O
- O
to O
- O
one O
corresponding O
to O
the O
tokens O
in O
the O
sentence O
. O

The O
dropout B-HyperparameterName
is O
applied O
to O
the O
embeddings O
, O
the O
outputs O
of O
BiLSTMs B-MethodName
, O
and O
the O
outputs O
of O
the O
ﬁrst O
MLP O
lay1https://pytorch.org/ers O
. O

For O
example O
, O
UCCA O
only O
generates O
a O
node O
when O
an O
unprocessed O
edge O
is O
met O
and O
the O
node O
is O
on O
it O
, O
and O
UCCA O
has O
separate O
actions O
to O
predictHyperparameter O
Value O
Max O
sentence O
length O
100 O
GloVe B-MethodName
embedding O
dim O
300 O
BERT B-MethodName
output O
dim O
1024 O
Lemma O
embedding O
dim O
200 O
POS B-TaskName
- O
tag O
embedding O
dim O
20 O
Dependency O
embedding O
dim O
20 O
Other O
feature O
dim O
10 O
BiLSTM B-MethodName
layers O
2 O
BiLSTM B-MethodName
dim O
300 O
MLP O
layers O
2 O
MLP O
dim O
50 O
Dropout O
0.2 O
Optimizer O
Adam O
Learning O
rate O
0.001 O
Adam O
 O
1 O
0.9 O
Adam O
 O
2 O
0.999 O
Table O
6 O
: O
Model O
hyperparameters O
. O

Secondly O
, O
we O
apply O
the O
best O
publicly O
available O
word O
embeddings O
for O
German O
and O
use O
them O
alongside O
our O
BIOﬁd O
dataset O
as O
an O
input O
for O
training O
high O
- O
performing O
neural O
mod1Biodiversity O
is O
the O
science O
which O
measures O
the O
variability O
and O
diversity O
of O
animals O
and O
plants.els O
for O
NER B-TaskName
, O
namely O
BiLSTM B-MethodName
, O
ELMo B-MethodName
, O
Flair O
and O
BERT B-MethodName
( O
Ahmed O
and O
Mehler O
, O
2018 O
; O
Peters O
et O
al O
. O
, O
2018 O
; O
Akbik O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

By O
using O
the O
optimized O
BiLSTM B-MethodName
model O
we O
achieve O
a O
new O
best O
F O
- O
score B-MetricName
of O
80.23 B-MetricValue
% O
regarding O
the O
recognition O
of O
taxonomic O
entities O
. O

In O
the O
same O
year O
, O
with O
the O
emergence O
of O
multilingual O
language O
models O
such O
as O
ELMo B-MethodName
, O
Flair O
and O
BERT B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Akbik O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
the O
performance O
of O
various O
NLP O
tasks O
, O
including O
NER B-TaskName
, O
was O
notably O
improved O
. O

Hence O
, O
for O
this O
task O
we O
included O
the O
LSTM B-MethodName
- O
based O
sentence O
boundary O
detector O
DeepEOS O
( O
Schweter O
and O
Ahmed O
, O
2019 O
) O
in O
our O
prepro2An O
example O
of O
such O
pages O
is O
given O
in O
Appendix O
C.cessing O
pipeline O
and O
trained O
it O
with O
1,361 O
sentences O
, O
which O
were O
manually O
extracted O
from O
the O
BIOﬁd O
corpus O
. O

As O
the O
recent O
work O
of O
Ahmed O
an O
Mehler O
( O
2018 O
) O
has O
shown O
, O
discarding O
subtle O
details O
is O
even O
beneﬁcial O
, O
whereas O
ﬁne O
- O
graded O
feature O
engineering O
for O
deep O
neural B-MethodName
networks I-MethodName
usually O
deteriorates O
the O
ﬁnal O
performance O
. O

Word O
Embeddings O
The O
language O
model O
of O
continuous O
space O
word O
representations O
( O
word2vec O
) O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
its O
variations O
by O
( O
Levy O
and O
Goldberg O
, O
2014 O
; O
Komninos O
and O
Manandhar O
, O
2016 O
) O
are O
the O
foundations O
of O
most O
ongoing O
research O
in O
NLP O
with O
neural B-MethodName
networks I-MethodName
. O

BiLSTM B-MethodName
We O
provide O
a O
brief O
overview O
of O
the O
conﬁgurations O
for O
the O
ﬁve O
neural O
models O
which O
we O
use O
throughout O
this O
paper O
. O

The O
model O
BiLSTMCRF B-MethodName
is O
similar O
to O
the O
one O
used O
in O
( O
Ahmed O
and O
Mehler O
, O
2018 O
) O
, O
which O
goes O
back O
to O
the O
work O
of O
( O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

The O
neural O
network O
consists O
of O
stacked O
LSTM B-MethodName
and O
CRF O
layers O
. O

Language O
Model O
Train O
Data O
BiLSTM B-MethodName
- O
a O
COW O
N O
/ O
A O
BIOﬁd O
Flair O
Wang2v O
. O

COW O
PCE O
BIOﬁd O
Flair O
ELMo B-MethodName
COW O
PCE+Leipzig O
BIOﬁd O
Flair O
BERT B-MethodName
COW O
PCE+BERT B-MethodName
- O
Base O
BIOﬁd O
BiLSTM B-MethodName
- O
b O
COW O
N O
/ O
A O
All O
Table O
3 O
: O
Overview O
of O
the O
model O
inputs O
. O

For O
BiLSTMb B-MethodName
we O
consider O
all O
merged O
training O
data O
( O
i.e. O
BIOﬁd O
+ O
GermEval O
+ O
CoNLL O
) O
Flair O
Wang2vec O
We O
further O
train O
a O
sequence O
labeling O
model O
using O
Flair10 O
. O

We O
build O
the O
model O
in O
9http://www.texttechnologylab.org/resources2018/ O
10http://github.com/zalandoresearch/flairthe O
same O
fashion O
as O
used O
by O
( O
Akbik O
et O
al O
. O
, O
2018 O
) O
following O
the O
guide O
given O
by O
the O
authors O
for O
the O
task O
” O
CoNLL-03 O
Named O
Entity O
Recognition O
( O
German O
) O
” O
, O
while O
keeping O
the O
pooled O
contextualized O
embeddings O
( O
PCE O
) O
and O
exchanging O
the O
GloVe B-MethodName
embeddings O
employed O
by O
the O
authors O
with O
Wang2vec O
embeddings O
trained O
on O
the O
COW O
corpus O
. O

Flair O
ELMo B-MethodName
In O
addition O
to O
the O
previous O
model O
, O
we O
train O
a O
Flair O
Sequence O
Tagging O
model O
by O
stacking O
an O
ELMo B-MethodName
embedding O
layer O
on O
top O
of O
the O
Flair O
Wang2vec O
model O
. O

The O
ELMo B-MethodName
embeddings O
were O
trained O
on O
a O
section O
of O
the O
Leipzig O
Corpora O
Collection O
( O
Goldhahn O
et O
al O
. O
, O
2012 O
) O
containing O
100,000 O
sentences O
from O
Wikipedia O
using O
default O
parameters O
. O

Flair O
BERT B-MethodName
Similarly O
, O
we O
added O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
to O
the O
Flair O
Wang2vec O
model O
. O

We O
used O
the O
recently O
published O
BERT B-MethodName
- O
Base O
, O
Multilingual O
Cased11pre O
- O
trained O
model O
for O
this O
purpose O
. O

For O
the O
optimized O
BiLSTM B-MethodName
Tagger O
, O
we O
achieve O
excellent O
results O
and O
establish O
a O
new O
state O
- O
of O
- O
the O
- O
art O
for O
the O
ﬁrst O
task O
of O
TR O
with O
80.23 O
% O
F O
- O
score B-MetricName
( O
see O
Table O
5 B-MetricValue
: O
BiLSTM B-MethodName
- O
a O
) O
. O

Therefore O
we O
omitted O
OTHER O
( O
3,143 O
sentences O
) O
from O
our O
further O
experiments O
which O
in O
turn O
increased O
the O
ﬁnal O
performance O
of O
NER.Model B-TaskName
Scores O
[ O
% O
] O
TAXON O
PERSON O
LOCATION O
ORGANIZATION O
TIME O
Overall O
Precision B-MetricName
77.42 B-MetricValue
58.92 O
85.05 O
N O
/ O
A O
70.67 O
77.49 O
Copious O
Recall B-MetricName
69.67 B-MetricValue
48.44 O
85.63 O
N O
/ O
A O
54.36 O
71.89 O
Nguyen O
( O
2019 O
) O
F1 B-MetricName
73.34 B-MetricValue
53.17 O
85.34 O
N O
/ O
A O
61.45 O
74.58 O
Precision B-MetricName
81.33 B-MetricValue
63.19 O
66.20 O
60.24 O
91.16 O
75.62 O
BiLSTM B-MethodName
- O
a O
Recall B-MetricName
79.16 B-MetricValue
77.45 O
57.35 O
67.57 O
88.16 O
74.98 O
F1 B-MetricName
80.23 B-MetricValue
69.60 O
61.46 O
63.69 O
89.63 O
75.30 O
Precision B-MetricName
75.94 B-MetricValue
61.25 O
67.58 O
61.64 O
90.59 O
73.58 O
Flair O
Wang2vec O
Recall B-MetricName
81.37 B-MetricValue
76.09 O
62.89 O
58.11 O
85.24 O
75.89 O
F1 B-MetricName
78.08 B-MetricValue
71.89 O
62.63 O
56.95 O
87.89 O
74.30 O
Precision B-MetricName
75.64 B-MetricValue
67.16 O
58.31 O
56.82 O
90.49 O
73.05 O
Flair O
ELMo B-MethodName
Recall B-MetricName
79.92 B-MetricValue
79.89 O
65.06 O
60.81 O
86.02 O
76.50 O
F1 B-MetricName
77.88 B-MetricValue
69.34 O
66.30 O
61.22 O
88.25 O
75.01 O
Precision B-MetricName
76.63 B-MetricValue
65.30 O
66.96 O
58.00 O
92.21 O
74.98 O
Flair O
BERT B-MethodName
Recall B-MetricName
77.38 B-MetricValue
81.02 O
61.89 O
58.00 O
90.33 O
76.22 O
F1 B-MetricName
77.01 B-MetricValue
72.31 O
64.32 O
58.00 O
91.26 O
75.59 O
Precision B-MetricName
80.45 B-MetricValue
88.61 O
72.72 O
81.21 O
87.63 O
79.35 O
BiLSTM B-MethodName
- O
b O
Recall B-MetricName
76.65 B-MetricValue
89.40 O
84.02 O
70.74 O
81.17 O
75.38 O
F1 B-MetricName
78.50 B-MetricValue
89.00 O
77.96 O
75.61 O
84.27 O
77.31 O
Table O
5 O
: O
Results O
for O
the O
task O
of O
German O
biological O
NER B-TaskName
with O
various O
neural B-MethodName
networks I-MethodName
models O
along O
the O
English O
baseline O
on O
the O
Copious O
dataset O
( O
T.H. O

With O
the O
popular O
deep O
language O
models O
Flair O
, O
ELMo B-MethodName
andBERT B-MethodName
, O
we O
interestingly O
stay O
below O
the O
performance O
of O
the O
BiLSTM B-MethodName
model O
( O
except O
for O
TIME O
) O
. O

Although O
we O
utilize O
the O
same O
pre O
- O
trained O
COW O
word O
embeddings O
for O
all O
models O
, O
we O
assume O
that O
the O
lower O
performance O
arises O
due O
to O
the O
language O
models O
themselves O
being O
trained O
on O
only O
a O
relatively O
small O
corpus O
( O
ELMo B-MethodName
: O
100,000 O
sentences O
) O
. O

However O
, O
for O
training O
ELMo B-MethodName
on O
larger O
corpora O
, O
such O
as O
the O
COW O
corpus O
, O
we O
would O
require O
many O
months O
of O
training O
time O
. O

For O
the O
pre O
- O
trained O
Flair O
and O
BERT B-MethodName
, O
we O
can O
only O
ﬁnetune O
the O
last O
tagging O
layer O
, O
not O
the O
whole O
language O
model O
itself O
. O

This O
stands O
in O
contrast O
to O
the O
BiLSTM B-MethodName
model O
which O
can O
be O
wholly O
targeted O
to O
our O
domain O
- O
speciﬁc O
training O
data O
. O

Data O
Merging O
for O
BiLSTM B-MethodName
Tagger O
For O
BiLSTM B-MethodName
- O
a O
, O
it O
can O
be O
noted O
that O
the O
performance O
of O
the O
standard O
categories O
PERSON O
, O
ORGANIZATION O
, O
and O
, O
especially O
LOCATION O
is O
inferior O
. O

Table O
5 O
: O
BiLSTM B-MethodName
- O
b O
shows O
the O
improvements O
in O
performance O
with O
the O
increased O
dataset O
. O

As O
for O
the O
baselines O
, O
we O
implemented O
two O
dictionary O
- O
based O
classiﬁers O
, O
support O
vector O
machine O
( O
SVM B-MethodName
) O
and O
random B-MethodName
forest I-MethodName
( O
RF O
) O
, O
and O
three O
pre O
- O
trained O
transformer O
models O
. O

We O
ﬁne O
- O
tuned O
SVM B-MethodName
with O
linear O
kernel O
and O
C=10 O
, O
and O
RF O
where O
max O
depth O
is O
set O
to O
100 O
. O

We O
employed O
a O
BERT B-MethodName
’s O
vocabulary O
to O
train O
dictionary O
- O
based O
models O
. O

We O
ﬁne O
- O
tuned O
transformer O
models O
employing O
the O
default O
settings O
from O
the O
Huggingface O
library O
( O
Wolf O
et O
al O
. O
, O
2019 O
): O
a. O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
is O
designed O
to O
pretrain O
bidirectional O
representations O
using O
masked O
language O
models O
. O

b. O
ALBERT B-MethodName
( O
Lan O
et O
al O
. O
, O
2019 O
) O
has O
signiﬁcantly O
fewer O
parameters O
than O
a O
traditional O
BERT B-MethodName
by O
two O
parameter O
reduction O
techniques O
. O

wF1 B-MetricName
( O
SD O
) O
MF1 B-MetricName
SVM B-MethodName
0.800 O
0.799 O
0.799 O
( O
0.001 O
) O
0.799 O
0.589 O
0.600 O
0.592 O
( O
0.001 O
) O
0.481 O
RF O
0.845 O
0.845 O
0.845 O
( O
0.002 O
) O
0.844 O
0.657 O
0.665 O
0.601 O
( O
0.003 O
) O
0.446 O
BERT B-MethodName
0.884 O
0.884 O
0.884 O
( O
0.003 O
) O
0.882 O
0.735 O
0.713 O
0.721 O
( O
0.004 O
) O
0.605 O
ALBERT B-MethodName
0.884 O
0.884 O
0.884 O
( O
0.003 O
) O
0.883 O
0.743 O
0.710 O
0.721 O
( O
0.004 O
) O
0.590 O
RoBERTa B-MethodName
0.886 O
0.886 O
0.886 O
( O
0.001 O
) O
0.885 O
0.739 O
0.716 O
0.723 O
( O
0.002 O
) O
0.612 O
BERTy0.886 B-MethodName
0.886 O
0.886 O
( O
0.003 O
) O
0.885 O
0.755 O
0.723 O
0.735 O
( O
0.005 O
) O
0.612 O
ALBERTy0.887 B-MethodName
0.887 O
0.887 O
( O
0.001 O
) O
0.887 O
0.752 O
0.718 O
0.729 O
( O
0.004 O
) O
0.602 O
RoBERTay0.891 B-MethodName
0.890 O
0.890 O
( O
0.001 O
) O
0.890 O
0.750 O
0.725 O
0.733 O
( O
0.003 O
) O
0.626 O
Table O
5 O
: O
Results O
for O
two O
tasks O
on O
our O
dataset O
. O

c. O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
is O
a O
robustly O
optimized O
BERT B-MethodName
, O
through O
pre O
- O
training O
on O
larger O
data O
and O
careful O
validation O
of O
hyperparameters O
. O

In O
addition O
, O
even O
though O
it O
is O
a O
marginal O
increment O
, O
the O
performance O
is O
improved O
continuously O
, O
especially O
for O
Task O
2 O
( O
from O
0.56 O
to O
0.62 O
for O
the O
BERT B-MethodName
case O
) O
. O

Although O
our O
model O
is O
not O
as O
performant O
as O
many O
others O
, O
it O
does O
not O
make O
use O
of O
neural B-MethodName
networks I-MethodName
, O
therefore O
we O
do O
not O
rely O
on O
word O
embeddings O
or O
any O
other O
data O
source O
other O
than O
the O
corpora O
themselves O
. O

In O
other O
words O
, O
the O
proposed O
framework O
is O
compatible O
with O
other O
supervised O
neural B-MethodName
networks I-MethodName
to O
boost O
their O
performance O
. O

Tang O
et O
al O
. O
( O
Tang O
et O
al O
. O
, O
2016a O
) O
proposed O
to O
make O
use O
of O
bidirectional O
Long O
ShortTerm O
Memory O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmid O
- O
huber O
, O
1997 O
) O
to O
encode O
the O
sentence O
from O
the O
left O
and O
right O
to O
the O
aspect O
- O
term O
. O

Word O
embedding O
technique O
has O
been O
wildly O
used O
in O
NLP O
models O
, O
e.g. O
, O
Glove O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

Recently O
, O
Bidirectional O
Encoder O
Representations O
from O
Transformer O
( O
BERT B-MethodName
) O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
replaces O
the O
embedding O
layer O
to O
contextdependent O
layer O
with O
the O
pre O
- O
trained O
bidirectional O
language O
model O
to O
capture O
the O
contextual O
representation O
. O

BERT B-MethodName
is O
complementary O
to O
the O
encoder O
of O
the O
proposed O
method O
. O

In O
term O
of O
word O
embedding O
, O
the O
pre O
- O
trained O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
is O
used O
as O
the O
in2https://inclass.kaggle.com/c/restaurant-reviews O
3http://times.cs.uiuc.edu/ O
wang296 O
/ O
Data/ O
4https://github.com/guillaumegenthial/sequence O
taggingClassiﬁer O
ModelsREST O
LAPTOP O
Accuracy O
Macro O
- O
F1 B-MetricName
Accuracy O
Macro O
- O
F1 B-MetricName
- O
CNN O
- O
ASP O
77.82 B-MetricValue
\ O
- O
72.46 O
\ O
- O
AE O
- O
LSTM B-MethodName
76.60 O
\ O
- O
68.90 O
\ O
- O
ATAE O
- O
LSTM B-MethodName
77.20 O
\ O
- O
68.70 O
\ O
- O
GCAE O
77.28 O
( O
0.32 O
) O
\- O
69.14 O
( O
0.32 O
) O
\TC O
- O
LSTMTC B-MethodName
- O
LSTM B-MethodName
77.97 O
( O
0.16 O
) O
67.55 O
( O
0.32 O
) O
68.42 O
( O
0.56 O
) O
62.42 O
( O
1.10 O
) O
TC O
- O
LSTM B-MethodName
( O
EMB B-MetricName
) O
77.18 O
( O
0.38 O
) O
65.97 O
( O
0.44 O
) O
67.51 O
( O
0.72 O
) O
60.31 O
( O
1.28 O
) O
TC O
- O
LSTM B-MethodName
( O
ST O
) O
78.19 O
( O
0.36 O
) O
67.65 O
( O
0.43 O
) O
68.47 O
( O
0.47 O
) O
62.54 O
( O
0.74 O
) O
TC O
- O
LSTM B-MethodName
( O
ASV O
AET O
) O
78.34 O
( O
0.18 O
) O
68.41 O
( O
0.92 O
) O
70.04 O
( O
0.53 O
) O
64.23 O
( O
0.71 O
) O
MemNetMemNet O
78.68 O
( O
0.23 O
) O
68.18 O
( O
0.58 O
) O
70.28 O
( O
0.32 O
) O
64.38 O
( O
0.86 O
) O
MemNet O
( O
EMB B-MetricName
) O
79.47 O
( O
0.38 O
) O
69.06 O
( O
0.21 O
) O
72.17 O
( O
0.44 O
) O
65.06 O
( O
0.73 O
) O
MemNet O
( O
ST O
) O
78.83 O
( O
0.20 O
) O
68.92 O
( O
0.20 O
) O
69.52 O
( O
0.36 O
) O
64.39 O
( O
0.67 O
) O
MemNet O
( O
ASV O
AET O
) O
80.58 O
( O
0.23 O
) O
70.06 O
( O
0.53 O
) O
73.21 O
( O
0.55 O
) O
65.88 O
( O
0.45 O
) O
IANIAN O
79.20 O
( O
0.19 O
) O
68.71 O
( O
0.59 O
) O
69.48 O
( O
0.52 O
) O
62.90 O
( O
0.99 O
) O
IAN O
( O
EMB B-MetricName
) O
79.46 O
( O
0.38 O
) O
69.45 O
( O
0.38 O
) O
70.89 O
( O
0.48 O
) O
65.27 O
( O
0.34 O
) O
IAN O
( O
ST O
) O
79.45 O
( O
0.11 O
) O
69.36 O
( O
0.71 O
) O
73.25 O
( O
0.81 O
) O
68.25 O
( O
0.76 O
) O
IAN O
( O
ASV O
AET O
) O
80.23 O
( O
0.17 O
) O
70.32 O
( O
1.00 O
) O
74.02 O
( O
0.42 O
) O
69.39 O
( O
0.75 O
) O
BILSTM B-MethodName
- O
ATT O
- O
GBILSTM B-MethodName
- O
ATT O
- O
G O
79.74 O
( O
0.22 O
) O
69.16 O
( O
0.53 O
) O
74.26 O
( O
0.35 O
) O
69.54 O
( O
0.53 O
) O
BILSTM B-MethodName
- O
ATT O
- O
G O
( O
EMB B-MetricName
) O
80.27 O
( O
0.44 O
) O
70.33 O
( O
0.51 O
) O
73.61 O
( O
0.30 O
) O
68.25 O
( O
0.63 O
) O
BILSTM B-MethodName
- O
ATT O
- O
G O
( O
ST O
) O
80.54 O
( O
0.23 O
) O
71.88 O
( O
0.19 O
) O
74.70 O
( O
0.41 O
) O
70.31 O
( O
0.60 O
) O
BILSTM B-MethodName
- O
ATT O
- O
G O
( O
ASV O
AET O
) O
81.11 O
( O
0.34 O
) O
72.19 O
( O
0.27 O
) O
75.44 O
( O
0.32 O
) O
70.52 O
( O
0.33 O
) O
TNet O
- O
ASTNet O
- O
AS O
80.56 O
( O
0.23 O
) O
71.17 O
( O
0.43 O
) O
76.75 O
( O
0.35 O
) O
71.88 O
( O
0.35 O
) O
TNet O
- O
AS O
( O
EMB B-MetricName
) O
80.96 O
( O
0.49 O
) O
69.99 O
( O
0.87 O
) O
76.45 O
( O
0.40 O
) O
71.52 O
( O
0.73 O
) O
TNet O
- O
AS O
( O
ST O
) O
80.76 O
( O
0.23 O
) O
71.32 O
( O
0.56 O
) O
76.88 O
( O
0.41 O
) O
71.74 O
( O
0.63 O
) O
TNet O
- O
AS O
( O
ASV O
AET O
) O
81.77 O
( O
0.20 O
) O
72.57 O
( O
0.32 O
) O
77.57 O
( O
0.31 O
) O
72.31 O
( O
0.69 O
) O
Table O
3 O
: O
Experimental O
results O
( O
% O
) O
. O

We O
implemented O
and O
veriﬁed O
four O
kinds O
of O
mainstream O
ATSA O
classiﬁers O
integrated O
into O
our O
model O
, O
i.e. O
, O
TC O
- O
LSTM B-MethodName
( O
Tang O
et O
al O
. O
, O
2016a O
) O
, O
MemNet O
( O
Tang O
et O
al O
. O
, O
2016b O
) O
, O
BILSTM B-MethodName
- O
ATT O
- O
G O
( O
Zhang O
and O
Liu O
, O
2017 O
) O
, O
IAN O
( O
Ma O
et O
al O
. O
, O
2017 O
) O
and O
TNet O
( O
Li O
et O
al O
. O
, O
2018b O
) O
. O

TC O
- O
LSTM B-MethodName
: O
Two O
LSTMs B-MethodName
are O
used O
to O
model O
the O
left O
and O
right O
context O
of O
the O
target O
separately O
, O
then O
the O
concatenation O
of O
two O
representations O
is O
used O
to O
predict O
the O
label O
. O

IAN O
: O
IAN O
adopts O
two O
LSTMs B-MethodName
to O
derive O
the O
representations O
of O
the O
context O
and O
the O
target O
phrase O
interactively O
and O
the O
concatenation O
is O
fed O
to O
the O
softmax O
layer O
. O

BILSTM B-MethodName
- O
ATT O
- O
G O
: O
It O
models O
left O
and O
right O
contexts O
using O
two O
attention O
- O
based O
LSTMs B-MethodName
5http://nlp.stanford.edu/data/glove.8B.300d.zipand O
makes O
use O
of O
a O
special O
gate O
layer O
to O
combine O
these O
two O
representations O
. O

TNet O
- O
AS O
: O
Without O
using O
an O
attention O
module O
, O
TNet O
adopts O
a O
convolutional O
layer O
to O
get O
salient O
features O
from O
the O
transformed O
word O
representations O
originated O
from O
a O
bidirectional O
LSTM B-MethodName
layer O
. O

clf(EMB B-MetricName
): O
We O
use O
CBOW B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
to O
train O
the O
word O
embedding O
vectors O
using O
both O
labeled O
and O
unlabeled O
data O
. O

And O
the O
resulting O
vectors O
, O
instead O
of O
pre O
- O
trained O
GloVe B-MethodName
vectors O
, O
are O
used O
to O
initialize O
the O
embedding O
matrix O
of O
the O
classiﬁer O
. O

Besides O
, O
we O
also O
include O
the O
results O
of O
several O
supervised O
models O
in O
the O
ﬁrst O
block O
, O
i.e. O
, O
CNN O
- O
ASP O
( O
Lam O
et O
al O
. O
, O
2018 O
) O
, O
AE O
- O
LSTM B-MethodName
, O
ATAELSTM B-MethodName
( O
Wang O
et O
al O
. O
, O
2016 O
) O
, O
GCAE O
( O
Li O
and O
Xue O
, O
2018 O
) O
, O
from O
the O
original O
paper O
. O

Without O
loss O
of O
general O
- O
Accuracy O
w/o O
sharing O
w/ O
sharing O
TC O
- O
LSTM B-MethodName
( O
ASV O
AET O
) O
78.34 O
77.65 O
MemNet O
( O
ASV O
AET O
) O
80.58 O
78.82 O
IAN O
( O
ASV O
AET O
) O
80.23 O
79.22 O
BILSTM B-MethodName
- O
ATT O
- O
G O
( O
ASV O
AET O
) O
81.11 O
78.36 O
TNet O
- O
AS O
( O
ASV O
AET O
) O
81.77 O
79.53 O
Table O
4 O
: O
Comparison O
between O
with O
or O
without O
sharing O
embedding O
on O
the O
REST O
dataset O
. O

The O
way O
we O
integrate O
external O
knowledge O
is O
performed O
by O
the O
notion O
of O
a O
regularizer O
, O
which O
is O
an O
independent O
component O
that O
can O
be O
connected O
to O
the O
two O
typical O
architectures O
, O
namely O
, O
continuous O
bag O
- O
of O
- O
words O
( O
CBOW B-MethodName
) O
and O
skip O
- O
gram O
( O
SG O
) O
, O
or O
used O
independently O
as O
a O
retroﬁtter O
. O

To O
provide O
a O
more O
generic O
solution O
, O
we O
propose O
a O
uniﬁed O
framework O
that O
learns O
word O
embeddings O
from O
context O
( O
e.g. O
, O
CBOW B-MethodName
or O
SG O
) O
together O
with O
the O
ﬂexibility O
of O
incorporating O
arbitrary O
external O
knowledge O
using O
the O
notion O
of O
a O
regularizer O
. O

For O
example O
, O
the O
CBOW B-MethodName
model O
can O
be O
formulated O
as O
maximizing O
L=1 O
|V||V|/summationdisplay O
i=1logp(wi|/summationdisplay O
0<|j|≤cυi+j),∀wi∈V O
( O
1 O
) O
whereυi+jrefers O
to O
the O
embedding O
of O
a O
word O
in O
wi+c O
i−c O
, O
andcdeﬁnes O
the O
window O
size O
of O
words O
adjacent O
to O
the O
word O
wi O
. O

In O
particular O
, O
the O
objective O
function O
for O
CBOW B-MethodName
with O
integrating O
the O
regularizer O
can O
be O
formulated O
as O
maximizing O
L=1 O
|V||V|/summationdisplay O
i=1logp(wi O
, O
ψ(wi)|/summationdisplay O
0<|j|≤cυi+j)(3 O
) O
where O
not O
only O
wi O
, O
but O
alsoR(wi)is O
predicted O
by O
the O
context O
words O
wi+jvia O
their O
embeddings O
υi+j O
. O

2.2 O
Parameter O
Estimation O
As O
shown O
in O
Equation O
3 O
, O
prior O
knowledge O
participates O
in O
the O
optimization O
process O
for O
predicting O
the O
current O
word O
and O
contributes O
to O
embedding O
updating O
during O
training O
a O
CBOW B-MethodName
model O
. O

beddings O
are O
updated O
in O
the O
same O
way O
as O
in O
normal O
CBOW B-MethodName
and O
SG O
models O
. O

6The O
lexicons O
are O
organized O
in O
the O
similar O
way O
as O
in O
Faruqui O
et O
al O
. O
( O
2015 O
) O
, O
where O
synonyms O
are O
grouped O
together O
and O
treated O
as O
a O
document O
for O
LDA O
learning O
. O
EmbeddingsMEN-3k O
SimLex-999 O
WordSim-353 O
γργργρ O
LDA O
57.17 O
58.86 O
20.39 O
22.12 O
55.48 O
54.81 O
CBOW B-MethodName
62.93 O
65.84 O
28.34 O
28.31 O
68.50 O
66.67 O
Yu O
and O
Dredze O
( O
2014)+PPDB O
65.35 O
65.84 O
35.56 O
33.30 O
72.75 O
72.43 O
+ O
WNsyn O
65.20 O
65.74 O
36.15 O
33.65 O
72.79 O
72.58 O
This O
work+LDA O
67.33 O
69.51 O
29.79 O
29.78 O
71.19 O
69.58 O
+ O
PPDB O
65.25 O
66.87 O
36.43 O
33.28 O
69.45 O
68.89 O
+ O
WNsyn O
64.42 O
66.98 O
33.86 O
33.69 O
66.13 O
67.11 O
SG O
64.79 O
66.71 O
26.97 O
26.59 O
68.88 O
67.80 O
Kiela O
et O
al O
. O
( O
2015)+PPDB O
61.13 O
60.04 O
36.47 O
34.29 O
70.14 O
68.76 O
+ O
WNsyn O
57.02 O
59.84 O
29.02 O
29.99 O
63.61 O
61.22 O
This O
work+LDA O
65.02 O
65.32 O
25.19 O
24.04 O
66.16 O
69.21 O
+ O
PPDB O
70.83 O
71.35 O
37.10 O
35.72 O
73.94 O
73.11 O
+ O
WNsyn O
66.58 O
68.14 O
36.72 O
35.91 O
68.50 O
67.90 O
Table O
1 O
: O
Word O
similarity O
results O
for O
joint O
learning O
on O
three O
datasets O
in O
terms O
of O
Pearson O
’s O
coefﬁcient O
correlation B-MetricName
( O
γ O
) O
and O
Spearman O
’s O
rank O
correlation B-MetricName
( O
ρ O
) O
in O
percentages O
. O

We O
experiment O
with O
three O
learning O
paradigms O
, O
namely O
CBOW B-MethodName
, O
SG O
and O
GloVe B-MethodName
. O

GloVe B-MethodName
is O
only O
tested O
in O
retroﬁtting O
since O
our O
regularizer O
is O
notcompatible O
with O
GloVe B-MethodName
learning O
objective O
in O
joint O
learning O
. O

Therefore O
we O
still O
use O
cosine O
for O
LDA O
embeddings O
. O
EmbeddingsMEN-3k O
SimLex-999 O
WordSim-353 O
γργργρ O
GloVe B-MethodName
66.84 O
66.97 O
28.87 O
27.52 O
59.78 O
61.46 O
Faruqui O
et O
al O
. O
( O
2015)+PPDB O
66.98 O
67.04 O
29.25 O
28.25 O
61.44 O
63.35 O
+ O
WNsyn O
64.29 O
63.92 O
27.32 O
24.39 O
57.40 O
58.88 O
This O
work+LDA O
59.65 O
60.23 O
22.25 O
22.70 O
55.65 O
57.57 O
+ O
PPDB O
68.99 O
68.99 O
31.35 O
29.85 O
62.31 O
63.96 O
+ O
WNsyn O
66.72 O
66.84 O
29.78 O
28.47 O
59.62 O
61.34 O
CBOW B-MethodName
62.93 O
65.84 O
28.34 O
28.31 O
68.50 O
66.67 O
Yu O
and O
Dredze O
( O
2014)+PPDB O
65.08 O
65.52 O
36.16 O
34.01 O
72.75 O
72.39 O
+ O
WNsyn O
65.34 O
65.77 O
35.68 O
33.33 O
72.72 O
72.74 O
Faruqui O
et O
al O
. O
( O
2015)+PPDB O
65.07 O
67.55 O
37.07 O
35.02 O
71.76 O
71.18 O
+ O
WNsyn O
63.71 O
66.44 O
30.15 O
29.83 O
71.24 O
69.39 O
This O
work+LDA O
50.07 O
56.64 O
21.47 O
23.01 O
41.56 O
47.27 O
+ O
PPDB O
65.30 O
67.68 O
37.34 O
35.74 O
72.01 O
72.05 O
+ O
WNsyn O
63.89 O
66.74 O
33.96 O
33.82 O
68.70 O
66.91 O
SG O
64.79 O
66.71 O
26.97 O
26.59 O
68.88 O
67.80 O
Kiela O
et O
al O
. O
( O
2015)+PPDB O
67.38 O
69.05 O
32.49 O
31.84 O
71.59 O
69.82 O
+ O
WNsyn O
64.58 O
67.02 O
29.43 O
28.12 O
69.15 O
68.36 O
Faruqui O
et O
al O
. O
( O
2015)+PPDB O
65.44 O
67.02 O
34.12 O
33.72 O
71.24 O
70.31 O
+ O
WNsyn O
65.65 O
66.71 O
28.25 O
27.61 O
70.21 O
69.47 O
This O
work+LDA O
64.02 O
65.33 O
24.64 O
24.28 O
59.43 O
60.60 O
+ O
PPDB O
67.17 O
69.09 O
34.93 O
34.57 O
72.63 O
71.15 O
+ O
WNsyn O
65.62 O
67.38 O
29.96 O
29.82 O
69.70 O
68.91 O
Table O
2 O
: O
Word O
similarity O
results O
for O
retroﬁtting O
on O
three O
datasets O
in O
terms O
of O
Pearson O
’s O
coefﬁcient O
correlation B-MetricName
( O
γ O
) O
and O
Spearman O
’s O
rank O
correlation B-MetricName
( O
ρ O
) O
in O
percentages O
. O

For O
comparison O
, O
we O
also O
include O
the O
results O
from O
the O
approaches O
proposed O
in O
previous O
studies O
, O
i.e. O
, O
Yu O
and O
Dredze O
( O
2014)9for O
CBOW B-MethodName
, O
Kiela O
et O
al O
. O
( O
2015)10for O
SG O
and O
Faruiqui O
et O
al O
. O
( O
2015)11for O
all O
initial O
embeddings O
. O

The O
classiﬁer O
is O
based O
on O
a O
bi O
- O
directional O
LSTM B-MethodName
model O
as O
described O
in O
Dai O
and O
Le O
( O
2015 O
) O
, O
with O
one O
hidden B-HyperparameterName
layer I-HyperparameterName
of O
1024 B-HyperparameterValue
units O
. O

Embeddings O
from O
different O
approaches O
are O
used O
as O
inputs O
for O
the O
LSTM B-MethodName
classiﬁer O
. O

Overall O
, O
our O
joint O
learning O
with O
CBOW B-MethodName
achieves O
the O
best O
performance O
on O
this O
task O
. O

This O
task O
demonstrates O
the O
potential O
of O
our O
framework O
for O
encoding O
external O
knowledge O
and O
using O
it O
to O
enrich O
the O
representa O
- O
Embeddings O
Accuracy O
Maas O
et O
al O
. O
( O
2011 O
) O
88.89 O
GloVe B-MethodName
90.66 O
Faruqui O
et O
al O
. O
( O
2015 O
) O
+ O
Retro O
90.43 O
This O
work O
+ O
Retro O
90.89 O
CBOW B-MethodName
91.29 O
Yu O
and O
Dredze O
( O
2014)+Joint O
91.14 O
+ O
Retro O
90.71 O
Faruqui O
et O
al O
. O
( O
2015 O
) O
+ O
Retro O
90.77 O
This O
work+Joint O
92.09∗ O
+ O
Retro O
91.81∗ O
SG O
91.30 O
Faruqui O
et O
al O
. O
( O
2015 O
) O
+ O
Retro O
91.03 O
Kiela O
et O
al O
. O
( O
2015)+Joint O
91.45 O
+ O
Retro O
91.14 O
This O
work+Joint O
92.07∗ O
+ O
Retro O
91.42 O
Table O
3 O
: O
Sentiment O
classiﬁcation O
results O
on O
IMDB O
data O
set O
( O
Maas O
et O
al O
. O
, O
2011 O
) O
. O

Our O
approach O
can O
be O
used O
independently O
as O
a O
retroﬁtter O
or O
jointly O
with O
CBOW B-MethodName
and O
SG O
to O
encode O
prior O
knowledge O
. O

BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
We O
experiment O
with O
two O
variants O
: O
BERT B-MethodName
BASE O
( O
110 O
M O
parameters O
) O
, O
and O
BERT B-MethodName
LARGE O
( O
340 O
M O
parameters O
) O
. O

RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
We O
experiment O
with O
RoBERTa B-MethodName
BASE O
( O
125 O
M O
parameters O
) O
and O
RoBERTa B-MethodName
LARGE O
( O
355 O
M O
parameters O
) O
. O

ALBERT B-MethodName
( O
Lan O
et O
al O
. O
, O
2020 O
) O
We O
experiment O
with O
version O
2 O
of O
ALBERT B-MethodName
BASE O
( O
11 O
M O
parameters O
) O
, O
ALBERT B-MethodName
LARGE O
( O
17 O
M O
parameters O
, O
ALBERT B-MethodName
XLARGE O
( O
58 O
M O
parameters O
) O
and O
ALBERT B-MethodName
XXLARGE O
( O
223 O
M O
parameters O
) O
. O

3.2 O
Input O
representation O
For O
our O
inputs O
, O
we O
add O
a O
start O
of O
sentence O
token O
( O
[ O
CLS O
] O
for O
BERT B-MethodName
, O
ALBERT B-MethodName
; O
< O
s O
> O
for O
RoBERTa B-MethodName
) O
. O

Separate O
sentences O
of O
a O
given O
input O
item O
are O
separated O
by O
a O
separator O
token O
, O
and O
the O
masked O
wordModel O
Accuracy O
BERT B-MethodName
BASE O
0.47 O
BERT B-MethodName
LARGE O
0.51 O
RoBERTa B-MethodName
BASE O
0.61 O
RoBERTa B-MethodName
LARGE O
0.66 O
ALBERT B-MethodName
BASE O
0.42 O
ALBERT B-MethodName
LARGE O
0.48 O
ALBERT B-MethodName
XLARGE O
0.56 O
ALBERT B-MethodName
XXLARGE O
0.57 O
Table O
1 O
: O
Connective O
prediction O
accuracy B-MetricName
on O
PDTB O
data O
to O
be O
predicted O
is O
denoted O
by O
[ O
MASK O
] O
for O
BERT B-MethodName
and O
ALBERT B-MethodName
, O
and O
< O
mask O
> O
for O
RoBERTa B-MethodName
. O

Accuracy O
generally O
improves O
with O
model O
size O
within O
model O
class O
, O
and O
among O
the O
three O
model O
classes O
, O
RoBERTa B-MethodName
shows O
the O
strongest O
performance O
overall O
. O

Comparing O
between O
relation O
types O
, O
we O
see O
that O
models O
( O
particularly O
BERT B-MethodName
and O
RoBERTa B-MethodName
) O
show O
much O
higher O
3It O
has O
also O
been O
observed O
PDTB-2 O
includes O
connectives O
that O
can O
signal O
more O
than O
one O
discourse O
relation O
( O
Pitler O
and O
Nenkova O
, O
2009 O
; O
Webber O
et O
al O
. O
, O
2019 O
) O
. O

PDTB-3 O
tries O
to O
resolve O
this O
connective O
ambiguity O
by O
introducing O
new O
relations O
in O
the O
annotations O
, O
but O
for O
the O
purposes O
of O
our O
preliminary O
test O
here O
, O
PDTB-2 O
is O
sufﬁcient O
. O
Model O
Expansion O
: O
Asynchronous O
: O
Concession O
: O
Causal O
: O
Conjunction O
Succession O
contra O
- O
expectation O
Result O
BERT B-MethodName
BASE O
0.73 O
0.46 O
0.18 O
0.20 O
BERT B-MethodName
LARGE O
0.74 O
0.5 O
0.25 O
0.24 O
RoBERTa B-MethodName
BASE O
0.76 O
0.67 O
0.43 O
0.3 O
RoBERTa B-MethodName
LARGE O
0.79 O
0.71 O
0.51 O
0.34 O
ALBERT B-MethodName
BASE O
0.42 O
0.52 O
0.37 O
0.09 O
ALBERT B-MethodName
LARGE O
0.49 O
0.61 O
0.38 O
0.17 O
ALBERT B-MethodName
XLARGE O
0.64 O
0.62 O
0.41 O
0.25 O
ALBERT B-MethodName
XXLARGE O
0.59 O
0.66 O
0.46 O
0.29 O
Table O
2 O
: O
Connective O
prediction O
accuracy B-MetricName
on O
PDTB O
data O
, O
broken O
down O
by O
speciﬁc O
discourse O
relations O
accuracy B-MetricName
on O
Expansion O
. O
Conjunction O
, O
more O
moderate O
performance O
on O
Asynchronous O
. O
Succession O
and O
Concession O
, O
and O
generally O
quite O
weak O
performance O
at O
predicting O
Causal O
connectives O
. O

ALBERT B-MethodName
deviates O
somewhat O
from O
the O
pattern O
of O
BERT B-MethodName
and O
RoBERTa B-MethodName
, O
in O
that O
its O
highest O
performance O
is O
instead O
typically O
on O
Asynchronous O
. O
Succession O
. O

We O
see O
that O
across O
the O
board O
, O
BERT B-MethodName
and O
RoBERTa B-MethodName
models O
have O
high O
rates O
of O
preferring O
andin O
cases O
of O
erroneous O
prediction O
. O

ALBERT B-MethodName
, O
by O
contrast O
, O
distributes O
errors O
across O
a O
wider O
range O
of O
connectives O
. O

What O
we O
see O
then O
, O
is O
a O
picture O
in O
which O
BERT B-MethodName
and O
RoBERTa B-MethodName
models O
seem O
to O
have O
settled O
on O
andas O
a O
common O
go O
- O
to O
connective O
, O
contributing O
to O
the O
high O
accuracy B-MetricName
of O
those O
models O
on O
the O
Expansion O
. O
Conjunction O
relation O
— O
while O
ALBERT B-MethodName
has O
less O
of O
a O
go O
- O
to O
and O
preference O
, O
consistent O
with O
ALBERT B-MethodName
’s O
lower O
accuracy B-MetricName
on O
Expansion O
. O
Conjunction O
, O
and O
slightly O
more O
balanced O
accuracies B-MetricName
overall O
. O

Causal O
Pair O
BERT B-MethodName
BASE O
0.73 O
0.27 O
0 O
BERT B-MethodName
LARGE O
0.6 O
0.43 O
0.03 O
RoBERTa B-MethodName
BASE O
0.4 O
0.46 O
0 O
RoBERTa B-MethodName
LARGE O
0.43 O
0.67 O
0.1 O
ALBERT B-MethodName
BASE O
0.9 O
0.1 O
0 O
ALBERT B-MethodName
LARGE O
0.6 O
0.4 O
0.03 O
ALBERT B-MethodName
XLARGE O
0.53 O
0.5 O
0.07 O
ALBERT B-MethodName
XXLARGE O
0.57 O
0.7 O
0.3 O
Table O
4 O
: O
Prediction O
accuracy B-MetricName
on O
contexts O
from O
Drenhaus O
et O
al O
. O
( O
2014 B-MetricValue
) O
. O

Most O
models O
hover O
around O
0 O
% O
accuracy B-MetricName
— O
only O
ALBERT B-MethodName
XXLARGE O
exceeds O
( O
narrowly O
) O
the O
roughly O
25 B-MetricValue
% O
threshold B-MetricName
that O
would O
be O
expected O
by O
chance O
. O

Pair O
BERT B-MethodName
BASE O
0.52 O
0.55 O
0.08 O
BERT B-MethodName
LARGE O
0.24 O
0.79 O
0.04 O
RoBERTa B-MethodName
BASE O
0.69 O
0.49 O
0.18 O
RoBERTa B-MethodName
LARGE O
0.61 O
0.21 O
0.04 O
ALBERT B-MethodName
BASE O
0.66 O
0.3 O
0.02 O
ALBERT B-MethodName
LARGE O
0.83 O
0.37 O
0.2 O
ALBERT B-MethodName
XLARGE O
0.06 O
0.98 O
0.04 O
ALBERT B-MethodName
XXLARGE O
0.09 O
0.89 O
0.06 O
Table O
6 O
: O
Prediction O
accuracy B-MetricName
in O
implicature O
test O
, O
with O
cancellation O
and O
reinforcement O
settings O
tence O
to O
understand O
the O
effects O
of O
“ O
which O
is O
to O
say O
” O
and O
“ O
in O
fact O
” O
) O
, O
then O
they O
should O
prefer O
before O
in O
the O
case O
of O
reinforcement O
, O
and O
after O
in O
the O
case O
of O
cancellation O
. O

Model O
Initial O
Medial O
Pair O
BERT B-MethodName
BASE O
0.93 O
0.45 O
0.38 O
BERT B-MethodName
LARGE O
0.91 O
0.48 O
0.39 O
RoBERTa B-MethodName
BASE O
0.85 O
0.39 O
0.24 O
RoBERTa B-MethodName
LARGE O
0.86 O
0.4 O
0.26 O
ALBERT B-MethodName
BASE O
0.95 O
0.46 O
0.41 O
ALBERT B-MethodName
LARGE O
0.95 O
0.24 O
0.2 O
ALBERT B-MethodName
XLARGE O
0.75 O
0.54 O
0.31 O
ALBERT B-MethodName
XXLARGE O
0.68 O
0.56 O
0.26 O
Table O
8 O
: O
Model O
preferences O
for O
after O
/before O
in O
sentenceinitial O
and O
sentence O
- O
medial O
position O
. O

Model O
performance O
is O
extremely O
weak O
across O
the O
board O
, O
with O
models O
assigning O
higher O
probability O
in O
the O
better O
context O
no O
more O
than O
3 O
% O
of O
the O
time O
— O
except O
for O
RoBERTa B-MethodName
LARGE O
in O
Reinforcement O
conditions O
, O
at O
21 O
% O
( O
chance O
level O
of O
50 O
% O
) O
. O

The O
other O
half O
of O
models O
exceed O
this O
chance O
- O
level O
percentage O
— O
particularly O
the O
BERT B-MethodName
models O
and O
ALBERT B-MethodName
BASE O
. O

Variation O
between O
models O
is O
fairly O
minor O
, O
though O
some O
RoBERTa B-MethodName
and O
ALBERT B-MethodName
variants O
at O
times O
distinguish O
themselves O
in O
coming O
closer O
to O
chancelevel O
performance O
when O
other O
models O
are O
close O
to O
0 O
% O
accuracy B-MetricName
. O

This O
could O
be O
attributable O
to O
larger O
pre O
- O
training O
data O
in O
the O
case O
of O
RoBERTa B-MethodName
, O
and O
in O
the O
case O
of O
ALBERT B-MethodName
, O
we O
speculate O
that O
some O
beneﬁt O
may O
be O
derived O
from O
the O
sentence O
order O
prediction O
loss O
( O
Lan O
et O
al O
. O
, O
2020 O
) O
, O
which O
may O
encourage O
sensitivity O
to O
certain O
discourse O
dynamics O
. O

Our O
work O
is O
also O
related O
to O
representation O
learning O
usig O
deep O
neural B-MethodName
networks I-MethodName
. O

To O
support O
our O
claim O
that O
the O
learnt O
representations O
of O
our O
model O
encode O
the O
semantic O
of O
question O
answer O
pairs O
better O
than O
pre O
- O
trained O
sentence O
representation O
models O
, O
we O
choose O
four O
baselines O
commonly O
used O
to O
encode O
sentences O
: O
1.Word2Vec B-MethodName
and O
Glove O
( O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Pennington O
et O
al O
. O
, O
2014 O
): O
We O
use O
the O
simple O
approach O
of O
averaging O
the O
word O
vectors O
for O
all O
words O
in O
a O
sentence O
. O

We O
observe O
that O
averaging O
word O
embeddings O
such O
as O
Glove O
or O
Word2Vec B-MethodName
performs O
better O
than O
the O
dedicated O
sentence O
representations O
in O
the O
WikiQA O
dataset O
. O

ModelWikiQA O
MAP B-MetricName
MRR B-MetricName
W.EGlove O
0.464 B-MetricValue
0.475 O
Word2Vec B-MethodName
0.4329 O
0.453 O
S.EInferSent O
0.399 O
0.404 O
Sent2Vec O
0.481 O
0.486 O
This O
work O
0.6771 O
0.6841 O
Table O
3 O
: O
Evaluation O
on O
quadruples O
. O

For O
the O
sequence O
generation O
model O
, O
we O
implement O
the O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
network O
with O
a O
layer O
of O
64 O
hidden B-HyperparameterName
units I-HyperparameterName
while O
the O
dimension O
of O
the O
input O
event O
vector O
representation O
is O
200 B-HyperparameterValue
. O

The O
skip O
- O
gram O
and O
continuous O
bag O
- O
of O
- O
words O
( O
CBOW B-MethodName
) O
models O
of O
Mikolov O
et O
al O
. O
( O
2013 O
) O
propose O
a O
simple O
singlelayer O
architecture O
based O
on O
the O
inner O
product O
between O
two O
word O
vectors O
. O

Additionally O
, O
researcher O
have O
been O
attempting O
to O
infuse O
knowledge O
into O
the O
language O
modeling O
process O
( O
Ahn O
et O
al O
. O
, O
2016 O
; O
Yang O
et O
al O
. O
, O
2016 O
; O
Ji O
et O
al O
. O
, O
2017 O
; O
He O
et O
al O
. O
, O
2017 O
; O
Clark O
et O
al O
. O
, O
2018).Most O
recently O
, O
pre O
- O
trained O
language O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
) O
, O
and O
XLNET O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
have O
achieved O
much O
success O
for O
language O
modeling O
and O
generation O
tasks O
. O

The O
feature O
embedding O
of O
each O
node O
is O
created O
by O
concatenating O
the O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
word O
embedding O
together O
with O
three O
randomly O
initialized O
embeddings O
for O
the O
features O
word O
lemma O
, O
upos O
and O
xpos O
provided O
by O
the O
syntactic O
parse O
. O

Then O
, O
we O
use O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
layers O
to O
encode O
three O
nodes O
sequences O
: O
( O
1 O
) O
nodes O
in O
the O
parser O
stack O
, O
( O
2 O
) O
nodes O
before O
the O
current O
node O
and O
( O
3 O
) O
nodes O
after O
the O
current O
node O
. O

If O
more O
than O
one O
node O
is O
involved O
( O
n O
> O
1 O
) O
, O
then O
we O
, O
in O
addition O
, O
predict O
the O
edge O
information O
by O
passing O
the O
feature O
embedding O
to O
an O
LSTM B-MethodName
layer O
, O
followed O
by O
feature O
- O
speciﬁc O
MLPs O
for O
predicting O
edge O
label O
and O
directions O
. O

We O
shared O
the O
parameters O
of O
word O
embeddings O
and O
LSTM B-MethodName
layers O
across O
frameworks O
, O
and O
separate O
the O
MLP O
parameters O
for O
each O
framework O
. O

This O
paper O
seeks O
to O
uncover O
patterns O
of O
sound O
change O
across O
Indo O
- O
Aryan O
languages O
using O
an O
LSTM B-MethodName
encoder O
- O
decoder O
architecture O
. O

We O
use O
an O
LSTM B-MethodName
- O
based O
encoder O
- O
decoder O
architecture O
to O
analyze O
a O
large O
data O
set O
of O
OIA O
etyma O
( O
ancestral O
forms O
) O
and O
medieval O
/ O
modern O
Indo O
- O
Aryan O
reﬂexes O
( O
descendant O
forms O
) O
extracted O
from O
a O
digitized O
etymological O
dictionary O
, O
with O
the O
goal O
of O
inferring O
patterns O
of O
sound O
change O
from O
input O
/ O
output O
string O
pairs O
. O

Computational O
methods O
have O
been O
applied O
to O
the O
related O
ﬁeld O
of O
historical O
linguistics O
to O
identify O
cognates O
( O
words O
that O
go O
back O
to O
a O
common O
ancestor O
) O
and O
infer O
relationships O
between O
languages O
( O
Rama O
et O
al O
. O
, O
2018 O
) O
as O
well O
as O
the O
reconstruction O
of O
ancestral O
words O
through O
Bayesian O
methods O
( O
Bouchard O
- O
C O
ˆot´e O
et O
al O
. O
, O
2013 O
) O
, O
gated O
neural B-MethodName
networks I-MethodName
( O
Meloni O
et O
al O
. O
, O
2019 O
) O
and O
non O
- O
neural O
sequence O
labeling O
methods O
( O
Ciobanu O
and O
Dinu O
, O
2020 O
) O
. O

Additionally O
, O
we O
represent O
the O
semantic O
proﬁle O
of O
each O
OIA O
etymon O
by O
generating O
embeddings O
of O
each O
etymon O
’s O
English O
language O
gloss O
using O
a O
pretrained O
BERT B-MethodName
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Wolf O
et O
al O
. O
, O
2019 O
) O
, O
though O
this O
approach O
does O
not O
fully O
encapsulate O
the O
OIA O
word O
’s O
semantics O
. O

A O
one O
- O
hot O
encoding O
of O
etymon O
IDs O
is O
costly O
, O
as O
there O
are O
13580 O
unique O
etyma O
in O
our O
dataset O
; O
instead O
, O
we O
combine O
information O
from O
BERT B-MethodName
embeddings O
and O
the O
input O
string O
in O
order O
to O
produce O
a O
unique O
embedding O
for O
each O
etymon O
in O
the O
data O
set O
. O

5 O
Model O
Our O
experiments O
use O
an O
LSTM B-MethodName
Encoder O
- O
Decoder O
with O
0th O
- O
order O
nonmonotonic O
hard O
attention O
( O
Wu O
and O
Cotterell O
, O
2019 O
) O
. O

The O
authors O
’ O
architecture O
works O
as O
follows O
: O
for O
each O
input O
x(for O
our O
purposes O
an O
OIA O
etymon O
) O
, O
a O
latent O
representation O
henc O
j2 O
R2Dis O
learned O
for O
each O
time O
step O
j2f1;:::;jxjg O
via O
a O
bidirectional O
LSTM B-MethodName
on O
the O
basis O
of O
the O
input O
symbol O
at O
time O
step O
j. O
For O
each O
output O
y(for O
our O
purposes O
a O
medieval O
/ O
modern O
Indo O
- O
Aryan O
reﬂex O
) O
, O
a O
latent O
representation O
hdec O
i2RDis O
learned O
via O
a O
forward O
LSTM B-MethodName
for O
each O
time O
step O
i2f1;:::;jyjg O
on O
the O
basis O
of O
the O
output O
symbol O
at O
time O
step O
i 1 O
. O

We O
use O
a O
one O
- O
hot O
encoding O
of O
language O
ID O
and O
POS B-TaskName
ID O
, O
and O
employ O
BERT B-MethodName
embeddings O
( O
reduced O
from O
768 O
to O
128 O
dimensions O
using O
principal B-MethodName
component I-MethodName
analysis I-MethodName
) O
to O
represent O
an O
etymon O
’s O
semantic O
proﬁle O
. O

Embeddings O
for O
etyma O
are O
represented O
by O
contatenating O
the O
ﬁrst O
and O
last O
states O
of O
a O
Bidirectional O
LSTM B-MethodName
encoding O
of O
the O
etymon O
string O
to O
the O
BERT B-MethodName
- O
based O
semantic O
embedding O
( O
denoted O
by O
e(glossi O
) O
) O
) O
. O

Formally O
, O
these O
embeddings O
consist O
of O
the O
following O
, O
for O
a O
given O
data O
point O
index O
i2f1;:::;jdatajg O
: O
L O
: O
zlang O
i O
= O
MLP O
( O
langi O
) O
P O
: O
zPOS B-TaskName
i O
= O
MLP O
( O
POS B-TaskName
i O
) O
S O
: O
zsem O
i O
= O
MLP O
( O
e(glossi O
) O
) O
E O
: O
zetym O
i O
= O
MLP O
( O
[ O
MLP O
( O
[ O
LSTM B-MethodName
( O
xi;1 O
: O
jxij)jxij O
; O
LSTM B-MethodName
( O
xi;jxij:1)jxij]);zsem O
i O
] O
) O
After O
one O
or O
more O
of O
these O
embeddings O
are O
concatenated O
to O
an O
input O
token O
, O
the O
resulting O
concatenation O
is O
passed O
to O
another O
MLP O
layer O
, O
which O
is O
then O
fed O
to O
the O
encoder O
- O
decoder O
architecture O
. O

10 O
Discussion O
and O
Outlook O
In O
this O
paper O
, O
we O
investigated O
the O
ability O
of O
LSTMbased B-MethodName
encoder O
- O
decoder O
architectures O
to O
capture O
recurrent O
patterns O
of O
sound O
change O
between O
OIA O
and O
medieval O
/ O
modern O
Indo O
- O
Aryan O
languages O
, O
as O
well O
as O
encode O
information O
regarding O
the O
genetic O
relationships O
between O
languages O
. O

We O
think O
our O
top O
- O
rankMethodExtended O
Features O
Sparse O
Features O
MT02 O
MT04 O
MT05 O
A O
VG O
MT02 O
MT04 O
MT05 O
A O
VG O
PRO O
40.30 O
38.12 O
37.69 O
38.70(+0.00 O
) O
40.63 O
38.46 O
38.24 O
39.11(+0.00 O
) O
KB O
- O
MIRA O
40.48 O
37.71 O
37.37 O
38.52(-0.18 O
) O
40.67 O
38.48 O
38.21 O
39.12(+0.01 O
) O
ListNet O
40.75∗38.69 O
+ O
38.31∗39.25(+0.55 O
) O
40.91∗38.77∗38.42 O
39.37(+0.26 O
) O
ListMLE O
40.40 O
38.21 O
38.04 O
38.88(+0.18 O
) O
40.63 O
38.68 O
38.24 O
39.18(+0.07 O
) O
ListMLE O
- O
T5 B-MethodName
41.02∗38.84 O
+ O
38.79 O
+ O
39.55(+0.85 O
) O
41.12∗38.91∗38.89∗39.64(+0.53 O
) O
ListMLE O
- O
TE O
41.15 O
+ O
39.01 O
+ O
39.16 O
+ O
39.77(+1.07 O
) O
41.25 O
+ O
39.00 O
+ O
39.27 O
+ O
39.84(+0.73 O
) O
Table O
3 O
: O
BLEU4 B-MetricName
in O
percentage O
for O
comparing O
of O
baseline O
systems O
and O
systems O
with O
listwise O
losses.+,∗ O
marks O
results O
that O
are O
signiﬁcant O
better O
than O
the O
baseline O
system O
with O
p<0.01andp<0.05 O
. O

( O
ListMLET5 B-MethodName
and O
ListMLE O
- O
TE O
refer O
to O
top-5 O
LisMLE O
and O
our O
top O
- O
rank O
enhanced O
ListMLE O
, O
respectively O
. O
) O

BLEU B-MetricName
in O
( O
a O
) O
top-5 O
ListMLE O
and O
( O
b O
) O
top O
- O
rank O
enhanced O
ListMLE O
Methods O
MT02 O
MT04 O
MT05 O
A O
VG O
PRO O
40.90 B-MetricValue
38.84 O
38.64 O
39.64(+0.00 O
) O
KB O
- O
MIRA O
41.09 O
38.49 O
38.62 O
39.40(-0.06 O
) O
ListNet O
41.49 O
+ O
39.25∗39.17∗39.97(+0.51 O
) O
ListMLE O
- O
T5 B-MethodName
41.26∗39.63 O
+ O
39.32∗40.07(+0.61 O
) O
ListMLE O
- O
TE O
41.85 O
+ O
39.96 O
+ O
39.88 O
+ O
40.56(+1.10 O
) O
Table O
4 O
: O
Comparison O
of O
baselines O
and O
listwise O
approaches O
with O
a O
larger O
k O
- O
best O
list O
on O
extended O
feature O
set O
. O

With O
our O
top O
- O
ranked O
enhanced O
method O
, O
we O
can O
get O
a O
better O
performance O
thanMethods O
MT02 O
MT04 O
MT05 O
A O
VG O
MERT O
37.72 O
37.13 O
36.77 O
37.21(+0.00 O
) O
PRO O
37.85 O
37.21 O
36.68 O
37.24(+0.03 O
) O
KB O
- O
MIRA O
37.97 O
37.28 O
36.58 O
37.28(+0.07 O
) O
ListNet O
37.71 O
37.47∗36.78 O
37.32(+0.11 O
) O
ListMLE O
37.54 O
37.54 O
36.65 O
37.24(+0.03 O
) O
ListMLE O
- O
T5 B-MethodName
37.90 O
37.32 O
36.84 O
37.35(+0.14 O
) O
ListMLE O
- O
TE O
38.03 O
37.49∗36.85 O
37.46(+0.25 O
) O
Table O
5 O
: O
Comparison O
of O
baseline O
and O
liswise O
approaches O
on O
basic O
feature O
set O
. O

In O
this O
work O
, O
we O
use O
a O
recurrent O
encoder O
– O
decoder O
system O
with O
long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
units O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
and O
an O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O

The O
sequence O
of O
embeddings O
is O
then O
processed O
by O
a O
bidirectional O
( O
Schuster O
and O
Paliwal O
, O
1997 O
) O
LSTM B-MethodName
network O
, O
that O
concatenates O
the O
hidden O
states O
from O
forward O
and O
backward O
layers O
and O
produces O
a O
sequence O
of O
annotations O
. O
The O
decoder O
is O
a O
conditional O
LSTM B-MethodName
( O
cLSTM B-MethodName
) O
network O
( O
Peris O
and O
Casacuberta O
, O
2018b O
) O
. O

A O
cLSTM B-MethodName
network O
is O
composed O
of O
several O
LSTM B-MethodName
transition O
blocks O
with O
an O
attention O
mechanism O
in O
between O
. O

We O
use O
two O
LSTM B-MethodName
blocks O
. O

Corpus O
Usage O
jSj O
jW O
j O
jVj O
EuroparlTrainEn2M46 O
M O
106k O
Es O
48 O
M O
160k O
Dev O
. O
En2k58k O
6.1k O
Es O
61k O
7.7k O
NewsTestEn51k1.2 O
M O
35k O
Commentary O
Es O
1.5 O
M O
49k6.3 O
NMT O
systems O
and O
AL O
setup O
Our O
NMT O
system O
was O
built O
using O
NMT O
- O
Keras O
( O
Peris O
and O
Casacuberta O
, O
2018a O
) O
and O
featured O
a O
bidirectional O
LSTM B-MethodName
encoder O
and O
a O
decoder O
with O
cLSTM B-MethodName
units O
. O

Following O
Britz O
et O
al O
. O
( O
2017 O
) O
, O
we O
set O
the O
dimension O
of O
the O
LSTM B-MethodName
, O
embeddings O
and O
attention O
model O
to O
512 O
. O

The O
system O
is O
based O
on O
an O
attention O
- O
free O
encoder O
- O
decoder O
neural O
architecture O
with O
a O
bidirectional O
LSTM B-MethodName
for O
encoding O
the O
input O
sequence O
and O
a O
unidirectional O
LSTM B-MethodName
for O
decoding O
and O
producing O
the O
output O
. O

This O
is O
achieved O
by O
simulating O
a O
FST O
using O
neural B-MethodName
networks I-MethodName
. O

9.00 O
74.00 O
french O
84.30 O
89.80 O
georgian O
97.80 O
98.40 O
old O
- O
french O
0.00 O
00.00 O
german O
37.40 O
42.50 O
greek O
81.10 O
85.90 O
old O
- O
saxon O
54.50 O
54.80 O
kannada O
99.00 O
98.00 O
haida O
96.00 O
93.00 O
pashto O
84.00 O
89.00 O
north O
- O
frisian O
15.00 O
69.00 O
hebrew O
85.70 O
87.20 O
persian O
95.60 O
97.70 O
old O
- O
english O
28.20 O
30.00 O
hindi O
89.40 O
90.60 O
portuguese O
84.00 O
84.50 O
polish O
87.80 O
90.40 O
hungarian O
79.50 O
86.50 O
quechua O
96.80 O
98.30 O
russian O
86.80 O
91.40 O
icelandic O
80.60 O
89.90 O
romanian O
82.00 O
88.00 O
Average B-MetricName
72.49 B-MetricValue
83.77 O
Table O
1 O
: O
Accuracy O
ﬁgures O
for O
all O
languages O
in O
the O
SIGMORPHON O
Shared O
Task O
2018 O
For O
all O
languages O
we O
used O
a O
two O
- O
layer O
encoder O
with O
200 O
LSTM B-MethodName
cells O
( O
in O
each O
direction O
- O
total O
400 O
cells O
per O
layer O
) O
and O
a O
two O
- O
layer O
decoder O
of O
200 O
unidirectional O
cells O
. O

For O
almost O
all O
languages O
, O
after O
correcting O
the O
bug O
, O
the O
accuracy B-MetricName
strongly O
in O
- O
creased O
; O
for O
Welsh O
we O
observed O
no O
increase O
, O
and O
only O
for O
2 B-MetricValue
languages O
did O
we O
observe O
a O
less O
than O
1 O
point O
decrease O
( O
probably O
due O
to O
weight B-HyperparameterName
initialization B-HyperparameterName
compounded O
by O
small O
models O
where O
the O
LSTMs B-MethodName
overcame O
the O
ﬁxed O
random O
weights B-HyperparameterName
of O
the O
dense O
layers O
) O
. O

2.2 O
The O
shape O
bias O
in O
neural B-MethodName
networks I-MethodName
Early O
connectionist O
models O
of O
associative O
learning O
demonstrated O
how O
it O
was O
possible O
to O
learn O
the O
shape O
bias O
via O
statistical O
regularities O
in O
lexical O
categories O
( O
Samuelson O
, O
2002 O
; O
Regier O
, O
2003 O
, O
2005 O
) O
. O

With O
the O
advent O
of O
situated O
agents O
and O
emergent O
language O
agents O
, O
the O
use O
of O
neural B-MethodName
networks I-MethodName
to O
study O
the O
learning O
process O
for O
the O
shape O
bias O
reemerged O
, O
reproducing O
similar O
results O
to O
earlier O
connectionist O
models O
supporting O
associative O
learning O
( O
Hill O
et O
al O
. O
, O
2019 O
) O
and O
demonstrating O
how O
existing O
perceptual O
biases O
can O
inﬂuence O
lexical O
category O
formation O
( O
Ohmer O
et O
al O
. O
, O
2021 O
) O
. O

This O
image O
goes O
through O
the O
vision O
modules O
and O
the O
output O
of O
the O
representation O
encoder O
acts O
as O
the O
initial O
hidden O
state O
of O
the O
production O
module O
, O
a O
single O
layered O
LSTM B-MethodName
. O

The O
LSTM B-MethodName
sequentially O
generates O
the O
speaker O
’s O
message O
. O

If O
the O
agent O
is O
the O
listener O
, O
the O
message O
mgenerated O
by O
the O
speaker O
is O
used O
as O
the O
input O
of O
the O
comprehension O
module O
, O
here O
again O
an O
LSTM B-MethodName
. O

The O
ﬁnal O
state O
of O
the O
LSTM B-MethodName
is O
combined O
by O
a O
dot O
product O
with O
the O
output O
of O
the O
representation O
encoder O
for O
each O
image O
in O
the O
set O
of O
images O
Iof O
a O
given O
game O
. O

In O
this O
ﬁnal O
section O
, O
we O
address O
what O
our O
results O
may O
teach O
us O
about O
the O
shape O
bias O
in O
human O
learners O
and O
neural B-MethodName
networks I-MethodName
. O

Other O
types O
of O
neural B-MethodName
networks I-MethodName
have O
been O
shown O
to O
develop O
a O
shape O
bias O
( O
1 O
) O
by O
increasing O
the O
size O
of O
models O
or O
( O
2 O
) O
by O
augmenting O
their O
input O
data O
( O
Hosseini O
et O
al O
. O
, O
2018 O
; O
Geirhos O
et O
al O
. O
, O
2019 O
; O
Hill O
et O
al O
. O
, O
2019 O
; O
Bhojanapalli O
et O
al O
. O
, O
2021 O
; O
Tuli O
et O
al O
. O
, O
2021 O
) O
. O

We O
have O
extended O
an O
LSTM B-MethodName
- O
based O
neural O
network O
designed O
for O
sequence O
tagging O
to O
additionally O
generate O
character O
- O
level O
sequences O
. O

Part O
- O
of O
- O
speech O
tagging O
is O
usually O
deﬁned O
as O
a O
sequence O
tagging O
problem O
and O
is O
solved O
with O
recurrent O
or O
convolutional O
neural B-MethodName
networks I-MethodName
using O
word O
- O
level O
softmax O
outputs O
or O
conditional O
random O
ﬁelds O
( O
Lample O
et O
al O
. O
, O
2016 O
; O
Strubell O
et O
al O
. O
, O
2017 O
; O
Chiu O
and O
Nichols O
, O
2016 O
) O
. O

We O
map O
each O
character O
to O
a O
randomly O
initialized O
30 O
- O
dimensional O
vectorbcj O
i O
= O
Emb30(cj O
i O
) O
, O
and O
apply O
a O
bi O
- O
directional O
LSTM B-MethodName
on O
these O
embeddings O
. O

echar(wi)is O
the O
concatenation O
of O
the O
25 O
- O
dimensional O
ﬁnal O
states O
of O
two O
LSTMs B-MethodName
. O

We O
use O
two O
types O
of O
recurrent O
cells O
: O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
and O
GRU O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
. O

We O
apply O
three O
layers O
of O
LSTM B-MethodName
with O
150dimensional O
hidden O
states O
on O
the O
embedding O
vectors O
: O
hj O
i O
= O
LSTM B-MethodName
hj 1 O
i;hj 1 O
i 1 O
j= O
1;2;3 O
whereh0 O
i O
= O
e(wi O
) O
. O

We O
also O
apply O
50 O
% O
dropout B-HyperparameterName
before O
each O
LSTM B-MethodName
layer O
. O

1.h3 O
iis O
the O
representation O
of O
the O
i O
- O
th O
word O
after O
feature O
extractor O
LSTMs B-MethodName
. O

2.bcj O
i O
= O
Emb30(cj O
i)is O
the O
same O
embedding O
of O
thej O
- O
th O
character O
of O
the O
word O
used O
in O
the O
character O
- O
level O
BiLSTM B-MethodName
described O
in O
Section O
3.1 O
. O

The O
initial O
state O
of O
the O
GRU O
is O
the O
output O
of O
the O
feature O
extractor O
LSTM B-MethodName
: O
s0 O
i O
= O
h3 O
i. O
All O
GRUs O
share O
the O
weights B-HyperparameterName
. O

5 O
Discussion O
5.1 O
Input O
vectors O
for O
lemma O
generation O
The O
initial O
versions O
of O
the O
lemma O
decoder O
did O
not O
get O
the O
state O
of O
the O
LSTM B-MethodName
below O
h3 O
iand O
positional O
embedding O
j O
ias O
inputs O
. O

5.2 O
Balancing O
different O
tasks O
Multitask O
learning O
in O
neural B-MethodName
networks I-MethodName
is O
usually O
complicated O
because O
of O
varying O
difﬁculty O
of O
individual O
tasks O
. O

The O
only O
trick O
we O
used O
was O
to O
apply O
dropout B-HyperparameterName
layers O
before O
feature O
extractor O
LSTMs B-MethodName
. O

2https://lucene.apache.org O
3While O
this O
BOW O
approach O
is O
not O
ideal O
in O
many O
ways O
, O
it O
performed O
equivalently O
to O
far O
more O
complicated O
approaches O
such O
as O
LSTMs B-MethodName
and O
GRUs O
, O
also O
noted O
by O
( O
Iyyer O
et O
al O
. O
, O
2015 O
) O
, O
likely O
due O
to O
the O
limited O
training O
data O
in O
this O
domain.ing O
cosine O
similarity O
. O

Embeddings O
: O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
trained O
on O
100 O
G O
tokens O
of O
Google O
News O
data,10 O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
trained O
on O
6 O
G O
tokens O
of O
wikipedia O
+ O
newspapers O
or O
42 O
G O
tokens O
of O
Common O
Crawl O
( O
CC O
) O
Web O
data,11and O
FastText O
( O
Joulin O
et O
al O
. O
, O
2017 O
) O
trained O
on O
600 O
G O
tokens O
of O
CC O
. O

The O
FastText O
model O
uses O
subword O
embeddings O
to O
approximate O
unknown O
words O
, O
also O
resulting O
in O
full O
coverage.n= O
2359 O
n= O
3836 O
USF O
EAT O
model O
span O
acc O
acc O
DSM O
2 O
76.01 O
% O
81.78 O
% O
DSM O
P=0 O
2 O
74.31 O
% O
78.62 O
% O
DSM O
10 O
76.98 O
% O
82.46 O
% O
DSM O
P=0 O
10 O
76.39 O
% O
79.30 O
% O
P(w2jw1 O
) O
2 O
73.76 O
% O
80.29 O
% O
logG22 O
71.98 O
% O
78.28 O
% O
PPMI O
2 O
68.80 O
% O
76.12 O
% O
MI22 O
73.51 O
% O
79.87 O
% O
P(w2jw1 O
) O
10 O
77.58 O
% O
84.02 O
% O
logG210 O
77.83 O
% O
83.00 O
% O
PPMI O
10 O
73.80 O
% O
81.18 O
% O
MI22 O
78.64 O
% O
83.92 O
% O
Combined O
2 O
77.66 O
% O
85.01 O
% O
Combined O
10 O
80.58 O
% O
85.74 O
% O
Combined O
mix O
80.46 O
% O
86.03 O
% O
miss O
: O
7 O
miss O
: O
119 O
word2vec O
– O
76.11 O
% O
77.78 O
% O
miss O
: O
2 O
miss O
: O
44 O
GloVe B-MethodName
– O
76.71 O
% O
79.80 O
% O
miss O
: O
7 O
GloVe B-MethodName
CC O
– O
80.12 O
% O
81.72 O
% O
FastText O
– O
82.24 O
% O
83.97 O
% O
miss O
: O
7 O
miss O
: O
116 O
word2vec O
wf O
– O
76.53 O
% O
79.25 O
% O
miss O
: O
2 O
miss O
: O
38 O
GloVe B-MethodName
wf O
– O
77.17 O
% O
81.07 O
% O
miss O
: O
2 O
GloVe B-MethodName
CC O
wf O
– O
80.75 O
% O
83.07 O
% O
FastText O
CC O
wf O
– O
83.21 O
% O
85.58 O
% O
Table O
1 O
: O
Multiple O
choice O
task O
( O
baseline O
: O
33.33 O
% O
) O
. O

mix O
45.36 O
% O
4.8 O
39.48 O
% O
6.4 O
miss O
: O
7 O
miss O
: O
119 O
w2v O
– O
38.98 O
% O
7.7 O
30.51 O
% O
14.8 O
miss O
: O
2 O
miss O
: O
44 O
GloVe B-MethodName
– O
39.22 O
% O
7.6 O
30.19 O
% O
13.8 O
miss O
: O
7 O
GloVe B-MethodName
CC O
– O
44.01 O
% O
5.7 O
34.26 O
% O
10.5 O
FastText O
CC O
– O
51.00 O
% O
4.1 O
40.34 O
% O
7.2 O
miss O
: O
7 O
miss O
: O
116 O
w2v O
wf O
– O
39.60 O
% O
7.4 O
31.29 O
% O
13.5 O
miss O
: O
2 O
miss O
: O
38 O
GloVe B-MethodName
wf O
– O
39.74 O
% O
7.3 O
31.07 O
% O
12.5 O
miss O
: O
2 O
GloVe B-MethodName
CC O
wf O
– O
44.50 O
% O
5.5 O
34.91 O
% O
9.7 O
FastText O
CC O
wf O
– O
51.43 O
% O
3.9 O
41.30 O
% O
6.4 O
Table O
2 O
: O
Open O
- O
vocabulary O
lexical O
access O
task O
. O

It O
is O
worth O
recalling B-MetricName
that O
Landauer O
and O
Dumais O
( O
1997 O
) O
suggested O
a O
solution O
to O
the O
problem O
which O
involved O
the O
use O
of O
Principal O
Component O
Analysis O
( O
PCA B-MethodName
) O
as O
a O
dimensionality O
reduction O
method O
over O
a O
distributional O
matrix O
. O

The O
use O
of O
PCA B-MethodName
was O
meant B-MetricName
to O
capture O
the O
main O
axes O
of O
variance O
over O
the O
limited O
data O
given O
to O
the O
model O
, O
allowing O
for O
fast O
generalisation O
. O

We O
have O
used O
simple O
cooccurrence O
prediction O
for O
the O
acquisition O
of O
word O
semantics O
and O
non O
- O
linear B-MethodName
regression I-MethodName
/ O
classiﬁcationfor O
task O
- O
speciﬁc O
competences O
, O
which O
goes O
well O
with O
claims O
that O
language O
can O
be O
acquired O
via O
generic O
cognitive O
functions O
. O

A O
Morphology O
- O
based O
Representation O
Model O
for O
LSTM B-MethodName
- O
based O
Dependency O
. O

We O
have O
tested O
our O
models O
on O
an O
LSTM B-MethodName
- O
based O
dependency O
parser O
with O
character O
- O
based O
word O
embeddings O
proposed O
by O
Ballesteros O
et O
al O
. O
( O
2015 O
) O
. O

We O
apply O
our O
approach O
to O
a O
transitionbased O
dependency O
parser O
by O
Ballesteros O
et O
al O
. O
( O
2015 O
) O
that O
uses O
stack O
Long O
Short O
Term O
Memory O
structures O
( O
LSTMs B-MethodName
) O
to O
predict O
the O
parser O
state O
. O

The O
rest O
of O
the O
paper O
is O
organized O
as O
follows O
: O
Section O
2 O
provides O
a O
brief O
description O
of O
the O
LSTM B-MethodName
- O
based O
dependency O
parser O
used O
in O
this O
study O
and O
introduces O
our O
embedding O
models O
. O

2 O
Parsing O
Model O
We O
use O
the O
LSTM B-MethodName
- O
based O
parser O
by O
Ballesteros O
et O
al O
. O
( O
2015 O
) O
. O

It O
is O
an O
improved O
version O
of O
a O
stateof O
- O
the O
- O
art O
transition O
- O
based O
dependency O
parser O
proposed O
by O
Dyer O
et O
al O
. O
( O
2015 O
) O
and O
uses O
stack O
LSTM B-MethodName
structures O
with O
push O
and O
pop O
operations O
to O
learn O
representations O
of O
the O
parser O
state O
. O

Instead O
of O
lookup O
- O
based O
word O
representations O
, O
bidirectional O
LSTM B-MethodName
modules O
are O
used O
to O
create O
character O
- O
based O
encodings O
of O
words O
. O

2.1 O
Character O
Embeddings O
of O
Words O
The O
character O
- O
based O
word O
embedding O
model O
using O
bi O
- O
LSTMs B-MethodName
in O
( O
Ballesteros O
et O
al O
. O
, O
2015 O
) O
is O
depicted O
in O
Figure O
1 O
. O

The O
authors O
compute O
character O
- O
based O
vector O
representations O
of O
words O
using O
bi O
- O
LSTMs B-MethodName
. O

2.2 O
Morphology O
- O
based O
Character O
Embeddings O
To O
improve O
the O
parsing O
performance O
of O
the O
LSTM B-MethodName
parser O
with O
character O
- O
based O
word O
embeddings O
mentioned O
in O
Section O
2.1 O
, O
we O
include O
the O
morphological O
information O
of O
words O
to O
the O
embedding O
model O
. O

The O
source O
code O
of O
our O
modiﬁed O
version O
of O
the O
LSTM B-MethodName
- O
based O
parser O
by O
Ballesteros O
et O
al O
. O
( O
2015 O
) O
can O
be O
found O
at O
https://github O
. O

The O
underlying O
contextualized O
method O
in O
our O
study O
is O
the O
pretrained O
BERT B-MethodName
base O
cased O
model1(Devlin O
et O
al O
. O
, O
2019 O
) O
. O

BERT B-MethodName
is O
trained O
using O
a O
transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
with O
masked O
language O
modelling O
( O
MLM O
) O
and O
next O
sentence O
prediction O
( O
NSP O
) O
tasks O
. O

NSP O
trains O
text O
- O
pair O
representations O
to O
predict O
whether O
the O
text O
- O
pair O
contains O
consecutive O
sentences O
from O
a O
monolingual O
corpus.2 O
We O
work O
with O
two O
BERT B-MethodName
variants O
. O

First O
, O
we O
explore O
aligning O
independently O
trained O
BERT B-MethodName
models O
, O
that O
is O
, O
models O
with O
separate O
model O
parameters O
for O
each O
language O
. O

For O
Spanish O
and O
English O
, O
since O
there O
is O
no O
pretrained O
BERT B-MethodName
Spanish O
model O
, O
we O
take O
the O
Spanish O
embeddings O
from O
the O
BERT B-MethodName
multilingual O
model O
and O
align O
it O
with O
the O
monolingual O
English O
model O
. O

We O
have O
also O
experimented O
with O
directly O
aligning O
embeddings O
obtained O
from O
the O
BERT B-MethodName
multilingual O
model O
, O
which O
is O
a O
joint O
model O
trained O
with O
the O
same O
model O
parameters O
with O
shared O
subword O
vocabulary O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

1To O
produce O
the O
contextualized O
representation O
for O
a O
word O
in O
context O
, O
we O
average B-MetricName
the O
12 B-MetricValue
hidden B-HyperparameterName
layers I-HyperparameterName
of O
the O
word O
’s O
subword O
representations O
in O
BERT B-MethodName
and O
then O
average B-MetricName
the O
subword O
representations O
as O
input O
for O
the O
cross O
- O
lingual O
alignment O
. O

2We O
have O
also O
experimented O
with O
ELMo B-MethodName
in O
lieu O
of O
BERT B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Che O
et O
al O
. O
, O
2018 O
) O
. O

However O
, O
as O
we O
reach O
similar O
conclusions O
in O
terms O
of O
relative O
performance O
, O
while O
BERT B-MethodName
- O
based O
cross O
- O
lingual O
embeddings O
outperform O
their O
ELMo B-MethodName
- O
based O
counterparts O
in O
absolute O
terms O
, O
we O
do O
not O
report O
ELMo B-MethodName
’s O
results O
for O
brevity O
. O

All O
the O
traditional O
word O
embedding O
models O
( O
e.g. O
, O
skipgram O
, O
CBOW B-MethodName
, O
fastText O
) O
provide O
such O
embeddings O
, O
and O
cross O
- O
lingual O
alignment O
is O
typically O
applied O
onthese O
type O
- O
level O
embeddings O
( O
Ruder O
et O
al O
. O
, O
2019 O
) O
. O

Contextualized O
models O
such O
as O
BERT B-MethodName
provide O
token O
- O
level O
embeddings O
by O
default O
: O
a O
natural O
way O
to O
align O
these O
embeddings O
is O
token O
- O
level O
alignment O
. O

Fasttext O
) O
type O
Models O
monolingual O
English O
BERT B-MethodName
model O
mono O
en O
monolingual O
Chinese O
BERT B-MethodName
model O
mono O
zh O
BERT B-MethodName
multilingual O
English O
model O
multi O
en O
BERT B-MethodName
multilingual O
Spanish O
model O
multi O
es O
Fasttext O
baseline O
fasttext O
Alignment O
techniques O
the O
original O
orthogonal O
linear O
transformation O
orig O
post O
- O
processing O
linear O
transformation O
after O
the O
orthogonal O
transformation O
mim O
Evaluation O
level O
Evaluated O
on O
token O
- O
level O
representations O
[ O
token O
] O
Evaluated O
on O
type O
- O
level O
representations O
[ O
type O
] O
Table O
1 O
: O
Different O
components O
used O
for O
the O
model O
conﬁgurations O
in O
our O
evaluation O
. O

BERT B-MethodName
variants O
( O
see O
x3.1 O
) O
are O
taken O
from O
Devlin O
et O
al O
. O
( O
2019 O
) O
. O

For O
comparison O
with O
BERT B-MethodName
, O
we O
also O
run O
fasttext O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
to O
produce O
baseline O
static O
embeddings O
using O
the O
same O
training O
Wikipedia O
corpora O
for O
English O
, O
Chinese O
and O
Spanish O
. O

Although O
BERT B-MethodName
was O
pretrained O
on O
a O
corpus O
comprising O
3.3B O
words O
, O
it O
is O
reasonable O
to O
assume O
that O
it O
is O
easier O
to O
procure O
abundant O
monolingual O
data O
than O
parallel O
data O
. O

For O
the O
EN O
– O
ZH O
Sentence O
Retrieval O
, O
aligning O
independently O
trained O
BERT B-MethodName
models O
outperforms O
aligning O
embeddings O
with O
shared O
vocabulary O
. O

Previous O
studies O
utilize O
pre O
- O
trained O
models O
on O
large O
- O
scale O
corpora O
such O
as O
BERT B-MethodName
, O
or O
perform O
reasoning O
on O
knowledge O
graphs O
. O

Pre O
- O
trained O
language O
models O
( O
PTLMs O
) O
with O
distributed O
representations O
, O
such O
as O
BERT B-MethodName
, O
RoBERTa B-MethodName
, O
XLNet B-MethodName
and O
ELECTRA(Devlin B-MethodName
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2019 O
; O
Clark O
et O
al O
. O
, O
2020 O
) O
, O
have O
performed O
well O
on O
many O
Natural O
Language O
Processing O
( O
NLP O
) O
tasks O
. O

2 O
Related O
Work O
Our O
work O
is O
related O
to O
commonsense O
reasoning O
and O
graph O
neural B-MethodName
networks I-MethodName
. O

Entity O
- O
GCN(Cao O
et O
al O
. O
, O
2019a O
) O
extracts O
an O
entitygraph O
from O
the O
context O
of O
the O
question O
, O
and O
models O
the O
inference O
process O
through O
graph O
neural B-MethodName
networks I-MethodName
. O

Graph O
Reasoning+XLNet(Lv B-MethodName
et O
al O
. O
, O
2020 O
) O
utilizes O
the O
evidence O
subgraphs O
extracted O
from O
ConceptNet O
and O
Wikipedia O
simultaneously O
to O
infer O
commonsense O
. O

The O
statement O
vector O
sin O
the O
above O
equation O
is O
obtained O
from O
a O
certain O
language O
encoder O
, O
which O
can O
either O
be O
a O
trainable O
sequence O
encoder O
like O
LSTM B-MethodName
or O
features O
extracted O
from O
pre O
- O
trained O
universal O
language O
encoders O
like O
BERT B-MethodName
/ O
RoBERTa B-MethodName
) O
. O

We O
use O
TransE O
( O
100 O
dimension O
) O
initialized O
by O
GloVe B-MethodName
to O
pre O
- O
train O
the O
knowledge O
embedding O
. O

The O
ﬁrst O
group O
are O
pre O
- O
trained O
models O
without O
external O
knowledge O
, O
including O
BERT B-MethodName
, O
XLNet B-MethodName
, O
RoBERTa B-MethodName
, O
ELECTRA B-MethodName
. O

The O
second O
group O
consists O
of O
models O
with O
external O
knowledge O
, O
include O
KagNet(Lin O
et O
al O
. O
, O
2019 O
) O
, O
RoBERTa+IR B-MethodName
, O
RoBERTa+KE B-MethodName
, O
HyKAS(Ma O
et O
al O
. O
, O
2019 O
) O
, O
RoBERTa+KEDGN B-MethodName
, O
XLNet+Graph B-MethodName
Reasoning(Lv O
et O
al O
. O
, O
2020 O
) O
. O

XLNet+Graph B-MethodName
Reasoning(Lv O
et O
al O
. O
, O
2020 O
) O
combines O
different O
knowledge O
sources O
( O
ConceptNet O
, O
WikiPedia O
) O
to O
construct O
multiple O
subgraphs O
, O
and O
infer O
the O
answers O
through O
the O
graph O
network O
attention O
mechanism O
. O

RoBERT+KE B-MethodName
and O
RoBERTa+IR B-MethodName
adopt O
RoBERTa B-MethodName
as O
the O
baseline O
and O
utilize O
the O
evidence O
from O
Wikipedia O
and O
search O
engine O
, O
respectively O
. O
Model O
NDevAcc.(%)NTestAcc O
. O

( O
% O
) O
BERT B-MethodName
- O
Large O
60.3 O
55.8 O
XLNet B-MethodName
- O
large O
74.8 O
62.1 O
RoBERTa B-MethodName
- O
Large O
72.8 O
69.1 O
ELECTRA B-MethodName
- O
Large O
77.9 O
72.1 O
BERT B-MethodName
- O
Large- O
RaB O
- O
PR O
63.3 O
60.8 O
XLNet B-MethodName
- O
large- O
RaB O
- O
PR O
75.1 O
71.9 O
RoBERTa B-MethodName
- O
Large- O
RaB O
- O
PR O
75.6 O
73.7 O
ELECTRA B-MethodName
- O
Large- O
RaB O
- O
PR O
76.1 O
72.3 O
Table O
1 O
: O
Comparisons O
with O
large O
pre O
- O
trained O
language O
model O
using O
the O
new O
split O
Model O
ODevAcc.(%)OTestAcc O
. O

( O
% O
) O
KagNet O
64.4 O
58.9 O
RoBERTa+IR B-MethodName
78.9 O
72.1 O
HyKAS O
- O
73.2 O
RoBERTa+KE B-MethodName
- O
73.3 O
RoBERTa+KEDGN B-MethodName
- O
74.4 O
XLNet+Graph B-MethodName
Reasoning O
79.3 O
75.3 O
RaB O
- O
PR O
81.6 O
76.7 O
Table O
2 O
: O
Comparison O
with O
ofﬁcial O
benchmark O
baseline O
methods O
using O
the O
ofﬁcial O
split O
5.4 O
Performance O
Analysis O
We O
conduct O
the O
experiments O
on O
our O
new O
splits O
to O
investigate O
whether O
our O
method O
can O
also O
work O
well O
on O
other O
universal O
language O
encoders O
( O
BERT B-MethodName
, O
XLNet B-MethodName
and O
ELECTRA B-MethodName
) O
. O

For O
the O
test O
set O
, O
Table O
1 O
shows O
that O
our O
methods O
using O
ﬁxed O
pre O
- O
trained O
language O
encoders O
outperform O
ﬁne O
- O
tuning O
in O
all O
settings O
, O
achieving O
60.8 O
% O
with O
BERT B-MethodName
- O
large O
, O
71.9 O
% O
with O
XLNet B-MethodName
- O
large O
, O
73.7 O
% O
with O
RoBERTa B-MethodName
- O
large O
, O
and O
72.3 O
% O
with O
ELECTRA B-MethodName
- O
large O
. O

Methods O
based O
on O
RoBERTa B-MethodName
and O
XLNet B-MethodName
are O
much O
better O
than O
other O
baseline O
methods O
, O
demonstrating O
the O
ability O
of O
language O
models O
to O
utilize O
commonsense O
knowledge O
in O
an O
implicit O
way O
. O

Compared O
with O
XLNet+Graph B-MethodName
Reasoning O
, O
our O
approach O
achieves O
a O
1.4 O
% O
absolute O
improvement O
, O
largely O
due O
to O
aggregating O
the O
multisource O
knowledge O
. O

( O
% O
) O
RaB O
- O
PR O
73.7 O
-w O
/ O
o O
WikiPedia4.1 O
72.9 O
-Replace O
CompGCN O
w O
/ O
GCN O
4.2 O
70.2 O
-Replace O
CompGCN O
w O
/ O
RGCN O
4.2 O
71.1 O
-Replace O
BiGRU O
w O
/ O
GRU O
4.3 O
70.7 O
-w O
/ O
o O
Attention O
4.4 O
71.5 O
-w O
/ O
o O
Negation O
Mode O
4.4 O
72.2 O
Table O
3 O
: O
Ablation O
experiment O
using O
the O
new O
split O
framework O
outperforms O
strong O
baselines O
including O
KagNet O
, O
HyKAS O
, O
XLNet+Graph B-MethodName
Reasoning O
, O
achieving O
an O
absolute O
improvement O
of O
1.4 O
points O
in O
accuracy B-MetricName
on O
the O
test O
data O
, O
reaching O
a O
new O
stateof O
- O
the O
- O
art O
performance O
. O

CMU O
ELMO O
FLAIR O
Batch O
size O
32 O
32 O
32 O
Max O
epochs B-HyperparameterName
150 B-HyperparameterValue
150 O
150 O
LSTM B-MethodName
hidden B-HyperparameterName
size I-HyperparameterName
256 B-HyperparameterValue
256 O
256 O
LSTM B-MethodName
layers O
1 O
2 O
1 O
Learning O
rate O
0.1 O
0.1 O
0.1 O
Patience O
3 O
4 O
3 O
Table O
1 O
: O
The O
most O
important O
parameters O
for O
the O
reproduced O
models O
( O
the O
other O
parameters O
remained O
as O
default O
library O
options O
) O
. O

In O
contrast O
to O
the O
other O
models O
, O
the O
BERT B-MethodName
model O
works O
in O
a O
ﬁne O
tune O
mode O
in O
which O
we O
used O
a O
pretrained O
language O
model O
and O
added O
a O
simple O
classiﬁcation O
layer O
on O
all O
heads O
of O
tokens O
( O
the O
ﬁrst O
BPE O
sub O
- O
token O
for O
each O
original O
word O
) O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

In O
order O
to O
ﬁne O
- O
tune O
the O
BERT B-MethodName
- O
base O
model O
, O
we O
used O
the O
’ O
huggingface O
’ O
library O
, O
version O
0.6.1 O
( O
pretrained O
BERT B-MethodName
, O
2019 O
) O
, O
with O
the O
following O
parameters O
: O
max O
sequence O
length O
– O
256 O
; O
batch B-HyperparameterName
size I-HyperparameterName
– O
16 B-HyperparameterValue
; O
learning B-HyperparameterName
rate I-HyperparameterName
– O
1e-5 O
; O
warmup O
proportion O
– O
0.4 B-HyperparameterValue
; O
number O
of O
train O
epoch O
– O
100 O
. O

The O
Stanford O
and O
BERT B-MethodName
models O
used O
the O
BIO O
tag O
encoding O
type O
, O
the O
other O
models O
were O
trained O
with O
the O
IOBES O
encoding O
type O
. O

In O
recent O
years O
, O
such O
analyses O
has O
gained O
popularity O
in O
the O
NLP O
community O
asresearchers O
have O
shifted O
their O
focus O
towards O
interpretability O
in O
neural B-MethodName
networks I-MethodName
( O
Alishahi O
et O
al O
. O
, O
2019 O
; O
Linzen O
et O
al O
. O
, O
2019 O
) O
. O

For O
a O
vocabulary O
V O
, O
at O
the O
time O
step O
t 1the O
network O
computes O
the O
probability O
distribution O
y O
tof O
possible O
target O
words O
as O
follows O
: O
et O
= O
Eyt 1 O
ht O
= O
f(et;ht 1 O
) O
at O
= O
Wht+b O
y O
t O
= O
Softmax O
( O
at)(3.1 O
) O
wherefconsists O
of O
one O
or O
many O
temporally O
compatible O
layers O
, O
such O
as O
LSTMs B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
or O
masked O
transformers O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

4 O
Methodology O
For O
our O
research O
, O
we O
require O
a O
NNLM O
that O
provides O
good O
performance O
without O
weight B-HyperparameterName
tying O
, O
so O
thatSemantic O
Similarity O
Semantic O
Relatedness O
Hybrid O
BrainBench O
Models O
WordSim O
- O
S O
SimLex999 O
WordSim O
- O
R O
MEN O
MTurk O
WordSim O
RW O
fMRI O
MEG O
Distributional O
Semantic O
Models O
Word2Vec B-MethodName
0.759 B-HyperparameterValue
0.400 O
0.555 O
0.725 O
0.660 O
0.645 O
0.637 O
0.687 O
0.677 O
GloVe B-MethodName
0.680 O
0.352 O
0.475 O
0.727 O
0.604 O
0.546 O
0.530 O
0.657 O
0.623 O
FastText O
0.782 O
0.391 O
0.585 O
0.742 O
0.678 O
0.668 O
0.647 O
0.680 O
0.682 O
Pretrained O
NNLM O
Representations O
Input O
Embs O
. O

The O
JLM O
network O
consists O
of O
a O
character O
level O
embedding O
input O
and O
two O
LSTM B-MethodName
layers O
of O
size O
8192 O
, O
which O
both O
incorporate O
a O
projection O
layer O
to O
reduce O
the O
hidden O
state O
dimensionality O
down O
to O
1024 O
. O

For O
this O
, O
we O
use O
the O
skip O
- O
gram O
implementation O
of O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
FastText O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
using O
the O
gensim O
package2and O
the O
Python O
implementation O
of O
Facebook O
’s O
FastText3re2https://radimrehurek.com/gensim/ O
3https://pypi.org/project/fasttext/spectively O
. O

Word2Vec B-MethodName
was O
trained O
with O
embeddings O
of O
size O
300and O
a O
context O
window O
of O
5 O
, O
while O
FastText O
uses O
the O
default O
settings O
with O
embedding O
size O
100 O
, O
window O
size O
5 O
, O
and O
ngrams O
of O
sizes O
from O
3to O
6 O
. O

We O
also O
train O
a O
Python O
implementation O
of O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
for O
100epochs B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0:05to O
construct O
word O
embeddings O
of O
size O
300 B-HyperparameterValue
. O

SST2 O
SST5 B-MethodName
TREC O
SICK O
- O
E O
MRPC B-DatasetName
Distributional O
Semantic O
Models O
Word2Vec B-MethodName
70.76 O
71.18 O
85.88 O
86.34 O
75.78 O
38.82 O
79.20 O
71.38 O
66.49 O
/ O
79.87 O
Glove O
68.22 O
69.59 O
84.58 O
86.94 O
73.86 O
36.15 O
78.60 O
70.83 O
69.57 O
/ O
80.97 O
FastText O
70.64 O
67.87 O
85.77 O
87.59 O
77.59 O
38.87 O
73.20 O
70.57 O
66.49 O
/ O
79.87 O
Pretrained O
NNLM O
Representations O
Input O
Embs O
. O

In O
comparison O
to O
the O
SOTA O
distributional O
models O
, O
the O
output O
embeddings O
tend O
to O
only O
beat O
FastText O
onSimLex999 O
andBrainBench O
, O
while O
also O
struggling O
in O
comparison O
to O
Word2Vec B-MethodName
on O
semantic O
relatedness O
and O
hybrid O
tasks O
. O

We O
expect O
the O
output O
embeddings O
to O
perform O
better O
than O
the O
input O
embeddings O
and O
otherModels O
SICK O
- O
R O
STS O
B O
Distributional O
Semantic O
Models O
Word2Vec B-MethodName
75.32 O
57.93 O
Glove O
71.64 O
56.03 O
FastText O
75.95 O
58.52 O
Pretrained O
NNLM O
Representaitons O
Input O
Embs O
. O

When O
predicting O
multiple O
classes O
( O
TREC O
, O
SST5 B-MethodName
) O
, O
the O
input O
embeddings O
perform O
marginally O
better O
than O
the O
output O
embeddings O
, O
though O
the O
AM O
embeddings O
perform O
best O
overall O
on O
both O
binary O
and O
multiclass O
datasets O
. O

Finally O
, O
when O
predicting O
6Results O
on O
these O
tasks O
conﬁrm O
this O
, O
with O
accuracy B-MetricName
at O
chance O
levels O
. O
NNLM O
Input O
NNLM O
Output O
NNLM O
Tied O
Models O
Train O
Validation O
Test O
Train O
Validation O
Test O
Train O
Validation O
Test O
Distributional O
Semantic O
Models O
Word2Vec B-MethodName
57.0 B-MetricValue
58.9 O
59.3 O
59.0 O
67.2 O
67.4 O
74.9 O
64.9 O
65.3 O
Glove O
61.8 O
62.5 O
63.1 O
66.3 O
74.4 O
74.5 O
90.5 O
76.8 O
77.4 O
FastText O
58.0 O
59.7 O
60.2 O
60.6 O
68.9 O
69.0 O
77.9 O
67.6 O
68.0 O
Pretrained O
NNLM O
Representaitons O
Inputs O
Embs O
. O

Indeed O
, O
LSTMs B-MethodName
are O
particularly O
good O
at O
learning O
dependency O
information O
such O
as O
subject O
- O
verb O
agreement O
( O
Linzen O
et O
al O
. O
, O
2016 O
) O
. O

See O
the O
appendix O
for O
training O
details O
, O
which O
closely O
follow O
the O
medium O
- O
sized O
LSTM B-MethodName
model O
presented O
by O
Zaremba O
et O
al O
. O
( O
2014 O
) O
with O
the O
Penn O
Treebank O
dataset O
( O
Marcus O
et O
al O
. O
, O
1993 O
) O
. O

We O
describe O
the O
SEx O
BiST O
parser O
( O
Semantically O
EXtended O
Bi O
- O
LSTM B-MethodName
parser O
) O
developed O
at O
Lattice O
for O
the O
CoNLL O
2018 O
Shared O
Task O
( O
Multilingual O
Parsing O
from O
Raw O
Text O
to O
Universal O
Dependencies O
) O
. O

The O
main O
characteristic O
of O
our O
work O
is O
the O
encoding O
of O
three O
different O
modes O
of O
contextual O
information O
for O
parsing O
: O
( O
i O
) O
Treebank O
feature O
representations O
, O
( O
ii O
) O
Multilingual O
word O
representations O
, O
( O
iii O
) O
ELMo B-MethodName
representations O
obtained O
via O
unsupervised O
learning O
from O
external O
resources O
. O

Methods O
such O
as O
Feed O
Forward O
Neural O
Network O
( O
FFN O
) O
( O
Chen O
and O
Manning O
, O
2014 O
) O
or O
LSTM B-MethodName
- O
based O
word O
representations O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
; O
Ballesteros O
et O
al O
. O
, O
2016 O
) O
have O
been O
proposed O
to O
provide O
ﬁne O
- O
grained O
token O
representations O
, O
and O
these O
methods O
provide O
state O
of O
the O
art O
performance O
. O

Following O
previous O
proposals O
promoting O
a O
model O
- O
transfer O
approach O
with O
lexicalized O
feature O
representations O
( O
Guo O
et O
al O
. O
, O
2016 O
; O
Ammar O
et O
al O
. O
, O
2016 O
; O
Lim O
and O
Poibeau O
, O
2017 O
) O
, O
we O
have O
developed O
the O
SEx O
BiST O
parser O
( O
Semantically O
EXtended O
BiLSTM B-MethodName
parser O
) O
, O
a O
multi O
- O
source O
trainable O
parser O
using O
three O
different O
contextualized O
lexical O
representations O
: O
Corpus O
representation O
: O
a O
vector O
representation O
of O
each O
training O
corpus O
. O

ELMo B-MethodName
representation O
: O
token O
- O
based O
representation O
integrating O
abundant O
contexts O
gathered O
from O
external O
resources O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

The O
function O
Char O
( O
that O
generates O
the O
character O
- O
level O
word O
vector O
ci O
) O
corresponds O
to O
a O
vector O
obtained O
from O
the O
hidden O
state O
representation O
hjof O
the O
LSTM B-MethodName
, O
with O
an O
initial O
state O
h0(m O
is O
the O
length O
of O
token O
ti)1 O
: O
1Note O
that O
irefers O
to O
the O
ithtoken O
in O
the O
sentence O
and O
that O
j O
refers O
to O
the O
jthcharacter O
of O
the O
ithtoken O
. O

Sohj= O
LSTM(ch)(h0 B-MethodName
, O
( O
ch O
1,ch2, O
.. O
ch O
m))j O
ci O
= O
wchm O
For O
LSTM B-MethodName
- O
based O
character O
- O
level O
representations O
, O
previous O
studies O
have O
shown O
that O
the O
last O
hidden B-HyperparameterName
layer I-HyperparameterName
hmrepresents O
a O
summary O
of O
all O
the O
information O
based O
on O
the O
input O
character O
sequences O
( O
Shi O
et O
al O
. O
, O
2017 B-HyperparameterValue
) O
. O

After O
some O
empirical O
experiments O
, O
we O
chose O
bidirectional O
LSTM B-MethodName
encoders O
rather O
than O
a O
single O
directional O
one O
and O
then O
introduced O
the O
hidden O
state O
Hiinto O
the O
two O
- O
layered O
Multi O
- O
Layer O
Perceptron O
( O
MLP O
) O
without O
bias O
terms O
for O
computing O
the O
attention O
weight B-HyperparameterName
ai O
: O
ai O
= O
Sofmax O
( O
watt2tanh(Watt1HiT O
) O
) O
ci O
= O
aiHi O
For O
training O
, O
we O
used O
the O
charter O
- O
level O
word O
representations O
for O
all O
the O
languages O
except O
Kazakh O
and O
Thai O
( O
see O
Section O
5 B-HyperparameterValue
) O
. O

2.3 O
Contextualized O
Representation O
ELMo B-MethodName
( O
Embedding O
from O
Language O
Model O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
) O
is O
a O
function O
that O
provides O
a O
representation O
based O
on O
the O
entire O
input O
sentence O
. O

ELMo B-MethodName
contextualized O
embedding O
is O
a O
new O
technique O
for O
word O
representation O
that O
has O
achieved O
state O
- O
of O
- O
theart O
performance O
across O
a O
wide O
range O
of O
language O
understanding O
tasks O
. O

As O
stated O
in O
the O
original O
paper O
by O
Peters O
et O
al O
. O
( O
2018 O
) O
, O
the O
goal O
is O
to O
“ O
learn O
a O
linear O
combination O
of O
the O
vectors O
stacked O
above O
each O
input O
word O
for O
each O
end O
task O
, O
which O
markedly O
improves O
performance O
over O
just O
using O
the O
top O
LSTM B-MethodName
layer O
” O
. O

We O
trained O
our O
language O
model O
with O
bidirectional O
LSTM B-MethodName
using O
ELMo B-MethodName
as O
an O
intermediate O
layer O
in O
the O
bidirectional O
language O
model O
( O
biLM O
) O
, O
and O
we O
used O
ELMo B-MethodName
embeddings O
to O
improve O
again O
the O
performance O
of O
our O
model O
. O

Ri O
= O
fxLM O
i O
; O
! O
hLM O
i;jj= O
1;:::;Lg O
= O
fhLM O
i;jj= O
0;:::;Lg(1 O
) O
ELMo B-MethodName
i O
= O
E(Ri O
; O
 O
) O
= O
 O
LX O
j=0sjhLM O
i;j O
( O
2 O
) O
In O
( O
1 O
) O
, O
xLM O
iand O
hLM O
i;0are O
word O
embedding O
vectors O
corresponding O
to O
the O
token O
layer O
. O

! O
hLM O
i;jisa O
hidden O
LSTM B-MethodName
vector O
consisting O
of O
a O
multi O
- O
layer O
and O
a O
bidirectional O
LSTM B-MethodName
layer O
. O

In O
( O
2 O
) O
, O
sjis O
softmax O
weight B-HyperparameterName
that O
is O
trainable O
to O
normalize O
multi O
- O
layer O
LSTM B-MethodName
layers O
. O

We O
used O
a O
1024 O
dimensions O
ELMo B-MethodName
embedding O
. O

However O
, O
the O
attention O
mechanism O
for O
the O
character O
- O
level O
word O
vector O
ciis O
focusing O
only O
on O
a O
limited O
number O
of O
features O
within O
the O
token O
, O
and O
2http://opus.nlpl.eu/ O
3https://github.com/jujbob/ O
multilingual O
- O
modelsthe O
word O
representation O
element O
wiis O
thus O
needed O
to O
transform O
a O
bidirectional O
LSTM B-MethodName
, O
as O
a O
way O
to O
capture O
the O
overall O
context O
of O
a O
sentence O
. O

Finally O
, O
a O
token O
is O
encoded O
as O
a O
vector O
gi O
: O
gi= O
BiLSTM(pos)(g0 B-MethodName
, O
( O
x1,x2, O
.. O
xn))i O
We O
transform O
the O
token O
vector O
gito O
a O
vector O
of O
the O
desired O
dimensionality O
by O
two O
- O
layered O
MLP O
with O
a O
bias O
term O
to O
classify O
the O
best O
candidate O
of O
universal O
part O
- O
of O
- O
speech O
( O
UPOS B-TaskName
): O
p0 O
i O
= O
Wpos2leaky O
relu(Wpos1giT O
) O
+ O
bpos O
y0 O
i= O
arg O
max O
jp0 O
ij O
Finally O
, O
we O
randomly O
initialize O
the O
UPOS B-TaskName
embedding O
as O
piand O
map O
the O
predicted O
UPOS B-TaskName
y0 O
i O
as O
a O
POS B-TaskName
vector O
: O
pi= O
Pos O
( O
y0 O
i;pos O
) O
4.2 O
Dependency O
Parser O
To O
take O
into O
account O
the O
predicted O
POS B-TaskName
vector O
on O
the O
main O
target O
task O
( O
i.e. O
parsing O
) O
, O
we O
concatenate O
the O
predicted O
POS B-TaskName
vector O
piwith O
the O
word O
representation O
wiand O
then O
we O
encode O
the O
resulting O
vector O
via O
BiLSTM B-MethodName
. O

This O
enriches O
the O
syntactic O
representations O
of O
the O
token O
by O
back O
- O
propagation O
during O
training O
: O
vi= O
BiLSTM(dep)(v0 B-MethodName
, O
( O
x1,x2, O
.. O
xn))i O
Following O
Dozat O
and O
Manning O
( O
2016 O
) O
, O
we O
used O
a O
deep O
bi O
- O
afﬁne O
classiﬁer O
to O
score B-MetricName
all O
the O
possible O
head O
and O
modiﬁer O
pairs O
Y O
= O
( O
h O
, O
m O
) O
. O

ELMo B-MethodName
. O

We O
used O
ELMo B-MethodName
weights B-HyperparameterName
to O
train O
speciﬁc O
models O
for O
ﬁve O
languages O
: O
Korean O
, O
French O
, O
English O
, O
Japanese O
and O
Chinese O
. O

ELMo B-MethodName
weights B-HyperparameterName
were O
pre O
- O
trained O
using O
the O
CoNLL O
resources O
provided4 O
. O

We O
included O
ELMo B-MethodName
only O
at O
the O
level O
of O
the O
input O
layer O
for O
both O
training O
and O
inference O
( O
we O
set O
up O
dropout B-HyperparameterName
to O
0.5 B-HyperparameterValue
and O
used O
1024 O
dimensions O
for O
the O
ELMo B-MethodName
embedding O
layer O
in O
our O
model O
) O
. O

All O
the O
other O
hyper O
- O
parameters O
are O
the O
same O
as O
for O
our O
other O
models O
( O
without O
ELMo B-MethodName
) O
. O

The O
seeds O
are O
integers O
randomly O
produced O
by O
the O
4http://hdl.handle.net/11234/1-1989 O
5https://github.com/allenai/allennlpCorpus O
UAS O
LAS O
Rank(UAS O
) O
Rank(LAS O
) O
Baseline(LAS O
) O
Overall O
( O
82 O
) O
78.71 O
73.02 O
2 O
4 O
65.80 O
Big O
treebanks O
only O
( O
61 O
) O
85.36 O
80.97 O
4 O
7 O
74.14 O
PUD O
treebanks O
only O
( O
5 O
) O
76.81 O
72.34 O
3 O
3 O
66.63 O
Small O
treebanks O
only O
( O
7 O
) O
75.67 O
68.12 O
2 O
3 O
55.01 O
Low O
- O
resource O
only O
( O
9 O
) O
37.03 O
23.39 O
4 O
5 O
17.17 O
Corpus O
Method O
UAS(Rank O
) O
LAS(Rank O
) O
afafribooms O
87.42 O
( O
7 O
) O
83.72 O
( O
8) O
grcperseus O
tr O
79.15 O
( O
4 O
) O
71.63 O
( O
8) O
grcproiel O
tr O
79.53 O
( O
5 O
) O
74.46 O
( O
8) O
arpadt O
75.96 O
( O
8) O
71.13 O
( O
10 O
) O
hyarmtdp O
tr O
, O
mu O
53.56 O
( O
1 O
) O
37.01 O
( O
1 O
) O
eubdt O
85.72 O
( O
7 O
) O
81.13 O
( O
8) O
brkeb O
tr O
, O
mu O
43.78 O
( O
3 O
) O
23.65 O
( O
5 O
) O
bgbtb O
92.1 O
( O
9 O
) O
88.02 O
( O
11 O
) O
bxrbdt O
tr O
, O
mu O
36.89 O
( O
3 O
) O
17.16 O
( O
4 O
) O
caancora O
92.83 O
( O
6 O
) O
89.56 O
( O
9 O
) O
hrset O
90.18 O
( O
8) O
84.67 O
( O
9 O
) O
cscac O
tr O
93.43 O
( O
2 O
) O
91 O
( O
2 O
) O
csﬁctree O
tr O
94.78 O
( O
1 O
) O
91.62 O
( O
3 O
) O
cspdt O
tr O
92.73 O
( O
2 O
) O
90.13 O
( O
7 O
) O
cspud O
tr O
89.49 O
( O
7 O
) O
83.88 O
( O
9 O
) O
daddt O
85.36 O
( O
8) O
80.49 O
( O
11 O
) O
nlalpino O
tr O
90.59 O
( O
2 O
) O
86.13 O
( O
5 O
) O
nllassysmall O
tr O
87.83 O
( O
2 O
) O
84.02 O
( O
4 O
) O
enewt O
tr O
, O
el O
86.9 O
( O
1 O
) O
84.02 O
( O
2 O
) O
engum O
tr O
, O
el O
88.57 O
( O
1 O
) O
85.05 O
( O
1 O
) O
enlines O
tr O
, O
el O
86.01 O
( O
1 O
) O
81.44 O
( O
2 O
) O
enpud O
tr O
, O
el O
90.83 O
( O
1 O
) O
87.89 O
( O
1 O
) O
etedt O
86.25 O
( O
7 O
) O
82.33 O
( O
7 O
) O
fooft O
tr O
, O
mu O
48.64 O
( O
9 O
) O
25.17 O
( O
17 O
) O
ﬁftb O
tr O
89.74 O
( O
4 O
) O
86.54 O
( O
6 O
) O
ﬁpud O
tr O
90.91 O
( O
4 O
) O
88.12 O
( O
6 O
) O
ﬁtdt O
tr O
88.39 O
( O
6 O
) O
85.42 O
( O
7 O
) O
frgsd O
tr O
, O
el O
89.5 O
( O
1 O
) O
86.17 O
( O
3 O
) O
frsequoia O
tr O
, O
el O
91.81 O
( O
1 O
) O
89.89 O
( O
1 O
) O
frspoken O
tr O
, O
el O
79.47 O
( O
2 O
) O
73.62 O
( O
3 O
) O
glctg O
tr O
84.05 O
( O
7 O
) O
80.63 O
( O
10 O
) O
gltreegal O
tr O
78.71 O
( O
2 O
) O
73.13 O
( O
3 O
) O
degsd O
82.09 O
( O
8) O
76.86 O
( O
11 O
) O
gotproiel O
73 O
( O
6 O
) O
65.3 O
( O
8) O
elgdt O
89.29 O
( O
8) O
86.02 O
( O
11 O
) O
hehtb O
66.54 O
( O
9 O
) O
62.29 O
( O
9 O
) O
hihdtb O
94.44 O
( O
8) O
90.4 O
( O
12 O
) O
huszeged O
80.49 O
( O
8) O
74.21 O
( O
10 O
) O
zhgsd O
tr O
, O
el O
71.48 O
( O
5 O
) O
68.09 O
( O
5 O
) O
idgsd O
85.03 O
( O
3 O
) O
77.61 O
( O
10 O
) O
gaidt O
79.13 O
( O
2 O
) O
69.1 O
( O
4)Corpus O
Method O
UAS(Rank O
) O
LAS(Rank O
) O
itisdt O
tr O
92.41 O
( O
6 O
) O
89.96 O
( O
8) O
itpostwita O
tr O
77.52 O
( O
6 O
) O
72.66 O
( O
7 O
) O
jagsd O
tr O
, O
el O
76.4 O
( O
6 O
) O
74.82 O
( O
6 O
) O
jamodern O
29.36 O
( O
8) O
22.71 O
( O
8) O
kkktb O
tr O
, O
mu O
39.24 O
( O
15 O
) O
23.97 O
( O
9 O
) O
kogsd O
tr O
, O
el O
88.03 O
( O
2 O
) O
84.31 O
( O
2 O
) O
kokaist O
tr O
, O
el O
88.92 O
( O
1 O
) O
86.32 O
( O
4 O
) O
kmr O
mg O
tr O
, O
mu O
38.64 O
( O
3 O
) O
27.94 O
( O
4 O
) O
laittb O
tr O
87.88 O
( O
8) O
84.72 O
( O
8) O
laperseus O
tr O
75.6 O
( O
3 O
) O
64.96 O
( O
3 O
) O
laproiel O
tr O
73.97 O
( O
6 O
) O
67.73 O
( O
8) O
lvlvtb O
tr O
82.99 O
( O
8) O
76.91 O
( O
11 O
) O
pcm O
nsc O
tr O
, O
mu O
18.15 O
( O
21 O
) O
11.63 O
( O
18 O
) O
sme O
giella O
tr O
, O
mu O
76.66 O
( O
1 O
) O
69.87 O
( O
1 O
) O
nobokmaal O
91.4 O
( O
5 O
) O
88.43 O
( O
11 O
) O
nonynorsk O
tr O
90.78 O
( O
8) O
87.8 O
( O
11 O
) O
nonynorsklia O
tr O
76.17 O
( O
2 O
) O
68.71 O
( O
2 O
) O
cuproiel O
77.49 O
( O
6 O
) O
70.48 O
( O
8) O
frosrcmf O
91.35 O
( O
5 O
) O
85.51 O
( O
7 O
) O
faseraji O
89.1 O
( O
7 O
) O
84.8 O
( O
10 O
) O
pllfg O
tr O
95.69 O
( O
8) O
92.86 O
( O
11 O
) O
plsz O
tr O
92.24 O
( O
9 O
) O
88.95 O
( O
10 O
) O
ptbosque O
89.77 O
( O
5 O
) O
86.84 O
( O
7 O
) O
rorrt O
89.8 O
( O
8) O
84.33 O
( O
10 O
) O
rusyntagrus O
tr O
93.1 O
( O
4 O
) O
91.14 O
( O
6 O
) O
rutaiga O
tr O
79.77 O
( O
1 O
) O
74 O
( O
2 O
) O
srset O
90.48 O
( O
10 O
) O
85.74 O
( O
11 O
) O
sksnk O
86.81 O
( O
11 O
) O
82.4 O
( O
11 O
) O
slssj O
tr O
87.18 O
( O
10 O
) O
84.68 O
( O
10 O
) O
slsst O
tr O
63.64 O
( O
3 O
) O
57.07 O
( O
3 O
) O
esancora O
91.81 O
( O
6 O
) O
89.25 O
( O
7 O
) O
svlines O
tr O
85.65 O
( O
4 O
) O
80.88 O
( O
6 O
) O
svpud O
tr O
83.44 O
( O
3 O
) O
79.1 O
( O
4 O
) O
svtalbanken O
tr O
89.02 O
( O
4 O
) O
85.24 O
( O
7 O
) O
thpud O
tr O
, O
mu O
0.33 O
( O
21 O
) O
0.12 O
( O
21 O
) O
trimst O
69.06 O
( O
7 O
) O
60.9 O
( O
11 O
) O
ukiu O
85.36 O
( O
10 O
) O
81.33 O
( O
9 O
) O
hsbufal O
tr O
, O
mu O
54.01 O
( O
2 O
) O
43.83 O
( O
2 O
) O
urudtb O
87.4 O
( O
7 O
) O
80.74 O
( O
10 O
) O
ugudt O
75.11 O
( O
6 O
) O
62.25 O
( O
9 O
) O
vivtb O
49.65 O
( O
6 O
) O
43.31 O
( O
8) O
Table O
1 O
: O
Ofﬁcial O
experiment O
results O
for O
each O
corpus O
, O
where O
tr O
( O
Treebank O
) O
, O
mu O
( O
Multilingual O
) O
andel O
( O
ELMo B-MethodName
) O
in O
the O
column O
Method O
denote O
the O
feature O
representation O
methods O
used O
( O
see O
Section O
2 O
and O
3 O
) O
. O

5.3 O
Hardware O
Resources O
The O
training O
process O
for O
all O
the O
language O
models O
with O
the O
ensemble O
and O
ELMo B-MethodName
was O
done O
using O
32 O
CPUs O
and O
7 O
GPUs O
( O
Geforce O
1080Ti O
) O
in O
approximately O
two O
weeks O
. O

In O
the O
testing O
phase O
on O
the O
TIRA O
platform O
, O
we O
submitted O
our O
models O
separately O
, O
since O
testing O
with O
a O
model O
trained O
with O
ELMo B-MethodName
takes O
around O
three O
hours O
. O

Corpus O
representation O
with O
ELMo B-MethodName
shows O
the O
best O
performance O
for O
parsing O
English O
and O
French O
. O

Language O
Model O
( O
ELMo B-MethodName
) O
. O

We O
used O
ELMo B-MethodName
embeddings O
for O
ﬁve O
languages O
: O
Korean O
, O
French O
, O
English O
, O
Japanese O
and O
Chinese O
( O
they O
are O
marked O
with O
elin O
the O
method O
column O
in O
Table O
1 O
) O
. O

The O
experiments O
with O
ELMo B-MethodName
models O
showed O
excellent O
overall O
performance O
. O

We O
also O
re O
- O
run O
the O
experiment O
by O
adding O
treebank O
and O
ELMo B-MethodName
representations O
. O

The O
results O
are O
shown O
in O
Table O
3 O
( O
emdenotes O
the O
use O
of O
the O
external O
word O
embedding O
and O
trand O
eldenotes O
treebank O
and O
ELMo B-MethodName
representations O
respectively O
. O
) O
. O

This O
is O
especially O
true O
regarding O
LAS O
when O
using O
ELMo B-MethodName
( O
el O
) O
, O
which O
means B-MetricName
this O
representation O
has O
a O
positive O
effect O
on O
relation O
labeling O
. O

7 O
Conclusion O
In O
this O
paper O
, O
we O
described O
the O
SEx O
BiST O
parser O
( O
Semantically O
EXtended O
Bi O
- O
LSTM B-MethodName
parser O
) O
developed O
at O
Lattice O
for O
the O
CoNLL O
2018 O
Shared O
Task O
. O

Our O
system O
was O
an O
extention O
of O
our O
2017 O
parser O
( O
Lim O
and O
Poibeau O
, O
2017 O
) O
with O
three O
deep O
contextual O
representations O
( O
multilingual O
word O
representation O
, O
corpus O
representations O
, O
ELMo B-MethodName
representation O
) O
. O

The O
generalization O
of O
ELMo B-MethodName
representation O
to O
new O
languages O
( O
beyond O
what O
we O
could O
do O
for O
the O
2018 O
evaluation O
) O
should O
also O
have O
a O
positive O
effect O
on O
the O
results O
. O

In O
the O
case O
of O
LSTMs B-MethodName
, O
where O
there O
are O
both O
hidden O
states O
and O
cell O
states O
, O
we O
learn O
a O
fully O
connected O
mapping O
for O
each O
. O

For O
all O
our O
models O
, O
we O
used O
GloVe B-MethodName
word O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

The O
encoder O
and O
decoder O
are O
both O
3 O
- O
layer O
LSTMs B-MethodName
with O
512 O
hidden B-HyperparameterName
units I-HyperparameterName
. O

It O
also O
includes O
a O
classiﬁer O
component O
which O
takes O
the O
hidden O
states O
( O
produced O
by O
the O
LSTM B-MethodName
language O
model O
) O
as O
input O
features O
and O
predicts O
the O
sentiment O
polarity O
. O

Intuitively O
, O
if O
we O
share O
the O
LSTM B-MethodName
layers O
of O
language O
models O
across O
languages O
, O
these O
layers O
are O
likely O
to O
process O
sentences O
from O
different O
languages O
in O
the O
same O
space O
, O
thus O
inducing O
language O
invariant O
features O
. O

Following O
previous O
work O
, O
we O
compute O
the O
probability O
of O
a O
sentence O
xby O
modeling O
the O
probability O
of O
a O
wordwkgiven O
the O
previous O
words O
: O
p(x O
) O
= O
jxjY O
k=1p(wkjw1;:::;wk 1 O
) O
( O
1 O
) O
For O
sentences O
in O
a O
certain O
language O
- O
domain O
pair O
( O
l;d O
) O
, O
the O
probabilities O
are O
computed O
using O
a O
twolayer O
LSTM B-MethodName
language O
model O
, O
which O
includes O
an O
embedding O
layer O
, O
two O
LSTM B-MethodName
layers O
and O
a O
linear O
decoding O
layer O
. O

hidden O
states O
: O
hk O
= O
LSTM B-MethodName
( O
hk 1;~ O
wk;lstm O
1 O
) O
( O
2 O
) O
where~ O
wkdenotes O
the O
embedding O
of O
word O
wk O
. O

These O
hidden O
states O
are O
then O
passed O
through O
the O
second O
LSTM B-MethodName
layer O
which O
is O
domain O
speciﬁc O
but O
language O
invariant O
, O
generating O
a O
sequence O
of O
ﬁnal O
hidden O
states O
: O
zk O
= O
LSTM B-MethodName
( O
zk 1;hk;d O
lstm O
2 O
) O
( O
3 O
) O
where O
the O
second O
LSTM B-MethodName
layer O
of O
domain O
dis O
parameterized O
by O
d O
lstm O
2 O
. O

The O
intuition O
of O
adopting O
a O
domain O
- O
speciﬁc O
LSTM B-MethodName
layer O
is O
that O
the O
distribution O
of O
sentences O
varies O
across O
domains O
. O

A O
language O
discriminator O
is O
trained O
to O
predict O
the O
language O
ID O
given O
the O
features O
by O
minimizing O
the O
cross O
entropy B-MetricName
loss O
, O
while O
the O
LSTM B-MethodName
network O
is O
trained O
to O
fool O
the O
discriminator O
by O
maximizing O
the O
loss O
: O
Jadv(emb;lstm O
1;ds O
lstm O
2;dis O
) O
= O
E(x;l)[ logp(ljx)](6 O
) O
whereemb O
= O
1 O
embjLj O
embdenotes O
the O
parameters O
of O
all O
the O
embedding O
layers O
and O
dis O
denotes O
the O
parameters O
of O
the O
language O
discriminator O
. O

3.5 O
The O
Full O
Objective O
Function O
Putting O
all O
the O
components O
together O
, O
the O
ﬁnal O
objective O
function O
is O
thus O
: O
Jfull(emb;lstm;dec;clf;dis O
) O
= O
X O
( O
l;d)2PJl;d O
lm+ O
 O
Jsenti  O
 O
Jadv O
( O
7 O
) O
wherelstm O
= O
lstm O
11 O
lstm O
2jDj O
lstm O
2 O
denotes O
the O
parameters O
of O
all O
the O
LSTM B-MethodName
layers O
, O
dec=1 O
decjLj O
decdenotes O
the O
parameters O
of O
all O
the O
decoding O
layers O
, O
 O
and O
 O
are O
the O
hyperparameters O
controlling O
the O
importance O
of O
the O
classiﬁcation O
objective O
and O
the O
language O
adversarial O
training O
objective O
. O

In O
the O
CLIDSA O
model O
, O
inputs O
from O
( O
lt;dt)do O
not O
go O
through O
the O
source O
- O
domain O
LSTM B-MethodName
layer O
( O
parameterized O
by O
ls O
lstm O
2 O
) O
, O
thus O
can O
not O
be O
forwarded O
to O
the O
sentiment O
classiﬁer O
for O
sentiment O
prediction O
. O

The O
key O
idea O
is O
simple O
: O
instead O
of O
viewing O
the O
sentiment O
classiﬁer O
as O
a O
linear O
classiﬁer O
that O
takes O
the O
domain O
- O
speciﬁc O
ﬁnal O
hidden O
statesz1;z2;:::;zjxjas O
input O
features O
, O
we O
consider O
the O
source O
- O
domain O
LSTM B-MethodName
layer O
and O
the O
linear O
classiﬁer O
together O
as O
a O
“ O
LSTM+Linear B-MethodName
” O
classiﬁer O
( O
parameterized O
by O
ds O
lstm O
2clf O
) O
that O
takes O
the O
domain O
invariant O
and O
language O
invariant O
hidden O
statesh1;h2;:::;hjxjas O
input O
features O
. O

this O
point O
of O
view O
, O
we O
can O
pass O
the O
ﬁrst O
- O
layer O
hidden O
states O
to O
the O
source O
- O
domain O
LSTM B-MethodName
layer O
and O
the O
sentiment O
classiﬁer O
to O
obtain O
the O
sentiment O
prediction O
at O
test O
time O
. O

At O
training O
time O
, O
the O
ﬁrst O
- O
layer O
hidden O
states O
generated O
from O
a O
target O
sentence O
are O
forwarded O
to O
the O
source O
- O
domain O
LSTM B-MethodName
layer O
( O
ds O
lstm O
2 O
) O
and O
the O
language O
discriminator O
( O
dis O
) O
to O
compute O
the O
adversarial O
loss O
. O

The O
LSTM B-MethodName
layers O
are O
trained O
to O
fool O
the O
language O
discriminator O
so O
that O
it O
can O
not O
distinguish O
the O
examples O
in O
( O
ls;ds)from O
those O
in O
( O
lt;dt O
) O
. O

For O
language O
modeling O
, O
we O
adopt O
the O
AWD O
- O
LSTM B-MethodName
language O
model O
( O
Merity O
et O
al O
. O
, O
2017 O
) O
with O
1150 O
hidden B-HyperparameterName
units I-HyperparameterName
and O
a O
weight B-HyperparameterName
dropout B-HyperparameterName
rate I-HyperparameterName
of O
0.5 B-HyperparameterValue
. O

The O
abbreviations O
fB O
, O
D O
, O
Mgstand O
forfBooks O
, O
DVD O
, O
Music O
g. O
EN O
- O
DE O
EN O
- O
FR O
EN O
- O
JA O
CLIDSAfull O
84.6 O
88.0 O
81.9 O
- O
decoder O
sharing O
83.0 O
87.0 O
78.9 O
- O
LSTM-1 B-MethodName
sharing O
82.4 O
87.1 O
81.4 O
- O
discriminator O
82.6 O
87.4 O
81.6 O
- O
joint O
training O
81.2 O
86.7 O
79.9 O
Table O
4 O
: O
Ablation O
results O
in O
the O
cross O
- O
lingual O
in O
- O
domain O
setting O
. O

We O
ﬁrst O
create O
a O
variant O
that O
does O
not O
share O
the O
decoding O
layers O
across O
domain O
, O
and O
another O
one O
that O
does O
not O
share O
the O
ﬁrst O
LSTM B-MethodName
layer O
across O
domain O
. O

Intuitively O
, O
parameter O
sharing O
would O
force O
the O
LSTM B-MethodName
layers O
to O
process O
sentences O
from O
different O
languages O
in O
the O
same O
space O
, O
thus O
inducing O
cross O
- O
lingual O
feature O
representation O
. O

Existing O
resources O
Recent O
work O
in O
NLP O
has O
investigated O
the O
ability O
of O
neural B-MethodName
networks I-MethodName
trained O
for O
natural O
language O
understanding O
to O
pick O
up O
onsubtle O
discourse O
cues O
( O
e.g O
, O
Upadhye O
et O
al O
. O
, O
2020 O
; O
Schuster O
et O
al O
. O
, O
2020 O
) O
. O

We O
further O
included O
5 O
fller O
premisehypothesis O
pairs O
from O
MNLI B-MethodName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
as O
attention O
checks O
( O
see O
Appendix O
Dfor O
exclusion O
criteria O
) O
. O

This O
caveat O
applies O
to O
change O
of O
state O
predicates O
and O
implicatives O
where O
the O
presupposition O
statement O
can O
not O
be O
mechanically O
derived O
from O
an O
expression O
in O
the O
trigger O
sentences O
. O
Measure O
SNLI O
MNLI B-MethodName
ANLI O
NOPE O
Unanimous O
agreement O
58.3 O
58.2 O
41.6 O
38.7 O
Individual O
lab O
. O

89.0 O
88.7 O
81.7 O
81.5No O
majority O
2.0 O
1.8 O
13.0 O
5.1 O
Table O
3 O
: O
Validation O
statistics O
( O
% O
) O
reported O
for O
SNLI O
, O
MNLI B-MethodName
, O
and O
ANLI O
compared O
with O
NOPE O
. O

5 O
Machine O
Learning O
Experiments O
5.1 O
Models O
To O
evaluate O
the O
ability O
of O
NLI O
models O
to O
infer O
presuppositions O
, O
we O
evaluated O
two O
baseline O
models O
, O
a O
Bag O
- O
of O
- O
Words O
( O
BOW O
) O
model O
and O
InferSent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
, O
as O
well O
as O
two O
pre O
- O
trained O
transformer O
models O
, O
RoBERTa B-MethodName
- O
large O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
and O
DeBERTa B-MethodName
- O
V2 O
- O
XLarge O
( O
He O
et O
al O
. O
, O
2020 O
) O
, O
which O
both O
recently O
achieved O
state O
- O
of O
- O
the O
- O
art O
performanceon O
the O
SuperGLUE B-DatasetName
natural O
language O
understanding O
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
. O

The O
BOW O
model O
produces O
a O
sentence O
representation O
from O
the O
mean B-MetricName
of O
FastText O
word O
vectors O
( O
Mikolov O
et O
al O
. O
, O
2018 B-MetricValue
) O
, O
and O
InferSent O
does O
so O
using O
a O
bidirectional O
LSTM B-MethodName
. O

We O
trained O
baselines O
end O
- O
toend O
on O
the O
combination O
of O
MNLI B-MethodName
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
SNLI O
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
, O
ANLI O
( O
Nie O
et O
al O
. O
, O
2020 O
) O
, O
and O
FEVER O
( O
Thorne O
et O
al O
. O
, O
2018 O
) O
. O

RoBERTa B-MethodName
- O
large O
and O
DeBERTa B-MethodName
- O
V2 O
- O
XLarge O
are O
both O
transformer O
- O
based O
masked O
language O
models O
with O
355 O
M O
and O
900 O
M O
parameters O
, O
respectively O
. O

We O
fne O
- O
tuned O
RoBERTa B-MethodName
fve O
times O
with O
different O
random B-HyperparameterName
seeds I-HyperparameterName
. O

Due O
to O
the O
high O
computational O
cost O
of O
fne O
- O
tuning O
DeBERTa B-MethodName
, O
we O
only O
fne O
- O
tuned O
it O
once O
. O

Model O
E O
E O
E O
{ O
N O
, O
C O
} O
{ O
N O
, O
C O
} O
E O
nonneg O
neg O
nonneg O
neg O
nonneg O
neg O
RoBERTa B-MethodName
DeBERTa89.9 B-MethodName
89.2 O
80.6 O
32.7 O
35.1 O
68.4 O
90.8 O
88.6 O
81.8 O
32.1 O
38.9 O
68.9 O
Table O
4 O
: O
Model O
accuracy B-MetricName
on O
non O
- O
negated O
and O
negated O
examples O
for O
different O
subsets O
of O
the O
main O
corpus O
. O

Accuracy O
on O
these O
examples O
is O
again O
much O
lower O
( O
39.2 O
% O
for O
RoBERTa B-MethodName
, O
and O
39.1 O
% O
for O
DeBERTa B-MethodName
) O
than O
accuracy B-MetricName
on O
the O
overall O
corpus O
, O
which O
suggests O
that O
the O
transformer O
models O
do O
not O
drawthe O
correct O
inferences O
for O
trigger O
sentences O
for O
which O
participants O
neither O
fully O
endorsed O
nor O
rejected O
the O
hypothesis.6Conclusion O
We O
presented O
NOPE O
, O
a O
dataset O
of O
naturally O
occur O
ring O
presuppositions O
involving O
a O
diverse O
set O
of O
presupposition O
triggers O
. O

We O
used O
this O
dataset O
to O
evaluate O
the O
pre O
- O
trained O
transformer O
- O
based O
models O
RoBERTa B-MethodName
and O
DeBERTa B-MethodName
. O

BabyBERTa B-MethodName
: O
Learning O
More O
Grammar O
With O
Small O
- O
Scale O
Child O
- O
Directed O
. O

In O
this O
work O
, O
we O
examined O
the O
grammatical O
knowledge O
of O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
when O
trained O
on O
a O
5 O
M O
word O
corpus O
of O
language O
acquisition O
data O
to O
simulate O
the O
input O
available O
to O
children O
between O
the O
ages O
1 O
and O
6 O
. O

Using O
the O
behavioral O
probing O
paradigm O
, O
we O
found O
that O
a O
smaller O
version O
of O
RoBERTa B-MethodName
- O
base O
that O
never O
predicts O
unmasked O
tokens O
, O
which O
we O
term O
BabyBERTa B-MethodName
, O
acquires O
grammatical O
knowledge O
comparable O
to O
that O
of O
pre O
- O
trained O
RoBERTa B-MethodName
- O
base O
- O
and O
does O
so O
with O
approximately O
15X O
fewer O
parameters O
and O
6,000X O
fewer O
words O
. O

When O
pre O
- O
trained O
on O
massive O
datasets O
on O
the O
order O
of O
billions O
of O
words O
, O
models O
like O
GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
achieve O
impressive O
scores B-MetricName
on O
benchmarks O
of O
natural O
language O
understanding O
tasks O
( O
Levesque O
et O
al O
. O
, O
2012 O
; O
Wang O
et O
al O
. O
, O
2019 O
) O
. O

By O
the O
time O
middle O
- O
class O
English O
- O
speaking O
children O
have O
acquired O
near O
adult O
- O
like O
grammatical O
knowledge O
( O
by O
about O
age O
6 O
( O
Kemp O
et O
al O
. O
, O
2005 O
) O
, O
though O
improvements O
can O
be O
detected O
long O
after O
) O
, O
they O
have O
been O
exposed O
to O
no O
more O
than O
10 O
- O
50 O
M O
words O
( O
Hart O
and O
Risley O
, O
1995 O
) O
- O
at O
least O
600 O
times O
fewer O
words O
than O
RoBERTa B-MethodName
. O

To O
remedy O
this O
, O
we O
present O
an O
acquisitionfriendly O
version O
of O
RoBERTa B-MethodName
, O
BabyBERTa B-MethodName
, O
that O
is O
trained O
on O
input O
which O
is O
both O
qualitatively O
and O
quantitatively O
comparable O
to O
that O
of O
the O
average B-MetricName
English O
- O
speaking O
6 B-MetricValue
- O
year O
- O
old O
. O

Model O
and O
training O
optimizations O
designed O
for O
suchRoBERTa B-MethodName
- O
base O
BabyBERTa B-MethodName
parameters O
125 O
M O
5 O
M O
data O
size O
160 O
GB O
0.02 O
GB O
words O
in O
data O
30B O
5 O
M O
batch B-HyperparameterName
size I-HyperparameterName
8 B-HyperparameterValue
K O
16 O
max O
sequence O
512 O
128 O
epochs B-HyperparameterName
> O
40 B-HyperparameterValue
10 O
max O
step O
500 O
260 O
hardware11024x O
V100 O
1x O
GTX1080 O
training O
time O
24hours O
2hours O
accuracy281.0 B-MetricName
80.5 O
Table O
1 O
: O
A O
comparison O
between O
RoBERTa B-MethodName
- O
base O
pretrained O
on O
30B O
words O
of O
web O
text O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
and O
BabyBERTa B-MethodName
pre O
- O
trained O
from O
scratch O
on O
5 O
M O
words O
of O
child O
- O
directed O
input.1GPU(s O
) O
used O
for O
training O
. O

Thus O
, O
in O
developing O
BabyBERTa B-MethodName
, O
we O
( O
i O
) O
used O
a O
smaller O
model O
size O
to O
avoid O
over-ﬁtting O
on O
the O
small O
acquisition O
data O
; O
and O
( O
ii O
) O
explored O
a O
large O
hyper O
- O
parameter O
space O
to O
identify O
conﬁgurations O
that O
work O
at O
acquisition O
- O
scale O
. O

To O
isolate O
the O
effect O
of O
data O
size O
on O
the O
acquisition O
of O
grammatical O
knowledge O
, O
Warstadt O
et O
al O
. O
( O
2020b O
) O
pre O
- O
trained O
RoBERTa B-MethodName
models O
on O
datasets O
varying O
in O
size O
. O

Their O
results O
showed O
that O
RoBERTa B-MethodName
learns O
linguistic O
features O
with O
only O
a O
few O
million O
words O
, O
but O
that O
it O
takes O
billions O
of O
words O
for O
the O
model O
to O
prefer O
to O
use O
linguistic O
generalizations O
over O
surface O
ones O
. O

Using O
this O
test O
suite O
, O
we O
found O
that O
RoBERTa B-MethodName
- O
base O
pre O
- O
trained O
from O
scratch O
did O
not O
perform O
well O
in O
the O
smalldata O
regime O
, O
and O
thus O
explored O
modiﬁcations O
of O
RoBERTa B-MethodName
- O
base O
via O
extensive O
hyper O
- O
parameter O
tuning O
. O

BabyBERTa B-MethodName
has O
approximately O
15X O
fewer O
parameters O
, O
and O
when O
trained O
on O
child O
- O
directed O
input O
- O
at O
least O
6,000X O
fewer O
words O
than O
the O
30B O
used O
to O
pre O
- O
train O
RoBERTa B-MethodName
- O
base O
- O
scored B-MetricName
within O
half O
a O
point O
of O
RoBERTa B-MethodName
- O
base O
. O

Furthermore O
, O
BabyBERTa B-MethodName
performs O
well O
above O
chance O
in O
a O
majority O
of O
13 O
grammatical O
phenomena O
evaluated O
. O

This O
demonstrates O
that O
masked O
language O
modeling O
can O
yield O
strong O
grammatical O
knowledge O
even O
when O
the O
input O
consists O
of O
a O
small O
corpus O
of O
child O
- O
directed O
language.2 O
Methods O
2.1 O
BabyBERTa B-MethodName
We O
introduce O
a O
scaled O
- O
down O
masked O
language O
model O
based O
on O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
with O
8 O
M O
parameters O
, O
8912 O
vocabulary O
items O
, O
and O
trained O
on O
no O
more O
than O
30 O
M O
words O
. O

We O
will O
refer O
to O
this O
model O
as O
BabyBERTa B-MethodName
to O
highlight O
both O
its O
origin O
in O
RoBERTa B-MethodName
and O
its O
use O
- O
case O
, O
small O
- O
scale O
language O
acquisition O
experiments O
. O

All O
hyper O
- O
parameters O
were O
identiﬁed O
by O
tuning O
BabyBERTa B-MethodName
on O
a O
masked O
word O
prediction O
task O
using O
a O
held O
- O
out O
portion O
of O
our O
corpus O
of O
transcribed O
child O
- O
directed O
speech O
as O
input O
. O

A O
detailed O
comparison O
between O
hyper O
- O
parameters O
of O
BabyBERTa B-MethodName
and O
RoBERTa B-MethodName
is O
available O
in O
Appendix O
A. O
Brieﬂy O
, O
BabyBERTa B-MethodName
uses O
only O
8 O
layers O
, O
8 O
attention O
heads O
, O
256 O
hidden B-HyperparameterName
units I-HyperparameterName
, O
and O
an O
intermediate O
size O
of O
1024 B-HyperparameterValue
. O

In O
line O
with O
RoBERTa B-MethodName
, O
we O
used O
dynamic O
masking O
. O

Consequently O
, O
when O
BabyBERTa B-MethodName
is O
trained O
on O
5 O
M O
words O
, O
BabyBERTa B-MethodName
actually O
receives O
5 O
10 O
= O
50 O
M O
words O
, O
which O
approximates O
the O
language O
experience O
of O
the O
average B-MetricName
English O
- O
learning O
6 B-MetricValue
- O
year O
- O
old O
. O

BabyBERTa B-MethodName
uses O
fewer O
epochs B-HyperparameterName
than O
RoBERTabase B-MethodName
which O
we O
estimated O
to O
be O
at O
least O
401 B-HyperparameterValue
. O

To O
isolate O
the O
inﬂuence O
of O
the O
unique O
aspects O
of O
child O
- O
directed O
speech O
on O
model O
performance O
, O
we O
also O
trained O
BabyBERTa B-MethodName
on O
corpora O
from O
two O
very O
different O
domains O
, O
matched O
approximately O
in O
size O
to O
AO O
- O
CHILDES O
: O
First O
, O
we O
obtained O
three O
small O
Wikipedia O
corpora O
by O
splitting O
a O
random O
collection O
1This O
lower O
bound O
is O
based O
on O
dividing O
the O
number O
of O
batches O
that O
can O
ﬁt O
the O
training O
data O
by O
the O
number O
of O
batches O
reported O
by O
Liu O
et O
al O
. O
( O
2019 O
) O
, O
40 O
109/ O
3106/ O
500103 O
40 O
, O
where O
40 O
109is O
an O
upper O
bound O
on O
the O
number O
of O
tokens O
after O
Byte O
- O
Pair O
encoding O
, O
and O
3 O
106is O
a O
lower O
bound O
on O
the O
number O
of O
tokens O
per O
batch.of O
articles O
in O
English O
Wikipedia O
into O
three O
sets O
of O
approximately O
500 O
K O
sentences O
each O
. O

2.3 O
Vocabulary O
BabyBERTa B-MethodName
uses O
a O
sub O
- O
word O
vocabulary O
based O
on O
Byte O
- O
Pair O
Encoding O
, O
introduced O
by O
Sennrich O
et O
al O
. O
( O
2016 O
) O
and O
later O
adopted O
in O
GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
. O

Instead O
of O
using O
the O
original O
50 O
K O
vocabulary O
used O
by O
RoBERTa B-MethodName
, O
we O
built O
a O
custom O
vocabulary O
of O
size O
8192 O
from O
a O
concatenation O
of O
AO O
- O
CHILDES O
, O
AO O
- O
Newsela O
and O
Wikipedia-1 O
, O
using O
the O
open O
- O
source O
Python O
API O
tokenizers2 O
. O

Because O
of O
the O
small O
vocabulary O
used O
by O
BabyBERTa B-MethodName
and O
the O
limited O
overlap O
between O
words O
in O
child O
- O
directed O
input O
and O
the O
set O
of O
words O
in O
BLiMP O
, O
we O
made O
our O
own O
grammar O
test O
suite O
. O

While O
retaining O
much O
of O
the O
organisational O
structure O
and O
philosophy O
of O
BLiMP O
, O
our O
evaluation O
data O
only O
includes O
words O
( O
and O
never O
sub O
- O
tokens O
) O
from O
the O
vocabulary O
of O
BabyBERTa B-MethodName
and O
is O
therefore O
more O
sensitive O
to O
differences O
in O
grammatical O
knowledge O
acquired O
in O
our O
setting O
. O

Because O
one O
of O
our O
aims O
is O
to O
compare O
grammatical O
knowledge O
obtained O
by O
BabyBERTa B-MethodName
trained O
on O
different O
corpora O
, O
we O
needed O
a O
single O
evaluation O
dataset O
that O
could O
be O
used O
with O
each O
corpus O
without O
biasing O
the O
results O
. O

Blue O
: O
RoBERTa B-MethodName
- O
base O
pre O
- O
trained O
by O
Liu O
et O
al O
. O
( O
2019 O
) O
on O
30B O
words O
. O

Orange O
: O
RoBERTa B-MethodName
- O
base O
pre O
- O
trained O
by O
Warstadt O
et O
al O
. O
( O
2020b O
) O
on O
10 O
M O
words O
. O

The O
following O
models O
were O
all O
trained O
on O
AO O
- O
CHILDES O
, O
5 O
M O
words O
of O
child O
- O
directed O
input O
: O
Green O
: O
RoBERTa B-MethodName
- O
base O
pretrained O
by O
us O
. O

Red O
: O
BabyBERTa B-MethodName
trained O
with O
the O
original O
masking O
strategy O
, O
where O
unmasked O
tokens O
are O
predicted O
. O

Purple O
: O
BabyBERTa B-MethodName
trained O
without O
predicting O
unmasked O
tokens O
. O

Figure O
1 O
summarizes O
the O
accuracies B-MetricName
across O
allparadigms O
in O
our O
grammar O
test O
suite O
, O
for O
RoBERTabase B-MethodName
and O
BabyBERTa B-MethodName
trained O
on O
different O
data5 O
. O

For O
complete O
results O
, O
see O
Appendix O
F. O
While O
RoBERTa B-MethodName
- O
base O
pre O
- O
trained O
on O
30B O
words O
by O
Liu O
et O
al O
. O
( O
2019 O
) O
performed O
reasonably O
well O
, O
the O
same O
architecture6performed O
considerably O
worse O
when O
trained O
only O
on O
5 O
M O
words O
of O
child O
- O
directed O
input O
. O

We O
also O
evaluated O
RoBERTabase B-MethodName
pre O
- O
trained O
from O
scratch O
by O
Warstadt O
et O
al O
. O
( O
2020b O
) O
on O
a O
similarly O
sized O
dataset O
consisting O
of O
10 O
M O
words O
of O
English O
Wikipedia O
and O
Smashwords O
, O
which O
achieved O
an O
average B-MetricName
accuracy B-MetricName
of O
64.5 B-MetricValue
, O
well O
below O
81.1 O
. O

The O
poor O
performance O
of O
RoBERTabase B-MethodName
pre O
- O
trained O
from O
scratch O
by O
us O
and O
by O
Warstadt O
et O
al O
. O
( O
2020b O
) O
clearly O
indicates O
that O
RoBERTa B-MethodName
- O
base O
does O
not O
perform O
well O
at O
acquisition O
- O
scale O
. O

This O
then O
raises O
the O
question O
of O
whether O
RoBERTa B-MethodName
- O
base O
could O
be O
adapted O
to O
work O
well O
at O
this O
scale O
. O

The O
results O
of O
a O
series O
of O
manual O
hyperparameter O
tuning O
studies O
, O
culminating O
in O
the O
model O
we O
call O
BabyBERTa B-MethodName
, O
demonstrate O
that O
RoBERTa B-MethodName
can O
be O
straightforwardly O
adapted O
to O
the O
scale O
at O
which O
acquisition O
takes O
place O
. O

While O
masked O
tokens O
are O
typically O
unmasked O
with O
a O
probability O
of O
0.1 O
, O
we O
found O
that O
setting O
this O
probability O
to O
zero O
- O
and O
thus O
removing O
unmasking O
altogether O
- O
yielded O
an O
enormous O
increase O
in O
accuracy B-MetricName
, O
from O
56.4 B-MetricValue
to O
80.5 O
- O
within O
half O
a O
point O
of O
RoBERTa B-MethodName
- O
base O
pre O
- O
trained O
on O
30B O
words O
. O

These O
results O
are O
strong O
evidence O
of O
the O
datahungriness O
of O
RoBERTa B-MethodName
- O
base O
, O
and O
show O
that O
a O
combination O
of O
downsizing O
the O
model O
and O
never O
predicting O
unmasked O
tokens O
can O
dramatically O
speed O
acquisition O
of O
grammatical O
knowledge O
at O
acquisitionscale O
. O

What O
additional O
factors O
may O
have O
contributed O
to O
the O
success O
of O
BabyBERTa B-MethodName
on O
our O
grammatical O
test O
suite O
? O

First O
, O
BabyBERTa B-MethodName
was O
trained O
exclusively O
on O
single O
sentences O
, O
whereas O
RoBERTa B-MethodName
- O
base O
was O
trained O
on O
multiple O
sentences O
per O
input O
. O

In O
an O
ofﬂine O
ablation O
study O
, O
we O
found O
that O
the O
grammatical O
competence O
of O
BabyBERTa B-MethodName
is O
slightly O
compromised O
when O
training O
on O
input O
that O
consists O
of O
more O
than O
one O
sentence O
, O
on O
the O
order O
of O
1 O
- O
2 O
points O
. O

Second O
, O
it O
is O
possible O
that O
BabyBERTa B-MethodName
’s O
custom O
vocabulary O
- O
based O
on O
the O
corpora O
from O
which O
words O
in O
our O
test O
suite O
are O
sampled O
- O
has O
a O
greater O
coverage O
of O
the O
words O
in O
our O
test O
suite O
. O

However O
, O
only O
24 O
out O
of O
the O
571 O
content O
words O
in O
our O
test O
suite O
are O
not O
in O
the O
vocabulary O
of O
RoBERTa B-MethodName
- O
base O
. O

When O
we O
probed O
RoBERTa B-MethodName
- O
base O
with O
proper O
nouns O
capitalized O
- O
thus O
achieving O
full O
overlap O
- O
the O
accuracy B-MetricName
increased O
by O
only O
1.9 B-MetricValue
points O
for O
RoBERTa B-MethodName
- O
base O
pre O
- O
trained O
by O
Liu O
et O
al O
. O
( O
2019 O
) O
, O
and O
1.4 O
points O
for O
RoBERTa B-MethodName
- O
base O
pre O
- O
trained O
by O
Warstadt O
et O
al O
. O
( O
2020b O
) O
. O

3.2 O
Comparing O
domains O
In O
this O
experiment O
, O
we O
further O
examined O
the O
grammatical O
ability O
of O
BabyBERTa B-MethodName
, O
to O
ask O
whether O
the O
strong O
performance O
of O
BabyBERTa B-MethodName
is O
speciﬁc O
to O
child O
- O
directed O
spoken O
input O
, O
or O
holds O
when O
trained O
with O
data O
that O
is O
more O
representative O
of O
that O
used O
in O
the O
NLP O
community O
. O

This O
gives O
a O
more O
in O
- O
depth O
look O
at O
when O
BabyBERTa B-MethodName
acquires O
each O
grammatical O
phenomenon O
, O
and O
yields O
useful O
information O
for O
researchers O
wishing O
to O
compare O
learning O
trajectories O
between O
TLMs O
and O
children O
. O

Strikingly O
, O
even O
though O
AO O
- O
CHILDES O
contains O
4X O
fewer O
words O
per O
sentence O
( O
6.4 O
vs O
24.7 O
) O
, O
the O
average B-MetricName
accuracy B-MetricName
of O
BabyBERTa B-MethodName
trained O
on O
AO O
- O
CHILDES O
is O
higher O
than O
Wikipedia-1 O
( O
77.2 B-MetricValue
and O
73.0 O
, O
respec O
- O
tively O
) O
. O

Furthermore O
, O
we O
found O
that O
BabyBERTa B-MethodName
trained O
on O
AO O
- O
Newsela O
achieves O
the O
best O
overall O
accuracy B-MetricName
( O
79.0 B-MetricValue
) O
. O

3.3 O
Scaffolding O
In O
addition O
to O
yielding O
better O
grammatical O
knowledge O
when O
training O
BabyBERTa B-MethodName
on O
AO O
- O
CHILDES O
compared O
to O
Wikipedia-1 O
, O
it O
is O
possible O
that O
childdirected O
speech O
is O
also O
a O
better O
starting O
point O
for O
further O
learning O
from O
more O
advanced O
language O
. O

Left O
panel O
: O
BabyBERTa B-MethodName
was O
either O
trained O
on O
AO O
- O
CHILDES O
before O
AO O
- O
Newsela O
( O
blue O
line O
) O
, O
or O
trained O
on O
AO O
- O
Newsela O
before O
AO O
- O
CHILDES O
( O
orange O
line O
) O
. O

Right O
panel O
: O
BabyBERTa B-MethodName
was O
either O
trained O
on O
Wikipedia-3 O
before O
AO O
- O
Newsela O
( O
blue O
line O
) O
, O
or O
trained O
on O
AO O
- O
Newsela O
before O
Wikipedia-3 O
( O
orange O
line O
) O
. O

example O
, O
it O
is O
possible O
that O
the O
grammar O
that O
BabyBERTa B-MethodName
acquires O
from O
AO O
- O
CHILDES O
is O
less O
tied O
to O
the O
peculiarities O
of O
the O
input O
, O
and O
would O
therefore O
provide O
better O
scaffolding O
for O
grammar O
induction O
after O
a O
domain O
- O
shift O
. O

We O
conducted O
two O
experiments O
: O
We O
trained O
BabyBERTa B-MethodName
either O
on O
a O
concatenation O
of O
AO O
- O
CHILDES O
and O
AO O
- O
Newsela O
, O
or O
Wikipedia-3 O
and O
AO O
- O
Newsela O
, O
and O
manipulated O
the O
order O
of O
training O
( O
order O
of O
concatenation O
vs. O
reverse)8 O
. O

To O
test O
that O
this O
ﬁnding O
did O
not O
result O
simply O
because O
BabyBERTa B-MethodName
performs O
better O
when O
trained O
on O
AO O
- O
Newsela O
last O
, O
and O
instead O
due O
to O
the O
scaffolding O
effects O
of O
early O
exposure O
to O
AO O
- O
CHILDES O
, O
we O
repeated O
the O
same O
experiment O
but O
with O
Wikipedia3 O
in O
place O
of O
AO O
- O
CHILDES O
. O

3.4 O
Domain O
diversity O
Finally O
, O
we O
asked O
whether O
we O
could O
improve O
the O
grammatical O
knowledge O
acquired O
by O
BabyBERTa B-MethodName
by O
extending O
the O
training O
data O
. O

BabyBERTa B-MethodName
was O
trained O
either O
on O
Wikipedia O
only O
( O
blue O
line O
) O
, O
or O
a O
more O
diverse O
set O
of O
corpora O
, O
AO O
- O
CHILDES O
+ O
AO O
- O
Newsela O
+ O
Wikipedia-3 O
, O
spanning O
3 O
different O
domains O
( O
orange O
line O
) O
. O

First O
, O
we O
observed O
that O
BabyBERTa B-MethodName
trained O
in O
the O
diverse O
training O
data O
condition O
achieved O
an O
average B-MetricName
accuracy B-MetricName
well O
above O
that O
of O
pre O
- O
trained O
RoBERTa B-MethodName
- O
base O
, O
which O
is O
notable O
given O
that O
the O
amount O
of O
data O
BabyBERTa B-MethodName
was O
exposed O
to O
is O
still O
far O
less O
than O
that O
of O
RoBERTa B-MethodName
- O
base O
pre O
- O
trained O
on O
30B O
words O
. O

Further O
, O
as O
predicted O
, O
we O
found O
that O
the O
average B-MetricName
accuracy B-MetricName
of O
BabyBERTa B-MethodName
trained O
only O
on O
Wikipedia O
was O
worse O
( O
83.8 B-MetricValue
vs. O
78.9 O
) O
. O

3.5 O
Holistic O
vs. O
MLM O
scoring O
The O
results O
reported O
in O
previous O
sections O
were O
computed O
using O
a O
method O
of O
scoring O
grammaticality O
where O
each O
sentence O
in O
a O
minimal O
pair O
is O
input O
to O
BabyBERTa B-MethodName
in O
whole O
and O
without O
masks O
. O

In O
this O
section O
, O
we O
compared O
these O
two O
methods O
by O
re O
- O
scoring O
Baby O
- O
Model O
& O
Data O
Unmasking O
Accuracy O
holistic O
MLM O
BabyBERTa B-MethodName
+ O
AO O
- O
CHILDES O
no O
77.9 O
78.0 O
+ O
AO O
- O
CHILDES O
yes O
60.5 O
77.6 O
RoBERTa B-MethodName
+ O
AO O
- O
CHILDES O
yes O
59.9 O
72.4 O
+ O
Warstadt O
, O
2020 O
yes O
64.4 O
78.1 O
+ O
Liu O
et O
al O
. O
, O
2019 O
yes O
82.5 O
90.2 O
Table O
2 O
: O
Overall O
accuracy B-MetricName
on O
our O
grammar O
test O
suite O
for O
two O
different O
scoring O
methods O
. O

The O
holistic O
method O
was O
proposed O
by O
Zaczynska O
et O
al O
. O
( O
2020 O
) O
and O
used O
to O
tune O
BabyBERTa B-MethodName
. O

Overall O
accuracy B-MetricName
values O
are O
averages B-MetricName
over O
10 B-MetricValue
replications O
( O
BabyBERTa B-MethodName
) O
, O
and O
3 O
replications O
( O
RoBERTa B-MethodName
- O
base O
except O
Liu O
et O
al O
. O
, O
2019 O
) O
. O

BERTa B-MethodName
on O
our O
grammatical O
test O
suite O
using O
MLM O
scoring O
. O

This O
includes O
all O
RoBERTa B-MethodName
models O
, O
and O
BabyBERTa B-MethodName
trained O
to O
predict O
unmasked O
tokens O
( O
unmasking O
= O
yes O
) O
. O

The O
only O
model O
whose O
accuracy B-MetricName
did O
not O
change O
noticeably O
is O
BabyBERTa B-MethodName
( O
unmasking O
= O
no O
) O
. O

Towards O
this O
end O
, O
we O
developed O
BabyBERTa B-MethodName
, O
a O
variation O
of O
RoBERTabase B-MethodName
with O
15X O
fewer O
parameters O
. O

When O
trained O
only O
on O
5 O
M O
words O
of O
child O
- O
directed O
input O
, O
BabyBERTa B-MethodName
achieved O
an O
overall O
accuracy B-MetricName
on O
our O
grammar O
test O
suite O
competitive O
with O
RoBERTa B-MethodName
- O
base O
. O

Performance O
, O
however O
, O
was O
far O
from O
perfect O
, O
and O
BabyBERTa B-MethodName
was O
at O
chance O
in O
evaluating O
the O
grammaticality O
of O
sentence O
pairs O
that O
contrast O
negativepolarity O
- O
item O
( O
NPI O
) O
licensing O
, O
gender O
agreement O
, O
ellipsis O
, O
superlative O
quantiﬁers O
, O
and O
island O
effects O
involving O
adjuncts O
. O

4.1 O
Limitations O
and O
Future O
Directions O
BabyBERTa B-MethodName
never O
predicts O
unmasked O
tokens O
, O
unlike O
RoBERTa B-MethodName
- O
base O
which O
inherits O
this O
method O
from O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

Given O
that O
unmasking O
is O
supposed O
to O
optimize O
performance O
on O
downstream B-TaskName
tasks I-TaskName
, O
it O
is O
not O
surprising O
that O
unmasking O
handicapped O
BabyBERTa B-MethodName
during O
pre O
- O
training O
. O

Without O
unmasked O
tokens O
in O
the O
input O
, O
BabyBERTa B-MethodName
is O
forced O
to O
attend O
to O
lexical O
context O
, O
instead O
of O
relying O
on O
the O
input O
to O
make O
predictions O
. O

However O
, O
this O
may O
leave O
BabyBERTa B-MethodName
vulnerable O
to O
the O
distribution O
- O
shift O
that O
occurs O
when O
ﬁne O
- O
tuning O
on O
a O
downstream B-TaskName
task O
. O

To O
support O
research O
on O
this O
front O
, O
we O
proposed O
and O
released O
BabyBERTa B-MethodName
, O
a O
TLM O
trained O
and O
optimized O
on O
developmentally O
plausible O
language O
data O
. O

Mikolov O
et O
al O
. O
( O
2013 O
) O
formulate O
the O
mapping O
as O
a O
least O
- O
square O
linear B-MethodName
regression I-MethodName
between O
the O
monolingual O
embeddings O
of O
the O
seed O
lexicon O
to O
minimize O
the O
mean B-MetricName
square O
error O
of O
the O
word O
translations O
. O

A O
linear B-MethodName
regression I-MethodName
shows O
a O
signiﬁcant O
relationship O
between O
accuracy B-MetricName
and O
phylogenetic O
distance O
( O
t(4838 O
) O
= O
 60:12,p O
< O
0:001 O
) O
. O

masculine O
/ O
feminine O
/ O
neuter O
, O
common O
/ O
neuter O
) O
, O
we O
ﬁt O
a O
linear B-MethodName
regression I-MethodName
model O
with O
the O
values O
of O
the O
accuracy B-MetricName
improvement O
( O
or O
degradation O
) O
from O
the O
baseline O
as O
the O
dependent O
variable O
, O
phylogenetic O
distance O
a O
continuous O
predictor O
, O
and O
system O
type O
a O
categorical O
predictor O
( O
common O
/ O
neuter O
is O
the O
reference O
level O
) O
. O

We O
run O
two O
logistic B-MethodName
regression I-MethodName
models O
. O

Previous O
work O
on O
computational O
CxG O
has O
explored O
how O
to O
discover O
potential O
constructions O
( O
Wible O
and O
Tsao O
, O
2010 O
; O
Forsberg O
et O
al O
. O
, O
2014 O
; O
Dunn O
, O
2017 O
) O
, O
the O
process O
of O
construction O
learning O
( O
Barak O
and O
Goldberg O
, O
2017 O
; O
Barak O
et O
al O
. O
, O
2017 O
) O
, O
and O
whether O
constructional O
information O
is O
implicitly O
encoded O
in O
models O
like O
BERT B-MethodName
( O
Tayyar O
Madabushi O
et O
al O
. O
, O
2020 O
) O
. O

We O
evaluate O
our O
model O
on O
the O
Stanford B-DatasetName
NLI I-DatasetName
corpus I-DatasetName
, O
the O
MultiGenre O
NLI O
corpus O
, O
and O
the O
Stanford B-DatasetName
Sentiment I-DatasetName
Treebank I-DatasetName
and O
ﬁnd O
that O
it O
consistently O
outperforms O
TreeLSTM B-MethodName
( O
Tai O
et O
al O
. O
, O
2015 O
) O
, O
the O
previous O
best O
known O
composition O
function O
for O
treestructured O
models O
. O

The O
Tree O
- O
structured O
recursive O
neural B-MethodName
networks I-MethodName
( O
TreeRNN O
) O
of O
Socher O
et O
al O
. O
( O
2010 O
) O
compose O
two O
child O
node O
vectors O
~hland O
~ O
hrusing O
this O
method O
: O
( O
1)~h O
= O
tanh O
( O
W O
" O
~hl O
~hr O
# O
+ O
~b O
) O
where O
~ O
hl,~hr,~h,~b2Rd1 O
, O
andW2Rd2d O
. O

The O
last O
composition O
function O
relevant O
to O
this O
paper O
is O
the O
Tree O
- O
structured O
long O
short O
- O
term O
memory O
networks O
( O
TreeLSTM B-MethodName
; O
Tai O
et O
al O
. O
, O
2015 O
; O
Zhu O
et O
al O
. O
, O
2015 O
; O
Le O
and O
Zuidema O
, O
2015 O
) O
, O
particularly O
the O
version O
over O
a O
constituency O
tree O
. O

TreeRNN O
TreeRNN O
/ O
LSTM B-MethodName
O O
( O
dd O
) O
No O
No O
1 O
CMS O
O O
( O
Vdd O
) O
Yes O
Yes O
1 O
= O
V O
MV O
- O
RNN O
O O
( O
Vdd O
) O
No O
Yes O
1 O
= O
V O
RNTN O
O O
( O
ddd O
) O
No O
Yes O
1 O
= O
d O
LMS O
( O
this O
work O
) O
O O
( O
ddemb O
) O
No O
Yes O
1 O
= O
demb O
Table O
1 O
: O
Summary O
of O
the O
models O
. O

extension O
of O
TreeRNN O
which O
adapts O
long O
shortterm O
memory O
( O
LSTM B-MethodName
; O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
networks O
. O

It O
shares O
the O
advantage O
of O
LSTM B-MethodName
networks O
in O
that O
it O
prevents O
the O
vanishing O
gradient O
problem O
( O
Hochreiter O
et O
al O
. O
, O
2001 O
) O
. O

Unlike O
TreeRNN O
, O
the O
output O
hidden O
state O
~hof O
TreeLSTM B-MethodName
is O
not O
directly O
calculated O
from O
the O
hidden O
states O
of O
its O
child O
nodes O
, O
~hland O
~ O
hr O
. O

Rather O
, O
each O
node O
in O
TreeLSTM B-MethodName
maintains O
a O
cell O
state O
~ O
c O
that O
keeps O
track O
of O
important O
information O
of O
its O
child O
nodes O
. O

TreeLSTM B-MethodName
selects O
which O
values O
to O
take O
from O
the O
new O
candidate O
~ O
gby O
passing O
it O
through O
an O
input O
gate O
~ O
i. O

( O
11)~ O
c=~fl O
 O
~ O
cl+~fr O
 O
~ O
cr+~i O
 O
~ O
g O
( O
12)~h=~ O
o O
 O
tanh O
( O
~ O
c O
) O
TreeLSTM B-MethodName
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
among O
the O
tree O
- O
structured O
models O
in O
various O
tasks O
, O
including O
natural O
language O
inference O
and O
sentiment O
classiﬁcation O
. O

However O
, O
there O
are O
non O
- O
tree O
- O
structured O
models O
on O
the O
market O
thatoutperform O
TreeLSTM B-MethodName
. O

To O
recapitulate O
, O
TreeRNN O
and O
TreeLSTM B-MethodName
reﬂect O
the O
principle O
of O
compositionality O
but O
can O
not O
capture O
the O
multiplicative O
interaction O
between O
two O
expressions O
. O

Nevertheless O
, O
it O
requires O
signiﬁcantly O
more O
parameters O
than O
TreeRNN O
or O
TreeLSTM B-MethodName
. O

Our O
model O
consists O
of O
three O
subparts O
: O
the O
LIFT O
layer O
, O
the O
composition O
layer O
, O
and O
the O
TreeLSTM B-MethodName
wrapper O
. O

3.2 O
LMS O
augmented O
with O
LSTM B-MethodName
components O
We O
augment O
the O
base O
model O
with O
LSTM B-MethodName
components O
( O
LMS O
- O
LSTM B-MethodName
) O
to O
circumvent O
the O
problem O
of O
long O
- O
term O
dependencies O
. O

As O
in O
the O
case O
of O
TreeLSTM B-MethodName
, O
we O
additionally O
manage O
cell O
states O
( O
~ O
cl,~ O
cr O
) O
. O

Since O
the O
LSTM B-MethodName
components O
operate O
on O
vectors O
, O
we O
reshapeHcand O
, O
Hl O
, O
andHrintod1column O
vectors O
respectively O
, O
and O
produce O
~ O
g,~hl O
, O
and O
~ O
hr O
. O

The O
output O
of O
the O
LSTM B-MethodName
components O
are O
calculated O
based O
on O
these O
vectors O
, O
and O
is O
reshaped O
back O
to O
ap O
dp O
dmatrix O
( O
eq O
. O

( O
20)~ O
c=~fl O
 O
~ O
cl+~fr O
 O
~ O
cr+~i O
 O
~ O
g O
( O
21)~h=~ O
o O
 O
tanh O
( O
~ O
c O
) O
( O
22)H O
= O
TO O
- O
MATRIX O
( O
~h O
) O
3.3 O
Simpliﬁed O
variants O
We O
implement O
two O
LMS O
- O
LSTM B-MethodName
variants O
with O
a O
simpler O
composition O
function O
as O
an O
ablation O
study O
. O

But O
unlike O
the O
full O
LMS O
- O
LSTM B-MethodName
which O
has O
two O
tanh O
layers O
, O
it O
only O
utilizes O
one O
. O

( O
24)Hcand O
= O
tanh O
( O
WCOMBHlHr+BCOMB O
) O
4 O
Experiments O
4.1 O
Implementation O
details O
As O
our O
interest O
is O
in O
the O
performance O
of O
composition O
functions O
, O
we O
compare O
LMS O
- O
LSTM B-MethodName
with O
TreeLSTM B-MethodName
, O
the O
previous O
best O
known O
composition O
function O
for O
tree O
- O
structured O
models O
. O

To O
allow O
for O
efﬁcient O
batching O
, O
we O
use O
the O
SPINN O
- O
PI O
- O
NT O
approach O
( O
Bowman O
et O
al O
. O
, O
2016 O
) O
, O
which O
implements O
standard O
TreeLSTM B-MethodName
using O
stack O
and O
buffer O
data O
structures O
borrowed O
from O
parsing O
, O
rather O
than O
tree O
structures O
. O

We O
use O
the O
300D O
reference O
GloVe B-MethodName
vectors O
( O
840B O
token O
version O
; O
Pennington O
et O
al O
. O
, O
2014 O
) O
for O
word O
representations O
. O

LABEL O
: O
Entailment O
We O
test O
our O
models O
on O
the O
Multi O
- O
Genre B-MethodName
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
corpus O
( O
MultiNLI O
; O
Williams O
et O
al O
. O
, O
2017 O
) O
. O

TreeLSTM B-MethodName
performs O
the O
best O
with O
one O
MLP O
layer O
, O
while O
LMSLSTM B-MethodName
displays O
the O
best O
performance O
with O
two O
MLP O
layers O
. O

On O
the O
SNLI O
test O
set O
, O
LMS O
- O
LSTM B-MethodName
has O
an O
additional O
1.3 O
% O
gain O
over O
TreeLSTM B-MethodName
. O

Also O
, O
both O
of O
the O
simpliﬁed O
variants O
of O
LMS O
- O
LSTM B-MethodName
outperform O
TreeLSTM B-MethodName
, O
but O
by O
a O
smaller O
margin O
. O

On O
the O
MultiNLI O
test O
sets O
, O
LMS O
- O
LSTM B-MethodName
scores B-MetricName
1.3 O
% O
higher O
on O
the O
matched O
test O
set O
and O
1.9 O
% O
higher O
on O
the O
mismatched O
test O
set O
. O
Model O
Params O
. O

Baselines O
CBOW B-MethodName
( O
Williams O
et O
al O
. O
, O
2017 O
) O
– O
– O
80.6 O
– O
65.2 O
64.6 O
BiLSTM B-MethodName
( O
Williams O
et O
al O
. O
, O
2017 O
) O
2.8 O
m O
– O
81.5 O
– O
67.5 O
67.1 O
Shortcut O
- O
Stacked O
BiLSTM B-MethodName
( O
Nie O
and O
Bansal O
, O
2017 O
) O
34.7 O
m O
– O
86.1 O
– O
74.6 O
73.6 O
DIIN O
( O
Gong O
et O
al O
. O
, O
2018 O
) O
– O
– O
88.0 O
– O
78.8 O
77.8 O
Existing O
Tree O
- O
Structured O
Model O
Runs O
300D O
TreeLSTM B-MethodName
( O
Bowman O
et O
al O
. O
, O
2016 O
) O
3.4 O
m O
84.4 O
80.9 O
– O
– O
– O
300D O
SPINN O
- O
PI O
( O
Bowman O
et O
al O
. O
, O
2016 O
) O
3.7 O
m O
89.2 O
83.2 O
– O
– O
– O
Our O
Experiments O
441D O
LMS O
( O
base O
) O
2.0(+11.6 O
m O
) O
79.7 O
76.5 O
– O
– O
– O
441D O
LMS O
- O
LSTM B-MethodName
( O
simpliﬁed O
, O
 WCOMB; tanh O
) O
3.3(+11.6)m O
90.5 O
84.1 O
– O
– O
– O
324D O
LMS O
- O
LSTM B-MethodName
( O
simpliﬁed O
, O
 tanh O
) O
2.2(+11.6)m O
92.5 O
84.5 O
– O
– O
– O
700D O
TreeLSTM B-MethodName
2.0(+11.6)m O
89.5 O
83.6 O
– O
– O
– O
576D O
LMS O
- O
LSTM B-MethodName
( O
full O
) O
4.6(+11.6)m O
86.0 O
84.9 O
– O
– O
– O
700D O
TreeLSTM B-MethodName
4.6(+30.2)m O
– O
– O
78.9 O
70.0 O
69.7 O
576D O
LMS O
- O
LSTM B-MethodName
( O
full O
) O
5.9(+30.2)m O
– O
– O
80.5 O
71.3 O
71.6 O
Table O
2 O
: O
Results O
on O
NLI O
classiﬁcation O
with O
sentence O
- O
to O
- O
vector O
encoders O
. O

The O
Shortcut O
- O
Stacked O
sentence O
encoder O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
result O
among O
non O
- O
attentionbased O
models O
, O
outperforming O
LMS O
- O
LSTM B-MethodName
. O

While O
this O
paper O
focuses O
on O
the O
design O
of O
the O
composition O
function O
, O
we O
expect O
that O
adding O
depth O
along O
the O
lines O
of O
Irsoy O
and O
Cardie O
( O
2014 O
) O
and O
shortcut O
connections O
to O
LMS O
- O
LSTM B-MethodName
would O
offer O
comparable O
results O
. O

For O
instance O
, O
modals O
express O
possibilities O
or O
necessities O
that O
areLMS O
- O
LSTM B-MethodName
TreeLSTM B-MethodName
Phenomenon O
Mat O
. O

We O
use O
the O
same O
code O
as O
Williams O
et O
al O
. O
( O
2017 O
) O
toModel O
Test O
Baselines O
MV O
- O
RNN O
( O
Socher O
et O
al O
. O
, O
2013 O
) O
44.4 O
RNTN O
( O
Socher O
et O
al O
. O
, O
2013 O
) O
45.7 O
Deep O
RNN O
( O
Irsoy O
and O
Cardie O
, O
2014 O
) O
49.8 O
TreeLSTM B-MethodName
( O
Tai O
et O
al O
. O
, O
2015 O
) O
51.0 O
TreeBiGRU O
w/ O
attention O
52.4 O
( O
Kokkinos O
and O
Potamianos O
, O
2017 O
) O
Our O
Experiments O
312D O
TreeLSTM B-MethodName
48.9 O
144D O
LMS O
- O
LSTM B-MethodName
50.1 O
Table O
4 O
: O
Five O
- O
way O
test O
set O
classiﬁcation O
accuracies B-MetricName
( O
% O
) O
on O
the O
Stanford B-DatasetName
Sentiment I-DatasetName
Treebank I-DatasetName
. O

While O
our O
implementation O
does O
not O
exactly O
reproduce O
Tai O
et O
al O
. O
’s O
( O
2015 O
) O
TreeLSTM B-MethodName
results O
, O
a O
comparison O
between O
our O
trained O
TreeLSTM B-MethodName
and O
LMS O
- O
LSTM B-MethodName
is O
consistent O
with O
the O
patterns O
seen O
in O
NLI O
tasks O
. O

We O
examine O
how O
well O
the O
constituent O
representations O
produced O
by O
LMS O
- O
LSTM B-MethodName
and O
TreeLSTM B-MethodName
encode O
syntactic O
category O
information O
. O

As O
a O
baseline O
, O
we O
train O
a O
bag O
- O
of O
- O
words O
( O
BOW O
) O
model O
which O
produces O
the O
hidden O
state O
of O
a O
given O
phrase O
by O
summing O
the O
GloVe B-MethodName
embeddings O
of O
the O
words O
of O
the O
phrase O
. O

The O
hidden O
state O
representations O
produced O
by O
LMS O
- O
LSTM B-MethodName
yield O
the O
best O
results O
on O
both O
3 O
- O
way O
and O
19 O
- O
way O
classiﬁcation O
tasks O
. O

Comparing O
LMS O
- O
LSTM B-MethodName
and O
TreeLSTM B-MethodName
representations O
, O
we O
see O
a O
5.1 O
% O
gain O
on O
the O
3 O
- O
way O
classiﬁcation O
and O
a O
5.5 O
% O
gain O
on O
the O
19 O
- O
way O
classiﬁcation.3 O
- O
way O
19 O
- O
way O
Model O
Train O
Test O
Train O
Test O
300D O
BOW O
86.4 O
85.6 O
82.7 O
82.1 O
700D O
TreeLSTM B-MethodName
93.2 O
91.2 O
90.0 O
86.6 O
576D O
LMS O
- O
LSTM B-MethodName
97.3 O
96.3 O
94.0 O
92.1 O
Table O
6 O
: O
Syntactic O
category O
classiﬁcation O
accuracies B-MetricName
( O
% O
) O
on O
SNLI O
development O
set O
, O
classiﬁed O
using O
the O
tags O
introduced O
in O
Bowman O
et O
al O
. O
( O
2015 B-MetricValue
) O
. O

Experimental O
results O
indicate O
that O
, O
while O
our O
model O
does O
not O
reach O
the O
state O
of O
the O
art O
on O
any O
of O
the O
three O
datasets O
under O
study O
, O
it O
does O
substantially O
outperform O
all O
known O
tree O
- O
structured O
models O
, O
and O
lays O
a O
strong O
foundation O
for O
future O
work O
on O
treestructured O
compositionality O
in O
artiﬁcial O
neural B-MethodName
networks I-MethodName
. O

• O
We O
introduce O
an O
attention O
- O
based O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNN O
) O
to O
capture O
the O
temporal O
signals O
of O
an O
entity O
. O

Ni O
et O
al O
. O
( O
2016 O
) O
use O
an O
adapted O
version O
of O
Word2Vec B-MethodName
, O
where O
each O
entity O
in O
a O
Wikipedia O
page O
is O
considered O
as O
a O
term O
. O

Recent O
work O
proposed O
a O
CNN O
with O
attention O
- O
based O
framework O
to O
model O
local O
context O
representations O
of O
textual O
pairs O
( O
Yin O
et O
al O
. O
, O
2016 O
) O
, O
or O
to O
combine O
with O
LSTM B-MethodName
to O
model O
time O
- O
series O
data O
( O
Ord O
´ O
o˜nez O
and O
Roggen O
, O
2016 O
; O
Lin O
et O
al O
. O
, O
2017 O
) O
for O
classiﬁcation O
and O
trend O
prediction O
tasks O
. O

Entity2Vec O
Model O
( O
E2V O
): O
or O
entity O
embedding O
learning O
using O
Skip O
- O
Gram O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
model O
. O

RankSVM B-MethodName
: O
Ceccarelli O
et O
al O
. O
( O
2013 O
) O
learned O
entity O
relatedness O
from O
a O
set O
of O
28 O
handcrafted O
features O
, O
using O
the O
traditional O
learning O
- O
to O
- O
rank O
method O
, O
RankSVM B-MethodName
. O

WLM O
performs O
poorer O
for O
the O
latter O
, O
that O
can O
be O
interpreted O
as O
link O
- O
based O
methods O
tend O
to O
slowly O
adaptModelPearson100 O
r100 O
nDCG O
( O
proxy O
) O
nDCG O
( O
human O
) O
@10 O
@30 O
@50 O
all O
all O
@3 O
@10 O
@20 O
@3 O
@10 O
@20BaselinesWLM O
27.6 O
28.3 O
24.0 O
19.4 O
12.1 O
0.63 O
0.59 O
0.62 O
0.50 O
0.46 O
0.52 O
RankSVM B-MethodName
28.5 O
34.7 O
31.4 O
20.7 O
27.5 O
0.65 O
0.61 O
0.64 O
0.52 O
0.61 O
0.65 O
Entity2Vec O
18.6 O
22.0 O
21.8 O
20.5 O
18.7 O
0.62 O
0.60 O
0.61 O
0.54 O
0.53 O
0.54 O
DeepWalk O
31.3 O
30.9 O
21.4 O
17.6 O
10.1 O
0.41 O
0.43 O
0.47 O
0.34 O
0.38 O
0.45 O
ParaVecs O
- O
DBOW O
18.6 O
22.0 O
21.8 O
20.5 O
16.0 O
0.62 O
0.60 O
0.61 O
0.50 O
0.50 O
0.55 O
ParaVecs O
- O
DM O
19.0 O
23.0 O
23.2 O
22.3 O
18.3 O
0.66 O
0.63 O
0.63 O
0.49 O
0.52 O
0.58Model O
AblationTS O
- O
CNN O
51.9 O
51.0 O
43.0 O
35.8 O
26.5 O
0.41 O
0.43 O
0.47 O
0.40 O
0.43 O
0.48 O
TS O
- O
CNN O
- O
Att O
( O
Base O
) O
57.9 O
49.7 O
44.7 O
37.1 O
24.9 O
0.43 O
0.44 O
0.49 O
0.38 O
0.45 O
0.50 O
Base+PV O
60.6 O
44.2 O
41.4 O
36.4 O
11.2 O
0.41 O
0.43 O
0.47 O
0.49 O
0.51 O
0.55 O
Base+DW O
43.5 O
36.5 O
35.7 O
32.7 O
31.0 O
0.44 O
0.48 O
0.53 O
0.47 O
0.51 O
0.52 O
Base+PV+DW O
56.9 O
46.1 O
43.4 O
32.9 O
28,4 O
0.41 O
0.44 O
0.48 O
0.49 O
0.54 O
0.57 O
Content O
Emb+Graph O
Emb O
48.9 O
40.1 O
49.9 O
37.5 O
27.9 O
0.67 O
0.62 O
0.70 O
0.61 O
0.69 O
0.65 O
Base+ O
Content O
Emb O
67.1 O
54.2 O
53.4 O
43.7 O
26.5 O
0.67 O
0.69 O
0.71 O
0.61 O
0.72 O
0.74 O
Base+ O
Graph O
Emb O
55.2 O
50.2 O
41.3 O
31.5 O
35.5 O
0.71 O
0.75 O
0.78 O
0.650.780.81 O
Trio O
58.6 O
54.3 O
50.2 O
45.4 O
43.5 O
0.75 O
0.78 O
0.83 O
0.740.820.85 O
Table O
3 O
: O
Performance O
of O
different O
models O
on O
task O
( O
1 O
) O
Pearson O
, O
Spearman O
’s O
rranking O
correlation B-MetricName
, O
and O
task O
( O
2 B-MetricValue
) O
recommendation O
( O
measured O
by O
nDCG O
) O
. O

The O
bi O
- O
directional O
LSTM B-MethodName
encoder O
takes O
the O
article O
xas O
an O
input O
and O
computes O
a O
sequence O
of O
encoder O
hidden O
states O
h1;h2;::hn O
. O

The O
last O
state O
hnbecomes O
the O
initial O
state O
of O
the O
LSTM B-MethodName
decoder O
which O
uses O
an O
attention O
mechanism O
to O
generate O
the O
output O
summary O
word O
by O
word O
. O

They O
report O
best O
performance O
using O
a O
combination O
of O
a O
Support O
Vector O
Machine O
( O
SVM B-MethodName
) O
model O
with O
Word2Vec B-MethodName
representations O
on O
the O
CTRW O
( O
Hayakawa O
and O
Ehrlich O
, O
1994 O
) O
and O
BEAN O
( O
Lahiri O
, O
2015 O
; O
Pavlick O
and O
Tetreault O
, O
2016 O
) O
datasets O
, O
obtaining O
84:4%accuracy B-MetricName
on O
the O
former O
and O
a O
Spearman O
’s O
of0:662on O
the O
latter O
. O

Next O
, O
an O
SVM B-MethodName
model O
is O
trained O
to O
ﬁnd O
a O
separating O
hyperplane O
between O
vector O
space O
representations O
( O
coming O
from O
Word2Vec B-MethodName
model O
trained O
on O
Google O
News O
corpus O
) O
of O
these O
formal O
and O
informal O
seeds O
. O

To O
compute O
the O
formality O
of O
a O
word O
sequence O
y O
, O
we O
use O
the O
weighted B-HyperparameterName
average B-MetricName
function O
from O
Niu O
et O
al O
. O
( O
2017 B-MetricValue
): O
F(y O
) O
= O
P O
wi2yjL(wi)j O
: O
L(wi)P O
wi2yjL(wi)j O
; O
( O
6 O
) O
whereL(wi)is O
the O
lexical O
formality O
score B-MetricName
from O
the O
SVM B-MethodName
model O
described O
above O
. O

The O
usage O
of O
two O
tokens O
keeps O
the O
input O
symmetric O
, O
making O
it O
easier O
for O
both O
the O
forward O
and O
backward O
LSTM B-MethodName
encoder O
networks O
to O
absorb O
the O
formality O
information O
at O
the O
start O
of O
generating O
their O
half O
of O
the O
encoding O
states O
. O

Skip O
- O
Gram O
Model O
Our O
model O
builds O
on O
word O
embeddings O
trained O
via O
the O
skip O
- O
gram O
model O
with O
negative O
sampling O
( O
SGNS O
) O
, O
proposed O
by O
Mikolov O
et O
al O
. O
( O
2013a O
, O
b O
) O
. O

Qualitative O
Nearest O
Neighbor O
Test O
To O
test O
whether O
the O
learned O
embedding O
vectors O
are O
semantically O
meaningful B-MetricName
, O
we O
chose O
some O
words O
from O
the O
new O
vocabulary O
and O
reported O
their O
nearest B-MethodName
neighbors I-MethodName
in O
the O
extended O
vocabulary O
. O

We O
expect O
the O
nearest B-MethodName
neighbors I-MethodName
to O
have O
a O
close O
semantic O
meanings B-MetricName
. O

The O
sequence O
encoder O
is O
based O
on O
a O
multi O
- O
layer O
transformer O
architecture O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
where O
inputs O
at O
the O
bottom O
layer O
combine O
the O
character O
- O
level O
features O
, O
POS B-TaskName
tag O
, O
named O
entity O
tags O
, O
and O
BERT B-MethodName
- O
based O
features O
. O

We O
rely O
on O
an O
existing O
state O
- O
of O
- O
the O
- O
art O
‘ O
end O
- O
toend O
’ O
coreference B-TaskName
resolution O
system O
based O
on O
the O
SpanBERT B-MethodName
language O
model O
( O
Joshi O
et O
al O
. O
, O
2020 O
) O
, O
henceforth O
SpanBERT B-MethodName
- O
coref O
. O

It O
builds O
directly O
on O
the O
coreference B-TaskName
systems O
of O
Joshi O
et O
al O
. O
( O
2019 O
) O
and O
Lee O
et O
al O
. O
( O
2018 O
) O
, O
the O
main O
innovation O
being O
its O
reliance O
on O
SpanBERT B-MethodName
in O
place O
of O
( O
respectively O
) O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
and O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

SpanBERT B-MethodName
and O
BERT B-MethodName
are O
transformer O
encoders O
pretrained O
on O
masked O
language O
modeling O
( O
Devlin O
et O
al O
. O
, O
2018 O
): O
a O
percentage O
of O
tokens O
is O
masked O
– O
substituted O
with O
a O
special O
token O
[ O
MASK O
] O
; O
the O
model O
has O
to O
predict O
these O
tokens O
based O
on O
their O
surrounding O
context O
. O

For O
training O
SpanBERT B-MethodName
, O
random O
contiguous O
sets O
of O
tokens O
– O
spans O
– O
are O
masked O
, O
and O
an O
additional O
Span O
Boundary O
Objective O
encourages O
meaningful B-MetricName
span O
representations O
at O
the O
span O
’s O
boundary O
tokens.2 O
3 O
Methods O
The O
SpanBERT B-MethodName
- O
coref O
system O
. O

We O
use O
the O
best O
system O
from O
Joshi O
et O
al O
. O
( O
2020 O
) O
, O
which O
is O
based O
on O
SpanBERT B-MethodName
- O
base O
( O
12 O
hidden B-HyperparameterName
layers I-HyperparameterName
of O
768 B-HyperparameterValue
units O
, O
with O
12 O
attention O
heads O
) O
. O

In O
SpanBERT B-MethodName
- O
coref O
, O
each O
span O
of O
text O
is O
represented O
by O
a O
ﬁxed O
- O
length O
vector O
computed O
from O
SpanBERT B-MethodName
token O
representations O
, O
obtained O
considering O
a O
surrounding O
context O
window O
of O
maximum O
384 O
tokens.3From O
span O
representations O
, O
a O
mention O
2In O
addition O
, O
the O
Next O
Sentence O
Prediction O
task O
used O
in O
BERT B-MethodName
is O
dropped O
in O
SpanBERT B-MethodName
, O
which O
allows O
the O
latter O
to O
be O
trained O
on O
larger O
contiguous O
segments O
of O
text O
. O

The O
probability O
that O
a O
mention O
xrefers O
to O
the O
entity O
e O
– O
P(Ex O
= O
e O
) O
– O
can O
be O
computed O
as O
the O
sum O
of O
the O
antecedent O
probabilities O
of O
the O
mentions O
of O
ein O
the O
previous O
discourse O
( O
Me O
): O
P(Ex O
= O
e O
) O
= O
X O
i2MeP(antecedent O
x O
= O
i)(3 O
) O
However O
, O
in O
SpanBERT B-MethodName
- O
coref O
this O
probability O
is O
conditioned O
both O
on O
the O
mention O
and O
its O
context O
( O
the O
model O
has O
observed O
both O
to O
compute O
a O
prediction O
) O
, O
whereas O
we O
need O
a O
distribution O
that O
is O
only O
conditioned O
on O
the O
context O
. O

Note O
that O
SpanBERTcoref B-MethodName
can O
be O
directly O
used O
for O
masked O
coreference B-TaskName
, O
token O
representation O
, O
end O
token O
representation O
, O
and O
a O
weighted B-HyperparameterName
sum O
( O
attention O
) O
over O
all O
of O
its O
token O
representations.since O
its O
vocabulary O
already O
includes O
the O
[ O
MASK O
] O
token O
. O

Therefore O
, O
we O
train O
a O
new O
instance O
of O
SpanBERT B-MethodName
- O
coref O
adapted O
to O
our O
purposes O
– O
Mm O
. O

As O
mentioned O
earlier O
, O
SpanBERT B-MethodName
- O
coref O
is O
jointly O
trained O
to O
detect O
mentions O
and O
to O
identify O
coreference B-TaskName
links O
between O
them O
. O

Our O
setup O
differs O
from O
that O
of O
Tily O
and O
Piantadosi O
( O
2009 O
) O
and O
Modi O
et O
al O
. O
( O
2017b O
) O
in O
that O
their O
human O
participants O
were O
given O
only O
the O
context O
preceding O
the O
mention O
, O
while O
our O
model O
is O
given O
context O
on O
both O
sides O
, O
in O
line O
with O
the O
bidirectional O
nature O
of O
BERT B-MethodName
, O
SpanBERT B-MethodName
and O
the O
state O
of O
the O
art O
in O
coreference B-TaskName
resolution O
. O

4 O
Evaluation O
Table O
1 O
reports O
the O
results O
of O
evaluation O
on O
OntoNotes O
test O
data O
for O
both O
Mu(the O
standard O
SpanBERT B-MethodName
- O
coref O
coreference B-TaskName
system O
) O
and O
our O
variantMm O
, O
trained O
with O
15 O
% O
of O
mentions O
masked O
in O
each O
document.6The O
table O
reports O
results O
for O
both O
5For O
hyperparameter O
tuning O
on O
development O
data O
, O
we O
use O
a O
faster O
but O
more O
coarse O
- O
grained O
method O
, O
described O
in O
Appendix O
A. O

To O
quantify O
the O
effect O
of O
predictability O
on O
mention O
type O
, O
we O
use O
multinomial O
logistic B-MethodName
regression I-MethodName
, O
using O
as O
the O
dependent O
variable O
the O
three O
- O
way O
referential O
choice O
with O
pronoun O
as O
the O
base O
level O
, O
and O
surprisal O
as O
independent O
variable.7The O
results O
of O
this O
surprisal O
- O
only O
regression O
are O
given O
in O
the O
top O
left O
segment O
of O
Table O
3 O
. O

Overall O
, O
the O
results O
corroborate O
the O
ﬁnding O
in O
Tily O
and O
Piantadosi O
( O
2009 O
) O
that O
full O
NPs O
are O
favoured O
, O
and O
pronouns O
and O
proper O
names O
disfavored O
, O
when O
surprisal O
is O
higher O
; O
and O
extend O
their O
ﬁnding O
, O
based O
on O
newspaper O
texts O
only O
, O
to O
a O
larger O
amount O
of O
data O
and O
more O
diverse O
genres O
of O
text O
( O
news O
, O
magazine O
articles O
, O
weblogs O
, O
religious O
texts O
, O
broadcast O
and O
telephone O
conversation).Predicting O
mention O
type O
Predicting O
mention O
length O
Proper O
name O
Full O
NP O
 O
s.e.z O
p O
 O
s.e.z O
p O
 O
s.e.t O
p O
Intercept O
-.63 O
.03 O
-23.8 O
- O
-.26 O
.02 O
-10.9 O
- O
1.87 O
.02 O
80.8 O
surprisal O
.31 O
.03 O
9.6 O
* O
.47 O
.03 O
16.4 O
* O
.25 O
.02 O
10.7 O
* O
Intercept O
-.24 O
.07 O
-3.6 O
- O
.04 O
.07 O
.6 O
- O
1.81 O
.05 O
40.1 O
distance O
3.13 O
.12 O
25.4 O
* O
3.10 O
.12 O
25.2 O
* O
.17 O
.02 O
7.1 O
* O
frequency O
.09 O
.03 O
3.1 O
* O
-.13 O
.03 O
-3.8 O
* O
-.13 O
.02 O
-5.4 O
* O
antecedent O
previous O
subject O
-1.31 O
.09 O
-13.9 O
* O
-1.10 O
.08 O
-13.7 O
* O
-.51 O
.06 O
-8.5 O
* O
mention O
subject O
.07 O
.07 O
1.0 O
.3 O
-0.50 O
.06 O
-7.7 O
* O
.04 O
.05 O
.8 O
.4 O
antecedent O
type O
proper O
name O
1.78 O
.08 O
22.8 O
* O
.41 O
.09 O
4.6 O
* O
-.21 O
.06 O
-3.2 O
* O
full O
NP O
-.17 O
.08 O
-2.2 O
* O
1.18 O
.06 O
18.1 O
* O
.42 O
.06 O
7.5 O
* O
surprisal O
.05 O
.04 O
1.5 O
.1 O
.23 O
.03 O
7.8 O
* O
.17 O
.02 O
7.4 O
* O
Table O
3 O
: O
( O
left O
) O
Two O
Multinomial O
logit O
models O
predicting O
mention O
type O
( O
baseline O
level O
is O
“ O
pronoun O
" O
) O
, O
( O
right O
) O
two O
linear B-MethodName
regression I-MethodName
models O
predicting O
mention O
length O
( O
number O
of O
tokens O
) O
of O
the O
masked O
mention O
, O
based O
on O
1 O
) O
surprisal O
alone O
and O
2 O
) O
shallow O
linguistic O
features O
+ O
surprisal O
. O

All O
predictors O
in O
models O
improved O
goodness O
- O
of-ﬁt O
to O
the O
data O
except O
“ O
target O
mention O
is O
subject O
” O
in O
the O
fuller O
linear B-MethodName
regression I-MethodName
model O
. O

We O
ﬁt O
linear B-MethodName
regression I-MethodName
models O
with O
mention O
length O
in O
number O
of O
tokens O
as O
the O
dependent O
variable O
( O
or O
number O
of O
characters O
, O
in O
Appendix O
C O
) O
, O
and O
, O
again O
, O
surprisal O
with O
and O
without O
shallow O
linguistic O
features O
as O
independent O
variables O
. O

Word O
embeddings O
such O
as O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
Glove O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
and O
FastText O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
express O
word O
similarities O
as O
continuous O
vectors O
in O
Euclidean O
space O
, O
and O
have O
brought O
significant O
advances O
to O
various O
applications O
( O
Collobert O
et O
al O
. O
, O
2011 O
; O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

Recently O
, O
it O
has O
become O
common O
to O
locally O
parametrize O
these O
models O
using O
rich O
features O
extracted O
by O
recurrent O
neural B-MethodName
networks I-MethodName
( O
such O
as O
LSTM B-MethodName
) O
, O
while O
enforcing O
consistent O
outputs O
through O
a O
simple O
linear O
- O
chain O
model O
, O
representing O
Markovian O
dependencies O
between O
successive O
labels O
. O

1 O
Introduction O
As O
with O
many O
other O
prediction O
tasks O
involving O
complex O
structured O
outputs O
, O
such O
as O
image O
segmentation O
( O
Chen O
et O
al O
. O
, O
2018 O
) O
, O
machine O
transla O
- O
tion O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
, O
and O
speech O
recognition O
( O
Hinton O
et O
al O
. O
, O
2012 O
) O
, O
deep O
neural B-MethodName
networks I-MethodName
( O
DNNs O
) O
for O
sequence O
labeling O
and O
shallow B-TaskName
parsing I-TaskName
have O
become O
standard O
tools O
for O
for O
information O
extraction O
( O
Collobert O
et O
al O
. O
, O
2011 O
; O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

We O
then O
feed O
the O
embedded O
sequence O
w O
= O
fw1;w2;:::;w O
Tginto O
a O
bidirectional O
LSTM B-MethodName
( O
Graves O
and O
Schmidhuber O
, O
2005 O
) O
. O

4.2 O
Training O
Details O
Our O
baseline O
is O
the O
BiLSTM+CRF B-MethodName
model O
from O
Lample O
et O
al O
. O
( O
2016 O
) O
, O
employing O
a O
bidirectional O
LSTM B-MethodName
with O
500 O
hidden B-HyperparameterName
units I-HyperparameterName
for O
input O
featurization O
to O
capture O
long O
- O
range O
dependencies O
in O
the O
input O
space O
. O

For O
training O
our O
models O
, O
we O
use O
the O
hyper O
- O
parameter O
settings O
from O
the O
LSTM+CRF B-MethodName
model O
of O
Lample O
et O
al O
. O
( O
2016 O
) O
. O

4.3 O
Results O
Table O
1 O
shows O
that O
overall O
performance O
on O
the O
UMass O
Citation O
dataset O
using O
the O
embedded O
- O
state O
latent O
CRF O
( O
95:18 O
) O
is O
marginally O
better O
than O
the O
baseline O
BiLSTM+CRF B-MethodName
model O
( O
95:07 O
) O
. O

Dataset O
CRF O
EL O
- O
CRF O
+ O
UM O
ASSCITATION O
95.07 O
95.18 O
0.11 O
CLEF O
EHEALTH O
68.66 O
70.32 O
1.66 O
Table O
1 O
: O
Entity O
- O
level O
F1 B-MetricName
scores B-MetricName
of O
the O
embeddedstate O
latent O
CRF O
and O
BiLSTM+CRF B-MethodName
baseline O
. O

Table O
1 O
demonstrates O
that O
EL O
- O
CRF O
outperforms O
the O
BiLSTM+CRF B-MethodName
on O
both O
datasets O
, O
with O
larger O
gains O
on O
the O
much O
smaller O
CLEF O
data O
. O

We O
introduce O
a O
neural B-MethodName
network I-MethodName
model O
that O
marries O
together O
ideas O
from O
two O
prominent O
strands O
of O
research O
on O
domain O
adaptation O
through O
representation O
learning O
: O
structural O
correspondence O
learning O
( O
SCL O
, O
( O
Blitzer O
et O
al O
. O
, O
2006 O
) O
) O
and O
autoencoder O
neural B-MethodName
networks I-MethodName
( O
NNs O
) O
. O

Below O
, O
we O
ﬁrst O
discuss O
two O
prominent O
ideas O
in O
feature O
representation O
learning O
: O
pivot O
features O
and O
autoencoder O
neural B-MethodName
networks I-MethodName
. O

The O
sentiment O
classiﬁer O
we O
employ O
, O
in O
this O
case O
as O
well O
as O
with O
our O
methods O
and O
with O
the O
SCL O
- O
MI O
and O
MSDA O
baselines O
, O
is O
a O
standard O
logistic B-MethodName
regression I-MethodName
classiﬁer.5 O
6 O
Experimental O
Protocol O
Following O
the O
unsupervised O
domain O
adaptation O
setup O
( O
Section O
2 O
) O
, O
we O
have O
access O
to O
unlabeled O
data O
from O
both O
the O
source O
and O
the O
target O
domains O
, O
which O
we O
use O
to O
train O
the O
representation O
learning O
models O
. O

At O
the O
same O
time O
, O
we O
detect O
a O
clear O
and O
evident O
beneﬁt O
when O
employing O
BERT B-MethodName
, O
a O
state O
- O
of O
- O
the O
- O
art O
deep O
language O
representation O
model O
, O
even O
in O
classiﬁcation O
scenarios O
with O
over O
30 O
different O
labels O
and O
limited O
amounts O
of O
training O
data O
. O

In O
order O
to O
contribute O
to O
these O
research O
efforts O
and O
following O
recent O
advancements O
in O
deep O
language O
representation O
models O
( O
Devlin O
et O
al O
. O
, O
2018 O
; O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
we O
test O
the O
potential O
of O
BERT B-MethodName
( O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
) O
for O
policy O
- O
topic O
classiﬁcation O
on O
both O
debate O
motions O
and O
manifestos O
. O

First O
, O
we O
examined O
the O
performance O
of O
Support O
Vector O
Machines O
( O
SVM B-MethodName
) O
trained O
using O
lexical O
( O
tfidf O
) O
or O
word O
embedding O
( O
w O
- O
emb O
) O
features O
, O
which O
act O
as O
strong O
traditional O
baselines O
. O

As O
ﬁnal O
skyline O
comparisons O
, O
we O
present O
the O
performance O
of O
( O
1)a O
pre O
- O
trained O
BERT B-MethodName
( O
large O
, O
cased O
) O
model O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
with O
a O
ﬁnal O
soft O
- O
max O
layer O
; O
and O
( O
2)the O
same O
pre O
- O
trained O
BERT B-MethodName
model O
, O
with O
a O
CNN O
and O
max O
- O
pooling O
layers O
before O
the O
soft O
- O
max O
layer O
. O

Motions O
: O
Quasi O
- O
sentence O
Classiﬁcation O
We O
tested O
the O
supervised O
pipelines O
at O
the O
quasisentence O
level O
and O
at O
the O
two O
levels O
of O
class O
label O
granularity O
( O
policy O
and O
domain O
) O
, O
which O
allowsModel O
Text O
Data O
Source O
Policy O
Domain O
representation O
Macro O
Micro O
Macro O
Micro O
Unigram O
overlap O
BOWMotion O
titles O
0.09 O
0.15 O
0.26 O
0.31 O
Motions O
0.09 O
0.21 O
0.23 O
0.38 O
Titles+motions O
0.10 O
0.23 O
0.25 O
0.39 O
Cosine O
similarity O
Tf O
- O
idfMotion O
titles O
0.23 O
0.34 O
0.44 O
0.49 O
Motions O
0.30 O
0.36 O
0.50 O
0.51 O
Titles+motions O
0.32 O
0.41 O
0.51 O
0.56 O
SVMTf B-MethodName
- O
idfMotions O
0.33 O
0.48 O
0.58 O
0.63 O
Manifestos O
0.29 O
0.40 O
0.53 O
0.56 O
Domain O
w O
- O
embMotions O
0.32 O
0.50 O
0.53 O
0.62 O
Manifestos O
0.25 O
0.41 O
0.45 O
0.53 O
Wiki O
w O
- O
embMotions O
0.35 O
0.51 O
0.55 O
0.65 O
Manifestos O
0.21 O
0.38 O
0.45 O
0.52 O
CNNDomain O
w O
- O
embMotions O
0.15 O
0.38 O
0.58 O
0.64 O
Manifestos O
0.19 O
0.30 O
0.37 O
0.51 O
Wiki O
w O
- O
embMotions O
0.13 O
0.29 O
0.50 O
0.57 O
Manifestos O
0.21 O
0.36 O
0.48 O
0.56 O
BERT B-MethodName
Large O
, O
casedMotions O
0.26 O
0.47 O
0.42 O
0.58 O
Manifestos O
0.32 O
0.47 O
0.52 O
0.57 O
+ O
Motions O
ﬁne O
- O
tuning O
0.39 O
0.50 O
0.60 O
0.67 O
BERT+CNN B-MethodName
Large O
, O
casedMotions O
0.27 O
0.48 O
0.42 O
0.56 O
Manifestos O
0.29 O
0.44 O
0.54 O
0.60 O
+ O
Motions O
ﬁne O
- O
tuning O
0.47 O
0.57 O
0.61 O
0.69 O
Table O
6 O
: O
F1 B-MetricName
scores B-MetricName
for O
similarity O
matching O
and O
classiﬁcation O
of O
debate O
motions O
at O
the O
quasi O
- O
sentence O
level O
. O

Concerning O
the O
SVM B-MethodName
and O
CNN O
baselines O
, O
training O
the O
classiﬁers O
on O
the O
large O
collection O
of O
annotated O
manifestos O
and O
then O
applying O
them O
to O
the O
motions O
does O
not O
lead O
to O
improvements O
in O
comparison O
to O
the O
performance O
of O
the O
same O
architectures O
on O
the O
motions O
alone O
. O

Finally O
, O
to O
further O
conﬁrm O
the O
large O
potential O
of O
BERT B-MethodName
, O
even O
in O
tasks O
which O
involve O
many O
labels O
, O
a O
lack O
of O
training O
data O
, O
and O
a O
very O
speciﬁc O
style O
of O
communication O
, O
we O
have O
obtained O
a O
clear O
improvement O
over O
all O
other O
systems O
when O
employing O
this O
state O
- O
of O
- O
the O
- O
art O
architecture O
, O
trained O
on O
manifesto O
quasi O
- O
sentences O
and O
further O
ﬁne O
- O
tuned O
on O
motions O
. O

Con O
- O
Model O
Text O
representation O
Policy O
Domain O
Macro O
Micro O
Macro O
Micro O
SVMTf B-MethodName
- O
idf O
0.39 O
0.54 O
0.58 O
0.66 O
Domain O
w O
- O
emb O
0.35 O
0.53 O
0.52 O
0.64 O
Wiki O
w O
- O
emb O
0.38 O
0.54 O
0.54 O
0.66 O
CNNDomain O
w O
- O
emb O
0.28 O
0.47 O
0.54 O
0.58 O
Wiki O
w O
- O
emb O
0.27 O
0.44 O
0.52 O
0.56 O
BERT B-MethodName
Large O
, O
cased O
0.42 O
0.58 O
0.58 O
0.64 O
BERT B-MethodName
+ O
CNN O
Large O
, O
cased O
0.42 O
0.58 O
0.60 O
0.70 O
Table O
7 O
: O
F1 B-MetricName
scores B-MetricName
for O
classiﬁcation O
of O
party O
political O
manifestos O
at O
the O
quasi O
- O
sentence O
level O
. O

versely O
, O
traditional O
SVM B-MethodName
baselines O
offer O
reasonable O
results O
, O
and O
we O
achieve O
state O
- O
of O
- O
the O
- O
art O
performances O
when O
employing O
BERT B-MethodName
. O

The O
third O
is O
the O
signiﬁcant O
contribution O
that O
the O
use O
of O
BERT B-MethodName
provides O
our O
supervised O
pipelines O
, O
which O
are O
able O
to O
achieve O
state O
- O
of O
- O
theart O
performance O
on O
both O
the O
motions O
and O
manifesto O
quasi O
- O
sentences O
. O

The O
generated O
dataset O
of O
topically O
labelled O
motions O
along O
with O
the O
trained O
BERT+CNN B-MethodName
classiﬁer O
can O
now O
pave O
the O
way O
for O
further O
work O
at O
the O
intersection O
of O
natural O
language O
processing O
and O
political O
science O
, O
which O
can O
beneﬁt O
from O
these O
ﬁne O
- O
grained O
policy O
position O
annotations O
: O
from O
analysing O
the O
sentiment O
of O
the O
motions O
to O
measuring O
the O
level O
of O
disagreement O
between O
members O
of O
the O
same O
party O
, O
and O
up O
to O
full O
- O
blown O
argumentation O
mining O
of O
each O
debate O
. O

In O
this O
paper O
, O
we O
propose O
a O
uniﬁed O
user O
geolocation O
method O
which O
relies O
on O
a O
fusion O
of O
neural B-MethodName
networks I-MethodName
. O

Moreover O
, O
we O
utilize O
a O
bidirectional O
LSTM B-MethodName
network O
augmented O
with O
an O
attention O
mechanism O
to O
identify O
the O
most O
location O
indicative O
words O
in O
textual O
content O
of O
tweets O
. O

We O
propose O
a O
uniﬁed O
user O
geolocation O
method O
that O
relies O
on O
a O
fusion O
of O
neural B-MethodName
networks I-MethodName
, O
incorporating O
different O
types O
of O
avail O
- O
able O
information O
: O
tweet O
message O
, O
users O
’ O
social O
relationships O
, O
and O
metadata O
ﬁelds O
embedded O
in O
tweets O
and O
proﬁles O
. O

For O
modeling O
the O
tweet O
text O
( O
and O
textual O
metadata O
ﬁelds O
) O
, O
we O
use O
bidirectional O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
networks O
augmented O
with O
a O
context O
- O
aware O
attention O
mechanism O
( O
Yang O
et O
al O
. O
, O
2016 O
) O
, O
which O
helps O
to O
identify O
the O
most O
location O
indicative O
words O
. O

Multinomial O
Na O
¨ıve O
Bayes O
( O
Han O
et O
al O
. O
, O
2012 O
, O
2014 O
; O
Wing O
and O
Baldridge O
, O
2011 O
) O
, O
logistic B-MethodName
regression I-MethodName
( O
Wing O
and O
Baldridge O
, O
2014 O
; O
Han O
et O
al O
. O
, O
2014 O
) O
, O
hierarchical O
logistic B-MethodName
regression I-MethodName
( O
Wing O
and O
Baldridge O
, O
2014 O
) O
, O
and O
multi O
- O
layer O
neural O
network O
with O
stacked O
denoising O
autoencoder O
( O
Liu O
and O
Inkpen O
, O
2015 O
) O
have O
realized O
geolocation O
predic O
- O
tion O
from O
text O
. O

Thomas O
and O
Hennig O
( O
2017 O
) O
have O
proposed O
a O
geolocation O
method O
that O
relies O
on O
the O
combination O
of O
individual O
neural B-MethodName
networks I-MethodName
trained O
on O
text O
andmetadata O
ﬁelds O
. O

For O
processing O
the O
tweet O
text O
, O
we O
utilize O
word O
embeddings O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
and O
bidirectional O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
unit O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
augmented O
with O
a O
context O
- O
aware O
attention O
mechanism O
( O
Yang O
et O
al O
. O
, O
2016 O
) O
( O
Section O
4.1 O
) O
. O

For O
representing O
textual O
metadata O
ﬁelds O
( O
i.e. O
, O
location O
, O
description O
, O
user O
name O
, O
and O
timezone O
) O
, O
we O
use O
word O
embeddings O
and O
bidirectional O
LSTM B-MethodName
networks O
with O
attentionmechanism O
. O

The O
embeddings O
of O
tweet O
words O
are O
then O
forwarded O
to O
an O
LSTM B-MethodName
layer O
. O

An O
LSTM B-MethodName
takes O
as O
input O
the O
words O
of O
a O
tweet O
and O
produces O
the O
word O
annotationsH= O
( O
h1 O
; O
h2 O
; O
: O
: O
: O
; O
h O
n O
) O
, O
where O
hiis O
the O
hidden O
state O
of O
the O
LSTM B-MethodName
at O
time O
- O
step O
i O
, O
summarizing O
all O
the O
information O
of O
the O
sentence O
up O
to O
wi O
. O

We O
use O
bidirectional O
LSTM B-MethodName
( O
BiLSTM B-MethodName
) O
in O
order O
to O
get O
annotations O
for O
each O
word O
that O
summarize O
the O
information O
from O
both O
directions O
of O
the O
message O
. O

A O
bidirectional O
LSTM B-MethodName
consists O
of O
a O
forward O
LSTM,  B-MethodName
! O
f O
, O
that O
reads O
the O
sentence O
from O
w1towT O
, O
and O
a O
backward O
LSTM B-MethodName
, O
 f O
, O
that O
reads O
the O
sentence O
from O
wTtow1 O
. O

We O
obtain O
the O
ﬁnal O
annotation O
for O
each O
word O
wi O
, O
by O
concatenating O
the O
annotations O
from O
both O
directions O
: O
hi=  O
! O
hik O
 hi O
; O
hi2R2L(4 O
) O
wherekdenotes O
the O
concatenation O
operation O
and O
Lthe O
size O
of O
each O
LSTM B-MethodName
. O

For O
metadata O
ﬁelds O
containing O
texts O
( O
i.e. O
, O
user O
description O
, O
user O
location O
, O
user O
name O
, O
and O
timezone O
) O
, O
we O
use O
an O
embedding O
layer O
and O
consequently O
forward O
the O
results O
to O
an O
LSTM B-MethodName
layer O
. O

The O
layers O
and O
the O
embeddings O
in O
our O
subnetworks O
have O
parameters O
like O
embedding O
dimension O
, O
LSTM B-MethodName
unit O
size O
, O
and O
attention O
context O
vector O
size O
. O

The O
dropout B-HyperparameterName
rate I-HyperparameterName
between O
layers O
is O
set O
to O
0.2.Model O
Embedding O
size O
LSTM B-MethodName
unit O
size O
Attention O
vector O
size O
TwitterUS O
WNUT O
TwitterUS O
WNUT O
TwitterUS O
WNUT O
Tweet O
text O
200 B-HyperparameterValue
300 O
100 O
200 O
200 O
400 O
User O
network O
200 O
400 O
100 O
200 O
200 O
400 O
Location O
100 O
200 O
100 O
200 O
200 O
400 O
Description O
100 O
200 O
100 O
200 O
200 O
400 O
User O
name O
100 O
200 O
100 O
200 O
200 O
400 O
Timezone O
100 O
200 O
100 O
200 O
200 O
400 O
Table O
1 O
: O
Parameter O
settings O
of O
the O
proposed O
models O
. O

The O
main O
reason O
is O
the O
effective O
representation O
of O
text O
, O
metadata O
, O
and O
network O
information O
, O
and O
unifying O
them O
through O
a O
fusion O
of O
neural B-MethodName
networks I-MethodName
. O

6 O
Conclusion O
and O
Future O
Work O
In O
this O
paper O
, O
we O
have O
proposed O
a O
uniﬁed O
user O
geolocation O
method O
which O
relies O
on O
a O
fusion O
of O
neural B-MethodName
networks I-MethodName
. O

For O
modeling O
tweet O
message O
and O
textual O
metadata O
ﬁelds O
, O
we O
utilized O
a O
bidirectional O
LSTM B-MethodName
network O
augmented O
with O
an O
attention O
mechanism O
to O
identify O
the O
most O
location O
indicative O
words O
. O

Recently O
, O
contextualized O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
( O
BERT B-MethodName
) O
models O
have O
established O
state O
- O
of O
- O
the O
- O
art O
performance O
for O
a O
variety O
of O
NLP O
tasks O
. O

However O
, O
there O
has O
not O
been O
much O
effort O
in O
exploring O
language O
transfer O
using O
BERT B-MethodName
for O
event O
extraction O
. O

Our O
work O
is O
the O
ﬁrst O
to O
experiment O
with O
two O
event O
architecture O
variants O
in O
a O
cross O
- O
lingual O
setting O
, O
to O
show O
the O
effectiveness O
of O
contextualized O
embeddings O
obtained O
using O
BERT B-MethodName
, O
and O
to O
explore O
and O
analyze O
its O
performance O
on O
Arabic O
. O

More O
recently O
, O
BERT B-MethodName
, O
a O
deep O
bidirectional O
representation O
which O
jointly O
conditions O
on O
both O
leftand O
right O
context O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
was O
proposed O
, O
which O
unlike O
MUSE O
, O
provides O
contextualized O
word O
embeddings O
, O
and O
has O
been O
shown O
to O
achieve O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
many O
NLP O
tasks O
. O

In O
particular O
, O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
propose O
a O
method O
based O
on O
BERT B-MethodName
for O
enhancing O
event O
trigger O
and O
argument O
extraction O
by O
generating O
more O
labeled O
data O
. O

We O
evaluate O
our O
framework O
using O
two O
embedding O
approaches O
: O
typebased O
unsupervised O
embeddings O
( O
MUSE O
) O
and O
contextualized O
embeddings O
( O
BERT B-MethodName
) O
. O

2 O
Methodology O
We O
treat O
trigger O
extraction O
as O
a O
sequence O
tagging O
problem O
for O
which O
we O
start O
by O
designing O
a O
ba O
- O
sic O
state O
- O
of O
- O
the O
- O
art O
approach O
for O
sequence O
tagging O
based O
on O
bidirectional O
Long O
Short O
Term O
Memory O
( O
bi O
- O
LSTM B-MethodName
) O
with O
word O
and O
character O
embeddings O
and O
a O
CRF O
layer O
on O
top O
of O
it O
. O

Then O
, O
we O
describe O
an O
approach O
that O
trains O
BERT B-MethodName
with O
a O
CRF O
layer O
for O
the O
task O
. O

2.1 O
Bi O
- O
LSTM B-MethodName
- O
Char O
- O
CRF O
networks O
The O
Bi O
- O
LSTM B-MethodName
- O
Char O
- O
CRF O
for O
sequence O
tagging O
model O
is O
a O
hierarchical O
neural O
network O
model O
based O
on O
three O
components O
: O
character O
- O
level O
using O
character O
embeddings O
, O
word O
- O
level O
using O
bi O
- O
LSTM B-MethodName
over O
word O
embeddings O
and O
sequence O
- O
level O
using O
CRF O
. O

Given O
an O
input O
character O
or O
word O
embeddings O
representation O
xtfor O
a O
given O
time O
step O
t O
, O
we O
use O
bidirectional O
LSTMs B-MethodName
by O
encoding O
features O
in O
their O
forward O
: O
fhi=    !LSTM B-MethodName
( O
xi)and O
backward O
bhi= O
    LSTM B-MethodName
( O
xi)directions O
and O
concatenating O
themhi= O
[ O
fhi;bhi]to O
capture O
information O
from O
both O
the O
past O
and O
future.2.1.2 O
Character O
Embeddings O
Character O
embeddings O
are O
used O
to O
capture O
orthographic O
patterns O
and O
to O
deal O
with O
out O
- O
ofvocabulary O
words O
, O
especially O
in O
the O
cross O
- O
lingual O
setting O
. O

We O
follow O
the O
same O
setup O
as O
Lample O
et O
al O
. O
( O
2016 O
) O
to O
obtain O
character O
embeddings O
using O
biLSTM B-MethodName
. O

Speciﬁcally O
, O
we O
concatenate O
both O
character O
and O
word O
- O
level O
features O
and O
use O
a O
bi O
- O
LSTM B-MethodName
on O
top O
of O
that O
. O

This O
layer O
simulates O
bi O
- O
LSTM B-MethodName
in O
its O
use O
of O
past O
and O
future O
tags O
to O
predict O
the O
current O
tag O
. O

s([x]N O
1;[i]N O
1 O
) O
= O
NX O
t=1([A][i]t 1;[i]t+ O
[ O
f][i]t;t)(1 O
) O
2.2 O
BERT B-MethodName
- O
CRF O
BERT B-MethodName
is O
a O
multi O
- O
layer O
bidirectional O
transformer O
encoder O
, O
an O
extension O
to O
the O
original O
Transformer O
model O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
. O

The O
pre O
- O
trained O
BERT B-MethodName
model O
provides O
a O
powerful O
contextualized O
representation O
which O
gives O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
for O
many O
NLP O
tasks O
. O

We O
use O
BERT B-MethodName
- O
CRF O
, O
which O
adds O
a O
CRF O
layer O
on O
top O
of O
BERT B-MethodName
’s O
contextualized O
embeddings O
layer O
. O

Zero O
- O
Shot O
learning O
experiments O
: O
As O
depicted O
in O
Figure O
3 O
, O
we O
train O
and O
ﬁne O
- O
tune O
on O
EN O
using O
multilingual O
embeddings O
( O
MUSE O
or O
BERT(multi B-MethodName
) O
) O
and O
test O
on O
ZH O
and O
AR O
assuming O
no O
resources O
for O
those O
languages O
. O

For O
Bi O
- O
LSTM B-MethodName
- O
Char O
- O
CRF O
, O
we O
train O
character O
embeddings O
using O
a O
single O
bi O
- O
LSTM B-MethodName
layer O
with O
100 O
hidden B-HyperparameterName
units I-HyperparameterName
and O
use O
another O
single O
layer O
of O
bi O
- O
LSTM B-MethodName
with O
300 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
to O
train O
on O
the O
concatenated O
word O
and O
char O
embeddings O
. O

For O
BERT B-MethodName
- O
CRF O
, O
we O
train O
monolingual O
EN O
and O
ZH O
using O
cased O
BERT B-MethodName
- O
Base O
and O
BERT B-MethodName
- O
ZH O
models4respectively O
and O
for O
all O
multilingual O
experiments O
, O
we O
use O
the O
recommended O
multi O
- O
cased O
BERT B-MethodName
- O
Base O
model.5All O
models O
were O
trained O
using O
12 B-HyperparameterValue
layers O
with O
768 O
hidden B-HyperparameterName
size I-HyperparameterName
and O
12 O
selfattention O
heads O
and O
110 O
Million O
parameters O
. O

We O
ﬁne O
tune O
all O
BERT B-MethodName
models O
with O
their O
default O
parameters O
. O

In O
3https://github.com/facebookresearch/ O
MUSE O
4No O
pre O
- O
trained O
BERT B-MethodName
model O
exists O
for O
Arabic O
. O

4 O
Results O
Table O
2 O
shows O
F1 B-MetricName
scores B-MetricName
of O
trigger O
identiﬁcation O
and O
classiﬁcation O
tested O
on O
EN O
, O
ZH O
and O
AR O
across O
two O
event O
architectures O
: O
Bi O
- O
LSTM B-MethodName
- O
Char O
- O
CRF O
and O
BERT B-MethodName
- O
CRF O
and O
using O
different O
embeddings O
and O
training O
schemes O
( O
ﬁne O
- O
grained O
performance O
analysis O
based O
on O
precision B-MetricName
, O
recall B-MetricName
scores B-MetricName
can O
be O
found O
in O
Appendix O
A O
) O
. O

6https://github.com/chakki-works/ O
seqevalModelTrain O
LangEmbed O
-dingsTest O
Lang O
EN O
ZH O
AR O
Ident O
Class O
Ident O
Class O
Ident O
Class O
yJi O
’s O
Cross O
- O
Entity’08 O
EN- O
N O
/ O
A O
68.3 O
- O
- O
- O
yLiao O
’s O
Cross O
- O
Event’10 O
- O
N O
/ O
A O
68.8 O
- O
- O
- O
yLi O
’s O
Joint O
- O
Beam’13 O
- O
70.4 O
67.5 O
- O
- O
- O
zChen O
’s O
DMCNN’15 O
Skip O
- O
Gram O
73.5 O
69.1 O
- O
- O
- O
zNguyen O
’s O
JRNN’16 O
C O
- O
BOW O
71.9 O
69.3 O
- O
- O
- O
zLius O
’s O
JMEE’18 O
Glove O
75.9 O
73.7 O
- O
- O
- O
zZhang O
’s O
GAIL’19 O
ELMo B-MethodName
73.9 O
72.0 O
- O
- O
- O
+ O
Feng O
’s O
HNN’16 O
Skip O
- O
Gram O
75.9 O
73.4 O
- O
- O
- O
+ O
Liu O
’s O
GMLATT’18 O
Skip O
- O
Gram O
74.1 O
72.4 O
- O
- O
- O
yLi’sMaxEnt’13 O
ZH- O
- O
- O
60.6 O
57.6 O
- O
yChen O
’s O
Rich O
- O
C’12 O
- O
- O
- O
66.7 O
63.2 O
- O
zHsi O
’s O
Multi’16 O
multi O
proj O
- O
- O
N O
/ O
A O
39.6 O
- O
+ O
Feng O
’s O
HNN’16 O
Skip O
- O
Gram O
- O
- O
68.2 O
63.0 O
- O
Bi O
- O
LSTM B-MethodName
- O
Char O
- O
CRFTest O
Lang O
FastText O
67.5 O
63.2 O
86.6 O
69.5 O
54.9 O
52.8 O
Test O
Lang O
MUSE68.9 O
62.5 O
29.6 O
25.0 O
20.3 O
18.7 O
EN O
- O
- O
61.3 O
48.8 O
53.0 O
42.2 O
EN+ZH O
69.5 O
65.8 O
77.2 O
70.6 O
- O
EN+AR O
70.6 O
66.9 O
- O
- O
56.1 O
53.2 O
All O
66.5 O
61.6 O
72.6 O
64.3 O
69.4 O
62.3 O
BERT B-MethodName
- O
CRFTest O
Lang O
Bert(Base O
) O
79.2 O
75.3 O
84.4 O
79.9 O
- O
Test O
Lang O
BERT B-MethodName
( O
multi)77.8 O
73.1 O
83.7 O
79.8 O
69.8 O
66.7 O
EN O
- O
- O
76.8 O
68.5 O
37.8 O
30.9 O
EN+ZH O
79.8 O
75.2 O
84.7 O
81.2 O
- O
EN+AR O
79.3 O
74.5 O
- O
- O
74.9 O
69.5 O
All O
79.2 O
73.5 O
87.7 O
83.2 O
73.2 O
68.9 O
Table O
2 O
: O
Comparison O
of O
performance O
with O
different O
train O
/ O
test O
language O
pairs O
using O
prior O
work O
baselines O
in O
the O
1sthalf O
and O
our O
method O
using O
Bi O
- O
LSTM B-MethodName
- O
Char O
- O
CRF O
and O
BERT B-MethodName
- O
CRF O
in O
the O
2ndhalf.y,+andzdenote O
baseline O
approaches O
using O
Discrete O
Only O
, O
Discrete O
+ O
Continuous O
and O
Continuous O
Only O
features O
respectively O
, O
whereas O
denotes O
our O
own O
approaches O
. O

We O
observe O
that O
in O
general O
our O
languageindependent O
( O
monolingual O
) O
Bi O
- O
LSTM B-MethodName
- O
Char O
- O
CRF O
and O
BERT B-MethodName
- O
CRF O
methods O
are O
on O
par O
with O
or O
outperform O
best O
attainable O
results O
. O

In O
particular O
, O
BERTCRF B-MethodName
trained O
monolingually O
using O
BERT(Base B-MethodName
) O
embeddings O
outperforms O
other O
baselines O
for O
both O
EN O
and O
ZH O
, O
with O
F1 B-MetricName
- O
scores B-MetricName
of O
79:2and75:3 O
on O
trigger O
identiﬁcation O
and O
classiﬁcation O
, O
respectively O
, O
amounting O
to O
a O
3:3%and1:6%gain O
for O
EN O
. O

On O
the O
other O
hand O
, O
although O
results O
using O
Bi O
- O
LSTM B-MethodName
- O
Char O
- O
CRF O
lag O
behind O
state O
- O
of O
- O
the O
- O
art O
for O
EN O
, O
incurring O
a O
loss O
of10:5%over O
trigger O
classiﬁcation O
, O
they O
are O
competitive O
for O
ZH O
, O
with O
scores B-MetricName
of O
86:6%and69:5%and O
gains O
of O
17:9%and6:2%over O
Feng O
’s O
HNN O
for O
trigger O
identiﬁcation O
and O
classiﬁcation O
respectively O
. O

4.2 O
Comparison O
between O
MUSE O
and O
BERT B-MethodName
Embeddings O
We O
observe O
a O
signiﬁcant O
difference O
in O
performance O
in O
favor O
of O
BERT B-MethodName
- O
CRF O
compared O
to O
BiLSTM B-MethodName
- O
Char O
- O
CRF O
with O
a O
gain O
of O
12.1 O
% O
, O
10.4 O
% O
, O
and O
13.9 O
% O
on O
the O
classiﬁcation O
task O
. O

The O
better O
performance O
of O
BERT B-MethodName
- O
CRF O
compared O
to O
BiLSTM B-MethodName
- O
Char O
- O
CRF O
can O
be O
attributed O
to O
the O
fact O
that O
BERT B-MethodName
is O
able O
to O
learn O
contextualized O
representation O
and O
long O
- O
range O
dependencies O
at O
different O
levels O
of O
granularity O
. O

Reconsidering O
the O
second O
example O
from O
the O
introduction O
, O
we O
notice O
that O
a O
Bi O
- O
LSTM B-MethodName
- O
Char O
- O
CRF O
fails O
to O
effectively O
associate O
it O
with O
position O
context O
clues O
. O

We O
notice O
that O
AR O
beneﬁts O
the O
most O
from O
multilingual O
training O
with O
an O
improvement O
of O
9.5 O
% O
and O
2.8 O
% O
on O
the O
classiﬁcation O
score B-MetricName
with O
BERT B-MethodName
- O
CRF O
and O
BiLSTM B-MethodName
- O
Char O
- O
CRF O
respectively O
. O

In O
particular O
, O
training O
with O
EN O
using O
BERT B-MethodName
- O
CRF O
was O
helpful O
for O
ZH O
with O
a O
performance O
not O
far O
from O
monolingual O
performance O
. O

The O
lower O
performance O
of O
EN O
! O
AR O
using O
BERT B-MethodName
- O
CRF O
raises O
questions O
about O
the O
quality O
of O
BERT(multi B-MethodName
) O
embedding O
model O
training O
for O
Arabic O
. O

Other O
approaches O
explore O
neural B-MethodName
networks I-MethodName
on O
top O
of O
linguistic O
features O
employing O
architectures O
like O
Dynamic O
Multi O
- O
Pooling O
CNNs O
( O
DMCNN O
) O
( O
Chen O
et O
al O
. O
, O
2015 O
) O
and O
bidirectional O
RNNs O
( O
JRNN O
) O
with O
manually O
crafted O
features O
( O
Nguyen O
et O
al O
. O
, O
2016 O
) O
. O

5.2 O
Cross O
- O
lingual O
Event O
Extraction O
Previous O
works O
for O
cross O
- O
lingual O
event O
extraction O
conducted O
in O
a O
semi O
- O
supervised O
way O
range O
from O
purely O
supervised O
approaches O
to O
those O
using O
ma O
- O
MUSE O
BERT B-MethodName
Davies O
is O
leaving O
to O
become O
chairman O
of O
the O
London O
School O
of O
EconomicsMovement O
: O
TransportPersonnel O
: O
End O
- O
Position O
The O
EU O
is O
set O
to O
release O
20 O
million O
euros O
( O
US O
21 O
) O
million O
in O
immediate O
humanitarian O
aid O
... O
Justice O
: O
Release O
- O
ParoleTransaction O
: O
Transfer O
- O
Money O
Palestinian O
uprising O
as O
Isreal O
removed O
all O
major O
checkpoints O
in O
the O
coastal O
territory O
. O
Conﬂict O
: O
DemonstrateConﬂict O
: O
Attack O
BERT(mono B-MethodName
) O
BERT(multi B-MethodName
all O
) O
... O
é O
 O
Jj O
. O
ÊË@ O
éJ O
Ê«I O
 O
Q O
 O
¯IJ O
kéÓ@Q O
 O
ªË@ O
 O
áÓ O
ÈA O
 O
JP@ O
ÕÎ O
 O
 O
ÕË O
” O
Arsenal O
has O
not O
been O
released O
from O
the O
ﬁne O
... O
” O
O O
Justice O
: O
Fine O
ÈA O
 O
 O
 O
úÍ@èPñJË@ O
ÈñjJK O
 O
à@ O
@Pñ O
 O
¯ O
ù O
 O
® O
J. O

K O
” O
The O
stone O
revolution O
must O
immediately O
turn O
into O
a O
ﬁght O
. O
”O O
Conﬂict O
: O
Attack O
由于月之海已经宣布年底前要解散，所以使得 O
... O
” O
Since O
’ O
the O
sea O
of O
the O
moon O
’ O
has O
been O
announced O
to O
be O
disbanded O
before O
the O
end O
of O
the O
year O
, O
... O
” O
B O
- O
Business O
: O
DeclareBankruptcyBusiness O
: O
End O
- O
Org O
Table O
3 O
: O
Examples O
of O
trigger O
extraction O
mislabeled O
by O
MUSE O
but O
correctly O
labeled O
by O
BERT B-MethodName
and O
those O
missed O
/ O
mislabeled O
with O
monolingual O
training O
only O
and O
corrected O
with O
multilingual O
BERT B-MethodName
model O
. O

More O
recently O
, O
BERT B-MethodName
was O
developed O
as O
an O
extension O
to O
the O
transformer O
architecture O
and O
achieved O
significant O
improvement O
in O
performance O
for O
many O
NLP O
tasks O
. O

We O
evaluate O
this O
approach O
using O
event O
trigger O
extraction O
architectures O
with O
type O
- O
based O
unsupervised O
embeddings O
( O
FastText O
and O
MUSE O
) O
and O
supervised O
embeddings O
tuned O
to O
the O
context O
( O
BERT B-MethodName
) O
. O

Although O
results O
using O
MUSE O
are O
lower O
for O
English O
, O
they O
are O
on O
par O
with O
Chinese O
baselines O
and O
better O
for O
Arabic O
compared O
to O
BERT.We B-MethodName
observe O
a O
generous O
boost O
in O
performance O
when O
English O
is O
added O
to O
the O
target O
language O
, O
and O
when O
all O
languages O
are O
combined O
together O
to O
train O
one O
cross O
- O
lingual O
model O
, O
especially O
for O
Arabic O
. O

We O
use O
a O
bi O
- O
directional O
Gated O
Recurrent O
Unit O
( O
GRU O
) O
cell O
( O
Cho O
et O
al O
. O
, O
2014 O
) O
which O
is O
a O
variation O
of O
Long O
Short O
- O
Term O
Memory O
( O
LSTM B-MethodName
) O
cell O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
that O
tries O
to O
solve O
the O
vanishing O
gradient O
problem O
. O

3.1 O
Neural O
Architectures O
for O
NER B-TaskName
With O
recent O
advances O
using O
deep O
neural B-MethodName
networks I-MethodName
, O
bi O
- O
directional O
long O
short O
- O
term O
memory O
networks O
with O
conditional O
random O
ﬁelds O
( O
BiLSTM B-MethodName
- O
CRF O
) O
have O
become O
standard O
for O
NER B-TaskName
( O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

A O
typical O
architecture O
consists O
of O
a O
BiLSTM B-MethodName
layer O
to O
learn O
feature O
representations O
from O
the O
input O
and O
a O
CRF O
layer O
to O
model O
the O
interdependencies O
between O
adjacent O
labels O
and O
perform O
joint O
inference O
. O

Ma O
and O
Hovy O
( O
2016 O
) O
introduce O
additional O
character O
- O
level O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNNs O
) O
to O
capture O
subword O
unit O
information O
. O

In O
this O
paper O
, O
we O
use O
a O
BiLSTM B-MethodName
- O
CRF O
with O
character O
- O
level O
modeling O
as O
our O
base O
model O
. O

We O
now O
brieﬂy O
review O
the O
BiLSTM B-MethodName
- O
CRF O
model O
. O

BiLSTMs B-MethodName
. O

Long O
Short O
Term O
Memory O
networks O
( O
LSTMs B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
are O
a O
variation O
of O
RNNs O
that O
are O
designed O
to O
avoid O
the O
vanishing O
/ O
exploding O
gradient O
problem O
( O
Bengio O
et O
al O
. O
, O
1994 O
) O
. O

Speciﬁcally O
, O
BiLSTMs B-MethodName
take O
as O
input O
a O
sequence O
of O
words O
x O
= O
fxkjk2Ng O
and O
output O
a O
sequence O
of O
hidden O
vectors O
: O
H O
= O
fhkjk2Ng O
BiLSTMs B-MethodName
combine O
a O
left O
- O
to O
- O
right O
( O
forward O
) O
and O
a O
right O
- O
to O
- O
left O
( O
backward O
) O
LSTM B-MethodName
to O
capture O
both O
left O
and O
right O
context O
. O

Formally O
, O
they O
produce O
a O
hidden O
vector O
~hi= O
[ O
  O
! O
hi O
; O
 hi]for O
each O
input O
, O
where  O
! O
hiand O
 hiare O
produced O
by O
the O
forward O
and O
the O
backward O
LSTMs B-MethodName
respectively O
; O
[ O
; O
] O
denotes O
vector O
concatenation O
. O

Following O
Wang O
et O
al O
. O
( O
2018 O
) O
, O
we O
use O
a O
BiLSTM B-MethodName
for O
character O
- O
level O
modeling O
. O

We O
concatenate O
the O
hidden O
vector O
of O
the O
space O
after O
a O
word O
from O
the O
forward O
LSTM B-MethodName
and O
the O
hidden O
vector O
of O
the O
space O
before O
a O
word O
from O
the O
backward O
LSTM B-MethodName
to O
form O
a O
character O
- O
level O
representation O
of O
the O
word O
: O
hc O
i= O
[ O
  O
! O
hc O
i O
; O
 hc O
i O
] O
. O

The O
wordlevel O
BiLSTM B-MethodName
then O
takes O
the O
concatenation O
of O
hc O
i O
and O
the O
word O
embedding O
as O
input O
xi= O
[ O
ei;hc O
i]to O
learn O
contextualized O
representations O
. O

The O
potential O
of O
a O
tag O
sequence O
can O
be O
computed O
as O
: O
St(y O
) O
= O
jyjY O
t=1Score O
( O
y[t];y[t 1 O
] O
) O
( O
3 O
) O
where O
y[t]is O
the O
tth O
element O
in O
y(y[ 1]is O
the O
start O
of O
the O
sequence O
) O
, O
and O
Score O
( O
y[t];y[t 1 O
] O
) O
= O
exp O
( O
tr(y[t];y[t 1])) O
exp O
( O
em(y[t O
] O
) O
) O
( O
4 O
) O
where O
tr(y[t];y[t 1])is O
the O
transition O
score B-MetricName
from O
y[t 1]toy[t O
] O
, O
andem(y[t])is O
the O
emission O
score B-MetricName
ofy[t]computed O
based O
on O
the O
output O
~htof O
the O
BiLSTM.3.2 B-MethodName
Learning O
from O
Imperfect O
Annotations O
Learning O
from O
multiple O
partially O
annotated O
datasets O
could O
be O
more O
generally O
thought O
of O
as O
learning O
from O
imperfect O
annotations O
. O

4 O
Model O
As O
mentioned O
above O
, O
we O
use O
a O
BiLSTM B-MethodName
- O
CRF O
with O
character O
- O
level O
modeling O
as O
our O
base O
model O
. O

The O
hidden B-HyperparameterName
sizes I-HyperparameterName
of O
the O
BiLSTMs B-MethodName
are O
tuned O
, O
and O
the O
best O
value O
we O
found O
is O
100 B-HyperparameterValue
for O
the O
character O
- O
level O
BiLSTM B-MethodName
, O
and O
300 O
for O
the O
word O
- O
level O
BiLSTM B-MethodName
. O

Second O
, O
we O
always O
favor O
the O
predictions O
of O
named O
entities O
over O
the O
predictions O
of O
non O
- O
entity.5 O
– O
Under O
the O
limited O
- O
supervision O
setting O
, O
for O
each O
global O
corpus O
, O
we O
add O
a O
new O
CRF O
and O
train O
it O
along O
with O
the O
LSTMs B-MethodName
. O

Additionally O
, O
MTMs O
are O
worse O
than O
all O
the O
uniﬁed O
models O
, O
because O
they O
only O
share O
the O
LSTM B-MethodName
layers O
, O
but O
lose O
all O
the O
knowledge O
in O
the O
CRFs O
when O
adapted O
to O
new O
corpora O
. O

2 O
Stereotypically O
gendered O
nouns O
used O
in O
referential O
experiments O
male O
female O
man O
woman O
boy O
girl O
father O
mother O
uncle O
aunt O
husband O
wife O
actor O
actress O
prince O
princess O
waiter O
waitress O
lord O
lady O
king O
queen O
son O
daughter O
nephew O
niece O
brother O
sister O
grandfather O
grandmother O
3 O
Referential O
Behavioral O
Results O
Statistical O
models O
with O
a O
categorical O
IC O
variable O
are O
given O
for O
LSTM B-MethodName
LMs O
in O
Table O
1 O
, O
for O
TransformerXL O
in O
Table O
3 O
, O
and O
for O
GPT-2 B-MethodName
XL O
in O
Table O
5 O
. O

Models O
with O
the O
continuous O
IC O
bias O
measure O
from O
Ferstl O
et O
al O
. O
( O
2011 O
) O
are O
given O
for O
LSTM B-MethodName
LMs O
are O
in O
Table O
2 O
, O
for O
TransformerXL O
in O
Table O
4 O
, O
and O
for O
GPT-2 B-MethodName
XL O
in O
Table O
6 O
. O

4 O
Referential O
Representational O
Results O
Statistical O
models O
with O
a O
categorical O
IC O
variable O
are O
given O
for O
LSTM B-MethodName
LMs O
in O
Table O
7 O
, O
for O
TransformerXL O
in O
Table O
9 O
, O
and O
for O
GPT-2 B-MethodName
XL O
in O
Table O
11 O
. O

are O
in O
Table O
8 O
, O
for O
TransformerXL O
in O
Table O
10 O
, O
and O
for O
GPT-2 B-MethodName
XL O
in O
Table O
12 O
. O

The O
full O
layer O
- O
wise O
results O
are O
given O
for O
GPT-2 B-MethodName
XL O
in O
Figure O
2 O
. O

Statistical O
models O
for O
the O
sentence O
completion O
experiments O
from O
( O
Rohde O
et O
al O
. O
, O
2011 O
) O
are O
given O
for O
LSTM B-MethodName
LMs O
are O
given O
in O
Table O
13 O
, O
for O
TransformerXL O
in O
Table O
14 O
, O
and O
for O
GPT-2 B-MethodName
XL O
in O
Table O
15 O
. O

Greater O
similarity O
corresponds O
to O
greater O
relationship O
between O
pronoun O
and O
antecedent.6 O
Syntactic O
Representational O
Results O
Statistical O
models O
ﬁtting O
the O
similarity O
between O
whoand O
the O
possible O
attachment O
positions O
are O
given O
for O
LSTM B-MethodName
LMs O
are O
given O
in O
Table O
19 O
, O
for O
TransformerXL O
in O
Table O
20 O
, O
and O
for O
GPT-2 B-MethodName
XL O
in O
Table O
21 O
. O

Statistical O
models O
ﬁtting O
the O
similarity O
between O
was O
/ O
were O
and O
the O
possible O
attachment O
positions O
are O
given O
for O
LSTM B-MethodName
LMs O
are O
given O
in O
Table O
22 O
, O
for O
TransformerXL O
in O
Table O
23 O
, O
and O
for O
GPT-2 B-MethodName
XL O
in O
Table O
24 O
. O

Additionally O
, O
the O
full O
layer O
- O
wise O
results O
of O
GPT-2 B-MethodName
XL O
comparing O
who O
to O
attachment O
positions O
are O
given O
in O
Figure O
3 O
and O
comparing O
the O
RC O
verb O
to O
possible O
attachment O
positions O
are O
given O
in O
Figure O
4 O
. O

This O
is O
complemented O
with O
an O
attentionbased O
LSTM B-MethodName
model O
( O
Bahdanau O
et O
al O
. O
, O
2014 O
) O
for O
transducing O
non O
- O
segmental O
multiword O
tokens O
. O

The O
tagger O
uses O
a O
Meta O
- O
BiLSTM B-MethodName
over O
the O
output O
of O
a O
sentence O
- O
based O
character O
model O
and O
a O
word O
model O
. O

The O
character O
BiLSTMs B-MethodName
use O
the O
full O
context O
of O
the O
sentence O
in O
contrast O
to O
most O
other O
taggers O
which O
use O
words O
only O
as O
context O
for O
the O
character O
model O
. O

This O
character O
model O
is O
combined O
with O
the O
word O
model O
in O
the O
Meta O
- O
BiLSTM B-MethodName
relatively O
late O
, O
after O
two O
layers O
of O
BiLSTMs B-MethodName
. O

For O
both O
the O
word O
and O
character O
models O
, O
we O
use O
two O
layers O
of O
BiLSTMs B-MethodName
with O
300 O
LSTM B-MethodName
cells O
per O
layer O
. O

5 O
Dependency O
Parsing O
We O
use O
a O
greedy O
transition O
- O
based O
parser O
( O
Nivre O
, O
2008 O
) O
based O
on O
the O
framework O
of O
Kiperwasser O
and O
Goldberg O
( O
2016b O
) O
where O
BiLSTMs B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Graves O
, O
2008 O
) O
learn O
representations O
of O
tokens O
in O
context O
, O
and O
are O
trained O
together O
with O
a O
multi O
- O
layer O
perceptron O
that O
predicts O
transitions O
and O
arc O
labels O
based O
on O
a O
few O
BiLSTM B-MethodName
vectors O
. O

In O
our O
parser O
, O
the O
vector O
representation O
xiof O
a O
word O
typewibefore O
it O
is O
passed O
to O
the O
BiLSTM B-MethodName
feature O
extractors O
is O
given O
by O
: O
xi O
= O
e(wi) O
◦ O
e(pi) O
◦ O
BiLSTM B-MethodName
( O
ch1 O
: O
m O
) O
. O

Here O
, O
e(wi)represents O
the O
word O
embedding O
and O
e(pi)the O
POS B-TaskName
tag O
embedding O
( O
Chen O
and O
Manning O
, O
2014 O
) O
; O
these O
are O
concatenated O
to O
a O
character O
- O
based O
vector O
, O
obtained O
by O
running O
a O
BiLSTM B-MethodName
over O
the O
charactersch1 O
: O
mofwi O
. O

With O
the O
aim O
of O
training O
multi O
- O
treebank O
models O
, O
we O
additionally O
created O
a O
variant O
of O
the O
parser O
which O
adds O
a O
treebank O
embedding O
e(tbi)to O
input O
vectors O
in O
a O
spirit O
similar O
to O
the O
language O
embeddings O
of O
Ammar O
et O
al O
. O
( O
2016 O
) O
and O
de O
Lhoneux O
et O
al O
. O
( O
2017a O
): O
xi O
= O
e(wi) O
◦ O
e(pi) O
◦ O
BiLSTM B-MethodName
( O
ch1 O
: O
m) O
◦ O
e(tbi O
) O
. O

We O
use O
the O
extended O
feature O
set O
of O
Kiperwasser O
and O
Goldberg O
( O
2016b O
) O
( O
top O
3 O
items O
on O
the O
stack O
together O
with O
their O
rightmost O
and O
leftmost O
depen9https://radimrehurek.com/gensim/ O
10An O
alternative O
strategy O
is O
to O
have O
the O
parser O
store O
embeddings O
for O
all O
words O
that O
appear O
in O
either O
the O
training O
data O
or O
pre O
- O
trained O
embeddings O
, O
but O
this O
uses O
far O
more O
memory O
. O
Character O
embedding O
dimension O
500 O
Character O
BiLSTM B-MethodName
layers O
1 O
Character O
BiLSTM B-MethodName
output O
dimension O
200 O
Word O
embedding O
dimension O
100 O
POS B-TaskName
embedding O
dimension O
20 O
Treebank O
embedding O
dimension O
12 O
Word O
BiLSTM B-MethodName
layers O
2 O
Word O
BiLSTM B-MethodName
hidden O
/ O
output O
dimension O
250 O
Hidden O
units O
in O
MLP O
100 O
Word O
dropout B-HyperparameterName
0.33 B-HyperparameterValue
α(for O
OOV O
vector O
training O
) O
0.25 O
Character O
dropout B-HyperparameterName
0.33 O
pagg(for O
exploration O
training O
) O
0.1 O
Table O
1 O
: O
Hyper O
- O
parameter O
values O
for O
parsing O
. O

That O
embedding O
was O
concatenated O
to O
the O
word O
embedding O
at O
the O
input O
of O
the O
BiLSTM B-MethodName
. O

The O
BiLSTM B-MethodName
output O
dimension O
of O
the O
character O
embedding O
layer O
was O
100 O
and O
the O
embedding O
dimension O
of O
the O
word O
and O
POS B-TaskName
embeddings O
were O
also O
100 O
. O

Both O
parser O
implementations O
use O
a O
BiLSTM B-MethodName
to O
create O
the O
character O
vector O
input O
to O
the O
network O
, O
so O
this O
seems O
more O
likely O
to O
be O
a O
result O
of O
the O
transition O
- O
based O
decoder O
leveraging O
features O
more O
than O
the O
graphbased O
one O
. O

It O
is O
based O
on O
the O
standard O
attention O
- O
based O
LSTM B-MethodName
encoder O
- O
decoder O
model O
, O
but O
makes O
use O
of O
multiple O
encoders O
to O
process O
all O
parts O
of O
the O
context O
as O
well O
as O
the O
lemma O
. O

3 O
Model O
Description O
Our O
model O
is O
based O
on O
the O
standard O
LSTM B-MethodName
encoder O
- O
decoder O
model O
with O
an O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O

Using O
bidirectional O
encoders O
, O
the O
ﬁnal O
hidden O
states O
produced O
by O
each O
encoder O
are O
concatena O
- O
tions O
of O
the O
respective O
forward O
and O
backward O
hidden O
states O
: O
hi= O
[ O
− O
→hi,← O
−hi O
] O
( O
2 O
) O
with O
− O
→hi O
= O
LSTM B-MethodName
( O
emb O
1, O
... O
,emb O
i),and O
( O
3 O
) O
← O
−hi O
= O
LSTM B-MethodName
( O
emb O
z, O
... O
,emb O
i O
) O
( O
4 O
) O
emb O
= O
emb O
1, O
... O
,emb O
zrepresents O
the O
respective O
sequence O
of O
embeddings O
, O
i.e. O
, O
either O
the O
embeddings O
of O
the O
lemma O
’s O
characters O
or O
the O
embeddings O
of O
the O
subword O
units O
of O
either O
context O
. O

The O
input O
to O
the O
decoder O
LSTM B-MethodName
at O
each O
timestep O
is O
the O
concatenation O
of O
all O
contexts O
and O
the O
embedding O
of O
the O
last O
output O
character O
. O

4.2 O
Baseline O
System O
The O
ofﬁcial O
baseline O
system O
of O
the O
shared O
task O
is O
a O
character O
- O
level O
LSTM B-MethodName
encoder O
- O
decoder O
model O
with O
attention O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
. O

Given O
the O
character O
embedding O
- O
context O
representations O
produced O
by O
the O
encoder O
, O
the O
LSTM B-MethodName
decoder O
generates O
the O
character O
sequence O
of O
the O
output O
inﬂected O
form O
, O
using O
an O
attention O
mechanism O
. O

In O
our O
model O
, O
we O
adopt O
a O
bidirectional O
RNN O
with O
LSTM B-MethodName
units O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

At O
time O
stept O
, O
we O
compute O
the O
hidden O
representations O
of O
an O
output O
prefix O
ytdenoted O
s1 O
tands2 O
tbased O
on O
an O
embedding O
of O
ytdenoted O
M[yt O
] O
, O
previous O
representations O
s1 O
t 1,s2 O
t 1 O
, O
and O
a O
context O
vector O
ctas O
, O
s1 O
t= O
LSTM B-MethodName
( O
s1 O
t 1;M[yt];ct);(3 O
) O
s2 O
t= O
LSTM B-MethodName
( O
s2 O
t 1;s1 O
t;ct O
) O
; O
( O
4 O
) O
P(yt+1jyt;x O
) O
= O
softmax O
( O
Ws2 O
t+W0ct);(5)where O
Mis O
the O
embedding O
table O
, O
and O
WandW0 O
are O
learnable O
parameters O
. O

LSTM B-MethodName
Logits O
LSTM B-MethodName
+ O
+ O
+ O
LSTM B-MethodName
Logits O
LSTM B-MethodName
+ O
+ O
+ O
Figure O
1 O
: O
An O
illustration O
of O
a O
two O
- O
layer O
LSTM B-MethodName
decoder O
with O
different O
ways O
of O
injecting O
the O
conditioning O
signal O
. O

When O
we O
incorporate O
the O
conditioning O
on O
zinto O
the O
LSTM B-MethodName
layers O
, O
each O
lookup O
table O
( O
e.g. O
, O
M1 O
andM2 O
) O
hasKrows O
andDLSTM B-MethodName
columns O
, O
where O
DLSTM B-MethodName
denotes O
the O
number O
of O
dimensions O
of O
the O
LSTM B-MethodName
states O
( O
512 O
in O
our O
case O
) O
. O

We O
combine O
the O
state O
of O
the O
LSTM B-MethodName
with O
the O
conditioning O
signal O
via O
simple O
addition O
. O

Then O
the O
LSTM B-MethodName
update O
equations O
take O
the O
form O
, O
si O
t= O
LSTM B-MethodName
( O
si O
t 1 O
+ O
Mi[z];input);(12 O
) O
fori2f1;2 O
g. O

We O
refer O
to O
the O
addition O
of O
the O
conditioning O
signal O
to O
the O
bottom O
and O
top O
LSTM B-MethodName
layers O
of O
the O
decoder O
as O
btandtprespectively O
. O

All O
of O
the O
models O
use O
a O
one O
- O
layer O
bidirectional O
LSTM B-MethodName
encoder O
and O
a O
twolayer O
LSTM B-MethodName
decoder O
. O

Each O
LSTM B-MethodName
layer O
in O
the O
encoder O
and O
decoder O
has O
a O
512dimensional O
hidden O
state O
. O

As O
seen O
in O
Section O
3 O
, O
the O
mixture O
components O
can O
be O
built O
by O
adding O
component O
- O
specific O
vectors O
to O
the O
logits O
( O
sf O
) O
, O
the O
top O
LSTM B-MethodName
layer O
( O
tp O
) O
or O
the O
bottom O
LSTM B-MethodName
layer O
( O
bt O
) O
in O
the O
decoder O
, O
or O
all O
of O
them O
( O
all O
) O
. O

For O
disambiguation O
, O
the O
parser O
associates O
words O
with O
BiLSTM B-MethodName
vectors O
and O
utilizes O
these O
vectors O
to O
assign O
scores B-MetricName
to O
candidate O
dependencies O
. O

For O
disambiguation O
, O
while O
early O
work O
on O
dependency O
parsing O
focused O
on O
global O
linear O
models O
, O
e.g. O
, O
structured O
perceptron O
( O
Collins O
, O
2002 O
) O
, O
recent O
work O
shows O
that O
deep O
learning O
techniques O
, O
e.g. O
, O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
, O
is O
able O
to O
significantly O
advance O
the O
state O
- O
of O
- O
the O
- O
art O
of O
the O
parsing O
accuracy B-MetricName
. O

Based O
on O
an O
efﬁcient O
ﬁrst O
- O
order O
Maximum O
Subgraph O
decoder O
, O
we O
implement O
a O
data O
- O
driven O
parser O
that O
scores B-MetricName
arcs O
based O
on O
stacked O
bidirectionalLSTM B-MethodName
( O
BiLSTM B-MethodName
) O
together O
with O
a O
multi O
- O
layer O
perceptron O
. O

3.3 O
Disambiguation O
with O
an O
LSTM B-MethodName
3.3.1 O
The O
Architecture O
A O
semantic O
graph O
mainly O
consists O
of O
two O
parts O
: O
the O
structural O
part O
and O
the O
label O
part O
. O

Following O
Kiperwasser O
and O
Goldberg O
( O
2016 O
) O
’s O
successful O
experience O
on O
syntactic O
tree O
parsing O
and O
Peng O
et O
al O
. O
( O
2017 O
) O
’s O
experience O
on O
semantic O
graph O
parsing O
, O
we O
employ O
a O
stacked O
bidirectional O
- O
LSTM B-MethodName
( O
BiLSTM B-MethodName
) O
based O
model O
to O
assign O
scores B-MetricName
. O

In O
our O
system O
, O
the O
BiLSTM B-MethodName
vectors O
associated O
with O
the O
input O
words O
are O
utilized O
to O
calculate O
scores B-MetricName
for O
the O
... O
LSTM B-MethodName
LSTM B-MethodName
LSTM B-MethodName
... O
Figure O
4 O
: O
The O
architecture O
of O
the O
network O
when O
processing O
He O
wants O
to O
go O
. O

The O
word O
- O
based O
embedding O
module O
applies O
the O
common O
lookup O
- O
table O
mechanism O
, O
while O
the O
character O
- O
based O
word O
embedding O
wiis O
implemented O
by O
extracting O
the O
features O
( O
denoted O
as O
c1;c2 O
; O
: O
: O
: O
; O
cn O
) O
within O
a O
character O
- O
based O
BiLSTM B-MethodName
: O
x1 O
: O
xn= O
BiLSTM B-MethodName
( O
c1 O
: O
cn O
) O
wi O
= O
x1+xn O
3.3.3 O
Lexical O
Feature O
Extractor O
The O
concatenation O
of O
word O
embedding O
wiand O
POS B-TaskName
- O
tag O
embedding O
piof O
each O
word O
in O
speciﬁc O
sentence O
is O
used O
as O
the O
input O
of O
BiLSTMs B-MethodName
to O
extract O
context O
- O
related O
feature O
vectors O
rifor O
each O
position O
i. O
ai O
= O
wipi O
r1 O
: O
rn= O
BiLSTM B-MethodName
( O
a1 O
: O
an)3.3.4 O
Factorized O
Scoring O
In O
our O
ﬁrst O
order O
model O
, O
the O
S O
CORE O
function O
evaluates O
the O
preference O
of O
a O
semantic O
dependency O
graph O
by O
considering O
every O
bilexical O
relation O
in O
this O
graph O
one O
by O
one O
. O

Hyper O
- O
parameter O
Val O
Randomly O
- O
initialized O
word O
embedding O
dimension O
100 B-HyperparameterValue
Pre O
- O
trained O
word O
embedding O
dimension O
100 O
Randomly O
- O
initialized O
character O
embedding O
dimension O
100 O
Character O
LSTM B-MethodName
layers O
for O
each O
direction O
2 O
Randomly O
- O
initialized O
POS B-TaskName
- O
Tag O
embedding O
dimension O
50 O
POS B-TaskName
- O
Tag O
dropout B-HyperparameterName
0.5 B-HyperparameterValue
Batch O
size O
32 O
BiLSTM B-MethodName
dimension O
for O
each O
direction O
150 O
BiLSTM B-MethodName
layers O
5 O
MLP O
hidden B-HyperparameterName
layers I-HyperparameterName
1 B-HyperparameterValue
MLP O
hidden B-HyperparameterName
layer I-HyperparameterName
dimension O
100 O
Table O
2 O
: O
Hyper O
- O
parameter O
setting O
of O
our O
model O
. O

Peng O
et O
al O
. O
( O
2017 O
) O
’s O
and O
Wang O
et O
al O
. O
( O
2018 O
) O
’s O
parsers O
utilize O
neural O
models O
, O
LSTMs B-MethodName
in O
particular O
, O
to O
score B-MetricName
either O
arcs O
or O
transitions O
. O

Comparing O
our O
results O
to O
the O
results O
obtained O
by O
parsers O
based O
on O
linear O
models O
, O
we O
can O
see O
the O
effectiveness O
of O
the O
BiLSTM B-MethodName
based O
disambiguation O
model O
. O

In O
this O
paper O
, O
we O
consider O
two O
POS B-TaskName
taggers O
: O
a O
symbol O
- O
reﬁned O
generative O
HMM O
tagger O
( O
SR O
- O
HMM O
) O
( O
Huang O
et O
al O
. O
, O
2009 O
) O
and O
a O
BiLSTMCRF B-MethodName
model O
when O
assisting O
Chinese O
SDG O
. O

For O
the O
neural O
tagging O
model O
, O
in O
addition O
to O
a O
BiLSTM B-MethodName
layer O
for O
encoding O
words O
, O
we O
set O
a O
BiLSTM B-MethodName
layer O
for O
encoding O
characters O
, O
which O
supports O
us O
to O
derive O
character O
- O
level O
representations O
for O
all O
words O
. O

In O
particular O
, O
vectors O
from O
the O
characterlevel O
LSTM B-MethodName
is O
concatenated O
with O
the O
pre O
- O
trained O
word O
embedding O
before O
feeding O
into O
the O
other O
word O
- O
level O
BiLSTM B-MethodName
network O
to O
capture O
contextual O
information O
. O

The O
LSTM B-MethodName
- O
based O
tagger O
can O
leverageModel O
LP O
LR O
LF O
Peking O
84.75 O
82.15 O
83.43 O
Ours O
85.49 O
84.11 O
84.79 O
Table O
4 O
: O
Labeled O
F1on B-MetricName
the O
test O
set O
of O
SemEval O
2015 O
for O
Chinese O
. O

Model O
POS B-TaskName
LP O
LR O
LF O
ZDSW O
Gold O
82.09 O
81.81 O
81.95 O
Ours O
Gold O
86.37 O
86.00 O
86.19 O
SR O
- O
HMM O
80.19 O
80.53 O
80.37 O
BiLSTM B-MethodName
- O
CRF O
81.13 O
81.74 O
81.43 O
Table O
5 O
: O
Labeled O
F1on B-MetricName
the O
test O
set O
of O
Chinese O
CCGBank O
. O

We O
introduce O
a O
new O
parser O
for O
semantic O
dependency O
analysis O
, O
which O
combines O
two O
promising O
parsing O
techniques O
, O
i.e. O
, O
decoding O
based O
on O
Maximum O
Subgraph O
algorithms O
and O
disambiguation O
based O
on O
BiLSTMs B-MethodName
. O

3)We O
experiment O
with O
a O
variety O
of O
subword O
- O
informed O
representation O
architectures O
, O
where O
the O
focus O
is O
on O
unsupervised O
, O
widely O
portable O
language O
- O
agnostic O
methods O
such O
as O
the O
ones O
based O
on O
character O
n O
- O
grams O
( O
Luong O
and O
Manning O
, O
2016 O
; O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
, O
Byte O
Pair O
Encodings O
( O
BPE O
) O
( O
Sennrich O
et O
al O
. O
, O
2016 O
; O
Heinzerling O
and O
Strube O
, O
2018 O
) O
, O
Morfessor O
( O
Smit O
et O
al O
. O
, O
2014 O
) O
, O
or O
BERT B-MethodName
- O
style O
pretraining O
and O
ﬁne O
- O
tuning O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
which O
relies O
on O
WordPieces O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
. O

For O
instance O
, O
we O
show O
that O
ﬁne O
- O
tuning O
pretrained O
multilingual O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Wu O
and O
Dredze O
, O
2019 O
) O
is O
a O
viable O
strategy O
for O
“ O
double O
” O
low O
- O
resource O
settings O
in O
the O
NER B-TaskName
and O
MTAG O
tasks O
, O
but O
it O
fails O
for O
the O
FGET O
task O
in O
the O
same O
setting O
; O
furthermore O
, O
its O
performance O
can O
be O
matched O
or O
surpassed O
by O
other O
subwordinformed O
methods O
in O
NER B-TaskName
and O
MTAG O
as O
soon O
as O
they O
obtain O
sufﬁcient O
embedding O
training O
data O
. O

We O
select O
16 O
languages O
in O
total O
spanning O
4 O
broad O
2For O
instance O
, O
as O
of O
April O
2019 O
, O
Wikipedia O
is O
available O
only O
in O
304 O
out O
of O
the O
estimated O
7,000 O
existing O
languages O
. O
Agglutinative O
Fusional O
Introﬂexive O
Isolating O
BM O
BXR O
MYV O
TE O
TR O
ZU O
EN O
FO O
GA O
GOT O
MT O
RUE O
AM O
HE O
YO O
ZH O
EMB B-MetricName
40 O
K O
372 O
K O
207 O
K O
5 O
M O
5 O
M O
69 O
K O
5 O
M O
1.6 O
M O
4.4 O
M O
18 O
K O
1.5 O
M O
282 O
K O
659 O
K O
5 O
M O
542 O
K O
5 O
M O
FGET O
29 O
K O
760 O
740 O
13 O
K O
60 O
K O
36 O
K O
60 O
K O
30 O
K O
56 O
K O
289 O
2.7 O
K O
1.5 O
K O
2.2 O
K O
60 O
K O
15 O
K O
60 O
K O
NER B-TaskName
345 O
2.4 O
K O
2.1 O
K O
9.9 O
K O
167 O
K O
425 O
8.9 O
M O
4.0 O
K O
7.6 O
K O
475 O
1.9 O
K O
1.6 O
K O
1.0 O
K O
107 O
K O
3.4 O
K O
– O
MTAG O
– O
– O
– O
1.1 O
K O
3.7 O
K O
– O
24 O
K O
– O
– O
3.4 O
K O
1.1 O
K O
– O
– O
5.2 O
K O
– O
4.0 O
K O
BERT B-MethodName
X O
X O
X O
X O
X O
X O
X O
Table O
1 O
: O
Overview O
of O
test O
languages O
and O
data O
availability O
. O

The O
BERT B-MethodName
row O
shows O
the O
languages O
supported O
by O
multilingual O
BERT B-MethodName
. O

We O
also O
compare O
with O
pretrained O
multilingualBERT B-MethodName
base(Devlin O
et O
al O
. O
, O
2019 O
) O
on O
the O
languages O
supported O
by O
this O
model O
. O

For O
each O
entity O
token O
, O
we O
ﬁrst O
use O
our O
subword O
- O
informed O
model O
to O
obtain O
word O
representations O
, O
and O
then O
feed O
the O
token O
embedding O
sequence O
into O
a O
bidirectional O
LSTM B-MethodName
with O
2 O
hidden B-HyperparameterName
layers I-HyperparameterName
of O
size O
512 B-HyperparameterValue
, O
followed O
by O
a O
projection O
layer O
which O
predicts O
the O
entity O
type O
. O

With O
BERT B-MethodName
, O
we O
input O
the O
entire O
entity O
mention O
and O
then O
use O
the O
representation O
of O
the O
special O
[ O
CLS O
] O
token O
for O
classiﬁcation O
. O

As O
suggested O
by O
Wu O
and O
Dredze O
( O
2019 O
) O
, O
BERT B-MethodName
hyper O
- O
parameters O
are O
more O
4We O
train O
fastText O
and O
skip O
- O
gram O
from O
word2vec O
with O
the O
same O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
that O
is O
used O
to O
train O
our O
subword O
- O
informed O
models O
on O
the O
corresponding O
data O
points.sensitive O
to O
smaller O
data O
sizes O
, O
so O
we O
tune O
them O
on O
the O
smallest O
data O
point O
with O
200training O
instances O
. O

As O
sequence O
labeling O
model O
, O
we O
train O
a O
bidirectional O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
; O
Plank O
et O
al O
. O
, O
2016 O
) O
, O
with O
two O
layers O
of O
size O
1024 O
and O
dropout B-HyperparameterName
0.4 B-HyperparameterValue
, O
using O
early O
stopping O
on O
the O
development O
set O
. O

For O
experiments O
involving O
multilingual O
BERT B-MethodName
, O
we O
ﬁne O
- O
tune O
all O
of O
BERT B-MethodName
’s O
layers O
and O
feed O
the O
ﬁnal O
layer O
into O
an O
LSTM B-MethodName
before O
classiﬁcation O
. O

Performance O
of O
BERT B-MethodName
. O

An O
interesting O
analysis O
regarding O
the O
( O
relative O
) O
performance O
of O
pretrained O
multilingual O
BERT B-MethodName
model O
emerges O
from O
Figures O
13 O
. O

A O
pretrained O
BERT B-MethodName
model O
encodes O
a O
massive O
amount O
of O
background O
knowledge O
available O
during O
its O
( O
multilingual O
) O
pretraining O
. O

However O
, O
as O
we O
supplement O
other O
subword O
- O
informed O
representation O
learning O
methods O
with O
more O
data O
for O
training O
the O
embeddings O
, O
the O
gap O
gets O
smaller O
until O
it O
almost O
completely O
vanishes O
: O
other O
methods O
now O
also O
get O
to O
see O
some O
of O
the O
distributional O
information O
which O
BERT B-MethodName
already O
consumed O
in O
its O
pretraining O
. O

BERT B-MethodName
performance O
on O
FGET O
versus O
MTAG O
and O
NER B-TaskName
concerns O
the O
very O
nature O
of O
the O
tasks O
at O
hand O
. O

Since O
BERT B-MethodName
has O
been O
pretrained O
on O
sentences O
, O
this O
setting O
is O
a O
natural O
ﬁt O
and O
makes O
ﬁne O
- O
tuning O
to O
these O
tasks O
easier O
: O
BERT B-MethodName
already O
provides O
a O
sort O
of O
“ O
contextual O
subword O
composition O
function O
” O
. O

This O
stands O
in O
contrast O
with O
the O
other O
subword O
- O
informed O
approaches O
. O
There O
, O
we O
might O
have O
good O
non O
- O
contextual O
subword O
embeddings O
and O
a O
pretrained O
“ O
non O
- O
contextual O
” O
composition O
function O
, O
but O
we O
have O
to O
learn O
how O
to O
effectively O
leverage O
the O
context O
for O
the O
task O
at O
hand O
( O
i.e. O
, O
by O
running O
an O
LSTM B-MethodName
over O
the O
subwordinformed O
token O
representations O
) O
from O
scratch.5 O
Truly O
Low O
- O
Resource O
Languages O
. O

The O
5Another O
factor O
at O
play O
is O
multilingual O
BERT B-MethodName
’s O
limited O
vocabulary O
size O
( O
100 O
K O
WordPiece O
symbols O
) O
, O
leaving O
on O
average B-MetricName
a O
bit O
under O
1 B-MetricValue
K O
symbols O
per O
language O
. O

Due O
to O
the O
different O
sizes O
of O
Wikipedias O
used O
for O
pretraining O
BERT B-MethodName
, O
some O
languages O
might O
even O
be O
represented O
with O
far O
fewer O
than O
1 O
K O
vocabulary O
entries O
, O
thereby O
limiting O
the O
effective O
language O
- O
speciﬁc O
model O
capacity O
. O

Therefore O
, O
it O
is O
not O
that O
surprising O
that O
monolingual O
subword O
- O
informed O
representations O
gradually O
surpass O
BERT B-MethodName
as O
more O
language O
- O
speciﬁc O
WE O
data O
becomes O
available O
. O

The O
results O
are O
obtained O
by O
training O
on O
the O
full O
WE O
data O
( O
except O
for O
BERT B-MethodName
) O
and O
the O
full O
task O
data O
of O
the O
corresponding O
languages O
. O

It O
is O
a O
transition O
- O
based O
parser O
with O
a O
BiLSTM B-MethodName
encoder O
, O
augmented O
with O
BERT B-MethodName
contextualized O
embeddings O
. O

Input O
feature O
embeddings O
are O
concatenate O
with O
BERT B-MethodName
embeddings O
for O
each O
token O
. O

Vector O
representations O
for O
the O
input O
tokens O
are O
then O
computed O
by O
two O
layers O
of O
shared O
and O
framework O
- O
speciﬁc O
bidirectional O
LSTMs B-MethodName
. O

3.2 O
Transition O
Classiﬁer O
To O
predict O
the O
next O
transition O
at O
each O
step O
, O
TUPA O
uses O
a O
BiLSTM B-MethodName
module O
followed O
by O
an O
MLP O
and O
a O
softmax O
layer O
for O
classiﬁcation O
( O
Kiperwasser O
and O
Goldberg O
, O
2016 O
) O
. O

The O
BiLSTM B-MethodName
module O
( O
illustrated O
in O
more O
detail O
in O
Figure O
4 O
) O
is O
applied O
before O
the O
transition O
se4While O
UCCA O
contains O
unanchored O
( O
implicit O
) O
nodes O
corresponding O
to O
non O
- O
instantiated O
arguments O
or O
predicates O
, O
the O
original O
TUPA O
disregards O
them O
as O
they O
are O
not O
included O
in O
standard O
UCCA O
evaluation O
. O

It O
consists O
of O
a O
pre O
- O
BiLSTM B-MethodName
MLP O
with O
feature O
embeddings O
( O
§ O
3.3 O
) O
and O
pre O
- O
trained O
contextualized O
embeddings O
( O
§ O
3.4 O
) O
concatenated O
as O
inputs O
, O
followed O
by O
( O
multiple O
layers O
of O
) O
a O
bidirectional O
recurrent O
neural O
network O
( O
Schuster O
and O
Paliwal,1997 O
; O
Graves O
, O
2008 O
) O
with O
a O
long O
short O
- O
term O
memory O
cell O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

3.4 O
Pre O
- O
trained O
Contextualized O
Embeddings O
Contextualized O
representation O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
have O
recently O
achieved O
state O
- O
of O
- O
the O
- O
art O
results O
on O
a O
diverse O
array O
of O
downstream B-TaskName
NLP O
tasks O
, O
gaining O
improved O
results O
compared O
to O
non O
- O
contextual O
representations O
. O

We O
usethe O
weighted B-HyperparameterName
sum O
of O
last O
four O
hidden B-HyperparameterName
layers I-HyperparameterName
of O
a O
BERT B-MethodName
pre O
- O
trained O
model O
as O
extra O
input O
features.5 O
BERT B-MethodName
uses O
a O
wordpiece O
tokenizer O
( O
Wu O
et O
al O
. O
, O
2016 B-HyperparameterValue
) O
, O
which O
segments O
all O
text O
into O
sub O
- O
word O
units O
, O
while O
TUPA O
uses O
the O
UDPipe O
tokenization O
. O

To O
maintain O
alignment O
between O
wordpieces O
and O
tokens O
, O
we O
use O
a O
summation O
of O
the O
outputs O
of O
BERT B-MethodName
vectors O
corresponding O
to O
the O
wordpieces O
of O
each O
token O
as O
its O
representation O
. O

4 O
Multi O
- O
Task O
Learning O
Whereas O
in O
the O
single O
- O
task O
setting O
TUPA O
is O
trained O
separately O
on O
each O
framework O
as O
described O
above O
, O
in O
the O
multi O
- O
task O
setting O
, O
all O
frameworks O
share O
a O
BiLSTM B-MethodName
for O
encoding O
the O
input O
. O

In O
addition O
, O
each O
framework O
has O
a O
framework O
- O
speciﬁc O
BiLSTM B-MethodName
, O
private O
to O
it O
. O

Each O
framework O
has O
its O
own O
MLP O
on O
top O
of O
the O
concatenation O
of O
the O
shared O
and O
framework O
- O
speciﬁc O
BiLSTM B-MethodName
( O
see O
Figure O
3 O
) O
. O

50 O
Shared O
BiLSTM B-MethodName
layers O
2 O
Shared O
BiLSTM B-MethodName
dim O
. O

500 O
Shared O
pre O
- O
BiLSTM B-MethodName
MLP O
layers O
1 O
Shared O
pre O
- O
BiLSTM B-MethodName
MLP O
dim O
. O

300 O
Private O
BiLSTM B-MethodName
layers O
2 O
Private O
BiLSTM B-MethodName
dim O
. O

500 O
Private O
pre O
- O
BiLSTM B-MethodName
MLP O
layers O
1 O
Private O
pre O
- O
BiLSTM B-MethodName
MLP O
dim O
. O

For O
node O
labels O
and O
properties O
and O
for O
edge O
attributes O
( O
when O
applicable O
) O
, O
an O
additional O
“ O
axis O
” O
( O
private O
BiLSTM B-MethodName
and O
MLP O
) O
is O
added O
per O
framework O
( O
e.g. O
, O
AMR O
node O
labels O
are O
predicted O
separately O
and O
with O
an O
identical O
architecture O
to O
AMR O
transitions O
, O
except O
the O
output O
dimension O
is O
different O
) O
. O

5.1 O
Hyperparameters O
We O
use O
dropout B-HyperparameterName
( O
Srivastava O
et O
al O
. O
, O
2014 B-HyperparameterValue
) O
between O
MLP O
layers O
, O
and O
recurrent O
dropout B-HyperparameterName
( O
Gal O
and O
Ghahramani O
, O
2016 B-HyperparameterValue
) O
between O
BiLSTM B-MethodName
layers O
, O
both O
withp= O
0:4 O
. O

6.3 O
Comparability O
with O
Previous O
Results O
Previous O
published O
results O
of O
applying O
TUPA O
to O
UCCA O
parsing O
( O
Hershcovich O
et O
al O
. O
, O
2017 O
, O
2018a O
, O
2019b O
, O
a O
) O
used O
a O
different O
version O
of O
the O
parser O
, O
without O
contextualized O
word O
representations O
from O
BERT B-MethodName
. O

While O
improvement O
is O
achieved O
uniformly O
over O
the O
previous O
TUPA O
scores B-MetricName
, O
even O
with O
BERT B-MethodName
, O
TUPA O
is O
outperformed O
by O
the O
shared O
task O
winners O
( O
Jiang O
et O
al O
. O
, O
2019 O
) O
. O

We O
also O
train O
and O
test O
TUPA O
with O
BERT B-MethodName
embeddings O
on O
v1.0 O
of O
the O
UCCA O
English O
Web O
Treebank O
( O
EWT O
) O
reviews O
dataset O
( O
Hershcovich O
et O
al O
. O
, O
2019a O
) O
. O

English O
- O
Wiki O
( O
open O
) O
TUPA O
( O
w/o O
BERT B-MethodName
) O
73.5 O
73.9 O
53.5 O
TUPA O
( O
w/ O
BERT B-MethodName
) O
77.8 O
78.3 O
57.4 O
Jiang O
et O
al O
. O
( O
2019 B-MetricValue
) O
80.5 O
81.0 O
58.8 O
English-20 O
K O
( O
open O
) O
TUPA O
( O
w/o O
BERT B-MethodName
) O
68.4 O
69.4 O
25.9 O
TUPA O
( O
w/ O
BERT B-MethodName
) O
74.9 O
75.7 O
44.0 O
Jiang O
et O
al O
. O
( O
2019 O
) O
76.7 O
77.7 O
39.2 O
German-20 O
K O
( O
open O
) O
TUPA O
( O
w/o O
BERT B-MethodName
) O
79.1 O
79.6 O
59.9 O
TUPA O
( O
w/ O
BERT B-MethodName
) O
81.3 O
81.6 O
69.2 O
Jiang O
et O
al O
. O
( O
2019 O
) O
84.9 O
85.4 O
64.1 O
French-20 O
K O
( O
open O
) O
TUPA O
( O
w/o O
BERT B-MethodName
) O
48.7 O
49.6 O
2.4 O
TUPA O
( O
w/ O
BERT B-MethodName
) O
72.0 O
72.8 O
45.8 O
Jiang O
et O
al O
. O
( O
2019 O
) O
75.2 O
76.0 O
43.3 O
Table O
4 O
: O
Test O
UCCA O
F O
- O
score B-MetricName
scores B-MetricName
( O
in O
% O
) O
on O
all O
edges O
, O
primary O
edges O
and O
remote O
edges O
, O
on O
the O
SemEval O
2019 O
Task O
1 O
data O
. O

The O
previous O
published O
TUPA O
scores B-MetricName
are O
shown O
( O
TUPA O
w/o O
BERT B-MethodName
) O
, O
as O
well O
as O
scores B-MetricName
for O
TUPA O
with O
BERT B-MethodName
contextualized O
embeddings O
, O
TUPA O
( O
w/ O
BERT B-MethodName
) O
, O
averaged B-MetricName
over O
three O
separately O
trained O
models O
in O
each O
setting O
, O
differing O
only O
by O
random O
seed O
( O
standard O
deviation O
< O
0.03 O
) O
; O
and O
the O
scores B-MetricName
for O
the O
best O
- O
scoring O
system O
from O
that O
shared O
task O
. O

Its O
input O
representation O
is O
augmented O
with O
BERT B-MethodName
contextualized O
embeddings O
. O

TUPA O
( O
w/o O
BERT B-MethodName
) O
71.0 O
72.1 O
47.0 O
TUPA O
( O
w/ O
BERT B-MethodName
) O
75.2 O
76.1 O
54.8 O
Table O
5 O
: O
Test O
UCCA O
F O
- O
score B-MetricName
scores B-MetricName
( O
in O
% O
) O
on O
all O
edges O
, O
primary O
edges O
and O
remote O
edges O
, O
on O
the O
UCCA O
EWT O
reviews O
data O
. O

TUPA O
( O
w/o O
BERT B-MethodName
) O
is O
from O
( O
Hershcovich O
et O
al O
. O
,2019a O
) O
. O

TUPA O
( O
w/ O
BERT B-MethodName
) O
is O
averaged B-MetricName
over O
three O
separately O
trained O
models O
in O
each O
setting O
, O
differing O
only O
by O
random O
seed O
( O
standard O
deviation O
< O
0.03 O
) O
. O

Its O
architecture O
is O
based O
on O
a O
bi O
- O
directional O
attention O
ﬂow O
( O
BiDAF O
) O
model O
( O
Gardner O
et O
al O
. O
, O
2018)3 O
, O
but O
also O
equipped O
with O
an O
explicit O
reasoning O
module O
that O
acts O
on O
entity O
- O
speciﬁc O
rela3Our O
implementation O
is O
based O
on O
the O
implementation O
publicly O
available O
in O
AllenNLP O
( O
Gardner O
et O
al O
. O
, O
2018).CNN O
CNN O
CNN O
LSTM B-MethodName
LSTM B-MethodName
LSTMStep B-MethodName
1 O
: O
Ingredients O
8 O
- O
12 O
oz O
( O
225 O
- O
350 O
g O
) O
gingersnap O
cookies O
( O
depending O
on O
how O
much O
crust O
you O
like O
! O
) O

Y O
ou're O
gon O
na O
love O
this O
one O
, O
I O
just O
know O
it!Step O
4 O
: O
 O
Bake O
Step O
2 O
: O
The O
Crust O
CNN O
LSTMChar B-MethodName
Embed O
Embed O
CNN O
Concat O
BiLSTMEmbed B-MethodName
CNN O
Concat O
BiLSTMChar B-MethodName
Embed O
fresh O
ground O
pepper O
gingersnap O
cookies O
ground O
cinnamon O
pumpkin O
 O
puree O
cream O
 O
cheese O
nutmeg O
vanilla O
EmbedPreheat O
your O
oven O
to O
350F O
( O
180C O
) O
. O

In O
word O
embedding O
layer O
, O
we O
use O
a O
pretrained O
GloVe B-MethodName
model O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
and O
extract O
wordlevel O
embeddings4 O
. O

Speciﬁcally O
, O
we O
obtain O
a O
step O
- O
level O
con4We O
also O
consider O
pretrained O
ELMo B-MethodName
embeddings O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
in O
our O
experiments O
but O
found O
out O
that O
the O
performance O
gain O
does O
not O
justify O
the O
computational O
overhead.textual O
embedding O
of O
the O
input O
recipe O
containing O
Tsteps O
asS= O
( O
s1;s2;:::;sT)where O
sirepresents O
the O
ﬁnal O
state O
of O
a O
BiLSTM B-MethodName
encoding O
the O
i O
- O
th O
step O
of O
the O
recipe O
obtained O
from O
the O
character O
and O
word O
- O
level O
embeddings O
of O
the O
tokens O
exist O
in O
the O
corresponding O
step O
. O

Then O
these O
embeddings O
are O
passed O
ﬁrst O
to O
a O
multilayer O
perceptron O
( O
MLP O
) O
and O
then O
its O
outputs O
are O
fed O
to O
a O
BiLSTM B-MethodName
. O

We O
then O
form O
a O
matrix O
Q02R2dMfor O
the O
question O
by O
concatenating O
the O
cell O
states O
of O
the O
BiLSTM B-MethodName
. O

For O
the O
visual O
ordering O
task O
, O
to O
represent O
the O
sequence O
of O
images O
in O
the O
answer O
with O
a O
single O
vector O
, O
we O
additionally O
use O
a O
BiLSTM B-MethodName
and O
deﬁne O
the O
answering O
embedding O
by O
the O
summation O
of O
the O
cell O
states O
of O
the O
BiLSTM B-MethodName
. O

We O
initialize O
each O
memory O
cell O
eirepresenting O
a O
speciﬁc O
entity O
by O
its O
CharCNN O
and O
pre O
- O
trained O
GloVe B-MethodName
embeddings6 O
. O

It O
is O
built O
on O
a O
2 O
- O
dimensional O
LSTM B-MethodName
model O
whose O
matrix O
of O
cell O
states O
represent O
our O
memory O
matrix O
E. O
Here O
, O
each O
rowiof O
the O
matrix O
Erefers O
to O
a O
speciﬁc O
entity O
eiand O
is O
updated O
after O
each O
recipe O
step O
tas O
follows O
: O
 O
i;t= O
R O
- O
RNN O
( O
 O
i;t 1;st O
) O
( O
1 O
) O
where O
stdenotes O
the O
embedding O
of O
recipe O
step O
t O
and O
 O
i;t= O
( O
hi;t;ei;t)is O
the O
cell O
state O
of O
the O
R O
- O
RNN O
at O
steptwithhi;tandei;tbeing O
thei O
- O
th O
row O
of O
the O
hidden O
state O
of O
the O
R O
- O
RNN O
and O
the O
dynamic O
representation O
of O
entity O
eiat O
the O
stept O
, O
respectively O
. O

For O
this O
purpose O
, O
we O
ﬁrst O
use O
a O
two O
- O
layer O
BiLSTM B-MethodName
to O
read O
the O
question O
- O
aware O
recipe O
Gand O
to O
encode O
the O
interactions O
among O
the O
words O
conditioned O
on O
the O
question O
. O

For O
each O
direction O
of O
BiLSTM B-MethodName
, O
we O
use O
its O
hidden O
state O
after O
reading O
the O
last O
token O
as O
its O
output O
. O

Similarly O
, O
we O
employ O
a O
second O
BiLSTM B-MethodName
, O
this O
time O
, O
over O
the O
entities O
Y O
, O
which O
results O
in O
another O
vector O
embedding O
f2R2dE1 O
. O

That O
is O
, O
it O
uses O
the O
static O
entity O
embeeddings O
initialized O
with O
GloVe B-MethodName
word O
vectors O
. O

We O
use O
the O
pre O
- O
trained O
language O
model O
BERT B-MethodName
as O
the O
encoder O
. O

Neural O
Architecture O
Our O
model O
builds O
the O
candidate O
graph O
nodes O
representation O
based O
on O
the O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
encoder O
outputs O
, O
i.e. O
, O
for O
each O
token O
wi O
, O
the O
contextualized O
vector O
from O
BERT B-MethodName
encoder O
is O
denoted O
as O
xi O
. O

Neural O
Architecture O
In O
this O
model O
, O
we O
also O
build O
the O
candidate O
span O
representation O
hbased O
on O
the O
BERT B-MethodName
encoder O
outputs O
due O
to O
a O
span O
’s O
correct O
label O
and O
its O
quality O
as O
a O
constituent O
depend O
heavily O
on O
the O
context O
in O
which O
it O
appears O
. O

The O
weights B-HyperparameterName
of O
pre O
- O
trained O
language O
model O
BERT B-MethodName
with O
whole O
word O
masking8are O
used O
to O
initialize O
the O
encoder O
of O
our O
models O
. O

8In O
our O
experiments O
, O
we O
use O
the O
BERT B-MethodName
- O
Large O
, O
uncased O
( O
Whole O
Word O
Masking O
) O
with O
24 O
- O
layer O
, O
1024 O
- O
hidden O
, O
16heads O
, O
and O
340 O
M O
parameters O
released O
by O
Google O
, O
https O
: O
//github.com O
/ O
google O
- O
research O
/ O
bert O
. O

Using O
TreeLSTM B-MethodName
, O
Zhang O
et O
al O
. O
( O
2017 O
) O
tried O
to O
integrate O
linguistic O
structure O
into O
QA O
implicitly O
. O

In O
contrast O
, O
( O
Xie O
and O
Eric O
, O
2017 O
) O
explicitly O
modeled O
candidate O
answers O
as O
sequences O
of O
constituents O
by O
encoding O
individual O
constituents O
using O
a O
chain O
oftrees O
LSTM B-MethodName
( O
CT O
- O
LSTM B-MethodName
) O
and O
tree O
- O
guided O
attention O
mechanism O
. O

As O
RNN O
cell O
, O
we O
used O
Long O
Short O
- O
Term O
Memory O
architecture O
( O
LSTM B-MethodName
) O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

Pan O
et O
al O
. O
( O
2017 O
) O
and O
Hu O
et O
al O
. O
( O
2017 O
) O
show O
that O
bi O
- O
directional O
LSTM B-MethodName
architectures O
provide O
more O
accurate O
representations O
of O
textual O
data O
. O

The O
common O
practice O
to O
form O
a O
bidirectional O
LSTM B-MethodName
is O
to O
concatenate O
the O
last O
vectors O
in O
forward O
and O
backward O
LSTMs B-MethodName
. O

Instead O
, O
we O
used O
a O
stepwise O
max O
pooling O
( O
SWMP O
) O
mechanism O
which O
takes O
the O
most O
important O
vectors O
from O
forward O
and O
backward O
LSTMs B-MethodName
in O
Equations O
3 O
and O
5 O
and O
concatenate O
them O
in O
Equations O
6 O
. O

Wi;t O
= O
E O
> O
Wk O
( O
1 O
) O
 !enci;t O
= O
LSTM B-MethodName
( O
 !enci;t 1;Wi;t O
) O
( O
2 O
) O
 !enci O
= O
SWMP O
( O
 !enci;t O
) O
( O
3 O
) O
 O
 enci;t O
= O
LSTM B-MethodName
( O
 enci;t+1;Wi;t O
) O
( O
4 O
) O
 O
 enci O
= O
SWMP O
( O
 enci;t O
) O
( O
5 O
) O
enci= O
[ O
 !enci O
; O
 enci O
] O
( O
6 O
) O
Using O
our O
encoding O
unit O
we O
encode O
questions O
and O
sentences O
and O
then O
concatenate O
the O
resulted O
vectors O
to O
generate O
a O
joint O
representation O
of O
questions O
and O
their O
answer O
sentences O
in O
Equation O
7 O
. O
encQS O
i= O
[ O
encQ O
i;encS O
i O
] O
( O
7 O
) O
In O
the O
next O
step O
, O
we O
need O
to O
encode O
the O
constituent O
answers O
. O

In O
both O
modules O
, O
we O
used O
an O
architecture O
similar O
to O
the O
one O
in O
the O
encoding O
unit O
with O
an O
additional O
attention O
unit O
. O
In O
the O
answer O
encoding O
unit O
, O
again O
the O
input O
to O
LSTM B-MethodName
cells O
are O
word O
embeddings O
generated O
by O
lookup O
table O
WA O
i;t O
. O
Two O
attention O
layers O
in O
this O
unit O
receive O
the O
output O
sequences O
of O
the O
forward O
and O
backward O
LSTM B-MethodName
cells O
and O
focus O
once O
on O
questions O
and O
once O
on O
sentences O
. O

The O
output O
is O
the O
concatenation O
of O
max O
- O
pooled O
vectors O
of O
LSTM B-MethodName
encoders O
. O

We O
used O
128 O
- O
dimensional O
LSTMs B-MethodName
for O
all O
recurrent O
networks O
and O
used O
’ O
Adam O
’ O
with O
parameters O
learning B-HyperparameterName
rate=0.001 I-HyperparameterName
, O
 B-HyperparameterValue
1= O
0:9 O
, O
 O
2= O
0:999 O
for O
optimization O
. O

We O
set O
batch B-HyperparameterName
size I-HyperparameterName
to O
32 B-HyperparameterValue
and O
dropout B-HyperparameterName
rate I-HyperparameterName
to O
0.5 B-HyperparameterValue
for O
all O
LSTMs B-MethodName
and O
embedding O
layers O
. O

As O
seen O
in O
Figure O
4 O
, O
the O
answers O
are O
mostly O
singular O
noun O
phrases O
after O
which O
with O
a O
signiﬁcant O
difference O
are O
proper O
nouns O
, O
verb O
phrases O
, O
and O
prepo O
- O
SQuAD B-DatasetName
Development O
set O
MS O
- O
MARCO O
Evaluation O
set O
Exact O
- O
match(% O
) O
F1(% B-MetricName
) O
BLEU B-MetricName
ROUGE B-MetricName
Logistic B-MethodName
Regression I-MethodName
( O
Rajpurkar O
et O
al O
. O
, O
2016 B-MetricValue
) O
40.00 O
% O
51.00 O
% O
- O
Uni O
- O
Attention O
Word O
- O
base O
( O
this O
work O
) O
55.12 O
% O
57.98 O
% O
35.6 O
35.1 O
Bi O
- O
Attention O
Word O
- O
base O
( O
this O
work O
) O
59.84 O
% O
63.08 O
% O
38.1 O
38.4 O
Uni O
- O
Attention O
Constituency O
- O
base O
( O
this O
work O
) O
73.82 O
% O
77.43 O
% O
39.6 O
39.9 O
TreeLSTM B-MethodName
( O
Zhang O
et O
al O
. O
, O
2017 O
) O
69.10 O
% O
78.38 O
% O
- O
BIDAF O
( O
Seo O
et O
al O
. O
, O
2016 O
) O
72.6 O
% O
80.7 O
% O
- O
CCNN O
( O
Xie O
and O
Eric O
, O
2017 O
) O
74.1 O
% O
82.6 O
% O
- O
R O
- O
net O
( O
Wang O
et O
al O
. O
, O
2017 O
) O
75.60 O
% O
82.80 O
% O
42.2 O
42.9 O
Bi O
- O
Attention O
Constituency O
- O
base O
( O
this O
work O
) O
80.72 O
% O
83.25 O
% O
42.1 O
42.7 O
Human O
Performance O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
82.30 O
% O
91.22 O
% O
- O
Table O
2 O
: O
The O
performances O
of O
different O
models O
in O
the O
exact O
match O
and O
F1 B-MetricName
metrics O
for O
SQuAD B-DatasetName
and O
BLEU B-MetricName
and O
ROUGE B-MetricName
for O
the O
MS O
- O
MARCO O
dataset O
. O

It O
operates O
an O
LSTM B-MethodName
over O
the O
constituents O
and O
uses O
the O
resulting O
hidden O
states O
to O
attend O
both O
to O
question O
and O
to O
the O
encompassing O
context O
sentence O
, O
thereby O
enriching O
the O
constituents O
representation O
with O
both O
. O

It O
is O
worth O
noting O
that O
our O
absolute O
scores B-MetricName
are O
weaker O
than O
these O
of O
Clark O
and O
Manning O
( O
2016a O
) O
, O
as O
they O
build O
on O
top O
of O
a O
similar O
but O
stronger O
mention O
- O
ranking O
baseline O
, O
which O
employs O
deeper O
neural B-MethodName
networks I-MethodName
and O
requires O
a O
much O
larger O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
to O
train O
( O
300 B-HyperparameterValue
epochs B-HyperparameterName
, O
including O
pretraining O
) O
. O

Wiseman O
et O
al O
. O
( O
2016 O
) O
extend O
this O
by O
employing O
LSTMs B-MethodName
to O
compute O
mention O
- O
chain O
representations O
which O
are O
then O
used O
to O
compute O
ranking O
scores B-MetricName
. O

Clark O
and O
Manning O
( O
2016a O
) O
build O
a O
similar O
resolver O
as O
in O
Wiseman O
et O
al O
. O
( O
2015b O
) O
but O
much O
stronger O
thanks O
to O
deeper O
neural B-MethodName
networks I-MethodName
and O
“ O
better O
mention O
detection O
, O
more O
effective O
, O
hyperparameters O
, O
and O
more O
epochs B-HyperparameterName
of O
training O
” O
. O

We O
then O
train O
a O
support O
vector O
machine O
( O
SVM B-MethodName
) O
to O
predict O
whether O
a O
text O
is O
advanced O
or O
elementary O
using O
these O
features O
. O

This O
system O
uses O
a O
multilayer O
perceptron O
classiﬁer O
and O
has O
been O
shown O
to O
outperform O
BERT B-MethodName
- O
based O
approaches O
on O
the O
OneStopEnglish O
dataset O
( O
Martinc O
et O
al O
. O
, O
2019 O
) O
. O

To O
do O
this O
, O
we O
initially O
create O
a O
highly O
competitive O
baseline O
by O
training O
an O
SVM B-MethodName
using O
the O
length O
of O
text O
and O
measures O
of O
lexical O
richness O
. O

We O
train O
a O
BiLSTM B-MethodName
to O
predict O
a O
supertag O
for O
each O
token O
and O
use O
the O
dependency O
parser O
of O
Kiperwasser O
and O
Goldberg O
( O
2016 O
) O
to O
predict O
dependency O
trees O
. O

5 O
Evaluation O
5.1 O
Experimental O
setup O
We O
trained O
one O
single O
- O
task O
model O
per O
graphbank O
and O
made O
use O
of O
a O
concatenation O
of O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
and O
Elmo O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
embeddings O
, O
without O
any O
ﬁnetuning O
. O

All O
of O
our O
models O
are O
sequence O
- O
to O
- O
sequence O
neural B-MethodName
networks I-MethodName
with O
multiple O
encoders O
and O
a O
single O
decoder O
. O

The O
discrete O
nature O
of O
such O
processes O
makes O
it O
difﬁcult O
to O
directly O
translate O
transducers O
into O
neural B-MethodName
networks I-MethodName
and O
to O
effectively O
train O
them O
using O
backpropagation O
. O

There O
have O
been O
various O
attempts O
to O
replace O
parts O
of O
the O
FST O
paradigm O
with O
neural B-MethodName
networks I-MethodName
( O
Aharoni O
and O
Goldberg O
, O
2016 O
) O
. O

Both O
encoders O
employ O
character O
/ O
tag O
embeddings O
and O
bidirectional O
LSTMs B-MethodName
, O
where O
the O
outputs O
are O
summed O
over O
the O
two O
directions O
. O

3.2 O
) O
, O
then O
it O
passes O
the O
embedded O
symbol O
to O
a O
unidirectional O
LSTM B-MethodName
. O

Both O
encoders O
( O
lemma O
and O
tag O
) O
and O
the O
decoder O
( O
listed O
as O
inﬂected O
) O
have O
three O
varying O
parameters O
: O
the O
size O
of O
the O
embedding O
, O
the O
number O
of O
hidden O
LSTM B-MethodName
cells O
and O
the O
number O
of O
LSTM B-MethodName
layers O
. O

We O
also O
varied O
the O
dropout B-HyperparameterName
rate I-HyperparameterName
for O
both O
the O
embedding O
and O
the O
LSTMs B-MethodName
and O
the O
whether O
to O
share O
the O
vocabulary O
and O
the O
embedding O
among O
the O
lemma O
and O
the O
decoder O
or O
not O
. O

Each O
token O
and O
lemma O
are O
encoded O
by O
a O
bidirectional O
character O
LSTM B-MethodName
, O
preceded O
by O
a O
character O
embedding O
, O
and O
the O
tag O
sequence O
of O
the O
corresponding O
token O
are O
encoded O
by O
a O
separate O
biLSTM B-MethodName
and O
tag O
embedding O
. O

We O
concatenate O
these O
and O
use O
another O
biLSTM B-MethodName
( O
context O
LSTM B-MethodName
) O
to O
create O
a O
single O
vector O
representation O
of O
the O
left O
/ O
right O
context O
. O

The O
context O
LSTM B-MethodName
is O
shared O
by O
the O
left O
and O
the O
right O
context O
. O

We O
include O
an O
analysis O
of O
the O
corpus O
properties O
, O
and O
experimental O
results O
with O
strong O
baselines O
and O
three O
different O
state O
- O
of O
- O
the O
- O
art O
deep O
learning O
models O
: O
an O
attention O
- O
based O
RNN O
, O
a O
reinforcement O
learning O
approach O
, O
and O
a O
BERT B-MethodName
- O
based O
transformer O
method O
. O

The O
ﬁnal O
method O
we O
test O
is O
a O
BERT B-MethodName
- O
based O
model O
introduced O
by O
Liu O
and O
Lapata O
( O
2019 O
) O
which O
uses O
BERT B-MethodName
embeddings O
as O
the O
pretrained O
encoder O
, O
stacked O
Transformer O
layers O
as O
a O
decoder O
and O
a O
ﬁne O
- O
tuning O
process O
to O
produce O
more O
natural O
abstractive O
summaries O
. O

We O
also O
use O
SumBasic3and O
a O
GloVe B-MethodName
vector4enhanced O
TextRank O
algorithm O
. O

4.2 O
Deep O
models O
4.2.1 O
Pointer O
– O
Generator O
We O
adapt O
the O
PyTorch O
re O
- O
implementation5of O
the O
original O
pointer O
– O
generator O
network O
( O
PGN O
) O
( O
See O
et O
al O
. O
, O
2017 O
) O
by O
inserting O
ELMo B-MethodName
embeddings O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
trained O
on O
PubMed O
texts,6hereafter O
refered O
to O
as O
PGN O
- O
E. O

The O
added O
ELMo B-MethodName
embeddings O
were O
computed O
by O
a O
pre O
- O
trained O
two O
- O
layered O
bidirectional O
language O
model O
( O
biLM O
) O
resulting O
in O
512 O
- O
dimensional O
word O
vectors O
, O
which O
is O
more O
than O
twice O
of O
the O
original O
pointer O
generator O
embedding O
size O
. O

Each O
single O
- O
layered O
LSTM B-MethodName
network O
includes O
256 O
hidden B-HyperparameterName
units I-HyperparameterName
in O
all O
models O
. O

4.2.3 O
Transformer O
We O
adapt O
the O
open O
- O
source O
code O
provided O
by O
Liu O
and O
Lapata O
and O
replace O
the O
“ O
BERT B-MethodName
- O
base O
- O
uncased O
” O
pre O
- O
trained O
model O
with O
SciBERT B-MethodName
( O
Beltagy O
et O
al O
. O
, O
2019 O
) O
. O

SciBERT B-MethodName
follows O
the O
BERT B-MethodName
model O
architecture O
, O
which O
is O
a O
multi O
- O
bidirectional O
transformer O
, O
by O
training O
an O
objective O
which O
predicts O
masked O
tokens O
and O
the O
next O
sentence O
, O
but O
is O
trained O
on O
scientiﬁc O
texts O
including O
PubMed O
. O

The O
input O
source O
content O
and O
target O
summaries O
were O
tokenised O
with O
BERT B-MethodName
’s O
subword O
tokeniser O
. O

We O
refer O
to O
this O
model O
as O
SciBERT B-MethodName
Abstractive O
, O
SciBERTA B-MethodName
for O
short O
. O
The O
speciﬁc O
hyperparameter O
values O
of O
the O
abstractive O
component O
are O
shown O
in O
Appendix O
A.1 O
. O

The O
maximum O
encoding O
text O
size O
is O
set O
to O
512 O
, O
because O
only O
210 O
input O
texts O
in O
our O
corpus O
are O
longer O
than O
512 O
, O
and O
512 O
is O
the O
maximum O
length O
in O
the O
original O
BERT B-MethodName
model O
position O
embeddings O
. O

We O
can O
observe O
from O
the O
CI O
overlap O
that O
SciBERTA B-MethodName
is O
signiﬁcantly O
better O
than O
the O
other O
two O
deep O
learning O
methods O
, O
whereas O
Lead-2 O
provides O
the O
most O
competitivebaseline O
. O

5 O
) O
where O
we O
see O
that O
the O
original O
PGN O
does O
not O
even O
beat O
the O
Lead-3 O
baseline O
, O
while O
the O
BERT B-MethodName
- O
based O
model O
outperforms O
the O
other O
two O
. O

The O
introduction O
of O
ELMo B-MethodName
vectors O
slightly O
alleviates O
this O
issue O
over O
the O
original O
PGNcoverage O
model O
. O

RL O
- O
EA O
has O
unigram O
overlap O
of O
61.35 O
% O
and O
SciBERTA B-MethodName
again O
shows O
highest O
diversity O
with O
only O
about O
53.86 O
% O
overlap O
with O
the O
Lead-2 O
baseline O
. O

We O
can O
see O
that O
SciBERTA B-MethodName
also O
outperforms O
the O
other O
methods O
with O
this O
measure O
, O
and O
although O
PGN O
- O
E O
and O
RL O
- O
EA O
were O
indistinguishable O
based O
on O
ROUGE B-MetricName
, O
PGN O
- O
E O
is O
actually O
producing O
more O
novel O
n O
- O
grams O
. O

PGN O
- O
E O
’s O
performance O
increases O
slightly O
over O
the O
original O
SciBERTA B-MethodName
scores B-MetricName
, O
which O
unfortunately O
drop O
slightly O
on O
the O
pruned O
training O
data O
. O

This O
indicates O
that O
SciBERTA B-MethodName
is O
more O
resilient O
to O
biased O
signals O
in O
the O
training O
data O
, O
but O
beneﬁts O
from O
more O
training O
data O
. O

The O
unigram O
overlap O
between O
the O
generated O
summaries O
and O
the O
Lead-2 O
baseline O
is O
also O
reduced O
across O
the O
board O
( O
PGN O
: O
59.61 O
% O
, O
PGNE O
: O
59.49 O
% O
, O
RL O
- O
EA:49.44 O
% O
, O
SciBERTA B-MethodName
: O
52.46%).ModelsROUGE-1 B-MetricName
ROUGE-2 B-MetricName
ROUGE B-MetricName
- O
L O
ROUGE B-MetricName
- O
SU O
F1 B-MetricName
CI O
F1 B-MetricName
CI O
F1 B-MetricName
CI O
F1 B-MetricName
CI O
Lead-2 O
45.4 B-MetricValue
( O
-0.2 O
, O
+0.2 O
) O
30.5 O
( O
-0.3 O
, O
+0.3 O
) O
38.1 O
( O
-0.2 O
, O
+0.2 O
) O
21.8 O
( O
-0.2 O
, O
+0.2 O
) O
SumBasic O
38.5 O
( O
-0.2 O
, O
+0.2 O
) O
20.4 O
( O
-0.2 O
, O
+0.2 O
) O
29.8 O
( O
-0.2 O
, O
+0.2 O
) O
15.0 O
( O
-0.2 O
, O
+0.2 O
) O
Textrank O
38.2 O
( O
-0.2 O
, O
+0.2 O
) O
21.8 O
( O
-0.3 O
, O
+0.3 O
) O
30.7 O
( O
-0.3 O
, O
+0.3 O
) O
16.6 O
( O
-0.2 O
, O
+0.2 O
) O
PGN O
- O
E O
46.2 O
( O
-0.2 O
, O
+0.3 O
) O
31.0 O
( O
-0.4 O
, O
+0.3 O
) O
41.2 O
( O
-0.3 O
, O
+0.3 O
) O
26.5 O
( O
-0.3 O
, O
+0.3 O
) O
RL O
- O
EA O
47.4 O
( O
-0.3 O
, O
+0.3 O
) O
31.7 O
( O
-0.3 O
, O
+0.3 O
) O
40.8 O
( O
-0.3 O
, O
+0.3 O
) O
25.8 O
( O
-0.3 O
, O
+0.3 O
) O
SciBERTA B-MethodName
48.3 O
( O
-0.3 O
, O
+0.3 O
) O
32.4 O
( O
-0.4 O
, O
+0.3 O
) O
42.4 O
( O
-0.3 O
, O
+0.4 O
) O
28.1 O
( O
-0.3 O
, O
+0.3 O
) O
Table O
4 O
: O
Results O
of O
extractive O
and O
abstractive O
models O
on O
RSC O
SUM O
( O
best O
in O
each O
section O
bold O
- O
faced O
) O
. O

Models O
ROUGE-1 B-MetricName
ROUGE-2 B-MetricName
ROUGE B-MetricName
- O
L O
Baseline O
Lead-3 O
( O
See O
et O
al O
. O
, O
2017 B-MetricValue
) O
40.3 O
17.7 O
36.6 O
Abstractive O
models O
PGN O
( O
See O
et O
al O
. O
, O
2017 O
) O
36.4 O
15.7 O
33.4 O
PGN+coverage O
( O
See O
et O
al O
. O
, O
2017 O
) O
39.5 O
17.3 O
36.3 O
RL O
- O
EA+rerank O
( O
Chen O
and O
Bansal O
, O
2018 O
) O
40.9 O
17.8 O
38.5 O
BERTSumAbs B-MethodName
( O
Liu O
and O
Lapata O
, O
2019 O
) O
41.7 O
19.4 O
38.8 O
Table O
5 O
: O
Performance O
of O
related O
algorithms O
from O
prior O
work O
on O
the O
non O
- O
anonymised O
CNN O
/ O
Daily O
Mail O
dataset O
. O

We O
choose O
to O
compare O
SciBERTA B-MethodName
as O
the O
best O
- O
performing O
system O
, O
in O
addition O
to O
RL O
- O
EA O
, O
which O
is O
less O
similar O
to O
the O
Lead-2 O
baseline O
than O
PGN O
- O
E O
and O
also O
has O
a O
different O
learning O
objective O
. O

8 O
shows O
the O
results O
, O
with O
SciBERTA B-MethodName
outperforming O
both O
RL O
- O
EA O
and O
the O
ground O
truth O
( O
GT O
) O
on O
all O
counts O
. O

SciBERTA B-MethodName
achieves O
the O
highest O
mean B-MetricName
score B-MetricName
on O
all O
four O
criteria O
at O
4.52 B-MetricValue
, O
4.81 O
, O
3.55 O
and O
3.48 O
respectively O
, O
whereas O
the O
ground O
truth O
has O
only O
3.33 O
overall O
quality O
and O
3.86 O
on O
average B-MetricName
, O
which O
is O
quite O
unexpected O
. O

Although O
we O
took O
care O
to O
distribute O
the O
summary O
lengths O
as O
evenly O
as O
possible O
, O
in O
general O
RL O
- O
EA O
favours O
longer O
summaries O
, O
yet O
the O
more O
succinct O
summaries O
of O
SciBERTA B-MethodName
are O
preferred O
even O
in O
the O
informativeness O
dimension O
. O

Though O
SciBERTA B-MethodName
performs O
the O
best O
in O
all O
the O
evaluation O
dimensions O
as O
well O
as O
the O
overall O
average B-MetricName
, O
it O
is O
signiﬁcantly O
better O
than O
the O
ground O
truth O
only O
in O
terms O
of O
the O
informativeness O
and O
outperforms O
RL O
- O
EA O
regarding O
grammaticality O
and O
overall O
quality O
. O

dimension O
SciBERTA B-MethodName
RL O
- O
EA O
GT O
Grammaticality O
4.52 O
3.29 O
4.24 O
Informativeness O
4.81 O
4.62 O
4.38 O
Relevance O
3.55 O
3.52 O
3.50 O
Overall O
quality O
3.48 O
2.71 O
3.33 O
Average B-MetricName
4.09 B-MetricValue
3.53 O
3.86 O
Table O
8 O
: O
Mean O
scores B-MetricName
of O
human O
evaluation O
on O
RSCSUM O
. O

Of O
these O
, O
the O
one O
by O
SciBERTA B-MethodName
is O
the O
most O
grammatically O
pleasing O
. O

Consequently O
, O
as O
part O
of O
a O
larger O
effort O
of O
enriching O
chemical O
NLP O
, O
in O
the O
future O
we O
will O
also O
release O
about O
40 O
K O
full O
text O
open O
access O
articles O
that O
have O
corresponding O
TOC O
summaries O
. O
We O
tested O
three O
state O
- O
of O
- O
the O
- O
art O
deep O
summarisation O
methods O
and O
found O
that O
a O
transformer O
- O
based O
method O
that O
uses O
pretrained O
scientiﬁc O
BERT B-MethodName
embeddings O
produces O
the O
best O
overall O
results O
in O
both O
quantitative O
evaluation O
and O
a O
qualitative O
study O
with O
three O
domain O
expert O
participants O
. O

They O
encode O
zero O
pronouns O
and O
their O
candidate O
antecedents O
by O
LSTM B-MethodName
( O
Yin O
et O
al O
. O
, O
2017 O
) O
, O
Attention O
( O
Yin O
et O
al O
. O
, O
2018b O
; O
Liu O
et O
al O
. O
, O
2017 O
) O
or O
BERT B-MethodName
( O
Song O
et O
al O
. O
, O
2020 O
; O
Aloraini O
and O
Poesio O
, O
2020 O
) O
, O
then O
measure O
the O
similarity O
between O
embeddings O
to O
ﬁnd O
out O
the O
best O
antecedents O
. O

After O
representing O
spans O
by O
highway O
LSTM B-MethodName
( O
Lee O
et O
al O
. O
, O
2018 O
) O
or O
transformers O
( O
Joshi O
et O
al O
. O
, O
2019 O
, O
2020 O
) O
, O
they O
calculate O
mention O
and O
antecedent O
scores B-MetricName
using O
feed O
forward O
neural O
network O
or O
machine O
reading O
comprehension O
method O
( O
Wu O
et O
al O
. O
, O
2019 O
) O
, O
and O
select O
the O
top O
K O
mentions O
and O
their O
antecedents O
with O
the O
top O
antecedent O
score B-MetricName
. O

Instead O
of O
using O
Word2vec O
and O
Elmo O
, O
Joshi O
et O
al O
. O
( O
2019 O
, O
2020 O
) O
apply O
BERT B-MethodName
to O
get O
better O
span O
representations O
. O

In O
the O
basic O
joint O
model O
, O
these O
scoring O
functions O
are O
computed O
as O
follows O
: O
su(ui O
) O
=( O
FFNNm(hui)ui2S O
FFNNz(hui)ui2 O
G O
sa(ui;uj O
) O
= O
FFNNa([hui;huj O
; O
( O
ui;uj O
) O
] O
) O
( O
3 O
) O
We O
use O
two O
different O
feed O
- O
forward O
neural B-MethodName
networks I-MethodName
FFNNmandFFNNzto O
score B-MetricName
spans O
and O
gaps O
separately O
, O
where O
huiis O
the O
span O
or O
gap O
representation O
foruiand O
will O
be O
described O
in O
detail O
in O
the O
next O
section O
. O

To O
get O
token O
embeddings O
eand O
gap O
embeddings O
g O
, O
we O
ﬁrstly O
apply O
BERT B-MethodName
to O
encode O
the O
document O
following O
the O
independent O
variant O
of O
splitting O
in O
Joshi O
et O
al O
. O
( O
2019 O
) O
and O
get O
the O
basic O
token O
embeddingse0 O
. O

After O
that O
, O
we O
concatenate O
the O
token O
embedding O
e0 O
i 1withe0 O
i O
, O
and O
map O
the O
concatenated O
embedding O
[ O
e0 O
i 1;e0 O
i]to O
the O
same O
dimension O
to O
get O
basic O
gap O
embedding O
g0 O
i. O
Formally O
, O
the O
basic O
token O
embedding O
e0 O
iand O
the O
basic O
gap O
embedding O
g0 O
iare O
computed O
as O
follows O
: O
e0 O
1;e0 O
2;:::;e0 O
d O
= O
BERT B-MethodName
( O
t1;t2;:::;td O
) O
g0 O
i O
= O
FFNNg([e0 O
i 1;e0 O
i])(6 O
) O
Based O
on O
basic O
embeddings O
e0andg0 O
, O
we O
use O
a O
gap O
- O
masked O
self O
- O
attention O
model O
to O
get O
ﬁnal O
embeddingseandg O
. O

4.3 O
Experimental O
Settings O
Our O
model O
reuses O
most O
of O
the O
hyperparameters O
from O
Joshi O
et O
al O
. O
( O
2019 O
) O
except O
: O
( O
1 O
) O
We O
use O
the O
ofﬁcial O
pretrained O
Chinese O
BERT B-MethodName
- O
base O
model3to O
encode O
tokens O
. O

The O
baseline O
is O
an O
end O
- O
to O
- O
end O
neural O
coreference B-TaskName
resolution O
model O
( O
Joshi O
et O
al O
. O
, O
2019 O
) O
using O
ofﬁcial O
Chinese O
BERT B-MethodName
- O
base O
model O
. O

For O
fair O
comparison O
, O
we O
tune O
hyperparameters O
and O
add O
a O
self O
- O
attention O
model O
which O
has O
the O
same O
size O
with O
the O
gap O
- O
masked O
self O
- O
attention O
model O
after O
the O
BERT B-MethodName
encoding O
layer O
on O
the O
baseline O
model O
. O

Table O
2 O
shows O
the O
results O
of O
zero O
pronoun O
resolu O
- O
NZ O
- O
F1 B-MetricName
Z O
- O
F1 B-MetricName
Gaps O
embedded O
BERT B-MethodName
58.96 B-MetricValue
27.96 O
FFNN O
+ O
tanh O
64.09 O
31.07 O
LSTM B-MethodName
64.15 O
31.21 O
Self O
- O
attention O
66.42 O
32.15 O
Gap O
- O
masked O
self O
- O
attention O
67.40 O
33.77 O
Table O
4 O
: O
Results O
of O
different O
encoders O
. O

“ O
Gaps O
embedded O
BERT B-MethodName
” O
encoder O
inserts O
all O
gaps O
into O
documents O
directly O
and O
uses O
one O
BERT B-MethodName
model O
to O
encode O
gaps O
and O
tokens O
simultaneously O
. O

“ O
FFNN O
+ O
tanh O
” O
encoder O
uses O
feedforward O
neural B-MethodName
networks I-MethodName
and O
tanh O
activation B-HyperparameterName
function O
to O
map O
token O
embeddings O
into O
gap O
embeddings O
. O

“ O
LSTM B-MethodName
” O
encoder O
and O
“ O
Self O
- O
attention O
” O
encoder O
apply O
LSTM B-MethodName
or O
self O
- O
attention O
model O
on O
token O
embeddings O
to O
get O
gap O
embeddings O
. O

These O
three O
encoders O
compute O
gap O
embeddings O
without O
updating O
token O
embeddings O
obtained O
by O
BERT B-MethodName
, O
where O
token O
representations O
will O
not O
be O
polluted O
but O
embeddings O
of O
gaps O
and O
tokens O
will O
be O
in O
different O
space O
. O

We O
present O
a O
fully O
unsupervised O
crosslingual O
semantic O
textual O
similarity O
( O
STS O
) O
metric O
, O
based O
on O
contextual O
embeddings O
extracted O
from O
BERT B-MethodName
– O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

With O
the O
advent O
of O
massively O
multilingual O
context O
representation O
models O
such O
as O
BERT B-MethodName
, O
which O
are O
trained O
on O
the O
concatenation O
of O
non O
- O
parallel O
data O
from O
each O
language O
, O
we O
show O
that O
the O
deadlock O
around O
parallel O
resources O
can O
be O
broken O
. O

Our O
results O
show O
that O
the O
unsupervised O
crosslingual O
STS O
metric O
using O
BERT B-MethodName
without O
ﬁne O
- O
tuning O
achieves O
performance O
on O
par O
with O
supervised O
or O
weakly O
supervised O
approaches O
. O

Massively O
multilingual O
context O
representation O
models O
, O
such O
as O
MUSE O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
and O
XLM O
( O
Lample O
and O
Conneau O
, O
2019 O
) O
, O
that O
are O
trained O
in O
an O
unsupervised O
manner O
with O
non O
- O
parallel O
data O
from O
eachlanguage O
, O
have O
shown O
improved O
performance O
in O
XNLI B-MetricName
classiﬁcation O
tasks O
using O
task O
- O
speciﬁc O
ﬁnetuning O
. O

In O
this O
paper O
, O
we O
propose O
a O
crosslingual O
STS O
metric O
based O
on O
fully O
unsupervised O
contextual O
embeddings O
extracted O
from O
BERT B-MethodName
without O
ﬁnetuning O
. O

In O
an O
intrinsic O
crosslingual O
STS O
evaluation O
and O
extrinsic O
parallel O
corpus O
ﬁltering O
and O
human O
translation O
error O
detection O
tasks O
, O
we O
show O
that O
our O
BERT B-MethodName
- O
based O
metric O
achieves O
performance O
on O
par O
with O
similar O
metrics O
based O
on O
supervised O
or O
weakly O
supervised O
approaches O
. O

2.3 O
Unsupervised O
crosslingual O
contextual O
embeddings O
with O
multilingual O
BERT B-MethodName
The O
above O
two O
mentioned O
embedding O
models O
produce O
static O
word O
embeddings O
that O
captures O
the O
semantic O
space O
to O
represent O
the O
training O
data O
. O

In O
contrast O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
uses O
a O
bidirectional O
transformer O
encoder O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
to O
capture O
the O
sentence O
context O
in O
the O
output O
embeddings O
, O
such O
that O
the O
embedding O
for O
the O
same O
word O
unit O
in O
different O
sentences O
would O
be O
different O
and O
better O
represented O
in O
the O
embedding O
space O
. O

Multilingual O
BERT B-MethodName
model O
is O
trained O
on O
the O
Wikipedia O
pages O
of O
104 O
languages O
with O
a O
shared O
subword O
vocabulary O
. O

Pires O
et O
al O
. O
( O
2019 O
) O
showed O
multilingual O
BERT B-MethodName
works O
well O
on O
different O
monolingual O
NLP O
tasks O
across O
different O
languages O
. O

Following O
the O
recommendation O
in O
Devlin O
et O
al O
. O
( O
2019 O
) O
, O
we O
use O
embeddings O
extracted O
from O
the O
ninth O
layer O
of O
the O
pretrained O
multilingual O
cased O
BERT B-MethodName
- O
Base O
model4to O
represent O
subword O
units O
in O
the O
two O
sentences O
in O
assessment O
for O
the O
crosslingual O
lexical O
semantic O
similarity O
. O

vecmap O
: O
BWE O
’s O
are O
produced O
with O
vecmap O
, O
trained O
on O
all O
WMT O
2013 O
ES O
and O
WMT O
2019 O
EN O
monolingual O
data O
, O
using O
Wikititles O
as O
bilingual O
lexicon.6 O
BERT B-MethodName
: O
BWE O
’s O
are O
obtained O
from O
pre O
- O
trained O
multilingual O
BERT B-MethodName
models O
. O

However O
, O
this O
is O
followed O
very O
closely O
by O
both O
the O
supervised O
BWE O
’s O
( O
YiSi-2 O
bivec O
) O
and O
BERT B-MethodName
( O
YiSi-2 O
bert O
) O
, O
which O
yield O
very O
similar O
results O
, O
and O
clearly O
outperform O
semisupervised O
BWE O
’s O
( O
YiSi-2 O
vecmap O
) O
. O

In O
contrast O
, O
weakly O
supervised O
BWE O
’s O
and O
BERT B-MethodName
behave O
much O
more O
reliably O
on O
this O
data O
. O

Overall O
, O
while O
MT O
and O
supervised O
BWE O
’s O
seem O
to O
work O
best O
with O
YiSi O
when O
large O
quantities O
of O
in O
- O
domain O
training O
data O
is O
available O
, O
the O
fully O
unsupervised O
alternative O
of O
using O
a O
pretrained O
BERT B-MethodName
model O
comes O
very O
close O
, O
and O
behaves O
much O
better O
in O
the O
face O
of O
out O
- O
of O
- O
domain O
data O
. O

BERT B-MethodName
: O
BWE O
’s O
obtained O
from O
pretrained O
multilingual O
BERT B-MethodName
models O
. O

Zipporah O
combines O
ﬂuency O
and O
adequacy O
features O
to O
score B-MetricName
sentence O
pairs O
; O
adequacy O
features O
are O
derived O
from O
existing O
parallel O
corpora O
, O
and O
the O
feature O
combination O
( O
logistic B-MethodName
regression I-MethodName
) O
is O
optimized O
on O
in O
- O
domain O
parallel O
data O
. O

Overall O
, O
the O
beneﬁts O
of O
supervised O
and O
weakly O
supervised O
approaches O
over O
using O
a O
pre O
- O
trained O
BERT B-MethodName
model O
for O
PCF O
appear O
to O
be O
minimal O
, O
even O
in O
very O
low O
- O
resource O
conditions O
such O
as O
this O
. O

PSC O
Translation O
Error O
Detection O
model O
ROC O
AUC B-MetricName
meanF1meanF2 B-MetricName
PSC.bivec O
0.807 B-MetricValue
0.160 O
0.281 O
PSC.vecmap O
0.717 O
0.136 O
0.241 O
BERT B-MethodName
0.702 O
0.132 O
0.234 O
WMT.bivec O
0.641 O
0.112 O
0.205 O
Table O
6 O
: O
Sentence O
- O
level O
translation O
error O
detection O
results O
on O
PSC O
test O
data O
, O
expressed O
in O
terms O
of O
Area O
under O
the O
ROC O
curve O
, O
mean B-MetricName
F1and B-MetricName
meanF2 B-MetricName
. O

BERT B-MethodName
: O
BWE O
’s O
obtained O
from O
pretrained O
multilingual O
BERT B-MethodName
models O
. O

However O
, O
BERT B-MethodName
models O
perform O
comparably O
to O
vector O
- O
mapped O
BWE O
’s O
trained O
with O
indomain O
data O
( O
PSC.vecmap O
) O
, O
and O
substantially O
better O
than O
BWE O
’s O
trained O
on O
large O
quantities O
of O
generic O
, O
out O
- O
of O
- O
domain O
parallel O
data O
( O
WMT O
) O
. O

We O
conclude O
that O
, O
in O
the O
absence O
of O
in O
- O
domain O
parallel O
data O
, O
for O
TEED O
applications O
, O
an O
unsupervised O
YiSi-2 O
method O
will O
perform O
at O
least O
as O
well O
as O
supervised O
methods O
trained O
on O
out O
- O
of O
- O
domain O
data.6 O
Conclusion O
We O
presented O
a O
fully O
unsupervised O
crosslingual O
semantic O
textual O
similarity O
( O
STS O
) O
metric O
, O
based O
on O
contextual O
embeddings O
extracted O
from O
BERT B-MethodName
without O
ﬁne O
- O
tuning O
. O

In O
this O
paper O
, O
we O
have O
only O
experimented O
with O
the O
contextual O
embeddings O
extracted O
from O
pretrained O
multilingual O
BERT B-MethodName
model O
. O

For O
domainspeciﬁc O
applications O
, O
such O
as O
the O
job O
advertisement O
domain O
in O
the O
PSC O
translation O
equivalence O
error O
detection O
task O
, O
the O
performance O
of O
YiSi2 O
could O
potentially O
be O
improved O
by O
ﬁne O
- O
tuning O
BERT B-MethodName
with O
in O
- O
domain O
data O
, O
something O
we O
plan O
to O
examine O
in O
the O
near O
future O
. O

The O
overwhelming O
majority O
of O
solutions O
employ O
either O
static O
word O
embeddings O
like O
word2vec O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
or O
more O
recent O
contextualised O
language O
models O
like O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

* O
The O
RuShiftEval O
baseline O
relies O
on O
CBOW B-MethodName
word O
embeddings O
and O
their O
local O
neighborhood O
similarity O
. O

To O
this O
end O
, O
we O
train O
logistic B-MethodName
regression I-MethodName
classiﬁers O
for O
binary O
classiﬁcation O
using O
English O
, O
German O
, O
Latin O
, O
Swedish O
and O
Italian O
data O
. O

F1 B-MetricName
English O
nouns O
number O
0.576 B-MetricValue
0.523 O
English O
verbs O
verb O
form O
, O
syntax O
0.750 O
0.733 O
German O
number O
, O
syntax O
, O
gender0.542 O
0.541 O
Swedish O
syntax O
, O
mood O
, O
voice O
, O
deﬁniteness O
, O
number0.839 O
0.797 O
Latin O
voice O
, O
number O
, O
degree O
, O
case O
, O
gender O
, O
mood O
, O
aspect O
, O
person O
, O
tense0.650 O
0.649 O
Italian O
number O
, O
tense O
, O
syntax0.778 O
0.723 O
Table O
3 O
: O
Categories O
with O
positive O
weights B-HyperparameterName
in O
binary O
classiﬁers O
of O
semantic O
change O
( O
logistic B-MethodName
regression I-MethodName
) O
. O

First O
, O
we O
automatically O
ﬁlter O
candidate O
pairs O
of O
a O
summary O
IUand O
a O
document O
sentence O
that O
do O
not O
satisfy O
a O
certain O
similarity O
criterion O
, O
which O
is O
is O
composed O
of O
several O
similarity O
scores B-MetricName
: O
the O
BERT B-MethodName
- O
based O
similarity O
measure O
BERTscore B-MetricName
( O
Zhang O
et O
al O
. O
, O
2019 B-MetricValue
) O
, O
an O
entailment O
score B-MetricName
based O
on O
RoBERTa B-MethodName
ﬁne O
- O
tuned O
on O
MNLI B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
and O
ROUGE-1 B-MetricName
precision B-MetricName
( O
Lin O
, O
2004).3Next O
, O
we O
ﬁlter O
the O
remaining O
pairs O
via O
crowdsourcing O
. O

We O
then O
measured O
the O
recall B-MetricName
of O
the O
alignments O
3For O
BERT B-MethodName
and O
ROUGE B-MetricName
scores B-MetricName
, O
the O
summary O
IUis O
considered O
as O
the O
candidate O
and O
the O
document O
sentence O
as O
the O
reference O
; O
for O
entailment O
score B-MetricName
, O
the O
summary O
IUis O
the O
hypothesis O
and O
the O
document O
sentence O
is O
the O
premise.obtained O
in O
Tasks O
2 B-MetricValue
and O
3 O
against O
the O
expert O
annotation O
( O
of O
6 O
topics O
, O
mentioned O
above O
) O
. O

First O
, O
we O
include O
challenging O
negative O
instances O
by O
selecting O
all O
non O
- O
aligning O
source O
- O
summary O
OIE O
combinations O
that O
have O
BERTscore B-MetricName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
above O
0.89 O
. O

Overall O
, O
we O
sampled O
219,772 O
negative O
examples O
, O
where O
22 O
% O
of O
them O
were O
selected O
for O
high O
BERTscore B-MetricName
. O

( O
c O
) O
and O
( O
d O
) O
are O
challenging O
negative O
examples O
with O
high O
BERTscore B-MetricName
, O
while O
( O
e O
) O
is O
taken O
from O
the O
naturally O
distributed O
negative O
sample O
. O

The O
similarity O
score B-MetricName
at O
this O
stage O
was O
a O
tuned O
combination O
of O
ROUGE B-MetricName
, O
RoBERTa B-MethodName
- O
MNLI B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 B-MetricValue
) O
and O
BERTscore B-MetricName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
. O

We O
follow O
the O
standard O
usage O
of O
RoBERTa B-MethodName
for O
paraphrasing O
tasks O
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O

Speciﬁcally O
, O
we O
take O
a O
RoBERTa B-MethodName
encoder O
ﬁne O
- O
tuned O
on O
MNLI B-MethodName
and O
augment O
it O
with O
a O
new O
binary O
classiﬁcation O
layer O
. O

We O
propose O
a O
novel O
distant O
supervision O
approach O
for O
this O
scenario O
, O
based O
on O
graph O
embeddings O
, O
BERT B-MethodName
, O
and O
label O
propagation O
, O
which O
projects O
the O
more O
- O
commonly O
- O
available O
labels O
for O
news O
media O
onto O
the O
trolls O
who O
cited O
these O
media O
. O

3.1 O
Embeddings O
We O
use O
two O
graph O
- O
based O
( O
user O
- O
to O
- O
hashtag O
and O
user O
- O
to O
- O
mentioned O
- O
user O
) O
and O
one O
text O
- O
based O
( O
BERT B-MethodName
) O
embedding O
representations O
. O

The O
embeddings O
extracted O
from O
this O
graph O
capture O
how O
similar O
troll O
users O
are O
according O
to O
the O
targets O
of O
their O
discussions O
on O
the O
social O
network.3.1.3 O
BERT B-MethodName
BERT B-MethodName
offers O
state O
- O
of O
- O
the O
- O
art O
text O
embeddings O
based O
on O
the O
Transformer O
architecture O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

We O
use O
the O
pre O
- O
trained O
BERT B-MethodName
- O
large O
, O
uncased O
model O
, O
which O
has O
24 O
- O
layers O
, O
1024 O
- O
hidden O
, O
16 O
- O
heads O
, O
and O
340 O
M O
parameters O
, O
which O
yields O
output O
embeddings O
with O
768 O
dimensions O
. O

Given O
a O
tweet O
, O
we O
generate O
an O
embedding O
for O
it O
by O
averaging O
the O
representations O
of O
the O
BERT B-MethodName
tokens O
from O
the O
penultimate O
layer O
of O
the O
neural O
network O
. O

We O
use O
an O
L2 O
- O
regularized O
logistic B-MethodName
regression I-MethodName
( O
LR O
) O
classiﬁer O
. O

For O
example O
, O
U2H O
kU2 O
M O
is O
a O
model O
trained O
on O
the O
concatenation O
of O
U2H O
and O
U2 O
M O
embeddings O
, O
while O
U2H O
BERT B-MethodName
represents O
the O
average B-MetricName
predictions O
of O
two O
models O
, O
one O
trained O
on O
U2H O
embeddings O
and O
one O
on O
BERT B-MethodName
. O

Looking O
at O
individual O
features O
, O
for O
both O
T1 O
and O
T2 O
, O
the O
embeddings O
from O
U2 O
M O
outperform O
those O
from O
U2H O
and O
from O
BERT B-MethodName
. O

Finally O
, O
the O
textual O
content O
on O
Twitter O
is O
quite O
noisy O
, O
and O
thus O
the O
BERT B-MethodName
embeddings O
perform O
slightly O
worse O
when O
used O
alone O
. O

Adding O
BERT B-MethodName
embeddings O
to O
the O
combination O
yields O
further O
improvements O
, O
and O
follows O
a O
similar O
trend O
, O
where O
feature O
concatenation O
works O
better O
, O
yielding O
89.2 O
accuracy B-MetricName
for O
T1 O
and O
78.2 B-MetricValue
for O
T2 O
( O
compared O
to O
89.0 O
accuracy B-MetricName
for O
T1 O
and O
78.0 B-MetricValue
for O
T2 O
for O
U2HU2MBERT B-MethodName
) O
. O

Moreover O
, O
it O
achieves O
78.5 O
accuracy B-MetricName
in O
the O
distant O
supervised O
case O
, O
which O
is O
only O
11 B-MetricValue
points O
behind O
the O
result O
for O
T1 O
, O
and O
is O
about O
10 O
points O
above O
the O
majority O
class O
baseline O
. O
MethodFull O
Supervision O
( O
T1 O
) O
Distant O
Supervision O
( O
T2 O
) O
Accuracy O
Macro O
F1 B-MetricName
Accuracy O
Macro O
F1 B-MetricName
Baseline O
( O
majority O
class O
) O
68.7 B-MetricValue
27.1 O
68.7 O
27.1 O
Kim O
et O
al O
. O
( O
2019 O
) O
84.0 O
75.0 O
N O
/ O
A O
N O
/ O
A O
BERT B-MethodName
86.9 O
83.1 O
75.1 O
60.5 O
U2H O
87.1 O
83.2 O
76.3 O
60.9 O
U2 O
M O
88.1 O
83.9 O
77.3 O
62.4 O
U2HU2 O
M O
88.3 O
84.1 O
77.9 O
64.1 O
U2HkU2 O
M O
88.7 O
84.4 O
78.0 O
64.6 O
U2HU2MBERT B-MethodName
89.0 O
84.4 O
78.0 O
65.0 O
U2HkU2MkBERT B-MethodName
89.2 O
84.7 O
78.2 O
65.1 O
U2HkU2MkBERT B-MethodName
+ O
LP1 O
89.3 O
84.7 O
78.3 O
65.1 O
U2HkU2MkBERT B-MethodName
+ O
LP2 O
89.6 O
84.9 O
78.5 O
65.7 O
Table O
3 O
: O
Predicting O
the O
role O
of O
the O
troll O
users O
using O
full O
vs. O
distant O
supervision O
. O

By O
building O
a O
connection O
between O
users O
, O
hashtags O
, O
and O
user O
mentions O
, O
we O
effectively O
ﬁltered O
out O
the O
noise O
and O
we O
focused O
only O
on O
the O
most O
sensitive O
type O
of O
context O
, O
thus O
automatically O
capturing O
features O
from O
this O
network O
via O
graph O
embeddings O
. O
Method O
Accuracy O
Macro O
F1 B-MetricName
Baseline O
( O
majority O
) O
46.5 B-MetricValue
21.1 O
BERT B-MethodName
61.8 O
60.4 O
U2H O
61.6 O
60.0 O
U2 O
M O
62.7 O
61.4 O
U2HU2 O
M O
63.5 O
61.8 O
U2HkU2 O
M O
63.8 O
61.9 O
U2HU2MBERT B-MethodName
63.7 O
61.8 O
U2HkU2MkBERT B-MethodName
64.0 O
62.2 O
Table O
4 O
: O
Leveraging O
user O
embeddings O
to O
predict O
the O
bias O
of O
the O
media O
cited O
by O
troll O
users O
. O

This O
is O
centered O
around O
the O
idea O
of O
apropensity O
score B-MetricName
, O
which O
Rosenbaum O
and O
Rubin O
( O
1983 B-MetricValue
) O
deﬁne O
as O
the O
probability O
of O
being O
assigned O
to O
a O
treatment O
group O
based O
on O
observed O
characteristics O
of O
the O
subject O
, O
P(zi|xi O
) O
, O
typically O
estimated O
with O
a O
logistic B-MethodName
regression I-MethodName
model O
. O

We O
estimate O
these O
probabilities O
by O
training O
a O
logistic B-MethodName
regression I-MethodName
model O
with O
word O
features O
. O

First O
, O
when O
building O
logistic B-MethodName
regression I-MethodName
models O
to O
estimate O
the O
propensity O
scores B-MetricName
, O
we O
adjusted O
the O
/lscript2regularization O
strength O
. O

We O
used O
logistic B-MethodName
regression I-MethodName
classiﬁers O
for O
sentiment O
classiﬁcation O
. O

Regularization O
When O
training O
the O
logistic B-MethodName
regression I-MethodName
model O
to O
create O
propensity O
scores B-MetricName
, O
we O
experimented O
with O
the O
following O
values O
of O
the O
inverse O
regularization O
parameter O
: O
λ∈ O
{ O
0.01,0.1,1.0,100.0,109 O
} O
, O
whereλ=109is O
essentially O
no O
regularization O
other O
than O
to O
keep O
the O
optimal O
parameter O
values O
ﬁnite O
. O

The O
standard O
way O
of O
deﬁning O
propensity O
scores B-MetricName
with O
logistic B-MethodName
regression I-MethodName
models O
is O
not O
designed O
to O
scale O
to O
the O
large O
number O
of O
variables O
used O
in O
text O
classiﬁcation O
. O

Our O
proposed O
method O
is O
slow O
because O
it O
requires O
training O
a O
logistic B-MethodName
regression I-MethodName
model O
forevery O
word O
in O
the O
vocabulary O
. O

This O
paper O
presents O
a O
RecurrentNeural O
Network O
based O
Encoder O
- O
Decoderarchitecture O
, O
in O
which O
an O
LSTM B-MethodName
- O
based O
de O
- O
coder O
is O
introduced O
to O
select O
, O
aggregate O
se O
- O
mantic O
elements O
produced O
by O
an O
attentionmechanism O
over O
the O
input O
elements O
, O
andto O
produce O
the O
required O
utterances O
. O

A O
re O
- O
curring O
problem O
in O
such O
systems O
is O
requiring O
anno O
- O
tated O
datasets O
for O
particular O
dialogue O
acts1(DAs).To O
ensure O
that O
the O
generated O
utterance O
representingthe O
intended O
meaning B-MetricName
of O
the O
given O
DA O
, O
the O
previ O
- O
ous O
RNN O
- O
based O
models O
were O
further O
conditionedon O
a O
1 O
- O
hot O
vector O
representation O
of O
the O
DA.Wenet O
al.(2015a O
) O
introduced O
a O
heuristic O
gate O
to O
en O
- O
sure O
that O
all O
the O
slot O
- O
value O
pair O
was O
accurately O
cap O
- O
tured O
during O
generation O
. O
Wen O
et O
al.(2015b O
) O
sub O
- O
sequently O
proposed O
a O
Semantically O
ConditionedLong O
Short O
- O
term O
Memory O
generator O
( O
SC O
- O
LSTM)which B-MethodName
jointly O
learned O
the O
DA O
gating O
signal O
andlanguage O
model O
. O
More O
recently O
, O
Encoder O
- O
Decoder O
networks(Vinyals O
and O
Le,2015;Li O
et O
al O
. O
,2015 O
) O
, O
especiallythe O
attentional O
based O
models O
( O
Wen O
et O
al O
. O
,2016b;Mei O
et O
al O
. O
,2015 O
) O
have O
been O
explored O
to O
solvethe O
NLG O
tasks O
. O

In O
order O
tobetter O
select O
, O
aggregate O
and O
control O
the O
seman O
- O
tic O
information O
, O
a O
Reﬁnement O
Adjustment O
LSTM B-MethodName
- O
based O
component O
( O
RALSTM B-MethodName
) O
is O
introduced O
to O
the1A O
combination O
of O
an O
action O
type O
and O
a O
set O
of O
slot O
- O
valuepairs O
. O

To O
sum O
up O
, O
wemake O
three O
key O
contributions O
in O
this O
paper:•We O
present O
an O
LSTM B-MethodName
- O
based O
componentcalledRALSTMcell B-MethodName
applied O
on O
the O
decoderside O
of O
an O
ARED O
model O
, O
resulting O
in O
an O
end O
- O
to O
- O
end O
generator O
that O
empirically O
shows O
sig O
- O
niﬁcant O
improved O
performances O
in O
compari O
- O
son O
with O
the O
previous O
approaches.•We O
extensively O
conduct O
the O
experiments O
toevaluate O
the O
models O
training O
from O
scratch O
oneach O
in O
- O
domain O
dataset.•We O
empirically O
assess O
the O
models O
’ O
ability O
to O
: O
learn O
from O
multi O
- O
domain O
datasets O
by O
pool O
- O
ing O
all O
available O
training O
datasets O
; O
and O
adaptto O
a O
new O
, O
unseen O
domain O
by O
limited O
feedingamount O
of O
in O
- O
domain O
data O
. O
We O
review O
related O
works O
in O
Section2 O
. O

The O
RNN O
- O
based O
Sequence O
to O
Sequencemodels O
have O
applied O
to O
solve O
variety O
of O
tasks O
: O
conversational O
modeling O
( O
Vinyals O
and O
Le,2015;Li O
et O
al O
. O
,2015,2016 O
) O
, O
machine O
translation O
( O
Luonget O
al O
. O
,2015;Li O
and O
Jurafsky,2016)For O
task O
- O
oriented O
dialogue O
systems O
, O
Wen O
et O
al.(2015a O
) O
combined O
a O
forward O
RNN O
generator O
, O
aCNN O
reranker O
, O
and O
a O
backward O
RNN O
reranker O
togenerate O
utterances O
. O
Wen O
et O
al.(2015b O
) O
proposedSC O
- O
LSTM B-MethodName
generator O
which O
introduced O
a O
controlsigmoid O
gate O
to O
the O
LSTM B-MethodName
cell O
to O
jointly O
learn O
thegating O
mechanism O
and O
language O
model O
. O

The O
Encoderpart O
is O
a O
BiLSTM B-MethodName
, O
the O
Aligner O
is O
an O
attentionmechanism O
over O
the O
encoded O
inputs O
, O
and O
the O
De O
- O
coder O
is O
the O
proposed O
RALSTM B-MethodName
model O
condi O
- O
tioned O
on O
a O
1 O
- O
hot O
representation O
vectors O
. O

At O
time O
stept O
, O
while O
the O
Reﬁne O
- O
ment O
cell O
computes O
new O
input O
tokensxtbased O
onthe O
original O
input O
tokens O
and O
the O
attentional O
DArepresentationdt O
, O
the O
Adjustment O
Cell O
calculateshow O
much O
information O
of O
the O
slot O
- O
value O
pairs O
canbe O
generated O
by O
the O
LSTM B-MethodName
Cell.3.1 O
EncoderThe O
slots O
and O
values O
are O
separated O
parameters O
usedin O
the O
encoder O
side O
. O

In O
this O
work O
, O
we O
use O
a O
1 O
- O
layer O
, O
BidirectionalLSTM B-MethodName
( O
Bi O
- O
LSTM B-MethodName
) O
to O
encode O
the O
sequence O
of O
slot O
- O
value O
pairs4embedding O
. O

The O
Bi O
- O
LSTM B-MethodName
consistsof O
forward O
and O
backward O
LSTMs B-MethodName
which O
read O
thesequence O
of O
slot O
- O
value O
pairs O
from O
left O
- O
to O
- O
right O
andright O
- O
to O
- O
left O
to O
produce O
forward O
and O
backward O
se O
- O
quence O
of O
hidden O
states O
( O
 !e1 O
, O
. O
. O
, O
 !eL O
) O
, O
and O
( O
 e1 O
, O
. O
. O
, O
 eL),respectively O
. O

The O
alignmentmodelais O
computed O
by O
: O
a(ei O
, O
ht 1)=v O
> O
atanh(Waei+Uaht 1)(5)whereva O
, O
Wa O
, O
Uaare O
the O
weight B-HyperparameterName
matrices O
to O
learn O
. O
Finally O
, O
the O
Aligner O
calculates O
dialogue O
act O
embed O
- O
dingdtas O
follows O
: O
dt O
= O
a Xi t O
, O
iei(6)whereais O
vector O
embedding O
of O
the O
action O
type.3.3 O
RALSTM B-MethodName
DecoderThe O
proposed O
semantic O
RALSTM B-MethodName
cell O
applied O
forDecoder O
side O
consists O
of O
three O
components O
: O
a O
Re-ﬁnement O
cell O
, O
a O
traditional O
LSTM B-MethodName
cell O
, O
and O
an O
Ad O
- O
justment O
cell O
: O
Firstly O
, O
instead O
of O
feeding O
the O
original O
input O
to O
- O
kenwtinto O
the O
RNN O
cell O
, O
the O
input O
is O
recomputedby O
using O
a O
semantic O
gate O
as O
follows O
: O
rt= (Wrddt+Wrhht 1)xt O
= O
rt wt(7)whereWrdandWrhare O
weight B-HyperparameterName
matrices O
. O
Element O
- O
wise O
multiplication plays O
a O
part O
inword O
- O
level O
matching O
which O
not O
only O
learns O
thevector O
similarity O
, O
but O
also O
preserves O
informationabout O
the O
two O
vectors O
. O
Wrhacts O
like O
a O
key O
phrasedetector O
that O
learns O
to O
capture O
the O
pattern O
of O
gener O
- O
ation O
tokens O
or O
the O
relationship O
between O
multipletokens O
. O

By O
this O
way O
, O
we O
canrepresent O
the O
whole O
sentence O
based O
on O
the O
reﬁnedinputs O
. O
Secondly O
, O
the O
traditional O
LSTM B-MethodName
network O
pro O
- O
posed O
byHochreiter O
and O
Schmidhuber(2014 O
) O
inwhich O
the O
input O
gateii O
, O
forget O
gateftand O
outputgatesotare O
introduced O
to O
control O
information O
ﬂowand O
computed O
as O
follows:0BB@itftotˆct1CCA=0BB@   tanh1CCAW4n,4n0@xtdtht 11A(8)wherenis O
hidden B-HyperparameterName
layer I-HyperparameterName
size O
, O
W4n,4nis O
model O
pa O
- O
rameters O
. O

The O
cell O
memory O
valuectis O
modiﬁed O
todepend O
on O
the O
DA O
representation O
as O
: O
ct O
= O
ft ct 1+it ˆct+ O
tanh(Wcrrt)˜ht O
= O
ot tanh(ct)(9)where˜htis O
the O
output O
. O
Thirdly O
, O
inspired O
by O
work O
ofWen O
et O
al.(2015b)in O
which O
the O
generator O
was O
further O
conditioned O
ona O
1 O
- O
hot O
representation O
vectorsof O
given O
dialogueact O
, O
and O
work O
ofLu O
et O
al.(2016 O
) O
that O
proposed O
avisual O
sentinel O
gate O
to O
make O
a O
decision O
on O
whetherthe O
model O
should O
attend O
to O
the O
image O
or O
to O
the O
sen O
- O
tinel O
gate O
, O
an O
additional O
gating O
cell O
is O
introducedon O
top O
of O
the O
traditional O
LSTM B-MethodName
to O
gate O
anothercontrolling O
vectors O
. O

First O
, O
starting O
fromthe O
1 O
- O
hot O
vector O
of O
the O
DAs0 O
, O
at O
each O
time O
steptthe O
proposed O
cell O
computes O
how O
much O
the O
LSTMoutput˜htaffects B-MethodName
the O
DA O
vector O
, O
which O
is O
computedas O
follows O
: O
at= (Waxxt+Wah˜ht)st O
= O
st 1 at(10)whereWax O
, O
Wahare O
weight B-HyperparameterName
matrices O
to O
belearned.atis O
called O
anAdjustmentgate O
since O
itstask O
is O
to O
control O
what O
information O
of O
the O
given O
DAhave O
been O
generated O
and O
what O
information O
shouldbe O
retained O
for O
future O
time O
steps O
. O

Final O
RALSTM B-MethodName
output O
isa O
combination O
of O
both O
outputs O
of O
the O
traditionalLSTM B-MethodName
cell O
and O
the O
Adjustment O
cell O
, O
and O
computedas O
follows O
: O
ht=˜ht+˜ha(12)Finally O
, O
the O
output O
distribution O
is O
computed O
byapplying O
a O
softmax O
functiong O
, O
and O
the O
distributioncan O
be O
sampled O
to O
obtain O
the O
next O
token O
, O
P(wt+1|wt O
, O
. O

] O
denotes O
the O
Attention O
- O
based O
Encoder O
- O
Decoder O
model O
. O
The O
best O
and O
second O
best O
models O
highlighted O
inboldanditalicface O
, O
respectively O
. O
ModelRestaurant O
Hotel O
Laptop O
TVBLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERRHLSTM B-MethodName
0.7466 B-MetricValue
0.74 O
% O
0.8504 O
2.67 O
% O
0.5134 O
1.10 O
% O
0.5250 O
2.50%SCLSTM B-MethodName
0.7525 O
0.38 O
% O
0.8482 O
3.07 O
% O
0.5116 O
0.79 O
% O
0.5265 O
2.31%Enc O
- O
Dec]0.7398 O
2.78 O
% O
0.8549 O
4.69 O
% O
0.5108 O
4.04 O
% O
0.5182 O
3.18%w O
/ O
o O
A]0.7651 O
0.99 O
% O
0.8940 O
1.82 O
% O
0.5219 O
1.64 O
% O
0.5296 O
2.40%w O
/ O
o O
R]0.7748 O
0.22%0.8944 O
0.48%0.5235 O
0.57%0.5350 O
0.72%RALSTM]0.7789 B-MethodName
0.16%0.8981 O
0.43%0.5252 O
0.42%0.5406 O
0.63%Table O
3 O
: O
Performance O
comparison O
of O
the O
proposed O
models O
on O
four O
datasets O
in O
terms O
of O
the O
BLEU B-MetricName
and O
theerror O
rate O
ERR(% O
) O
scores B-MetricName
. O

The O
results O
were O
averaged B-MetricName
over O
5 B-HyperparameterValue
randomly O
initialized O
networks.bolddenotesthe O
best O
model O
. O
ModelRestaurant O
Hotel O
Laptop O
TVBLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERRw O
/ O
o O
A O
0.7619 B-MetricValue
2.26 O
% O
0.8913 O
1.85 O
% O
0.5180 O
1.81 O
% O
0.5270 O
2.10%w O
/ O
o O
R O
0.7733 O
0.23 O
% O
0.8901 O
0.59 O
% O
0.5208 O
0.60 O
% O
0.5321 O
0.50%RALSTM0.7779 B-MethodName
0.20%0.8965 O
0.58%0.5231 O
0.50%0.5373 O
0.49%differ O
depending O
on O
the O
initialization B-HyperparameterName
, O
we O
also O
re O
- O
port O
the O
results O
which O
were O
averaged B-MetricName
over O
5 O
ran O
- O
domly O
initialized O
networks O
. O

This O
paper O
presents O
a O
RecurrentNeural O
Network O
based O
Encoder O
- O
Decoderarchitecture O
, O
in O
which O
an O
LSTM B-MethodName
- O
based O
de O
- O
coder O
is O
introduced O
to O
select O
, O
aggregate O
se O
- O
mantic O
elements O
produced O
by O
an O
attentionmechanism O
over O
the O
input O
elements O
, O
andto O
produce O
the O
required O
utterances O
. O

A O
re O
- O
curring O
problem O
in O
such O
systems O
is O
requiring O
anno O
- O
tated O
datasets O
for O
particular O
dialogue O
acts1(DAs).To O
ensure O
that O
the O
generated O
utterance O
representingthe O
intended O
meaning B-MetricName
of O
the O
given O
DA O
, O
the O
previ O
- O
ous O
RNN O
- O
based O
models O
were O
further O
conditionedon O
a O
1 O
- O
hot O
vector O
representation O
of O
the O
DA.Wenet O
al.(2015a O
) O
introduced O
a O
heuristic O
gate O
to O
en O
- O
sure O
that O
all O
the O
slot O
- O
value O
pair O
was O
accurately O
cap O
- O
tured O
during O
generation O
. O
Wen O
et O
al.(2015b O
) O
sub O
- O
sequently O
proposed O
a O
Semantically O
ConditionedLong O
Short O
- O
term O
Memory O
generator O
( O
SC O
- O
LSTM)which B-MethodName
jointly O
learned O
the O
DA O
gating O
signal O
andlanguage O
model O
. O
More O
recently O
, O
Encoder O
- O
Decoder O
networks(Vinyals O
and O
Le,2015;Li O
et O
al O
. O
,2015 O
) O
, O
especiallythe O
attentional O
based O
models O
( O
Wen O
et O
al O
. O
,2016b;Mei O
et O
al O
. O
,2015 O
) O
have O
been O
explored O
to O
solvethe O
NLG O
tasks O
. O

In O
order O
tobetter O
select O
, O
aggregate O
and O
control O
the O
seman O
- O
tic O
information O
, O
a O
Reﬁnement O
Adjustment O
LSTM B-MethodName
- O
based O
component O
( O
RALSTM B-MethodName
) O
is O
introduced O
to O
the1A O
combination O
of O
an O
action O
type O
and O
a O
set O
of O
slot O
- O
valuepairs O
. O

To O
sum O
up O
, O
wemake O
three O
key O
contributions O
in O
this O
paper:•We O
present O
an O
LSTM B-MethodName
- O
based O
componentcalledRALSTMcell B-MethodName
applied O
on O
the O
decoderside O
of O
an O
ARED O
model O
, O
resulting O
in O
an O
end O
- O
to O
- O
end O
generator O
that O
empirically O
shows O
sig O
- O
niﬁcant O
improved O
performances O
in O
compari O
- O
son O
with O
the O
previous O
approaches.•We O
extensively O
conduct O
the O
experiments O
toevaluate O
the O
models O
training O
from O
scratch O
oneach O
in O
- O
domain O
dataset.•We O
empirically O
assess O
the O
models O
’ O
ability O
to O
: O
learn O
from O
multi O
- O
domain O
datasets O
by O
pool O
- O
ing O
all O
available O
training O
datasets O
; O
and O
adaptto O
a O
new O
, O
unseen O
domain O
by O
limited O
feedingamount O
of O
in O
- O
domain O
data O
. O
We O
review O
related O
works O
in O
Section2 O
. O

The O
RNN O
- O
based O
Sequence O
to O
Sequencemodels O
have O
applied O
to O
solve O
variety O
of O
tasks O
: O
conversational O
modeling O
( O
Vinyals O
and O
Le,2015;Li O
et O
al O
. O
,2015,2016 O
) O
, O
machine O
translation O
( O
Luonget O
al O
. O
,2015;Li O
and O
Jurafsky,2016)For O
task O
- O
oriented O
dialogue O
systems O
, O
Wen O
et O
al.(2015a O
) O
combined O
a O
forward O
RNN O
generator O
, O
aCNN O
reranker O
, O
and O
a O
backward O
RNN O
reranker O
togenerate O
utterances O
. O
Wen O
et O
al.(2015b O
) O
proposedSC O
- O
LSTM B-MethodName
generator O
which O
introduced O
a O
controlsigmoid O
gate O
to O
the O
LSTM B-MethodName
cell O
to O
jointly O
learn O
thegating O
mechanism O
and O
language O
model O
. O

The O
Encoderpart O
is O
a O
BiLSTM B-MethodName
, O
the O
Aligner O
is O
an O
attentionmechanism O
over O
the O
encoded O
inputs O
, O
and O
the O
De O
- O
coder O
is O
the O
proposed O
RALSTM B-MethodName
model O
condi O
- O
tioned O
on O
a O
1 O
- O
hot O
representation O
vectors O
. O

At O
time O
stept O
, O
while O
the O
Reﬁne O
- O
ment O
cell O
computes O
new O
input O
tokensxtbased O
onthe O
original O
input O
tokens O
and O
the O
attentional O
DArepresentationdt O
, O
the O
Adjustment O
Cell O
calculateshow O
much O
information O
of O
the O
slot O
- O
value O
pairs O
canbe O
generated O
by O
the O
LSTM B-MethodName
Cell.3.1 O
EncoderThe O
slots O
and O
values O
are O
separated O
parameters O
usedin O
the O
encoder O
side O
. O

In O
this O
work O
, O
we O
use O
a O
1 O
- O
layer O
, O
BidirectionalLSTM B-MethodName
( O
Bi O
- O
LSTM B-MethodName
) O
to O
encode O
the O
sequence O
of O
slot O
- O
value O
pairs4embedding O
. O

The O
Bi O
- O
LSTM B-MethodName
consistsof O
forward O
and O
backward O
LSTMs B-MethodName
which O
read O
thesequence O
of O
slot O
- O
value O
pairs O
from O
left O
- O
to O
- O
right O
andright O
- O
to O
- O
left O
to O
produce O
forward O
and O
backward O
se O
- O
quence O
of O
hidden O
states O
( O
 !e1 O
, O
. O
. O
, O
 !eL O
) O
, O
and O
( O
 e1 O
, O
. O
. O
, O
 eL),respectively O
. O

The O
alignmentmodelais O
computed O
by O
: O
a(ei O
, O
ht 1)=v O
> O
atanh(Waei+Uaht 1)(5)whereva O
, O
Wa O
, O
Uaare O
the O
weight B-HyperparameterName
matrices O
to O
learn O
. O
Finally O
, O
the O
Aligner O
calculates O
dialogue O
act O
embed O
- O
dingdtas O
follows O
: O
dt O
= O
a Xi t O
, O
iei(6)whereais O
vector O
embedding O
of O
the O
action O
type.3.3 O
RALSTM B-MethodName
DecoderThe O
proposed O
semantic O
RALSTM B-MethodName
cell O
applied O
forDecoder O
side O
consists O
of O
three O
components O
: O
a O
Re-ﬁnement O
cell O
, O
a O
traditional O
LSTM B-MethodName
cell O
, O
and O
an O
Ad O
- O
justment O
cell O
: O
Firstly O
, O
instead O
of O
feeding O
the O
original O
input O
to O
- O
kenwtinto O
the O
RNN O
cell O
, O
the O
input O
is O
recomputedby O
using O
a O
semantic O
gate O
as O
follows O
: O
rt= (Wrddt+Wrhht 1)xt O
= O
rt wt(7)whereWrdandWrhare O
weight B-HyperparameterName
matrices O
. O
Element O
- O
wise O
multiplication plays O
a O
part O
inword O
- O
level O
matching O
which O
not O
only O
learns O
thevector O
similarity O
, O
but O
also O
preserves O
informationabout O
the O
two O
vectors O
. O
Wrhacts O
like O
a O
key O
phrasedetector O
that O
learns O
to O
capture O
the O
pattern O
of O
gener O
- O
ation O
tokens O
or O
the O
relationship O
between O
multipletokens O
. O

By O
this O
way O
, O
we O
canrepresent O
the O
whole O
sentence O
based O
on O
the O
reﬁnedinputs O
. O
Secondly O
, O
the O
traditional O
LSTM B-MethodName
network O
pro O
- O
posed O
byHochreiter O
and O
Schmidhuber(2014 O
) O
inwhich O
the O
input O
gateii O
, O
forget O
gateftand O
outputgatesotare O
introduced O
to O
control O
information O
ﬂowand O
computed O
as O
follows:0BB@itftotˆct1CCA=0BB@   tanh1CCAW4n,4n0@xtdtht 11A(8)wherenis O
hidden B-HyperparameterName
layer I-HyperparameterName
size O
, O
W4n,4nis O
model O
pa O
- O
rameters O
. O

The O
cell O
memory O
valuectis O
modiﬁed O
todepend O
on O
the O
DA O
representation O
as O
: O
ct O
= O
ft ct 1+it ˆct+ O
tanh(Wcrrt)˜ht O
= O
ot tanh(ct)(9)where˜htis O
the O
output O
. O
Thirdly O
, O
inspired O
by O
work O
ofWen O
et O
al.(2015b)in O
which O
the O
generator O
was O
further O
conditioned O
ona O
1 O
- O
hot O
representation O
vectorsof O
given O
dialogueact O
, O
and O
work O
ofLu O
et O
al.(2016 O
) O
that O
proposed O
avisual O
sentinel O
gate O
to O
make O
a O
decision O
on O
whetherthe O
model O
should O
attend O
to O
the O
image O
or O
to O
the O
sen O
- O
tinel O
gate O
, O
an O
additional O
gating O
cell O
is O
introducedon O
top O
of O
the O
traditional O
LSTM B-MethodName
to O
gate O
anothercontrolling O
vectors O
. O

First O
, O
starting O
fromthe O
1 O
- O
hot O
vector O
of O
the O
DAs0 O
, O
at O
each O
time O
steptthe O
proposed O
cell O
computes O
how O
much O
the O
LSTMoutput˜htaffects B-MethodName
the O
DA O
vector O
, O
which O
is O
computedas O
follows O
: O
at= (Waxxt+Wah˜ht)st O
= O
st 1 at(10)whereWax O
, O
Wahare O
weight B-HyperparameterName
matrices O
to O
belearned.atis O
called O
anAdjustmentgate O
since O
itstask O
is O
to O
control O
what O
information O
of O
the O
given O
DAhave O
been O
generated O
and O
what O
information O
shouldbe O
retained O
for O
future O
time O
steps O
. O

Final O
RALSTM B-MethodName
output O
isa O
combination O
of O
both O
outputs O
of O
the O
traditionalLSTM B-MethodName
cell O
and O
the O
Adjustment O
cell O
, O
and O
computedas O
follows O
: O
ht=˜ht+˜ha(12)Finally O
, O
the O
output O
distribution O
is O
computed O
byapplying O
a O
softmax O
functiong O
, O
and O
the O
distributioncan O
be O
sampled O
to O
obtain O
the O
next O
token O
, O
P(wt+1|wt O
, O
. O

] O
denotes O
the O
Attention O
- O
based O
Encoder O
- O
Decoder O
model O
. O
The O
best O
and O
second O
best O
models O
highlighted O
inboldanditalicface O
, O
respectively O
. O
ModelRestaurant O
Hotel O
Laptop O
TVBLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERRHLSTM B-MethodName
0.7466 B-MetricValue
0.74 O
% O
0.8504 O
2.67 O
% O
0.5134 O
1.10 O
% O
0.5250 O
2.50%SCLSTM B-MethodName
0.7525 O
0.38 O
% O
0.8482 O
3.07 O
% O
0.5116 O
0.79 O
% O
0.5265 O
2.31%Enc O
- O
Dec]0.7398 O
2.78 O
% O
0.8549 O
4.69 O
% O
0.5108 O
4.04 O
% O
0.5182 O
3.18%w O
/ O
o O
A]0.7651 O
0.99 O
% O
0.8940 O
1.82 O
% O
0.5219 O
1.64 O
% O
0.5296 O
2.40%w O
/ O
o O
R]0.7748 O
0.22%0.8944 O
0.48%0.5235 O
0.57%0.5350 O
0.72%RALSTM]0.7789 B-MethodName
0.16%0.8981 O
0.43%0.5252 O
0.42%0.5406 O
0.63%Table O
3 O
: O
Performance O
comparison O
of O
the O
proposed O
models O
on O
four O
datasets O
in O
terms O
of O
the O
BLEU B-MetricName
and O
theerror O
rate O
ERR(% O
) O
scores B-MetricName
. O

The O
results O
were O
averaged B-MetricName
over O
5 B-HyperparameterValue
randomly O
initialized O
networks.bolddenotesthe O
best O
model O
. O
ModelRestaurant O
Hotel O
Laptop O
TVBLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERR O
BLEU B-MetricName
ERRw O
/ O
o O
A O
0.7619 B-MetricValue
2.26 O
% O
0.8913 O
1.85 O
% O
0.5180 O
1.81 O
% O
0.5270 O
2.10%w O
/ O
o O
R O
0.7733 O
0.23 O
% O
0.8901 O
0.59 O
% O
0.5208 O
0.60 O
% O
0.5321 O
0.50%RALSTM0.7779 B-MethodName
0.20%0.8965 O
0.58%0.5231 O
0.50%0.5373 O
0.49%differ O
depending O
on O
the O
initialization B-HyperparameterName
, O
we O
also O
re O
- O
port O
the O
results O
which O
were O
averaged B-MetricName
over O
5 O
ran O
- O
domly O
initialized O
networks O
. O

More O
- O
over O
, O
a O
comparison O
between O
the O
models O
with O
gat O
- O
ing O
the O
DA O
vector O
also O
indicates O
that O
the O
proposedmodels O
( O
w/o O
R O
, O
RALSTM B-MethodName
) O
have O
signiﬁcant O
im O
- O
proved O
performance O
on O
both O
the O
evaluation O
metricsacross O
the O
four O
domains O
compared O
to O
theSCLSTMmodel B-MethodName
. O

TheRALSTMcell B-MethodName
without O
the O
Reﬁne O
- O
ment O
cell O
is O
similar O
as O
theSCLSTMcell B-MethodName
. O

How O
- O
ever O
, O
it O
obtained O
the O
results O
much O
better O
than O
theSCLSTM B-MethodName
baselines O
. O

This O
stipulates O
the O
neces O
- O
sary O
of O
the O
LSTM B-MethodName
encoder O
and O
the O
Aligner O
in O
ef O
- O
fectively O
partial O
learning O
the O
correlated O
order O
be O
- O
tween O
slot O
- O
value O
representation O
in O
the O
DAs O
, O
espe O
- O
cially O
for O
the O
unseen O
domain O
where O
there O
is O
onlyone O
training O
example O
for O
each O
DA O
. O

Table3fur O
- O
ther O
demonstrates O
the O
stable O
strength O
of O
our O
mod O
- O
els O
since O
the O
results O
’ O
pattern O
stays O
unchanged O
com O
- O
pared O
to O
those O
in O
Table2.Figure3shows O
a O
comparison O
of O
three O
models(Enc O
- O
Dec O
, O
SCLSTM B-MethodName
, O
andRALSTM B-MethodName
) O
which O
weretrained O
from O
scratch O
on O
the O
unseen O
laptop O
domainin O
varied O
proportion O
of O
training O
data O
, O
from1%to100 O
% O
. O

It O
clearly O
shows O
that O
theRALSTMoutper B-MethodName
- O
forms O
the O
previous O
models O
in O
all O
cases O
, O
while O
theEnc O
- O
Dechas O
a O
much O
greater O
ERR O
score B-MetricName
comparingto O
the O
two O
models O
. O
A O
comparison O
of O
top O
responses O
generated O
forsome O
input O
DAs O
between O
different O
models O
areshown O
in O
Table4 O
. O

which O
would O
you O
preferHLSTM B-MethodName
the O
satellite O
notus O
19 O
is O
a O
budget O
priced O
laptop O
with O
a O
500 O
gb O
drive O
and O
a128 O
gb O
drive O
. O

which O
one O
do O
you O
prefer O
[ O
128 O
gb]SCLSTM B-MethodName
the O
satellite O
notus O
19 O
is O
in O
the O
budget O
price O
range O
and O
has O
a O
500 O
gb O
drive O
and O
the O
portege O
thanatos O
98 O
has O
a O
128 O
gbdrive O
. O

which O
one O
do O
you O
prefer O
[ O
expensive]RALSTM B-MethodName
the O
satellite O
notus O
19 O
is O
in O
the O
budget O
price O
range O
with O
a O
500 O
gb O
drive O
, O
while O
the O
portege O
thanatos O
98 O
is O
in O
theexpensive O
price O
range O
and O
has O
a O
128 O
gb O
drive O
. O

which O
one O
do O
you O
prefer O
# O
Model O
Generated O
Responses O
from O
TV O
Domain2Input O
DAinformcount(count=”73 O
” O
, O
type=”television O
” O
, O
hasusbport=”dontcare O
” O
, O
hdmiport=”2 O
” O
, O
screensizerange=”dontcare”)Referencethere O
are O
73 O
televisions O
if O
you O
do O
not O
care O
about O
whether O
it O
has O
usb O
port O
or O
not O
, O
with O
2 O
hdmi O
ports O
and O
if O
you O
do O
notcare O
about O
the O
screen O
sizeHLSTM B-MethodName
there O
are O
73 O
televisions O
with O
2 O
hdmi O
ports O
and O
if O
you O
do O
not O
care O
about O
the O
number O
of O
usb O
ports O
[ O
screensizerange]Enc O
- O
Dec O
there O
are O
73 O
televisions O
with O
2 O
hdmi O
ports O
if O
you O
do O
not O
care O
about O
usb O
ports O
[ O
screensizerange]SCLSTM B-MethodName
there O
are O
73 O
televisions O
with O
2 O
hdmi O
ports O
if O
you O
do O
not O
care O
about O
the O
screen O
size O
and O
if O
you O
do O
not O
care O
about O
thenumber O
of O
usb O
portsRALSTM B-MethodName
there O
are O
73 O
televisions O
with O
2 O
hdmi O
ports O
if O
you O
do O
not O
care O
about O
screen O
size O
or O
usb O
portsAll2Tv O
there O
are O
73 O
televisions O
with O
2 O
hdmi O
ports O
if O
you O
do O
not O
care O
about O
screen O
size O
or O
usb O
portsproduce O
some O
errors O
( O
missing O
and O
misplaced O
in O
- O
formation O
) O
, O
the O
proposed O
models O
( O
RALSTM B-MethodName
andthe O
modelsAll2*trained O
by O
pooling O
all O
datasetstogether O
) O
can O
generate O
appropriate O
sentences O
. O

The O
re O
- O
sults O
are O
consistent O
with O
the O
Figure3 O
, O
in O
which O
theRALSTMhas B-MethodName
better O
performance O
than O
theEnc O
- O
DecandSCLSTMon B-MethodName
all O
domains O
in O
terms O
of O
the O
BLEUand B-MetricName
the O
ERR O
scores B-MetricName
, O
while O
theEnc O
- O
Dechas O
difﬁ-culties O
in O
reducing O
the O
ERR O
score B-MetricName
. O

This O
indicatesthe O
relevant O
contribution O
of O
the O
proposed O
compo O
- O
nent O
Reﬁnement O
and O
Adjustment O
cells O
to O
the O
orig O
- O
inal O
ARED O
architecture O
, O
in O
which O
the O
Reﬁnementwith O
attentional O
gating O
can O
effectively O
select O
andaggregate O
the O
information O
before O
putting O
them O
intothe O
traditional O
LSTM B-MethodName
cell O
, O
while O
the O
Adjustmentwith O
gating O
DA O
vector O
can O
effectively O
control O
the(a O
) O
An O
example O
from O
the O
Laptop O
domain O
. O

TheRALSTMmodel B-MethodName
outperforms O
the O
pre O
- O
vious O
model O
in O
both O
cases O
where O
the O
sufﬁcient O
in O
- O
domain O
data O
is O
used O
( O
as O
in O
Figure5 O
- O
left O
) O
and O
thelimited O
in O
- O
domain O
data O
is O
used O
( O
Figure5 O
- O
right).The O
Figure5 O
- O
rightalso O
indicates O
that O
theRALSTMmodel B-MethodName
can O
adapt O
to O
a O
new O
, O
unseen O
domain O
fasterthan O
the O
previous O
models.6 O
Conclusion O
and O
Future O
WorkWe O
present O
an O
extension O
of O
ARED O
model O
, O
inwhich O
an O
RALSTM B-MethodName
component O
is O
introduced O
toselect O
and O
aggregate O
semantic O
elements O
producedby O
the O
Encoder O
, O
and O
to O
generate O
the O
required O
sen O
- O
tence O
. O

They O
use O
a O
Support O
Vector O
Machine O
( O
SVM B-MethodName
) O
classiﬁer O
- O
LIBLINEAR O
implementation O
( O
Fan O
et O
al O
. O
, O
2008 O
) O
along O
with O
31 O
lexical O
and O
syntactic O
features O
, O
to O
distinguish O
between O
the O
anaphoric O
and O
the O
non O
- O
anaphoric O
class O
. O

For O
the O
remaining O
26 O
anaphors O
, O
either O
the O
antecedent O
isT1 O
T2 O
T3 O
T4 O
T50200400600800 B-MethodName
766 O
87596 O
3 O
POS B-TaskName
String O
TemplateNo O
. O

For O
the O
latter O
, O
we O
use O
BERT B-MethodName
( O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
) O
base O
uncased O
word O
- O
piece O
model O
for O
English O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
it O
currently O
provides O
the O
most O
powerful O
word O
embeddings O
taking O
into O
account O
a O
large O
left O
and O
right O
context O
. O

For O
both O
the O
subtasks O
, O
we O
experiment O
with O
a O
simple O
Multilayer O
Perceprton O
( O
MLP O
) O
and O
bidirectional O
Long O
Short O
Term O
Memory O
( O
bi O
- O
LSTM B-MethodName
) O
networks O
. O

For O
the O
bi O
- O
LSTM B-MethodName
, O
we O
have O
embedding O
layer O
, O
timedistributed O
translate O
layer O
, O
Bi O
- O
LSTM B-MethodName
( O
RNN O
) O
layer O
, O
batch O
normalization O
layer O
, O
dropout B-HyperparameterName
layer O
and O
prediction O
layer O
. O

In O
case O
of O
BERT B-MethodName
, O
we O
ﬁne O
tune O
the O
pretrained O
BERT B-MethodName
model O
. O

We O
also O
treat O
the O
result O
as O
incorrect O
when O
the O
system O
gives O
multiple O
antecedents O
for O
the O
same O
one O
- O
anaphora O
( O
as O
currently O
there O
is O
no O
way O
the O
system O
can O
make O
a O
decision O
in O
such O
a O
case).Our O
experiments O
show O
that O
, O
the O
pre O
- O
trained O
ﬁne O
tuned O
BERT B-MethodName
model O
renders O
robust O
and O
high O
scores B-MetricName
for O
both O
the O
subtasks O
. O

This O
is O
expected O
as O
BERT B-MethodName
has O
been O
previously O
shown O
to O
give O
promising O
scores B-MetricName
on O
a O
number O
of O
classiﬁcation O
tasks O
. O

Now O
, O
instead O
of O
the O
gold O
vectors O
, O
the O
resolution O
model O
is O
fed O
the O
one O
- O
Task O
P O
R O
F O
One O
- O
Anaphora O
Detection O
FT O
, O
MLP O
65.49 O
79.35 O
71.76 O
FT O
, O
Bi O
- O
LSTM B-MethodName
64.22 O
71.74 O
67.77 O
BERT B-MethodName
( O
ﬁne O
- O
tuned O
) O
78.87 O
89.35 O
83.78 O
Antecedent O
Selection O
FT O
, O
MLP O
55.24 O
61.29 O
58.11 O
FT O
, O
Bi O
- O
LSTM B-MethodName
58.97 O
65.74 O
62.17 O
BERT B-MethodName
( O
ﬁne O
- O
tuned O
) O
63.07 O
72.33 O
67.38 O
Final O
Model O
See O
Figure O
2 O
. O

For O
word O
and O
context O
representation O
, O
we O
experimented O
with O
pre O
- O
trained O
fastText O
and O
BERT B-MethodName
word O
embeddings O
. O

Reference O
- O
based O
metrics O
such O
as O
ROUGE B-MetricName
or O
BERTScore B-MetricName
evaluate O
the O
content O
quality O
of O
a O
summary O
by O
comparing O
the O
summary O
to O
a O
reference O
. O

In O
this O
work O
, O
we O
analyze O
the O
token O
alignments O
used O
by O
ROUGE B-MetricName
and O
BERTScore B-MetricName
to O
compare O
summaries O
and O
argue O
that O
their O
scores B-MetricName
largely O
can O
not O
be O
interpreted O
as O
measuring O
information O
overlap O
. O

on O
their O
lexical O
overlap O
, O
whereas O
more O
recent O
methods O
, O
such O
as O
BERTScore B-MetricName
( O
Zhang O
et O
al O
. O
, O
2020 B-MetricValue
) O
, O
do O
so O
based O
on O
the O
similarity O
of O
the O
summary O
tokens O
’ O
contextualized O
word O
embeddings O
. O

However O
, O
it O
is O
not O
clear O
whether O
metrics O
such O
as O
ROUGE B-MetricName
and O
BERTScore B-MetricName
evaluate O
summaries O
based O
on O
how O
much O
information O
they O
have O
in O
common O
with O
the O
reference O
or O
some O
less O
desirable O
dimension O
of O
similarity O
, O
such O
as O
whether O
the O
two O
summaries O
discuss O
the O
same O
topics O
( O
see O
Fig O
. O

In O
this O
work O
, O
we O
demonstrate O
that O
ROUGE B-MetricName
and O
BERTScore B-MetricName
largely O
do O
not O
measure O
how O
much O
information O
two O
summaries O
have O
in O
common O
. O

Our O
analysis O
casts O
ROUGE B-MetricName
and O
BERTScore B-MetricName
into O
a O
uniﬁed O
framework O
in O
which O
the O
similarity O
of O
two O
summaries O
is O
calculated O
based O
on O
an O
alignment O
be O
- O
tween O
the O
summaries O
’ O
tokens O
( O
§ O
3 B-MetricValue
) O
. O

Overall O
, O
both O
analyses O
support O
the O
conclusion O
that O
ROUGE B-MetricName
and O
BERTScore B-MetricName
largely O
do O
not O
measure O
information O
overlap O
. O

By O
viewing O
QAEval O
as O
inducing O
an O
alignment O
between O
two O
summaries O
and O
reasoning O
about O
its O
behavior O
, O
we O
demonstrate O
evidence O
that O
it O
measures O
information O
overlap O
much O
more O
strongly O
than O
either O
ROUGE B-MetricName
or O
BERTScore B-MetricName
does O
, O
supporting O
that O
QA O
- O
based O
metrics O
are O
a O
promising O
direction O
for O
future O
research O
( O
§ O
7 B-MetricValue
) O
. O

The O
contributions O
of O
this O
work O
include O
( O
1 O
) O
an O
analysis O
which O
reveals O
that O
ROUGE B-MetricName
and O
BERTScore B-MetricName
largely O
do O
not O
measure O
the O
information O
overlap O
between O
two O
summaries O
, O
( O
2 B-MetricValue
) O
evidence O
that O
many O
other O
evaluation O
metrics O
likely O
suffer O
from O
the O
problem O
, O
and O
( O
3 O
) O
preliminary O
results O
which O
show O
that O
QAEval O
does O
measure O
information O
quality O
better O
than O
ROUGE B-MetricName
and O
BERTScore B-MetricName
. O

Metrics O
such O
as O
ROUGE B-MetricName
and O
BERTScore B-MetricName
calculate O
the O
similarity O
of O
two O
summaries O
either O
by O
how O
much O
lexical O
overlap O
they O
have O
or O
how O
similar O
the O
summaries O
’ O
contextual O
word O
embeddings O
are O
( O
discussed O
in O
more O
detail O
in O
§ O
3 B-MetricValue
) O
. O

3 O
A O
Common O
Framework O
The O
focus O
of O
our O
analysis O
will O
be O
primarily O
on O
two O
evaluation O
metrics O
, O
ROUGE B-MetricName
and O
BERTScore B-MetricName
. O

Since O
a O
unigram O
may O
appear O
multiple O
times O
in O
a O
summary O
, O
the O
alignment O
may O
not O
be O
unique O
, O
however O
, O
its O
weight B-HyperparameterName
will O
equal O
M. O
BERTScore B-MetricName
calculates O
a O
similarity O
score B-MetricName
between O
two O
pieces O
of O
text O
based O
on O
the O
pairwise O
cosine O
similarities O
of O
their O
tokens O
’ O
BERT B-MethodName
embeddings O
. O

To O
calculate O
recall B-MetricName
, O
BERTScore B-MetricName
ﬁrst O
aligns O
every O
reference O
token O
to O
its O
most O
- O
similar O
summary O
token O
( O
Eq O
. O

AR O
= O
f(i;j;B O
ij O
) O
: O
8i;j= O
arg O
max O
kBikg(2 O
) O
BERTScore B-MetricName
Recall B-MetricName
= O
W(AR)=m O
( O
3 B-MetricValue
) O
A O
similar O
procedure O
is O
followed O
to O
calculate O
precision B-MetricName
, O
but O
instead O
, O
every O
summary O
token O
is O
aligned O
to O
its O
most O
- O
similar O
reference O
token O
, O
and O
the O
sum O
of O
the O
similarities O
is O
normalized O
by O
the O
number O
of O
summary O
tokens O
. O

Because O
the O
token O
alignments O
are O
selected O
via O
the O
max O
operation O
, O
BERTScore B-MetricName
’s O
alignment O
is O
unique O
, O
unlike O
for O
ROUGE B-MetricName
. O

By O
formulating O
ROUGE B-MetricName
and O
BERTScore B-MetricName
in O
a O
framework O
based O
on O
token O
alignments O
, O
we O
can O
reason O
about O
their O
behaviors O
by O
examining O
the O
tokens O
2Our O
analysis O
focuses O
on O
the O
unigram O
variant O
of O
ROUGE B-MetricName
, O
called O
ROUGE-1 B-MetricName
. O

These O
annotated O
phrases O
can O
be O
used O
to O
reason O
about O
ROUGE B-MetricName
and O
BERTScore B-MetricName
: O
If O
a O
large O
proportion O
of O
their O
token O
alignments O
is O
between O
phrases O
that O
express O
the O
same O
information O
, O
then O
their O
scores B-MetricName
can O
potentially O
be O
interpreted O
as O
representing O
the O
summaries O
’ O
information O
overlap O
. O

The O
distribution O
of O
the O
proportion O
of O
ROUGE B-MetricName
and O
BERTScore B-MetricName
explained O
by O
SCU O
matches O
is O
presented O
in O
Figure O
3 B-MetricValue
. O

Since O
only O
a O
relatively O
small O
fraction O
of O
the O
overall O
metric O
scores B-MetricName
comes O
from O
phrases O
with O
the O
same O
information O
, O
this O
suggests O
that O
ROUGE B-MetricName
and O
BERTScore B-MetricName
’s O
values O
can O
not O
be O
interpreted O
as O
a O
measure O
of O
information O
overlap O
. O

5 O
Category O
- O
Based O
Analysis O
The O
second O
analysis O
of O
ROUGE B-MetricName
and O
BERTScore B-MetricName
focuses O
on O
grouping O
token O
alignments O
into O
categories O
( O
§ O
5.1 B-MetricValue
) O
, O
then O
using O
those O
categories O
to O
reasonabout O
how O
much O
of O
the O
metrics O
’ O
scores B-MetricName
is O
explained O
by O
information O
or O
topic O
matches O
( O
§ O
5.2 O
) O
. O

For O
example O
, O
a O
“ O
noun O
” O
category O
would O
select O
only O
the O
token O
indices O
that O
correspond O
to O
nouns O
. O
C(S)denotes O
the O
application O
of O
a O
category O
to O
summary O
S. O
Each O
category O
is O
used O
to O
ﬁlter O
an O
alignment O
A O
used O
by O
ROUGE B-MetricName
or O
BERTScore B-MetricName
to O
a O
categoryspeciﬁc O
alignment O
between O
tokens O
which O
belong O
to O
that O
category O
only O
, O
denoted O
AC O
: O
AC O
= O
f(i;j;w O
) O
: O
( O
i;j;w O
) O
2A O
; O
i2C(R);j2C(S)g(6 O
) O
For O
the O
“ O
noun O
” O
category O
, O
ACwould O
be O
the O
subset O
of O
token O
alignments O
between O
nouns O
in O
RandS. O

Then O
, O
the O
contribution O
ofCis O
deﬁned O
as O
the O
ratio O
between O
ACandA O
: O
Contribution O
C O
= O
W(AC O
) O
W(A)(7 O
) O
The O
contribution O
of O
Ccan O
be O
interpreted O
as O
the O
portion O
of O
ROUGE B-MetricName
or O
BERTScore B-MetricName
that O
can O
be O
explained O
by O
matches O
between O
tokens O
in O
category O
C O
( O
see O
Fig O
. O

7 O
) O
of O
every O
category O
to O
ROUGE B-MetricName
( O
R O
) O
and O
BERTScore B-MetricName
( O
BS O
) O
on O
TAC O
2008 B-MetricValue
and O
CNN O
/ O
DailyMail O
indicate O
the O
metrics O
are O
largely O
matching O
nouns O
and O
stopwords O
rather O
than O
tuples O
which O
express O
information O
( O
e.g. O
, O
VB+NSUBJ O
+ O
DOBJ O
) O
. O

rics O
largely O
follow O
the O
same O
trend O
: O
Noun- O
and O
stopword O
- O
based O
matches O
explain O
the O
vast O
majority O
of O
the O
token O
alignments O
used O
by O
both O
ROUGE B-MetricName
and O
BERTScore B-MetricName
, O
whereas O
the O
dependency O
tuple O
categories O
explain O
very O
little O
of O
the O
overall O
scores.3For B-MetricName
instance O
, O
on O
TAC O
2008 B-MetricValue
, O
noun O
phrase O
and O
stopword O
matches O
contribute O
58.7 O
% O
and O
54.6 O
% O
to O
ROUGE B-MetricName
, O
whereas O
the O
dependency O
tuple O
with O
the O
largest O
contribution O
, O
VB+DOBJ O
only O
contributes O
1.3 B-MetricValue
% O
. O

When O
the O
speciﬁc O
categories O
are O
grouped O
by O
content O
type O
in O
Table O
2 O
, O
it O
becomes O
even O
more O
apparent O
that O
topic O
and O
stopwords O
matches O
explain O
most O
of O
ROUGE B-MetricName
and O
BERTScore.4We B-MetricName
ﬁnd O
that O
topic O
, O
stopword O
, O
and O
information O
matches O
explain O
70.6 B-MetricValue
% O
, O
54.6 O
% O
, O
and O
2.2 O
% O
of O
ROUGE B-MetricName
on O
TAC’08 O
. O

6 O
Other O
Evaluation O
Metrics O
The O
analyses O
thus O
far O
have O
exploited O
the O
structure O
of O
ROUGE B-MetricName
and O
BERTScore B-MetricName
to O
reason O
about O
the O
extent O
to O
which O
they O
measure O
information O
overlap O
between O
two O
summaries O
. O

Score O
 O
ROUGE-1 B-MetricName
1.00 O
0.59 O
Pyramid O
Score O
0.59 O
1.00 O
AutoSummENG O
0.83 O
0.61 O
0.22 O
BERTScore B-MetricName
0.74 B-MetricValue
0.59 O
0.15 O
BEwT O
- O
E O
0.81 O
0.62 O
0.19 O
MeMoG O
0.68 O
0.52 O
0.16 O
METEOR O
0.91 O
0.63 O
0.28 O
MoverScore O
0.79 O
0.61 O
0.18 O
NPowER O
0.81 O
0.60 O
0.21 O
PyrEval O
0.47 O
0.35 O
0.12 O
QAEval O
- O
F O
1 O
0.59 O
0.57 O
0.02 O
ROUGE-2 B-MetricName
0.79 O
0.58 O
0.21 O
S30.92 O
0.63 O
0.29 O
Table O
3 O
: O
The O
summary O
- O
level O
Pearson O
correlations B-MetricName
of O
various O
metrics O
to O
ROUGE-1 B-MetricName
and O
the O
Pyramid O
Score O
( O
 O
is O
the O
difference O
between O
them O
) O
. O

In O
the O
next O
section O
, O
we O
run O
a O
preliminary O
analysis O
of O
QAEval O
to O
understand O
if O
this O
recently O
proposed O
metric O
behaves O
different O
than O
ROUGE B-MetricName
and O
BERTScore B-MetricName
. O

In O
this O
section O
, O
we O
brieﬂy O
examine O
QAEval O
and O
demonstrate O
evidence O
that O
it O
addresses O
some O
of O
the O
shortcomings O
of O
ROUGE B-MetricName
and O
BERTScore B-MetricName
. O

We O
ﬁnd O
that O
QAEval O
can O
be O
explained O
by O
SCU O
matches O
far O
more O
than O
ROUGE B-MetricName
or O
BERTScore B-MetricName
on O
average B-MetricName
. O

The O
average B-MetricName
proportion O
, O
42 B-MetricValue
% O
, O
is O
higher O
than O
ROUGE B-MetricName
( O
25 B-MetricValue
% O
) O
and O
BERTScore B-MetricName
( O
15 B-MetricValue
% O
) O
, O
indicating O
QAEval O
captures O
information O
similarity O
better O
than O
the O
other O
two O
metrics O
. O

6 O
summarizes O
this O
distribution O
for O
QAEval O
, O
ROUGE B-MetricName
, O
and O
BERTScore B-MetricName
( O
Fig O
. O

3 O
) O
, O
demonstrating O
that O
46 O
% O
of O
summaries O
have O
at O
least O
50 O
% O
of O
their O
QAEval O
score B-MetricName
explained O
by O
SCU O
matches O
, O
whereas O
this O
is O
true O
for O
less O
than O
4 B-MetricValue
% O
and O
0 O
% O
of O
ROUGE B-MetricName
and O
BERTScore B-MetricName
. O

Overall O
, O
this O
experiment O
provides O
preliminary O
evidence O
that O
QAEval O
is O
indeed O
measuring O
information O
quality O
more O
than O
either O
ROUGE B-MetricName
or O
BERTScore B-MetricName
, O
and O
suggests O
that O
QA O
- O
based O
evaluation O
metrics O
are O
an O
exciting O
direction O
for O
future O
research O
. O

Our O
work O
can O
be O
viewed O
as O
more O
direct O
evidence O
about O
what O
ROUGE B-MetricName
and O
BERTScore B-MetricName
measure O
. O

10 O
Conclusion O
In O
this O
work O
, O
we O
argued O
that O
ROUGE B-MetricName
, O
BERTScore B-MetricName
, O
and O
many O
other O
proposed O
metrics O
for O
evaluating O
the O
content O
quality O
of O
summaries O
largely O
do O
not O
compare O
summaries O
based O
on O
their O
information O
overlap O
. O

We O
explored O
recent O
deep O
learning O
approaches O
for O
the O
recognition O
of O
sentiment O
, O
such O
as O
Bidirectional O
Long O
Short O
- O
Term O
Memory O
( O
BiLSTM B-MethodName
) O
and O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
( O
BERT B-MethodName
) O
. O

The O
key O
contribution O
of O
these O
studies O
includes O
: O
Detailed O
description O
of O
the O
procedure O
of O
building O
PolEmo O
2.0 O
: O
manually O
annotated O
corpus O
of O
consumer O
reviews O
from O
4 O
domains O
( O
medicine O
, O
school O
, O
hotels O
, O
products O
) O
at O
2 O
levels O
of O
sentiment O
granularity O
( O
document O
, O
sentence O
) O
; O
Detailed O
analysis O
of O
manual O
annotation O
with O
regard O
to O
frequently O
occurring O
errors O
; O
Development O
of O
methods O
based O
on O
deep O
learning O
( O
BiLSTM B-MethodName
, O
BERT B-MethodName
) O
, O
adapted O
to O
PolEmo O
2.0 O
corpus O
, O
also O
using O
sentiment O
lexicon O
generated O
from O
plWordNet B-DatasetName
4.0 O
Emo O
; O
Performing O
tests O
on O
sets O
prepared O
for O
the O
analysis O
of O
the O
quality O
of O
methods O
( O
1 O
) O
evaluated O
on O
texts O
within O
a O
given O
domain O
, O
( O
2 O
) O
evaluated O
on O
texts O
from O
various O
domains O
( O
3 O
) O
trained O
on O
texts O
that O
do O
not O
include O
a O
given O
domain O
and O
tested O
on O
a O
given O
domain O
; O
Comparison O
of O
deep O
learning O
methods O
with O
classic O
methods O
( O
Logistic B-MethodName
Regression I-MethodName
) O
, O
especially O
in O
the O
context O
of O
the O
ability O
to O
generalize O
the O
problem O
of O
recognizing O
sentiment O
and O
providing O
semantic O
representation O
, O
which O
is O
as O
independent O
of O
the O
domain O
as O
possible O
; O
Making O
PolEmo O
2.0 O
corpus O
available O
under O
an O
open O
license.2 O
Related O
Work O
There O
are O
several O
well O
- O
known O
resources O
annotated O
with O
sentiment O
for O
English O
, O
e.g. O
: O
MPQA O
3.0 O
( O
Deng O
and O
Wiebe O
, O
2015 O
) O
, O
the O
Stanford B-DatasetName
Sentiment I-DatasetName
Treebank I-DatasetName
( O
Socher O
et O
al O
. O
, O
2013 O
) O
, O
Amazon O
Product O
Data O
( O
He O
and O
McAuley O
, O
2016 O
) O
, O
Pros O
And O
Cons O
Dataset O
( O
Ganapathibhotla O
and O
Liu O
, O
2008 O
) O
, O
corpora O
developed O
within O
the O
Semantic O
Evaluation O
workshops O
( O
Nakov O
et O
al O
. O
, O
2016 O
; O
Pontiki O
et O
al O
. O
, O
2016 O
) O
, O
SentiWordNet B-DatasetName
( O
Baccianella O
et O
al O
. O
, O
2010 O
) O
or O
Opinion O
Lexicon O
( O
Hu O
and O
Liu O
, O
2004 O
) O
. O

Most O
of O
the O
systems O
participating O
in O
the O
PolEval2017 O
competition O
used O
Tree O
LSTM B-MethodName
adapted O
to O
dependency O
trees O
, O
including O
the O
best O
system O
, O
which O
reached O
an O
accuracy B-MetricName
of O
79 B-MetricValue
% O
on O
this O
data O
. O

The O
SVM B-MethodName
model O
trained O
on O
a O
subset O
of O
1500 O
texts O
( O
containing O
equal O
amounts O
of O
hate O
speech O
and O
non O
- O
hate O
speech O
) O
obtained O
the O
precision B-MetricName
of O
0.8 B-MetricValue
( O
Troszy O
´ O
nski O
and O
Wawer O
, O
2017 O
) O
. O

Using O
SVM B-MethodName
with O
a O
rich O
set O
of O
features O
we O
obtained O
90,06 O
% O
( O
F1 B-MetricName
- O
score B-MetricName
) O
in O
the O
task O
of O
distinguishing O
between O
genuine O
SNs O
, O
counterfeited O
SNs O
and O
nonletters O
. O

4 O
Multi O
- O
Level O
Sentiment O
Recognition O
Recently O
deep O
neural B-MethodName
networks I-MethodName
show O
relatively O
good O
performance O
among O
all O
available O
methods O
of O
processing O
such O
information O
( O
Glorot O
et O
al O
. O
, O
2011 O
) O
. O

We O
selected O
the O
same O
classiﬁers O
for O
the O
recognition O
tasks O
as O
in O
( O
Koco O
´ O
n O
et O
al O
. O
, O
2019 O
): O
( O
1 O
) O
Logistic B-MethodName
Regression I-MethodName
as O
a O
fastText O
recognition O
model O
( O
Joulin O
et O
al O
. O
, O
2017 O
) O
with O
KGR10 O
word O
embeddings O
( O
Koco´n O
and O
Gawor O
, O
2018 O
) O
providing O
a O
baseline O
for O
text O
classiﬁcation O
; O
( O
2 O
) O
BiLSTM B-MethodName
( O
Zhou O
et O
al O
. O
, O
2016 O
) O
in O
two O
variants O
: O
KGR10 O
embeddings O
as O
features O
only O
and O
KGR10 O
embeddings O
extended O
with O
general O
polarity O
information O
from O
sentiment O
dictionary O
described O
in O
( O
Koco O
´ O
n O
et O
al O
. O
, O
2019 O
) O
; O
( O
3 O
) O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
with O
additional O
sequence O
classiﬁcation O
layer O
. O

We O
changed O
the O
architecture O
of O
BiLSTM B-MethodName
and O
BERT B-MethodName
architecture O
. O

In O
case O
of O
BiLSTM B-MethodName
, O
instead O
of O
ﬁxed O
input O
length O
we O
changed O
the O
model O
to O
work O
with O
text O
of O
any O
length O
. O

The O
input O
tensor O
shape O
is O
( O
None O
, O
300 O
) O
for O
embedding O
- O
only O
variant O
( O
BiLSTM B-MethodName
) O
and O
( O
None O
, O
306 O
) O
for O
embedding+dictionary O
variant O
( O
BiLSTMd B-MethodName
) O
. O

Next O
layers O
remain O
the O
same O
, O
i.e. O
( O
1 O
) O
BiLSTM B-MethodName
layer O
with O
1024 O
hidden B-HyperparameterName
units I-HyperparameterName
, O
( O
2 B-HyperparameterValue
) O
dropout B-HyperparameterName
layer O
( O
ratio O
0.2 B-HyperparameterValue
) O
. O

For O
BERT B-MethodName
we O
used O
the O
same O
architecture O
as O
in O
( O
Koco O
´ O
n O
et O
al O
. O
, O
2019 O
) O
for O
the O
whole O
texts O
, O
but O
we O
changed O
it O
for O
sentences O
. O

In O
case O
of O
evaluation O
for O
a O
single O
domain O
for O
each O
label O
, O
fastText O
( O
using O
Logistic B-MethodName
Regression I-MethodName
) O
outperformed O
other O
classiﬁers O
in O
16 O
out O
of O
28 O
distinguishable O
cases O
. O

BERT B-MethodName
classiﬁer O
performs O
much O
better O
( O
14 O
out O
of O
28 O
cases O
) O
in O
domain O
- O
out O
knowledge O
transfer O
evaluation O
( O
DOT O
) O
. O

7 O
Conclusions O
BERT B-MethodName
’s O
performance O
is O
below O
the O
expectations O
of O
this O
advanced O
method O
in O
case O
of O
the O
classiﬁcation O
of O
the O
whole O
texts O
. O

Looking O
at O
both O
tables O
( O
8 O
and O
9 O
) O
, O
BERT B-MethodName
’s O
results O
are O
the O
best O
in O
64 O
out O
of O
182 O
label O
- O
speciﬁc O
cases O
. O

BiLSTM B-MethodName
outperformed O
other O
methods O
in O
48 O
cases O
. O

Overall O
BiLSTM B-MethodName
performance O
is O
better O
in O
88 O
out O
of O
182 O
cases O
. O

BERT B-MethodName
dominance O
( O
when O
distinguishing O
between O
BiLSTM B-MethodName
and O
BiLSTMd B-MethodName
) O
is O
observed O
in O
DOT O
and O
all O
sentence O
cases O
. O

The O
values O
of O
the O
general O
F1 B-MetricName
, O
micro O
AUC B-MetricName
and O
macro O
AUC B-MetricName
are O
the O
highest O
for O
BiLSTM B-MethodName
variants O
( O
see O
Table O
6 B-MetricValue
) O
. O

ing O
using O
the O
ELMo B-MethodName
deep O
word O
representations O
method O
( O
Peters O
et O
al O
. O
, O
2018 O
) O
, O
with O
the O
use O
of O
the O
large O
KGR10 O
corpus O
presented O
in O
work O
( O
Koco O
´ O
n O
et O
al O
. O
, O
2019a O
) O
. O

We O
also O
want O
to O
train O
the O
basic O
BERT B-MethodName
model O
with O
the O
use O
of O
KGR10 O
to O
investigate O
whether O
it O
will O
improve O
the O
quality O
of O
sentiment O
recognition O
. O

This O
objective O
can O
be O
achieved O
by O
other O
complex O
methods O
such O
as O
OpenAI B-MethodName
GPT-2 B-MethodName
( O
Radford O
et O
al O
. O
, O
2019 O
) O
and O
domain O
dictionaries O
construction O
methods O
utilising O
WordNet B-DatasetName
( O
Koco O
´ O
n O
and O
Marci O
´ O
nczuk O
, O
2016 O
) O
. O

One O
of O
our O
motivations O
for O
our O
submission O
to O
this O
CoNLL O
2017 O
multilingual O
dependency O
parsing O
shared O
task O
is O
to O
provide O
a O
baseline O
to O
evaluate O
to O
what O
extent O
recent O
advances O
in O
neural B-MethodName
network I-MethodName
models O
and O
training O
do O
in O
fact O
improve O
performance O
over O
“ O
traditional O
” O
recurrent O
neural B-MethodName
networks I-MethodName
. O

But O
in O
the O
case O
of O
neural B-MethodName
networks I-MethodName
, O
it O
is O
common O
to O
decompose O
the O
parametrisation O
of O
these O
features O
into O
a O
matrix O
for O
the O
feature O
role O
( O
e.g. O
front O
- O
of O
- O
the O
- O
queue O
) O
and O
a O
vector O
for O
the O
feature O
value O
( O
e.g. O
a O
word O
) O
. O

Recent O
advances O
in O
optimisation O
methods O
for O
neural B-MethodName
networks I-MethodName
— O
such O
as O
AdaGrad O
and O
mini O
- O
batch O
– O
are O
obvious O
modiﬁcations O
to O
compare O
against O
the O
reported O
results O
. O

We O
use O
CNN O
and O
bi O
- O
directional O
LSTM B-MethodName
based O
models O
with O
attention O
. O

4.3 O
Settings O
and O
Hyperparameters O
We O
use O
a O
300 O
- O
dimenstional O
embedding O
layer O
which O
is O
initialized O
with O
FastText O
( O
Joulin O
et O
al O
. O
, O
2016 O
) O
based O
free O
- O
trained O
embeddings O
for O
both O
CNN O
and O
the O
bi O
- O
directional O
LSTM B-MethodName
based O
models O
. O
We O
use O
a O
128 O
- O
dimensional O
hidden B-HyperparameterName
layer I-HyperparameterName
for O
the O
bidirectional O
LSTM B-MethodName
and O
a O
32 B-HyperparameterValue
- O
dimensional O
ﬁlter O
with O
kernels O
of O
sizef1;3;5;7gfor O
CNN O
. O

Dataset O
LSTM(% B-MethodName
) O
LSTM+ASWA(% B-MethodName
) O
LSTM+NASWA(% B-MethodName
) O
IMDB O
89.1 O
( O
1.34 O
) O
90.2 O
( O
0.32 O
) O
90.3 O
( O
0.17 O
) O
Diabetes O
87.7 O
( O
1.44 O
) O
87.7 O
( O
0.60 O
) O
87.8 O
( O
0.55 O
) O
SST O
81.9 O
( O
1.11 O
) O
82.0 O
( O
0.60 O
) O
82.1 O
( O
0.57 O
) O
Anemia O
91.6 O
( O
0.49 O
) O
91.8 O
( O
0.34 O
) O
91.9 O
( O
0.36 O
) O
AgNews O
95.5 O
( O
0.32 O
) O
96.1 O
( O
0.17 O
) O
96.1 O
( O
0.10 O
) O
Tweet O
84.7 O
( O
1.79 O
) O
83.8 O
( O
0.45 O
) O
83.9 O
( O
0.45 O
) O
Table O
3 O
: O
Performance O
statistics O
obtained O
from O
10 O
differently O
seeded O
LSTM B-MethodName
based O
models O
. O

We O
observe O
similar O
results O
for O
the O
LSTM B-MethodName
based O
models O
in O
Table O
3 O
. O

We O
observe O
similar O
behaviours O
for O
the O
LSTM B-MethodName
based O
models O
in O
Figure O
3b O
. O

We O
observe O
similar O
, O
but O
relatively O
worse O
results O
for O
the O
LSTM B-MethodName
based O
models O
in O
Figure O
4b O
. O

trend O
of O
unstable O
attention O
distributions O
over O
both O
CNN O
and O
LSTM B-MethodName
based O
attention O
distribution O
. O

Our O
results O
on O
LSTM B-MethodName
based O
models O
are O
provided O
in O
the O
attached O
supplementary O
material O
. O

We O
note O
that O
the O
observations O
for O
LSTM B-MethodName
models O
are O
, O
in O
most O
cases O
, O
similar O
to O
the O
behaviour O
of O
the O
CNN O
based O
models O
. O

Thedogaredogs O
< O
E O
> O
barkings1s2s3dog O
Figure O
4 O
: O
The O
neural O
baseline O
system O
for O
track O
2 O
of O
task O
2 O
: O
A O
bidirectional O
LSTM B-MethodName
encoder O
, O
conditioned O
on O
embeddings O
of O
the O
left O
context O
word O
The O
, O
right O
context O
word O
areand O
a O
whole O
token O
embedding O
of O
the O
lemma O
dog O
, O
is O
used O
to O
encode O
the O
character O
sequence O
( O
d O
, O
o O
, O
g O
) O
into O
representation O
vectors O
s1 O
, O
s2ands3 O
. O

An O
LSTM B-MethodName
decoder O
with O
an O
attention O
mechanism O
generates O
the O
contextually O
appropriate O
output O
word O
form O
dogs O
. O

A O
bidirectional O
LSTM B-MethodName
encoder O
is O
used O
to O
encode O
the O
lemma O
into O
representation O
vectors O
. O

The O
input O
vectors O
e1 O
; O
: O
: O
: O
; O
e O
mare O
then O
encoded O
into O
representations O
s1 O
; O
: O
: O
: O
; O
s O
mby O
a O
bidirectional O
LSTM B-MethodName
encoder O
. O

Finally O
, O
a O
decoder O
with O
additive O
attention O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
is O
used O
for O
generating O
the O
output O
word O
form O
w O
= O
w1 O
; O
: O
: O
: O
; O
w O
nbased O
on O
the O
representations O
s1 O
; O
: O
: O
: O
; O
s O
m. O
The O
baseline O
system O
uses O
100 O
- O
dimensional O
embeddings O
and O
the O
LSTM B-MethodName
hidden O
dimension O
for O
both O
the O
encoder O
and O
decoder O
is O
of O
size O
100 O
. O

Both O
encoder O
and O
decoder O
LSTM B-MethodName
networks O
are O
single O
layer O
networks O
. O

During O
training O
, O
30 O
% O
dropout B-HyperparameterName
is O
applied O
on O
all O
input O
and O
recurrent O
connections O
in O
the O
encoder O
and O
decoder O
LSTM B-MethodName
networks O
. O

More O
recently O
, O
neural O
models O
of O
composition O
( O
Socher O
et O
al O
. O
, O
2012 O
; O
Bowman O
et O
al O
. O
, O
2015 O
) O
and O
large O
networks O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
have O
been O
extremely O
successful O
. O

3 O
Methods O
3.1 O
Density O
Matrix O
Models O
We O
now O
introduce O
the O
three O
novel O
methods O
that O
we O
propose O
for O
building O
density O
matrices.3.1.1 O
BERT2DM B-MethodName
Algorithm O
1 O
: O
BERT2DM B-MethodName
training O
foreach O
sentence O
sin O
a O
corpus O
do O
Processswith O
BERT B-MethodName
. O

Extract O
and O
store O
the O
contextualised O
embeddings O
produced O
by O
BERT B-MethodName
for O
the O
words O
ins O
. O

Apply O
PCA B-MethodName
/ O
SVD O
to O
the O
remaining O
contextualised O
embeddings O
. O

foreach O
wordvin O
the O
vocabulary O
do O
Compute O
the O
density O
matrix O
of O
vas O
JvK O
= O
X O
i2ind(v)  O
! O
vi  O
! O
vi O
> O
; O
where O
ind(v)are O
the O
indices O
at O
which O
the O
wordvoccurs O
in O
the O
corpus O
and  O
! O
vi O
is O
the O
reduced O
embedding O
for O
v. O
end O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
produces O
contextualised O
embeddings O
for O
words O
and O
sentences O
. O

Given O
a O
sentence O
, O
it O
produces O
vectors O
for O
each O
word O
that O
are O
speciﬁc O
to O
that O
particular O
context O
( O
BERT B-MethodName
actually O
models O
subword O
units O
, O
but O
we O
average B-MetricName
the O
subword O
embeddings O
of O
a O
word O
to O
obtain O
a O
contextualised O
word O
embedding O
) O
. O

BERT2DM B-MethodName
uses O
the O
contextualised O
embeddings O
of O
BERT B-MethodName
to O
build O
density O
matrices O
that O
encode O
multiple O
senses O
of O
a O
word O
. O

BERT B-MethodName
is O
applied O
to O
a O
corpus O
and O
the O
contextualised O
embeddings O
for O
a O
word O
ware O
combined O
to O
compute O
w O
’s O
density O
matrix O
according O
to O
equation O
( O
1 O
) O
. O

Since O
the O
vectors O
produced O
by O
BERT B-MethodName
are O
fairly O
large O
, O
we O
apply O
a O
dimensionality O
reduction O
step O
( O
either O
PCA B-MethodName
or O
SVD O
) O
over O
all O
content O
word O
embeddings O
before O
combining O
to O
form O
a O
density O
matrix O
. O

3.1.2 O
Word2DM O
Word2DM O
is O
an O
extension O
of O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013a O
, O
b O
) O
skip O
- O
gram O
with O
negative O
samplingAlgorithm O
2 O
: O
Word2DM O
training O
foreach O
wordvin O
the O
vocabulary O
do O
Randomly O
initialise O
a O
nmmatrixBv O
. O

The O
objective O
function O
at O
each O
target O
- O
context O
prediction O
is O
then O
: O
J( O
) O
= O
log(tr(AtAc))+KX O
k=1log( tr(AtAwk O
) O
) O
( O
2 O
) O
whereAtandAcare O
the O
the O
density O
matrices O
of O
the O
target O
and O
context O
words O
respectively O
, O
A1;A2;:::;A O
Kare O
the O
density O
matrices O
of O
Knegative O
samples O
, O
and O
is O
the O
set O
of O
weights B-HyperparameterName
of O
the O
intermediary O
matrices O
Bt;BcandB1;B2;:::;B O
K. O
Word2DM O
is O
a O
straightforward O
extension O
of O
Word2Vec B-MethodName
for O
learning O
density O
matrices O
. O

The O
objective O
function O
being O
maximised O
( O
equation O
4 O
) O
has O
the O
same O
gradient O
as O
Word2Vec B-MethodName
and O
therefore O
does O
not O
lead O
to O
suboptimal O
training O
updates.3.2 O
Composition O
methods O
The O
composition O
methods O
we O
use O
are O
based O
on O
methods O
in O
( O
Lewis O
, O
2019a O
; O
Coecke O
and O
Meichanetzidis O
, O
2020 O
) O
. O

For O
our O
word O
embedding O
baselines O
we O
use O
embeddings O
produced O
by O
three O
existing O
models O
- O
Word2Vec B-MethodName
, O
GloVe B-MethodName
, O
and O
FastText O
. O

For O
the O
phrase O
big O
house O
with O
vectors  O
! O
bigand   !house O
the O
different O
compositional O
distributional O
methods O
will O
be O
computed O
as O
follows O
: O
Add O
: O
  O
! O
big+   !house O
Mult O
: O
  O
! O
big O
 O
   !house O
Tensor O
: O
  O
! O
big  O
! O
big>   !house O
4.1.2 O
Sentence O
encoders O
We O
compare O
our O
models O
to O
two O
well O
- O
known O
neural O
sentence O
encoders O
- O
InferSent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
and O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

We O
also O
compare O
our O
models O
to O
BERT B-MethodName
as O
a O
sentence O
encoder O
. O

BERT B-MethodName
produces O
an O
embedding O
for O
the O
entire O
sentence O
by O
adding O
a O
special O
classiﬁcation O
token O
( O
[ O
CLS O
] O
) O
to O
the O
start O
of O
every O
sequence O
. O

When O
BERT B-MethodName
is O
used O
in O
a O
sentence O
- O
level O
task O
, O
the O
[ O
CLS O
] O
embedding O
can O
be O
used O
as O
a O
semantic O
representation O
for O
the O
entire O
sentence O
. O

For O
the O
pre O
- O
trained O
word O
embeddings O
required O
for O
step O
1 O
of O
the O
above O
procedure O
, O
we O
use O
17dimensional O
word O
embeddings O
, O
trained O
with O
the O
gensim O
implementation4of O
Word2Vec B-MethodName
on O
the O
combined O
ukWaC+Wackypedia O
corpus O
. O

We O
present O
results O
for O
four O
different O
BERT2DM B-MethodName
models O
. O

Two O
of O
these O
cluster O
the O
BERT B-MethodName
representations O
into O
senses O
before O
dimensionality O
reduction O
, O
while O
the O
other O
two O
do O
not O
. O

We O
also O
vary O
the O
dimensionality O
reduction O
algorithm O
between O
PCA B-MethodName
and O
SVD O
, O
to O
test O
whether O
or O
not O
centering O
the O
contextual O
embeddings O
before O
dimensionality O
reduction O
makes O
any O
difference O
. O

Training O
BERT2DM B-MethodName
takes O
only O
a O
few O
hours O
on O
a O
16 O
- O
core O
CPU O
( O
Intel O
Xeon O
Gold O
6130 O
) O
but O
requires O
around O
4.5 O
GB O
of O
memory O
per O
1 O
million O
words O
that O
it O
is O
trained O
on O
. O

For O
multi O
- O
sense O
Word2DM O
and O
BERT2DM B-MethodName
we O
trained O
four O
models O
each O
, O
with O
different O
hyperparameter O
settings O
( O
as O
described O
in O
section O
4.2 O
and O
listed O
in O
table O
2 O
) O
. O

RG O
WS O
MC O
SL O
MEN O
Word2Vec B-MethodName
.818 O
.662 O
.765 O
.404 O
.781 O
GloVe B-MethodName
.826 O
.571 O
.732 O
.399 O
.773 O
FastText O
.767 O
.517 O
.682 O
.404 O
.768 O
Context2DM O
.228 O
.234 O
.331 O
.094 O
.267 O
Word2DM O
.541 O
.473 O
.452 O
.157 O
.540 O
MS O
- O
Word2DM O
- O
cos O
, O
5 O
senses O
.768 O
.556 O
.670 O
.290 O
.680 O
- O
cos O
, O
10 O
senses O
.727 O
.580 O
.659 O
.256 O
.682 O
- O
dot O
, O
5 O
senses O
.662 O
.578 O
.568 O
.247 O
.663 O
- O
dot O
, O
10 O
senses O
.679 O
.596 O
.612 O
.281 O
.663 O
BERT2DM B-MethodName
- O
PCA B-MethodName
.452 O
.275 O
.388 O
.226 O
.351 O
- O
SVD O
.428 O
.317 O
.392 O
.234 O
.327 O
- O
PCA B-MethodName
+ O
cluster O
.383 O
.219 O
.381 O
.153 O
.251 O
- O
SVD O
+ O
cluster O
.315 O
.205 O
.294 O
.091 O
.207 O
Table O
2 O
: O
Spearman O
obtained O
on O
word O
similarity O
tasks O
. O

5http://compling.eecs.qmul.ac.uk/resources/Verb O
Mult O
Add O
Tensor O
Phaser O
Word2Vec B-MethodName
.215 O
.256 O
.299 O
.231 O
GloVe B-MethodName
.332 O
.098 O
.304 O
.397 O
FastText O
.181 O
.281 O
.198 O
.137 O
BERT B-MethodName
.140 O
InferSent1 O
.207 O
InferSent2 O
.174 O
Context2DM O
-.069 O
-.025 O
-.058 O
-.025 O
-.064 O
Word2DM O
-.022 O
.057 O
.010 O
.057 O
-.007 O
MS O
- O
Word2DM O
- O
cos O
, O
5 O
senses O
.235 O
.195 O
.254 O
.195 O
.328 O
- O
cos O
, O
10 O
senses O
.248 O
.204 O
.210 O
.204 O
.217 O
- O
dot O
, O
5 O
senses O
.216 O
.145 O
.280 O
.145 O
.311 O
- O
dot O
, O
10 O
senses O
.157 O
.195 O
.170 O
.195 O
.325 O
BERT2DM B-MethodName
- O
PCA B-MethodName
-.055 O
-.016 O
-.101 O
-.016 O
-.114 O
- O
SVD O
.072 O
.170 O
.075 O
.170 O
.067 O
- O
PCA B-MethodName
+ O
cluster O
.105 O
.170 O
.062 O
.170 O
.130 O
- O
SVD O
+ O
cluster O
.090 O
-.002 O
.057 O
-.002 O
.049 O
Table O
3 O
: O
Spearman O
obtained O
on O
ML2008 O
. O

The O
BERT2DM B-MethodName
models O
perform O
worst O
of O
all O
our O
models O
, O
but O
still O
demonstrate O
some O
ability O
to O
judge O
word O
similarity O
. O

There O
is O
no O
clear O
performance O
difference O
between O
using O
PCA B-MethodName
or O
SVD O
for O
dimensionality O
reduction O
. O

Clustering O
the O
BERT B-MethodName
representations O
before O
dimensionality O
reduction O
leads O
to O
worse O
correlation B-MetricName
scores B-MetricName
. O

Column O
headings O
specify O
the O
composition O
methods O
used O
to O
compute O
the O
phrase O
represen6https://github.com/mfaruqui/eval-word-vectorsVerb O
Mult O
Add O
Tensor O
Phaser O
Word2Vec B-MethodName
.209 O
.203 O
.268 O
.204 O
GloVe B-MethodName
.304 O
.211 O
.252 O
.256 O
FastText O
.210 O
.187 O
.154 O
.185 O
BERT B-MethodName
.266 O
InferSent1 O
.241 O
InferSent2 O
.194 O
Context2DM O
.037 O
-.027 O
.012 O
-.021 O
.036 O
Word2DM O
-.059 O
.001 O
.019 O
-.064 O
-.039 O
MS O
- O
Word2DM O
- O
cos O
, O
5 O
senses O
.187 O
.286 O
.206 O
.289 O
.365 O
- O
cos O
, O
10 O
senses O
.091 O
.237 O
.161 O
.200 O
.323 O
- O
dot O
, O
5 O
senses O
-.016 O
.091 O
-.002 O
.010 O
.077 O
- O
dot O
, O
10 O
senses O
-.021 O
.116 O
.025 O
.131 O
.118 O
BERT2DM B-MethodName
PCA B-MethodName
-.024 O
-.107 O
-.001 O
-.097 O
-.046 O
SVD O
-.105 O
-.304 O
-.078 O
-.057 O
-.074 O
PCA B-MethodName
+ O
cluster O
-.056 O
.030 O
.008 O
.007 O
-.028 O
SVD O
+ O
cluster O
.045 O
.013 O
.031 O
.037 O
-.029 O
Table O
4 O
: O
Spearman O
obtained O
on O
GS2011 O
. O

These O
do O
not O
apply O
to O
the O
sentence O
encoders O
( O
BERT B-MethodName
and O
InferSent O
) O
. O

In O
some O
cases O
BERT2DM B-MethodName
achieves O
correlation B-MetricName
scores B-MetricName
that O
are O
comparable O
to O
multi O
- O
sense O
Word2DM O
and O
the O
baselines O
. O

But O
in O
general O
the O
BERT2DM B-MethodName
density O
matrices O
can O
not O
reliably O
be O
used O
to O
achieve O
disambiguation O
. O

For O
a O
density O
matrix O
=P O
ipi  O
! O
vi  O
! O
vi O
> O
the O
VNE O
is O
deﬁned O
as O
S( O
) O
= O
 tr(ln O
): O
( O
5)Verb O
Mult O
Add O
Tensor O
Phaser O
Word2Vec B-MethodName
.270 O
.155 O
.334 O
.260 O
GloVe B-MethodName
.413 O
.219 O
.297 O
.231 O
FastText O
.302 O
.176 O
.175 O
.264 O
BERT B-MethodName
.471 O
InferSent1 O
.370 O
InferSent2 O
.372 O
Context2DM O
-.025 O
.015 O
-.063 O
-.033 O
.005 O
Word2DM O
-.047 O
.019 O
-.092 O
.043 O
.074 O
MS O
- O
Word2DM O
- O
cos O
, O
5 O
senses O
.266 O
.203 O
.270 O
.329 O
.500 O
- O
cos O
, O
10 O
senses O
.214 O
.208 O
.263 O
.304 O
.397 O
- O
dot O
, O
5 O
senses O
-.067 O
.068 O
-.103 O
.022 O
.126 O
- O
dot O
, O
10 O
senses O
-.059 O
.082 O
-.040 O
.068 O
.126 O
BERT2DM B-MethodName
- O
PCA B-MethodName
.025 O
.031 O
.122 O
.056 O
.056 O
- O
SVD O
-.071 O
.001 O
-.042 O
-.037 O
-.056 O
- O
PCA B-MethodName
+ O
cluster O
-.232 O
-.073 O
-.141 O
-.155 O
-.232 O
- O
SVD O
+ O
cluster O
-.132 O
-.117 O
-.154 O
-.089 O
-.172 O
Table O
5 O
: O
Spearman O
obtained O
on O
GS2012 O
. O

For O
these O
experiments O
, O
we O
only O
report O
results O
for O
one O
variant O
of O
multi O
- O
sense O
Word2DM O
( O
cosine O
similarity O
, O
5 O
senses O
) O
and O
two O
variants O
of O
BERT2DM B-MethodName
( O
SVD O
and O
PCA B-MethodName
) O
. O

The O
results O
show O
that O
both O
BERT2DM B-MethodName
and O
multisense O
Word2DM O
successfully O
encode O
how O
ambiguous O
words O
are O
. O

Word2DM O
exhibits O
a O
very O
low O
correlation B-MetricName
and O
Context2DM O
( O
not O
plotted O
) O
shows O
none O
. O
Verb O
Mult O
Add O
Tensor O
Phaser O
Word2Vec B-MethodName
.201 B-MetricValue
.222 O
.194 O
.190 O
GloVe B-MethodName
.152 O
.154 O
.127 O
.083 O
FastText O
.081 O
.285 O
.073 O
.196 O
BERT B-MethodName
.314 O
InferSent1 O
.187 O
InferSent2 O
.190 O
Context2DM O
-.017 O
-.074 O
-.037 O
-.064 O
-.006 O
Word2DM O
.149 O
-.118 O
.114 O
-.014 O
.081 O
MS O
- O
Word2DM O
- O
cos O
, O
5 O
senses O
.135 O
.075 O
.190 O
.147 O
.309 O
- O
cos O
, O
10 O
senses O
.171 O
.008 O
.207 O
.118 O
.288 O
- O
dot O
, O
5 O
senses O
-.026 O
.034 O
.039 O
.117 O
.275 O
- O
dot O
, O
10 O
senses O
.094 O
.016 O
.070 O
.053 O
.345 O
BERT2DM B-MethodName
- O
PCA B-MethodName
.084 O
.082 O
.089 O
.081 O
.187 O
- O
SVD O
.037 O
.160 O
.054 O
.128 O
.037 O
- O
PCA B-MethodName
+ O
cluster O
.075 O
.075 O
.073 O
.067 O
-.011 O
- O
SVD O
+ O
cluster O
.036 O
-.058 O
.020 O
.052 O
.017 O
Table O
6 O
: O
Spearman O
obtained O
on O
KS2013 O
- O
CoNLL O
. O

Model O
Pearson O
rSpearman O
Context2DM O
-.081 O
-.043 O
Word2DM O
.053 O
.112 O
MS O
- O
Word2DM O
.296 O
.295 O
BERT2DM B-MethodName
SVD O
.367 O
.418 O
BERT2DM B-MethodName
PCA B-MethodName
.405 O
.463 O
Table O
7 O
: O
Correlation B-MetricName
coefﬁcient O
between O
the O
VNE O
of O
a O
word O
’s O
density O
matrix O
and O
the O
number O
of O
WordNet B-DatasetName
synsets O
to O
which O
the O
word O
belongs O
. O

Verb O
Mult O
Add O
Tensor O
Phaser O
Context2DM O
0.344 O
0.354 O
0.429 O
0.354 O
0.007 O
Word2DM O
0.103 O
0.257 O
0.707 O
0.257 O
0.019 O
MS O
- O
Word2DM O
0.953 O
1.751 O
1.537 O
1.751 O
0.468 O
BERT2DM*0.667 B-MethodName
0.132 O
0.938 O
0.132 O
0.060 O
Table O
8 O
: O
Average B-MetricName
VNE O
before O
and O
after O
composition O
on O
ML2008 O
. O

* O
SVD O
variant O
Verb O
Mult O
Add O
Tensor O
Phaser O
Context2DM O
0.174 O
0.187 O
0.359 O
0.402 O
0.000 O
Word2DM O
0.245 O
0.522 O
1.096 O
0.797 O
0.038 O
MS O
- O
Word2DM O
1.444 O
1.821 O
1.906 O
4.012 O
0.224 O
BERT2DM*0.763 B-MethodName
0.008 O
0.961 O
0.174 O
0.003 O
Table O
9 O
: O
Average B-MetricName
VNE O
before O
and O
after O
composition O
on O
GS2011 O
. O

Equipped O
with O
a O
compositional O
frame O
- O
Verb O
Mult O
Add O
Tensor O
Phaser O
Context2DM O
0.167 O
0.137 O
0.417 O
0.387 O
0.000 O
Word2DM O
0.140 O
0.623 O
1.445 O
0.908 O
0.005 O
MS O
- O
Word2DM O
1.340 O
1.181 O
2.043 O
3.534 O
0.029 O
BERT2DM*0.659 B-MethodName
0.000 O
1.080 O
0.014 O
0.000 O
Table O
10 O
: O
Average B-MetricName
VNE O
before O
and O
after O
composition O
on O
GS2012 O
. O

* O
SVD O
variant O
Verb O
Mult O
Add O
Tensor O
Phaser O
Context2DM O
0.233 O
0.183 O
0.524 O
0.537 O
0.000 O
Word2DM O
0.161 O
0.604 O
1.405 O
0.889 O
0.010 O
MS O
- O
Word2DM O
1.224 O
1.453 O
2.032 O
3.704 O
0.033 O
BERT2DM*0.689 B-MethodName
0.000 O
1.124 O
0.026 O
0.000 O
Table O
11 O
: O
Average B-MetricName
VNE O
before O
and O
after O
composition O
on O
KS2013 O
- O
CoNLL O
. O

This O
revealed O
that O
the O
density O
matrices O
built O
by O
two O
of O
our O
models O
( O
multi O
- O
sense O
Word2DM O
and O
BERT2DM B-MethodName
) O
reﬂect O
true O
word O
level O
ambiguity O
. O

We O
test O
two O
methods O
of O
system O
combination O
: O
linear O
combination O
and O
an O
SVM B-MethodName
reranker O
. O

2.5 O
RNNs O
and O
synthetic O
training O
data O
Recurrent O
encoder O
- O
decoder O
neural B-MethodName
networks I-MethodName
( O
RNNs O
) O
can O
generate O
a O
target O
sequence O
given O
an O
input O
sequence O
. O

This O
ensembling O
process O
is O
a O
common O
technique O
intended O
to O
stabilize O
neural B-MethodName
networks I-MethodName
, O
and O
lessen O
the O
impact O
of O
local O
optima O
. O

Our O
SVM B-MethodName
reranker O
includes O
four O
features O
: O
( O
1 O
) O
the O
normalized O
score B-MetricName
produced O
by O
D O
IREC O
TL+ O
, O
( O
2 B-MetricValue
) O
the O
normalized O
score B-MetricName
produced O
by O
the O
RNN O
ensemble O
, O
( O
3 B-MetricValue
) O
a O
binary O
indicator O
of O
the O
presence O
of O
a O
prediction O
in O
a O
corpus O
, O
and O
( O
4 O
) O
the O
normalized O
probability O
assigned O
to O
the O
prediction O
by O
a O
character O
language O
model O
. O

For O
the O
language O
models O
that O
inform O
our O
SVM B-MethodName
reranker O
, O
we O
use O
the O
entire O
Persian O
corpus O
, O
training O
data O
only O
for O
English O
and O
Polish O
, O
and O
the O
afﬁx O
- O
match O
method O
for O
German O
and O
Spanish O
( O
Section O
2.6 O
) O
. O

Notably O
, O
the O
simple O
linear O
combination O
, O
which O
has O
no O
access O
to O
language O
models O
, O
performs O
slightly O
better O
on O
average B-MetricName
than O
the O
SVM B-MethodName
reranker O
, O
and O
seems O
to O
be O
more O
stable O
as O
well O
. O

This O
is O
illustrated O
by O
the O
fact O
that O
in O
a O
referential O
game O
involving O
a O
speaker O
and O
a O
listener O
neural B-MethodName
networks I-MethodName
optimizing O
accurate O
transmission O
over O
a O
discrete O
channel O
, O
the O
emergent O
messages O
fail O
to O
achieve O
an O
optimal O
length O
. O

1 O
Introduction O
Recent O
emergent O
- O
communication O
studies O
, O
renewed O
by O
the O
astonishing O
success O
of O
neural B-MethodName
networks I-MethodName
, O
are O
often O
motivated O
by O
a O
desire O
to O
develop O
neural O
network O
agents O
eventually O
able O
to O
verbally O
interact O
with O
humans O
( O
Havrylov O
and O
Titov O
, O
2017 O
; O
Lazaridou O
et O
al O
. O
, O
2017 O
) O
. O

To O
facilitate O
such O
interaction O
, O
neural B-MethodName
networks I-MethodName
’ O
emergent O
language O
should O
possess O
many O
natural O
- O
language O
- O
like O
properties O
. O

This O
inefﬁciency O
was O
related O
to O
neural B-MethodName
networks I-MethodName
’ O
“ O
innate O
preference O
” O
for O
long O
messages O
. O

For O
both O
Speaker O
and O
Listener O
, O
we O
experiment O
with O
either O
standard O
or O
modiﬁed O
LSTM B-MethodName
architectures O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

Standard O
Speaker O
is O
a O
singlelayer O
LSTM B-MethodName
. O

First O
, O
Speaker O
’s O
inputs O
iare O
mapped O
by O
a O
linear O
layer O
into O
an O
initial O
hidden O
state O
of O
Speaker O
’s O
LSTM B-MethodName
cell O
. O

Then O
, O
the O
message O
mis O
generated O
symbol O
by O
symbol O
: O
the O
current O
sequence O
is O
fed O
to O
the O
LSTM B-MethodName
cell O
that O
outputs O
a O
new O
hidden O
state O
. O

Standard O
Listener O
is O
also O
a O
singlelayer O
LSTM B-MethodName
. O

As O
shown O
in O
Figure O
1 O
, O
Impatient O
Listener O
consists O
of O
a O
modiﬁed O
Standard O
Listener O
that O
, O
instead O
of O
guessing O
iafter O
consuming O
the O
entire O
message O
m= O
( O
m0;:::;mt O
) O
, O
makes O
a O
prediction O
^ikfor O
each O
symbol O
mk.2This O
modiﬁcation O
takes O
advantage O
of O
the O
recurrent O
property O
of O
the O
LSTM B-MethodName
, O
however O
, O
could O
be O
adapted O
to O
any O
causal O
sequential O
neural O
network O
model O
. O

The O
agent O
is O
composed O
of O
a O
single O
- O
layer O
LSTM B-MethodName
cell O
and O
one O
shared O
linear O
layer O
followed O
by O
a O
softmax O
. O

5 O
Conclusion O
We O
demonstrated O
that O
a O
standard O
communication O
system O
, O
where O
standard O
Speaker O
and O
Listener O
LSTMs B-MethodName
are O
trained O
to O
solve O
a O
simple O
reconstruction O
game O
, O
leads O
to O
long O
messages O
, O
close O
to O
the O
maximal O
threshold B-MetricName
. O

Surprisingly O
, O
if O
these O
messages O
are O
long O
, O
LSTM B-MethodName
agents O
rely O
only O
on O
a O
small O
number O
of O
informative O
message O
symbols O
, O
located O
at O
the O
end O
. O

C O
Class O
Imbalance O
In O
addition O
to O
the O
classiﬁer O
type O
and O
size O
, O
we O
also O
tested O
the O
inﬂuence O
of O
the O
class O
( O
im)balance O
of O
the O
1Misaligned O
sentences O
were O
skipped O
. O
Encoder O
Size O
Avg O
300 O
pmeans B-MetricName
( O
Avg O
+ O
Max+Min O
) O
900 O
Random O
LSTM B-MethodName
4096 O
InferSent O
4096 O
QuickThought O
2400 O
LASER O
1024 O
BERT B-MethodName
768 O
Table O
2 O
: O
Encoders O
and O
their O
dimensionalities O
. O

Depending O
on O
the O
memory O
and O
CPU O
requirements O
we O
trained O
either O
the O
models O
sequentially O
( O
e.g. O
the O
parser O
is O
memory O
expensive O
and O
the O
linear O
classiﬁer O
for O
part O
- O
of O
- O
speech O
tagging O
is O
multi O
- O
threaded O
) O
or O
multiple O
models O
in O
parallel O
( O
the O
morphological O
analysis O
require O
far O
less O
memory O
and O
CPU O
time O
- O
practically O
the O
decision B-MethodName
trees I-MethodName
for O
all O
languages O
in O
the O
competition O
were O
built O
and O
pruned O
in O
parallel O
) O
. O

3 O
System O
Description O
Before O
we O
proceed O
with O
the O
description O
of O
the O
modules O
we O
must O
note O
that O
some O
of O
our O
methods O
rely O
on O
decision B-MethodName
trees I-MethodName
( O
DT O
) O
that O
are O
built O
using O
a O
custom O
designed O
algorithm O
that O
relies O
on O
the O
constituency O
matrix O
to O
speed O
- O
up O
computation O
for O
the O
Information O
Gain O
( O
IG O
) O
( O
Equation O
1 O
) O
. O

However O
, O
for O
a O
few O
languages O
it O
seems O
that O
the O
decision B-MethodName
tree I-MethodName
approach O
is O
not O
optimal O
, O
yielding O
low O
per O
- O
formances O
. O

After O
determining O
the O
best O
labeling O
strategy O
we O
generated O
a O
decision B-MethodName
tree I-MethodName
using O
the O
following O
features O
: O
ﬁrst O
four O
letters O
, O
last O
four O
letters O
, O
wordform O
( O
if O
occurrence O
frequency O
was O
higher O
than O
10 O
in O
the O
training O
data O
) O
. O

To O
capture O
localdependencies O
between O
words O
we O
used O
a O
context O
window O
of O
5 O
centered O
on O
the O
current O
word O
. O
In O
this O
case O
, O
we O
preferred O
to O
use O
decision B-MethodName
trees I-MethodName
, O
mainly O
because O
of O
reduced O
computational O
requirements O
and O
the O
small O
- O
footprint O
of O
the O
output O
models O
. O

The O
word O
embeddings O
were O
computed O
by O
applying O
the O
Continuous O
Bag O
of O
Words O
( O
CBOW B-MethodName
) O
model O
on O
the O
permitted O
raw O
- O
text O
resources O
. O

We O
use O
decision B-MethodName
trees I-MethodName
for O
most O
of O
the O
tasks O
, O
a O
CRF O
model O
for O
UPOS B-TaskName
tagging O
and O
the O
models O
RBG O
Parser O
creates O
for O
the O
last O
task O
of O
parsing O
. O

The O
decision B-MethodName
tree I-MethodName
approach O
used O
, O
while O
simple O
, O
brought O
interesting O
results O
, O
like O
ﬁrst O
place O
for O
Czech O
( O
CLTT O
) O
, O
Italian O
, O
Irish O
or O
Russian O
. O

Lemmatization O
, O
also O
based O
on O
decision B-MethodName
trees I-MethodName
, O
unfortunately O
worked O
really O
well O
on O
only O
a O
small O
number O
of O
languages O
. O

Again O
, O
while O
not O
a O
best O
performer O
, O
the O
decision B-MethodName
tree I-MethodName
algorithm O
we O
used O
has O
shown O
very O
good O
performance O
compared O
to O
more O
complex O
algorithms O
in O
the O
competition O
. O

We O
used O
a O
decision B-MethodName
tree I-MethodName
model O
because O
it O
is O
a O
predictable O
and O
understandable O
model O
, O
that O
, O
for O
this O
initial O
set O
of O
experiments O
allowed O
us O
to O
obtain O
signiﬁcant O
insight O
on O
how O
we O
should O
create O
features O
and O
output O
labels O
, O
something O
that O
using O
a O
neural O
network O
would O
not O
allow O
. O

The O
only O
place O
we O
used O
baseline O
UDPipe O
ﬁles O
was O
in O
the O
tokenization O
and O
sentence O
splitting O
where O
our O
decision B-MethodName
tree I-MethodName
approach O
with O
no O
tuning O
produced O
results O
signiﬁcantly O
below O
the O
baseline O
. O

During O
the O
last O
weeks O
after O
the O
shared O
task O
ended O
, O
we O
have O
replaced O
the O
decision B-MethodName
tree I-MethodName
algorithm O
with O
our O
own O
implementation O
of O
a O
linear O
classiﬁer O
, O
and O
have O
obtained O
superior O
results O
. O

However O
there O
the O
footprint O
of O
the O
model O
obtained O
using O
the O
linear O
classiﬁer O
is O
, O
in O
some O
cases O
, O
1000 O
times O
larger O
than O
that O
of O
the O
decision B-MethodName
tree I-MethodName
classiﬁer O
( O
i.e. O
the O
Ancient O
Greek O
XPOS B-TaskName
linear O
model O
size O
is O
4.5 O
GB O
, O
whereas O
the O
DT O
model O
is O
only O
4 O
MB O
) O
. O

While O
one O
might O
consider O
that O
training O
independent O
models O
for O
each O
morphological O
attribute O
would O
provide O
better O
results O
, O
decision B-MethodName
trees I-MethodName
, O
Linear O
Classiﬁer O
and O
DNN O
performed O
signiﬁcantly O
better O
, O
when O
trained O
to O
output O
all O
the O
morphological O
features O
at O
once O
( O
softmax O
one O
- O
of O
- O
n O
encoded O
, O
not O
multitask O
learning O
) O
. O

We O
use O
bidirectional O
LSTMs B-MethodName
( O
BiLSTMs B-MethodName
) O
as O
the O
base O
model O
to O
make O
unit O
- O
level O
predictions O
. O

Since O
sentence O
boundaries O
and O
MWTs O
usually O
require O
a O
larger O
context O
to O
determine O
( O
e.g. O
, O
periods O
following O
abbreviations O
or O
the O
ambiguous O
word O
“ O
des O
” O
in O
French O
) O
, O
we O
incorporate O
token O
- O
level O
information O
into O
a O
two O
- O
layer O
BiLSTM B-MethodName
as O
follows O
( O
see O
also O
Figure O
1 O
) O
. O

The O
ﬁrst O
layer O
BiLSTM B-MethodName
operates O
directly O
on O
raw O
units O
, O
and O
makes O
an O
initial O
prediction O
over O
the O
categories O
. O

To O
help O
capture O
local O
unit O
patterns O
more O
easily O
, O
we O
also O
combine O
the O
ﬁrstlayer O
BiLSTM B-MethodName
with O
1 O
- O
D O
convolutional O
networks O
, O
by O
using O
a O
one O
hidden B-HyperparameterName
layer I-HyperparameterName
convolutional O
network O
( O
CNN O
) O
with O
ReLU O
nolinearity O
at O
its O
ﬁrst O
layer O
, O
giving O
an O
effect O
a O
little O
like O
a O
residual O
connection O
( O
He O
et O
al O
. O
, O
2016 B-HyperparameterValue
) O
. O

The O
output O
of O
the O
CNN O
is O
simply O
added O
to O
the O
concatenated O
hidden O
states O
of O
the O
BiLSTM B-MethodName
for O
downstream B-TaskName
computation O
: O
hRNN O
1= O
[ O
  O
! O
h1 O
; O
 h1 O
] O
= O
BiLSTM B-MethodName
1(x O
) O
; O
( O
1 O
) O
hCNN O
1 O
= O
CNN O
( O
x O
) O
; O
( O
2 O
) O
h1 O
= O
hRNN O
1+hCNN O
1;(3 O
) O
[ O
s(tok O
) O
1;s(sent O
) O
1;s(MWT O
) O
1 O
] O
= O
W1h1 O
; O
( O
4 O
) O
where O
xis O
the O
input O
character O
representations O
, O
3In O
this O
case O
, O
we O
deﬁne O
a O
syllable O
as O
a O
consecutive O
run O
of O
alphabetic O
characters O
, O
numbers O
, O
or O
individual O
symbols O
, O
together O
with O
any O
leading O
white O
spaces O
before O
them O
. O

BiLSTM1Step B-MethodName
tInput O
Unitt1 O
- O
D O
CNN!",$(&'())!",$(+,- O
) O
! O

To O
incorporate O
token O
- O
level O
information O
at O
the O
second O
layer O
, O
we O
use O
a O
gating O
mechanism O
to O
suppress O
representations O
at O
non O
- O
token O
boundaries O
before O
propagating O
hidden O
states O
upward O
: O
g1 O
= O
h1 O
 O
(s(tok O
) O
1 O
) O
( O
5 O
) O
h2= O
[ O
  O
! O
h2 O
; O
 h2 O
] O
= O
BiLSTM B-MethodName
2(g1);(6 O
) O
[ O
s(tok O
) O
2;s(sent O
) O
2;s(MWT O
) O
2 O
] O
= O
W2h2 O
; O
( O
7 O
) O
where O
 O
is O
an O
element O
- O
wise O
product O
broadcast O
over O
all O
dimensions O
of O
h1for O
each O
unit O
. O

This O
can O
be O
viewed O
as O
a O
simpler O
alternative O
to O
multiresolution O
RNNs O
( O
Serban O
et O
al O
. O
, O
2017 O
) O
, O
where O
the O
ﬁrst O
- O
layer O
BiLSTM B-MethodName
operates O
at O
the O
unit O
level O
, O
and O
the O
second O
layer O
operates O
at O
the O
token O
level O
. O

To O
combine O
predictions O
from O
both O
layers O
of O
the O
BiLSTM B-MethodName
, O
we O
simply O
sum O
the O
scores B-MetricName
to O
obtain O
s(X)=s(X O
) O
1+s(X O
) O
2 O
, O
whereX2ftok O
, O
sent O
, O
MWTg O
. O

Speciﬁcally O
, O
we O
train O
a O
sequence O
- O
to O
- O
sequence O
model O
using O
a O
BiLSTM B-MethodName
encoder O
with O
an O
attention O
mechanism O
( O
Bahdanau O
et O
al O
. O
, O
2015 O
) O
in O
the O
form O
of O
a O
multi O
- O
layer O
perceptron O
( O
MLP O
) O
. O

Once O
the O
encoder O
hidden O
states O
henc O
are O
obtained O
with O
a O
single O
- O
layer O
BiLSTM B-MethodName
, O
each O
decoder O
step O
is O
unrolled O
as O
follows O
: O
hdec O
j O
= O
LSTM B-MethodName
dec(Eyj 1;hdec O
j 1 O
) O
; O
( O
10 O
) O
 O
ij O
/ O
exp(u O
> O
 O
tanh(W O
 O
[ O
hdec O
j;henc O
i]));(11 O
) O
cj O
= O
X O
i O
 O
ijhenc O
i O
; O
( O
12 O
) O
P(yj O
= O
wjy O
< O
j)/u O
> O
wtanh(W[hdec O
j;cj]):(13 O
) O
Here O
, O
wis O
a O
character O
index O
in O
the O
output O
vocabulary O
, O
y0a O
special O
start O
- O
of O
- O
sequence O
symbol O
in O
the O
vocabulary O
, O
and O
hdec O
0the O
concatenation O
of O
the O
last O
hidden O
states O
of O
each O
direction O
of O
the O
encoder O
. O
To O
bring O
the O
symbolic O
and O
neural O
systems O
together O
, O
we O
train O
them O
separately O
and O
use O
the O
following O
protocol O
during O
evaluation O
: O
for O
each O
MWT O
, O
we O
ﬁrst O
look O
it O
up O
in O
the O
dictionary O
, O
and O
return O
the O
expansion O
recorded O
there O
if O
one O
can O
be O
found O
. O

As O
in O
that O
work O
, O
the O
core O
of O
the O
tagger O
is O
a O
highway O
BiLSTM B-MethodName
( O
Srivastava O
et O
al O
. O
, O
2015 O
) O
with O
inputs O
coming O
from O
the O
concatenation O
of O
three O
sources O
: O
( O
1 O
) O
a O
pretrained O
word O
embedding O
, O
from O
the O
word2vec O
embeddings O
provided O
with O
the O
task O
when O
available O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
, O
and O
from O
fastText O
embeddings O
otherwise O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
; O
( O
2 O
) O
a O
trainable O
frequent O
word O
embedding O
, O
for O
all O
words O
that O
occurred O
at O
least O
seven O
times O
in O
the O
training O
set O
; O
and O
( O
3 O
) O
a O
character O
- O
level O
embedding O
, O
generated O
from O
a O
unidirectional O
LSTM B-MethodName
over O
characters O
in O
each O
word O
. O

UPOS B-TaskName
is O
predicted O
by O
ﬁrst O
transforming O
each O
word O
’s O
BiLSTM B-MethodName
state O
with O
a O
fully O
- O
connected O
( O
FC O
) O
layer O
, O
then O
applying O
an O
afﬁne O
classiﬁer O
: O
hi O
= O
BiLSTM(tag B-MethodName
) O
i(x1;:::;xn);(14 O
) O
v(u O
) O
i O
= O
FC(u)(hi O
) O
; O
( O
15 O
) O
P  O
y(u O
) O
ikjX O
= O
softmaxk  O
W(u)v(u O
) O
i O
: O
( O
16 O
) O
To O
predict O
XPOS B-TaskName
, O
we O
similarly O
start O
with O
transforming O
the O
BiLSTM B-MethodName
states O
with O
an O
FC O
layer O
. O

The O
highway O
BiLSTM B-MethodName
takes O
as O
input O
pretrained O
word O
embeddings O
, O
frequent O
word O
and O
lemma O
embeddings O
, O
character O
- O
level O
word O
embeddings O
, O
summed O
XPOS B-TaskName
and O
UPOS B-TaskName
embeddings O
, O
and O
summed O
UFeats O
embeddings O
. O

In O
( O
Dozat O
et O
al O
. O
, O
2017 O
) O
, O
unlabeled O
attachments O
are O
predicted O
by O
scoring O
each O
word O
iand O
its O
potential O
heads O
with O
a O
biafﬁne O
transformation O
ht O
= O
BiLSTM(parse B-MethodName
) O
t O
( O
x1;:::;xn);(20 O
) O
v(ed O
) O
i;v(eh O
) O
j O
= O
FC(ed)(hi);FC(eh)(hj O
) O
; O
( O
21 O
) O
s(e O
) O
ij= O
[ O
v(eh O
) O
j;1]>U(e)[v(ed O
) O
i;1 O
] O
; O
( O
22 O
) O
= O
Deep O
- O
Biaff(e)(hi;hj O
) O
; O
( O
23 O
) O
P  O
y(e O
) O
ijjX O
= O
softmaxj  O
s(e O
) O
i O
; O
( O
24 O
) O
where O
v(ed O
) O
iis O
wordi’sedge O
- O
dependent O
representation O
and O
v(eh O
) O
iitsedge O
- O
head O
representation O
. O

This O
approach O
, O
however O
, O
does O
not O
explicitly O
take O
into O
consideration O
relative O
locations O
of O
heads O
and O
dependents O
during O
prediction O
; O
instead O
, O
such O
predictive O
location O
information O
must O
be O
implicitly O
learned O
by O
the O
BiLSTM B-MethodName
. O

The O
tokenizer O
/ O
sentence O
segmenter O
uses O
BiLSTMs B-MethodName
with O
64d O
hidden O
states O
in O
each O
direction O
and O
takes O
32d O
character O
embeddings O
as O
input O
. O

For O
the O
convolutional O
component O
we O
use O
ﬁlter O
sizes O
of O
1 O
and O
9 O
, O
and O
for O
each O
ﬁlter O
size O
we O
use O
64 O
channels O
( O
same O
as O
one O
direction O
in O
the O
BiLSTM B-MethodName
) O
. O

The O
convolutional O
outputs O
are O
concatenated O
in O
the O
hidden B-HyperparameterName
layer I-HyperparameterName
, O
before O
an O
afﬁne O
transform O
is O
applied O
to O
serve O
as O
a O
residual O
connection O
for O
the O
BiLSTM B-MethodName
. O

For O
the O
MWT O
expander O
, O
we O
use O
BiLSTMs B-MethodName
with O
256d O
hidden O
states O
in O
each O
direction O
as O
the O
encoder O
, O
a O
512d O
LSTM B-MethodName
decoder O
, O
64d O
character O
embeddings O
as O
input O
, O
and O
dropout B-HyperparameterName
rate I-HyperparameterName
p=:5for O
the O
inputs O
and O
hidden O
states O
. O

The O
lemmatizer O
uses O
BiLSTMs B-MethodName
with O
100d O
hidden O
states O
in O
each O
direction O
of O
the O
encoder O
, O
50d O
character O
embeddings O
as O
input O
, O
and O
dropout B-HyperparameterName
rate I-HyperparameterName
p=:5for O
the O
inputs O
and O
hidden O
states O
. O

The O
decoder O
is O
an O
LSTM B-MethodName
with O
200d O
hidden O
states O
. O

We O
use O
2 O
- O
layer O
200d O
BiLSTMs B-MethodName
for O
the O
tagger O
and O
3 O
- O
layer O
400d O
BiLSTMs B-MethodName
for O
the O
parser O
. O

We O
employ O
dropout B-HyperparameterName
in O
all O
feedforward O
connections O
with O
p=:5and O
all O
recurrent O
connections O
( O
Gal O
and O
Ghahramani O
, O
2016 B-HyperparameterValue
) O
with O
p=:25(exceptp=:5 O
in O
the O
tagger O
BiLSTM B-MethodName
) O
. O

One O
of O
the O
greatest O
opportunities O
for O
further O
gains O
is O
through O
the O
use O
of O
context O
- O
sensitive O
word O
embeddings O
, O
such O
as O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
ULMﬁt O
( O
Howard O
and O
Ruder O
, O
2018 O
) O
. O

They O
concatenate O
these O
two O
types O
of O
features O
and O
feed O
them O
into O
neural B-MethodName
networks I-MethodName
to O
learn O
mixed O
representations O
. O

One O
of O
the O
most O
important O
reasons O
is O
that O
it O
is O
easier O
for O
attention O
mechanisms O
to O
learn O
long O
- O
range O
dependencies O
, O
which O
is O
a O
key O
challenge O
in O
sequential O
data O
modeling O
, O
than O
recurrent O
and O
convolutional O
neural B-MethodName
networks I-MethodName
. O

We O
use O
pre O
- O
trained O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
vectors O
as O
the O
embeddings O
for O
word O
tokens O
, O
and O
randomly O
initialize O
the O
embeddings O
for O
entity O
types O
then O
update O
them O
during O
the O
training O
procedure O
. O

With O
the O
structure O
which O
combines O
BiLSTM B-MethodName
with O
CNN O
, O
this O
method O
can O
capture O
both O
the O
sequence O
and O
chunk O
information O
of O
words O
to O
beneﬁt O
ED O
. O

6)SELF O
is O
proposed O
by O
Hong O
et O
al O
. O
( O
2018 O
) O
, O
which O
integrates O
BiLSTM B-MethodName
into O
GAN O
structure O
, O
in O
order O
to O
distinguish O
the O
authentic O
information O
from O
spurious O
features O
. O

Following O
Liu O
et O
al O
. O
( O
2019 O
) O
, O
we O
train O
a O
BiLSTM B-MethodName
- O
CRF O
model O
on O
the O
training O
set O
, O
then O
apply O
it O
on O
the O
test O
set O
to O
get O
the O
predicted O
entity O
type O
sequences O
. O

The O
F1score B-MetricName
of O
the O
BiLSTM B-MethodName
- O
CRF O
model O
on O
the O
test O
set O
is O
82.7 O
% O
. O

Among O
them O
, O
there O
are O
several O
approaches O
which O
utilize O
the O
entity O
type O
information O
in O
their O
neural B-MethodName
networks I-MethodName
. O

Liu O
et O
al O
. O
( O
2017 O
) O
utilize O
the O
entity O
type O
embedding O
directly O
as O
local O
context O
of O
the O
current O
word O
, O
and O
calculate O
the O
attention O
values O
between O
them O
; O
others O
( O
Liu O
et O
al O
. O
, O
2018b O
, O
2019 O
; O
Nguyen O
and O
Grishman O
, O
2018 O
) O
concatenate O
the O
entity O
type O
embeddings O
with O
the O
work O
token O
embeddings O
, O
in O
order O
to O
integrate O
these O
two O
types O
of O
features O
into O
mixed O
representations O
with O
the O
help O
of O
neural B-MethodName
networks I-MethodName
. O

Second O
, O
we O
ask O
how O
stable O
probing O
task O
results O
are O
across O
different O
classiﬁers O
( O
e.g. O
, O
MLP O
vs. O
Naive B-MethodName
Bayes I-MethodName
) O
. O

As O
non O
- O
parametric O
methods O
, O
we O
consider O
: O
( O
i O
) O
average B-MetricName
word O
embeddings O
as O
a O
popular O
baseline O
, O
( O
ii O
) O
the O
concatenation O
of O
average B-MetricName
, O
min O
and O
max O
pooling O
( O
pmeans B-MetricName
) O
( O
R¨uckl´e O
et O
al O
. O
, O
2018 B-MetricValue
) O
; O
and O
Random O
LSTMs B-MethodName
( O
Conneau O
et O
al O
. O
, O
2017 O
; O
Wieting O
and O
Kiela O
, O
2019 O
) O
, O
which O
feed O
word O
embeddings O
to O
randomly O
initialized O
LSTMs B-MethodName
, O
then O
apply O
a O
pooling O
operation O
across O
time O
- O
steps O
. O

As O
parametric O
methods O
, O
we O
consider O
: O
InferSent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
, O
which O
induces O
a O
sentence O
representation O
by O
learning O
a O
semantic O
entailment O
relationship O
between O
two O
sentences O
; O
QuickThought O
( O
Logeswaran O
and O
Lee O
, O
2018 O
) O
which O
reframes O
the O
popular O
SkipThought O
model O
( O
Kiros O
et O
al O
. O
, O
2015 O
) O
in O
a O
classiﬁcation O
context O
; O
LASER O
( O
Artetxe O
and O
Schwenk O
, O
2019 O
) O
derived O
from O
massively O
multilingual O
machine O
translation O
models O
, O
and O
BERT B-MethodName
base O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
where O
we O
average B-MetricName
token O
embeddings O
of O
the O
last O
layer O
for O
a O
sentence O
representation O
. O

4 O
Experiments O
Experimental O
Setup O
To O
the O
SentEval O
toolkit O
( O
Conneau O
and O
Kiela O
, O
2018 O
) O
, O
which O
addresses O
both O
probing O
and O
downstream B-TaskName
tasks I-TaskName
and O
offers O
Logistic B-MethodName
Regression I-MethodName
( O
LR O
) O
and O
MLP O
classiﬁers O
on O
top O
of O
representations O
, O
we O
added O
implementations O
of O
8https://www.kaggle.com/crowdflower/ O
twitter O
- O
airline O
- O
sentiment O
9https://github.com/hilalbenzer/ O
turkish O
- O
sentiment O
- O
analysis O
10http://study.mokoron.com/Random O
Forest O
( O
RF O
) O
and O
Naive B-MethodName
Bayes I-MethodName
( O
NB O
) O
from O
scikit O
- O
learn O
as O
other O
popular O
but O
‘ O
simple O
’ O
classiﬁers O
. O

The O
same O
embeddingsare O
used O
for O
the O
random O
LSTM B-MethodName
. O

For O
average B-MetricName
BERT B-MethodName
, O
we O
use O
the O
base O
- O
multilingual O
- O
cased O
model O
. O

Both O
the O
tagger O
and O
the O
parser O
use O
bi O
- O
LSTM B-MethodName
layers O
on O
word O
embeddings O
. O

For O
almost O
all O
languages O
, O
the O
parser O
is O
based O
on O
three O
bi O
- O
LSTM B-MethodName
layers O
( O
300neurons O
/ O
layer O
) O
and O
the O
tagger O
on O
two O
biLSTM B-MethodName
layers(200neurons O
/ O
layer O
) O
. O

For O
languages O
with O
less O
than O
1500 O
sentences O
, O
we O
used O
a O
parser O
composed O
of O
only O
one O
bi O
- O
LSTM B-MethodName
layer O
and O
smaller O
lReLU O
layers O
. O

As O
we O
said O
earlier O
, O
the O
tagger O
takes O
into O
account O
characters O
by O
using O
a O
LSTM B-MethodName
on O
each O
character O
and O
using O
a O
linear O
attention O
layer O
on O
the O
LSTM B-MethodName
outputs O
, O
so O
we O
did O
expect O
great O
results O
on O
features O
. O

We O
studied O
the O
following O
parameters O
: O
dropout B-HyperparameterName
on O
word O
embeddings O
, O
dropout B-HyperparameterName
on O
character O
embeddings O
, O
word O
embeddings O
merging O
strategy O
( O
whether O
to O
sum O
or O
concatenate O
the O
word O
embedding O
coming O
from O
characters O
and O
the O
pre O
/ O
post O
trained O
word O
embeddings O
) O
, O
case O
sensitivity O
, O
number O
of O
bi O
- O
LSTM B-MethodName
layers O
, O
number O
of O
neurons O
for O
bi O
- O
LSTM B-MethodName
, O
lReLU O
layers O
sizes O
, O
lReLU O
layers O
dropout B-HyperparameterName
, O
and O
some O
optimizer B-HyperparameterName
parameters O
like O
the O
learning B-HyperparameterName
rate I-HyperparameterName
. O

number O
of O
bi O
- O
LSTM B-MethodName
The O
number O
of O
bi O
- O
LSTM B-MethodName
layers O
deeply O
affects O
the O
score B-MetricName
probably O
because O
of O
the O
vanishing O
gradient O
problem O
. O

Best O
scores B-MetricName
were O
achieved O
for O
one O
to O
three O
bi O
- O
LSTM B-MethodName
. O

Even O
if O
we O
lacked O
the O
time O
to O
ﬁnalize O
this O
contribution O
, O
we O
did O
manage O
to O
reach O
a O
f1 O
score B-MetricName
close O
to O
UDPipe O
with O
random B-MethodName
forest I-MethodName
classiﬁer O
on O
English O
LinES O
corpus O
( O
-0.2 O
% O
tokens O
, O
5 B-MetricValue
% O
sentences O
) O
. O

We O
also O
stand O
below O
TurkuNLP O
for O
morphological O
features O
on O
big O
treebanks O
( O
93.68 O
% O
for O
93.82 O
% O
) O
, O
they O
also O
used O
the O
biLSTM B-MethodName
architecture O
but O
they O
inferred O
tags O
without O
sub O
- O
categories O
. O

Wieting O
and O
Gimpel O
( O
Wieting O
and O
Gimpel O
, O
2018 O
) O
compared O
three O
encoding O
mechanisms O
: O
WORD O
, O
TRIGRAM O
and O
LSTM B-MethodName
. O

The O
WORD O
model O
( O
Wieting O
et O
al O
. O
, O
2016 O
) O
averages B-MetricName
the O
embedding O
for O
each O
word O
in O
the O
sentence O
into O
a O
ﬁxed O
length O
vector O
embedding O
for O
the O
sentence O
; O
the O
TRIGRAM O
model O
( O
Huang O
et O
al O
. O
, O
2013 O
) O
averages B-MetricName
over O
character O
trigrams O
; O
and O
the O
LSTM B-MethodName
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
approach O
averages B-MetricName
over O
the O
ﬁnal O
hidden O
states O
to O
obtain O
the O
sentence O
embedding O
. O

Encoders O
are O
trained O
on O
paraphrase O
pairs O
( O
s;s0 O
) O
with O
a O
margin O
based O
loss O
function O
l(s;s0;t;t0 O
) O
= O
max(0; cos[g(s);g(s0 O
) O
] O
+ O
cos[g(s);g(t)])+ O
max(0; cos[g(s);g(s0 O
) O
] O
+ O
cos[g(s0);g(t0 O
) O
] O
) O
where O
g O
is O
one O
of O
( O
WORD O
, O
TRIGRAM O
, O
LSTM B-MethodName
) O
and(t;t0)is O
a O
negative O
sample O
selected O
from O
amegabatch O
, O
an O
aggregation O
of O
mminibatches O
( O
Wieting O
and O
Gimpel O
, O
2018).3 O
We O
evaluate O
the O
WORD O
model O
trained4on O
PARANMT O
, O
P O
ARABANK O
and O
P O
ARABANK O
2 O
( O
our O
work O
) O
. O

3.8 O
Improving O
contextualized O
encoders O
with O
paraphrastic O
data O
Paraphrastic O
data O
can O
be O
used O
to O
ﬁne O
- O
tune O
contextualized O
encoders O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

To O
generate O
the O
training O
data O
, O
we O
extract O
, O
for O
eachQQP B-DatasetName
MNLI B-MethodName
STS O
- O
B O
MRPC B-DatasetName
BERT B-MethodName
87.90 O
83.86 O
88.40 O
84.00 O
pBERT B-MethodName
88.14 O
82.64 O
88.59 O
86.55 O
Table O
4 O
: O
F1 B-MetricName
scores B-MetricName
are O
reported O
for O
QQP B-DatasetName
and O
MRPC B-DatasetName
, O
Spearman O
correlations B-MetricName
are O
reported O
for O
STS O
- O
B O
, O
and O
accuracy B-MetricName
scores B-MetricName
are O
reported O
for O
MNLI B-MethodName
. O

Numbers O
reported O
on O
Dev O
set O
Type O
BERT B-MethodName
pBERT B-MethodName
F1 B-MetricName
HasAns O
76.81 B-MetricValue
74.21 O
NoAns O
71.44 O
74.95 O
Total O
74.12 O
74.58 O
Exact O
Match O
HasAns O
70.34 O
68.00 O
NoAns O
71.44 O
74.95 O
Total O
70.89 O
71.48 O
Table O
5 O
: O
SQuAD B-DatasetName
2.0 O
results O
on O
dev O
set O
. O

We O
then O
use O
this O
BERT B-MethodName
ﬁne O
- O
tuned O
on O
paraphrases O
( O
henceforth O
pBERT B-MethodName
) O
for O
ﬁne O
- O
tuning O
on O
SQuAD B-DatasetName
2.0 O
( O
Rajpurkar O
et O
al O
. O
, O
2018 O
) O
and O
4 O
NLP O
tasks O
present O
in O
the O
General O
Language O
Understanding O
Evaluation O
( O
GLUE B-DatasetName
) O
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
): O
Quora O
Question O
Pairs O
( O
QQP B-DatasetName
) O
( O
Chen O
et O
al O
. O
, O
2017 O
) O
, O
Multi O
- O
Genre B-MethodName
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
( O
MNLI B-MethodName
) O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
, O
the O
Semantic O
Textual O
Similarity O
Benchmark O
( O
STS O
- O
B O
) O
( O
Agirre O
et O
al O
. O
, O
2016 O
) O
, O
and O
the O
Microsoft O
Research O
Paraphrase O
Corpus O
( O
MRPC B-DatasetName
) O
( O
Dolan O
et O
al O
. O
, O
2004 O
) O
. O

Fine O
- O
tuning O
on O
our O
paraphrase O
corpus O
also O
improves O
performance O
on O
SQuAD B-DatasetName
, O
a O
questionanswering O
task O
, O
while O
slightly O
degrading O
performance O
on O
MNLI B-MethodName
. O

Overall O
, O
simple O
ﬁne O
- O
tuning O
of O
BERT B-MethodName
on O
our O
corpus O
leads O
to O
improvements O
ondownstream B-TaskName
tasks O
, O
in O
particular O
when O
the O
task O
is O
related O
to O
paraphrase O
detection O
. O

Convolutional O
neural B-MethodName
networks I-MethodName
have O
been O
introduced O
by O
Yin O
and O
Sch O
¨utze O
( O
2015 O
) O
and O
Chen O
et O
al O
. O
( O
2018 O
) O
, O
and O
further O
augmented O
with O
LSTMs B-MethodName
( O
Kubal O
and O
Nimkar O
, O
2018 O
) O
and O
attention O
mechanisms O
( O
Fan O
et O
al O
. O
, O
2018).5 O
Conclusions O
and O
future O
work O
A O
presumed O
goal O
for O
building O
a O
sentential O
paraphrase O
resource O
is O
to O
capture O
different O
ways O
of O
expressing O
the O
same O
thing O
: O
diversity O
matters O
. O

We O
evaluate O
two O
methods O
for O
generating O
paired O
training O
data O
, O
which O
is O
then O
used O
to O
train O
T5 B-MethodName
models O
for O
free O
and O
controlled O
generation O
. O

We O
then O
ﬁne O
tune O
pretrained O
T5 B-MethodName
seq2seq O
models O
using O
literal O
/ O
metaphoric O
pairs O
, O
as O
they O
allow O
for O
easy O
implementation O
of O
control O
codes O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
. O

Deep O
learning O
has O
driven O
the O
ﬁeld O
in O
recent O
years O
, O
particularly O
autoencoder O
and O
LSTM B-MethodName
networks O
( O
Gupta O
et O
al O
. O
, O
2018 O
; O
Prakash O
et O
al O
. O
, O
2016 O
) O
and O
transformer O
- O
based O
methods O
( O
Li O
et O
al O
. O
, O
2019 O
; O
Egonmwan O
and O
Chali O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2019 O
) O
. O

We O
then O
ﬁll O
the O
masks O
with O
the O
best O
ﬁt O
word O
from O
the O
source O
vocabulary O
using O
FitterBERT B-MethodName
, O
a O
ﬁlter O
layer O
built O
on O
top O
of O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
,Stowe O
et O
al O
. O
( O
2021)MetaNet O
Silver O
# O
Sentences O
248k O
360k O
Unique O
Mappings O
8.5k O
650 O
Unique O
Domains O
1k O
550 O
Avg O
. O

FitterBERT B-MethodName
is O
inspired O
by O
FitBERT B-MethodName
( O
Havens O
and O
Stal O
, O
2019 O
) O
, O
but O
is O
three O
orders O
of O
magnitude O
faster O
. O

Second O
, O
the O
average B-MetricName
sentence O
length O
is O
much O
longer O
, O
as O
the O
FrameNet O
annotations O
used O
cover O
a O
much O
more O
diverse O
set O
of O
sentences O
than O
the O
Gutenberg O
poetry O
corpus.4 O
Methods O
4.1 B-MetricValue
Seq2seq O
Models O
For O
our O
free O
and O
controlled O
generation O
models O
, O
we O
use O
the O
T5 B-MethodName
system O
for O
sequence O
to O
sequence O
generation O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
. O

As O
we O
are O
ﬁne O
- O
tuning O
a O
large O
pretrained O
T5 B-MethodName
model O
, O
it O
may O
be O
the O
case O
that O
models O
can O
automatically O
generate O
metaphors O
from O
valid O
conceptual O
mappings O
as O
it O
has O
seen O
these O
metaphors O
before O
. O

To O
incorporate O
controlled O
generation O
into O
T5 B-MethodName
, O
we O
include O
target O
and O
source O
information O
into O
the O
preﬁx O
, O
which O
then O
matches O
the O
format O
" O
Activate O
metaphors O
from O
TARGET O
toSOURCE O
: O
" O
. O

Table O
2 O
: O
Data O
inputs O
for O
T5 B-MethodName
seq2seq O
metaphoric O
paraphrase O
generation O
. O

SentBERT B-MethodName
SentBERT B-MethodName
( O
Reimers O
and O
Gurevych O
, O
2019 O
) O
provides O
sentence O
transformers O
to O
generate O
sentential O
vectors O
. O

SentBERT B-MethodName
has O
proven O
effective O
for O
a O
wide O
variety O
of O
similarity O
tasks O
, O
and O
should O
also O
be O
effective O
at O
determining O
paraphrase O
quality O
between O
literal O
and O
metaphoric O
sentences O
. O

MoverScore O
MoverScore O
( O
Zhao O
et O
al O
. O
, O
2019 O
) O
is O
a O
metric O
that O
uses O
BERT B-MethodName
and O
Earth O
Mover O
Distance O
to O
measure O
similarity O
between O
two O
sentences O
. O

It O
uses O
contextual O
embeddings O
similar O
to O
SentBERT B-MethodName
, O
and O
has O
the O
potential O
to O
better O
represent O
sense O
- O
speciﬁc O
meanings B-MetricName
like O
those O
associated O
with O
metaphoricity O
. O

Perplexity O
Transformer O
- O
based O
language O
mod O
- O
els O
such O
as O
the O
GPT B-MethodName
family O
( O
Radford O
et O
al O
. O
, O
2019 O
; O
Brown O
et O
al O
. O
, O
2020 O
) O
are O
extremely O
effective O
at O
producing O
text O
. O

We O
here O
use O
the O
abstract O
/ O
concreteness O
ratings O
from O
Köper O
and O
Schulte O
i O
m O
Walde O
( O
2017 O
) O
, O
which O
are O
based O
on O
Word2Vec B-MethodName
embeddings O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
. O

Novelty O
Classiﬁcation O
We O
train O
a O
BERT B-MethodName
regression O
model O
on O
the O
metaphoric O
novelty O
scores B-MetricName
from O
Do O
Dinh O
et O
al O
. O
( O
2018 O
) O
. O

We O
further O
explore O
each O
component O
and O
the O
relevant O
metrics O
’ O
performance O
: O
6.1 O
Fluency O
Perplexity O
under O
GPT-2 B-MethodName
performs O
best O
for O
ﬂuency O
, O
far O
outperforming O
any O
other O
metric O
, O
which O
is O
expected O
as O
this O
was O
the O
intended O
use O
of O
this O
metric O
. O

MoverScore O
and O
SentBERT B-MethodName
compared O
to O
the O
context O
yield O
the O
best O
correlation B-MetricName
. O

Speciﬁcally O
, O
SentBERT B-MethodName
cosine O
similarity O
achieves O
a O
strong O
correlation B-MetricName
value O
of O
.65 B-MetricValue
, O
making O
it O
a O
valuable O
metric O
for O
evaluating O
this O
component O
. O

Wang O
et O
al O
. O
( O
2016 O
) O
applied O
window O
LSTM B-MethodName
model O
for O
surface O
segmentation O
. O

Last O
, O
while O
Gulcehre O
et O
al O
. O
( O
2017 O
) O
use O
a O
lan O
- O
guage O
model O
implemented O
with O
recurrent O
neural B-MethodName
networks I-MethodName
, O
we O
employ O
a O
statistical O
language O
model O
, O
which O
is O
better O
adapted O
to O
our O
settings O
with O
small O
data O
sets O
. O

Sun O
et O
al O
. O
( O
2015 O
) O
; O
Francis O
- O
Landau O
et O
al O
. O
( O
2016 O
) O
proposed O
using O
convolutional O
neural B-MethodName
networks I-MethodName
( O
CNN O
) O
with O
Siamese O
( O
symmetric O
) O
architecture O
to O
capture O
the O
similarity O
between O
texts O
. O

The O
interaction O
- O
focused O
method O
tries O
to O
build O
local O
interactions O
( O
e.g. O
, O
cosine O
similarity O
) O
between O
two O
pieces O
of O
text O
, O
and O
then O
uses O
neural B-MethodName
networks I-MethodName
to O
learn O
the O
ﬁnal O
matching O
score B-MetricName
based O
on O
the O
local O
interactions O
. O

One O
straightforward O
way O
to O
combine O
multiple O
semantic O
matching O
signals O
is O
to O
apply O
a O
linear B-MethodName
regression I-MethodName
layer O
to O
learn O
a O
static O
weight B-HyperparameterName
for O
each O
matching O
signal(Francis O
- O
Landau O
et O
al O
. O
, O
2016 B-HyperparameterValue
) O
. O

To O
overcome O
the O
issue O
of O
the O
static O
weights B-HyperparameterName
in O
linear B-MethodName
regression I-MethodName
, O
we O
apply O
rank O
aggregation O
to O
combine O
multiple O
semantic O
matching O
signals O
captured O
by O
two O
neural O
models O
on O
multiple O
text O
pairs O
. O

Given O
the O
tweet O
t O
, O
we O
aim O
to O
assign O
labels O
y= O
( O
y1;:::;y O
n)for O
each O
word O
in O
the O
tweet O
t. O
yi=8 O
< O
: O
B O
w O
iis O
a O
begin O
word O
of O
a O
mention O
; O
I O
w O
iis O
a O
non O
- O
begin O
word O
of O
a O
mention O
; O
O O
w O
iis O
not O
a O
mention O
word O
: O
In O
our O
implementation O
, O
we O
apply O
an O
LSTM B-MethodName
- O
CRF O
based O
NER B-TaskName
tagging O
model O
which O
automaticallyModel O
Overview O
 O
Mention O
Detection O
and O
Candidate O
GenerationTweet O
Data O
Convolution O
Neural O
 O
Network O
with O
Max O
-PoolingNeural O
Relevance O
Model O
 O
with O
Kernel O
-PoolingSemantic O
Matching O
Knowledge O
Base O
Linking O
ResultsRank O
AggregationFigure O
1 O
: O
An O
overview O
of O
aggregated O
semantic O
matching O
for O
entity O
disambiguation O
. O

learns O
contextual O
features O
for O
sequence O
tagging O
via O
recurrent O
neural B-MethodName
networks I-MethodName
( O
Lample O
et O
al O
. O
, O
2016 O
) O
. O

To O
take O
advantage O
of O
different O
semantic O
matching O
models O
on O
different O
text O
pairs O
, O
a O
straightforward O
approach O
is O
using O
a O
linear B-MethodName
regression I-MethodName
layer O
to O
combine O
multiple O
semantic O
matching O
signals O
( O
Francis O
- O
Landau O
et O
al O
. O
, O
2016 O
) O
. O

We O
train O
an O
LSTM B-MethodName
- O
CRF O
based O
tagger O
( O
Lample O
et O
al O
. O
, O
2016 O
) O
for O
mention O
detection O
by O
using O
the O
NEEL O
training O
dataset O
. O

Sun O
et O
al O
. O
( O
2015 O
) O
; O
FrancisLandau O
et O
al O
. O
( O
2016 O
) O
apply O
convolutional O
neural B-MethodName
networks I-MethodName
for O
entity O
linking O
. O

Eshel O
et O
al O
. O
( O
2017 O
) O
use O
recurrent O
neural B-MethodName
networks I-MethodName
to O
model O
the O
mention O
contexts O
. O

To O
identify O
uncertain O
points O
along O
the O
separating O
hyperplane O
of O
an O
SVM B-MethodName
the O
following O
approach O
is O
proposed O
. O

Encoder O
and O
decoder O
RNN O
have O
layer O
normalized O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
LSTM B-MethodName
cells O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
with O
size O
512 O
. O

Learner O
The O
Learner O
is O
an O
SVM1with B-MethodName
linear O
kernel O
. O

The O
latent O
variable O
is O
a O
vector O
with O
50 O
dimensions O
and O
the O
SVM B-MethodName
is O
trained O
on O
this O
representation O
. O

The O
two O
largest O
absolute O
coefﬁcients O
of O
the O
trained O
SVM B-MethodName
’s O
linear O
kernel O
identify O
the O
most O
important O
dimensions O
. O

Given O
that O
we O
use O
an O
SVM B-MethodName
for O
classiﬁcation O
, O
we O
do O
not O
expect O
a O
strong O
effect O
for O
adding O
large O
numbers O
of O
additional O
training O
instances O
, O
given O
that O
the O
majority O
of O
those O
data O
points O
will O
not O
be O
positioned O
close O
to O
the O
decision O
boundary O
. O

As O
an O
improvement O
over O
UDPipe O
2.0 O
, O
we O
use O
the O
“ O
frozen O
” O
contextualized O
embeddings O
on O
the O
input O
( O
BERT B-MethodName
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
in O
the O
same O
way O
as O
Straka O
et O
al O
. O
( O
2019 O
) O
. O

Right O
: O
Token O
representation O
encoder O
architecture O
. O
Input O
word O
Mr. O
Pretrained O
regular O
embeddings O
. O
Trained O
embeddings O
. O
Mr. O
GRUGRUGRUPretrained O
contextualized O
embeddings O
....... O
LSTMLSTMLSTMLSTMLSTMLSTM B-MethodName
............ O
Mr. O
Merksamerisleadingthebuy O
- O
out.udef O
qnamedlead O
v1theqbuy+out O
n1mister O
n1proper O
qcompoundudef O
qnamedlead O
v1theqbuy+out O
n1mister O
n1proper O
qcompoundMr.Merksamerisleadingthebuy-out.udef O
qnamedlead O
v1theqbuy+out O
n1mister O
n1proper O
qcompoundNode O
RepresentationCreate O
new O
node O
softmaxNode O
prop O
1 O
softmax O
... O
Node O
propP O
softmax O
... O
Underlying O
Token O
RepresentationtanhNode O
prop O
1 O
embed O
.... O
Node O
propP O
embed O
.... O
+New O
Node O
Representation(b O
) O
Left O
: O
First O
AddNodes O
operation O
. O

pre O
- O
trained O
FastText O
word O
embeddings O
of O
dimension O
300(Mikolov O
et O
al O
. O
, O
2018),4 O
pre O
- O
trained O
( O
“ O
frozen O
” O
) O
contextual O
BERT B-MethodName
embeddings O
of O
dimension O
768 O
( O
Devlin O
et O
al O
. O
, O
2019).5We O
average B-MetricName
the O
last O
four O
layers O
of O
the O
BERT B-MethodName
model O
and O
we O
produce O
a O
word O
embedding O
for O
a O
token O
as O
an O
average B-MetricName
of O
the O
corresponding O
BERT B-MethodName
subword O
embeddings O
. O

Therefore O
, O
we O
expected O
that O
utilization O
of O
BERT B-MethodName
embeddings O
would O
improve O
results O
considerably O
, O
which O
was O
the O
case O
, O
as O
demonstrated O
in O
Section O
4.1 O
. O

4https://fasttext.cc/docs/en/ O
english-vectors.html O
5We O
use O
the O
Base O
English O
Uncased O
model O
from O
https://github.com/google-research/bert O
.All O
available O
embeddings O
for O
a O
token O
are O
concatenated O
and O
processed O
with O
two O
bidirectional O
LSTM B-MethodName
layers O
with O
residual O
connections O
. O

We O
processed O
token O
embeddings O
using O
two O
layers O
of O
bidirectional O
LSTMs B-MethodName
with O
residual O
connections O
and O
a O
dimension O
of O
768 O
. O

We O
employed O
dropout B-HyperparameterName
with O
rate O
0.3 B-HyperparameterValue
before O
and O
after O
every O
LSTM B-MethodName
layer O
and O
on O
all O
node O
representations O
, O
and O
utilized O
also O
word O
dropout B-HyperparameterName
( O
zeroing O
the O
whole O
WE O
for O
a O
given O
word O
) O
with O
a O
rate O
of O
0.2 B-HyperparameterValue
. O

All O
reported O
scores B-MetricName
are O
macro O
- O
averaged B-MetricName
F1 B-MetricName
scores B-MetricName
of O
allSystem O
Tops O
Labels O
Properties O
Anchors O
Edges O
Attributes O
All O
Original O
ST O
submission O
75.12 B-MetricValue
% O
6 O
63.99 O
% O
7 O
56.53 O
% O
6 O
69.53 O
% O
6 O
62.17 O
% O
7 O
7.85 O
% O
4 O
74.74 O
% O
6 O
Bugﬁx O
ST O
submission O
81.47 O
% O
6 O
73.06 O
% O
1 O
69.95 O
% O
1 O
77.23 O
% O
3 O
73.89 O
% O
5 O
7.87 O
% O
4 O
83.96 O
% O
3 O
99 O
% O
training O
data O
80.59 O
% O
6 O
73.06 O
% O
1 O
70.18 O
% O
1 O
77.35 O
% O
3 O
74.27 O
% O
5 O
7.96 O
% O
4 O
84.14 O
% O
3 O
No O
BERT B-MethodName
embeddings O
70.50 O
% O
8 O
70.71 O
% O
4 O
67.01 O
% O
4 O
76.02 O
% O
4 O
65.02 O
% O
6 O
5.30 O
% O
6 O
78.99 O
% O
5 O
Ensemble O
81.13 O
% O
6 O
73.39 O
% O
1 O
70.82 O
% O
1 O
77.57 O
% O
3 O
75.85 O
% O
4 O
8.28 O
% O
3 O
85.05 O
% O
3 O
HIT O
- O
SCIR O
( O
Che O
et O
al O
. O
, O
2019 O
) O
90.41 O
% O
2 O
70.85 O
% O
3 O
69.86 O
% O
1 O
77.61 O
% O
2 O
79.37 O
% O
1 O
12.40 O
% O
1 O
86.20 O
% O
1 O
SJTU O
– O
NICT O
( O
Li O
et O
al O
. O
, O
2019 O
) O
91.50 O
% O
1 O
71.24 O
% O
2 O
68.73 O
% O
2 O
77.62 O
% O
1 O
77.74 O
% O
2 O
9.40 O
% O
2 O
85.27 O
% O
2 O
SUDA O
– O
Alibaba O
( O
Zhang O
et O
al O
. O
, O
2019b O
) O
86.01 O
% O
5 O
69.50 O
% O
4 O
68.24 O
% O
3 O
77.11 O
% O
3 O
76.85 O
% O
3 O
8.16 O
% O
3 O
83.96 O
% O
3 O
Saarland O
( O
Donatelli O
et O
al O
. O
, O
2019 O
) O
86.70 O
% O
4 O
71.33 O
% O
1 O
61.11 O
% O
5 O
75.08 O
% O
5 O
75.01 O
% O
4 O
— O
81.87 O
% O
4 O
Table O
3 O
: O
Overall O
results O
, O
macro O
- O
averaged B-MetricName
on O
all O
frameworks O
. O

In O
order O
to O
asses O
the O
BERT B-MethodName
embeddings O
effect O
, O
we O
further O
evaluated O
a O
version O
of O
our O
system O
with O
- O
out O
them O
. O

The O
macro O
- O
averaged B-MetricName
allperformance O
without O
BERT B-MethodName
embeddings O
is O
substantially O
lower O
, O
79 O
% O
compared O
to O
84 O
% O
. O

Generally O
all O
metrics O
decrease O
without O
BERT B-MethodName
embeddings O
, O
showing O
that O
contextual O
embeddings O
help O
“ O
everywhere O
” O
. O

We O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
comments O
. O
System O
Tops O
Labels O
Properties O
Anchors O
Edges O
Attributes O
All O
Bugﬁx O
ST O
submission O
87.39 O
% O
8 O
97.29 O
% O
1 O
94.50 O
% O
5 O
99.02 O
% O
6 O
88.32 O
% O
8 O
— O
94.66 O
% O
5 O
99 O
% O
training O
data O
88.36 O
% O
8 O
97.38 O
% O
1 O
94.57 O
% O
5 O
99.04 O
% O
6 O
88.47 O
% O
8 O
— O
94.75 O
% O
4 O
No O
BERT B-MethodName
embeddings O
80.70 O
% O
9 O
96.24 O
% O
2 O
92.19 O
% O
7 O
98.45 O
% O
8 O
80.06 O
% O
10 O
— O
91.75 O
% O
8 O
Ensemble O
89.06 O
% O
7 O
97.51 O
% O
1 O
94.86 O
% O
4 O
99.12 O
% O
3 O
89.72 O
% O
8 O
— O
95.17 O
% O
2 O
HIT O
- O
SCIR O
( O
Che O
et O
al O
. O
, O
2019 O
) O
92.65 O
% O
3 O
93.00 O
% O
4 O
95.33 O
% O
3 O
99.28 O
% O
1 O
92.54 O
% O
2 O
— O
95.08 O
% O
2 O
SJTU O
– O
NICT O
( O
Li O
et O
al O
. O
, O
2019 O
) O
93.26 O
% O
2 O
94.89 O
% O
3 O
95.49 O
% O
2 O
99.27 O
% O
2 O
92.39 O
% O
3 O
— O
95.50 O
% O
1 O
SUDA O
– O
Alibaba O
( O
Zhang O
et O
al O
. O
, O
2019b O
) O
91.13 O
% O
6 O
90.27 O
% O
8 O
91.51 O
% O
7 O
98.16 O
% O
8 O
89.84 O
% O
7 O
— O
92.26 O
% O
7 O
Saarland O
( O
Donatelli O
et O
al O
. O
, O
2019 O
) O
85.87 O
% O
8 O
96.82 O
% O
1 O
93.55 O
% O
5 O
99.05 O
% O
5 O
90.95 O
% O
6 O
— O
94.69 O
% O
4 O
( O
a O
) O
DM O
framework O
System O
Tops O
Labels O
Properties O
Anchors O
Edges O
Attributes O
All O
Bugﬁx O
ST O
submission O
94.48 O
% O
6 O
95.94 O
% O
1 O
92.61 O
% O
2 O
99.00 O
% O
4 O
76.06 O
% O
7 O
— O
90.96 O
% O
4 O
99 O
% O
training O
data O
86.49 O
% O
9 O
96.05 O
% O
1 O
92.70 O
% O
2 O
99.00 O
% O
3 O
76.37 O
% O
7 O
— O
90.89 O
% O
4 O
No O
BERT B-MethodName
embeddings O
67.57 O
% O
12 O
95.14 O
% O
3 O
90.72 O
% O
7 O
98.47 O
% O
8 O
68.22 O
% O
10 O
— O
87.58 O
% O
8 O
Ensemble O
87.35 O
% O
8 O
96.19 O
% O
1 O
93.04 O
% O
2 O
99.02 O
% O
3 O
78.20 O
% O
7 O
— O
91.51 O
% O
1 O
HIT O
- O
SCIR O
( O
Che O
et O
al O
. O
, O
2019 O
) O
96.03 O
% O
3 O
89.30 O
% O
5 O
93.10 O
% O
1 O
99.12 O
% O
1 O
79.65 O
% O
3 O
— O
90.55 O
% O
4 O
SJTU O
– O
NICT O
( O
Li O
et O
al O
. O
, O
2019 O
) O
96.30 O
% O
1 O
93.14 O
% O
4 O
91.57 O
% O
5 O
99.11 O
% O
2 O
80.27 O
% O
1 O
— O
91.19 O
% O
3 O
SUDA O
– O
Alibaba O
( O
Zhang O
et O
al O
. O
, O
2019b O
) O
86.55 O
% O
8 O
84.51 O
% O
8 O
85.03 O
% O
8 O
97.51 O
% O
8 O
75.22 O
% O
7 O
— O
85.56 O
% O
8 O
Saarland O
( O
Donatelli O
et O
al O
. O
, O
2019 O
) O
93.50 O
% O
6 O
95.21 O
% O
2 O
92.20 O
% O
4 O
99.00 O
% O
3 O
78.32 O
% O
6 O
— O
91.28 O
% O
1 O
( O
b O
) O
PSD O
framework O
System O
Tops O
Labels O
Properties O
Anchors O
Edges O
Attributes O
All O
Bugﬁx O
ST O
submission O
82.82 O
% O
6 O
89.99 O
% O
3 O
91.21 O
% O
1 O
92.67 O
% O
4 O
84.76 O
% O
7 O
— O
89.12 O
% O
4 O
99 O
% O
training O
data O
83.79 O
% O
6 O
90.19 O
% O
3 O
91.19 O
% O
1 O
92.88 O
% O
4 O
85.09 O
% O
6 O
— O
89.37 O
% O
4 O
No O
BERT B-MethodName
embeddings O
73.91 O
% O
8 O
84.52 O
% O
5 O
85.76 O
% O
3 O
89.08 O
% O
5 O
76.73 O
% O
7 O
— O
83.43 O
% O
7 O
Ensemble O
84.59 O
% O
6 O
90.86 O
% O
2 O
92.00 O
% O
1 O
93.52 O
% O
3 O
86.55 O
% O
6 O
— O
90.29 O
% O
3 O
HIT O
- O
SCIR O
( O
Che O
et O
al O
. O
, O
2019 O
) O
85.23 O
% O
5 O
89.45 O
% O
3 O
89.54 O
% O
2 O
94.29 O
% O
2 O
88.77 O
% O
3 O
— O
90.75 O
% O
2 O
SJTU O
– O
NICT O
( O
Li O
et O
al O
. O
, O
2019 O
) O
87.72 O
% O
3 O
89.42 O
% O
4 O
77.53 O
% O
4 O
93.37 O
% O
3 O
87.82 O
% O
4 O
— O
89.90 O
% O
3 O
SUDA O
– O
Alibaba O
( O
Zhang O
et O
al O
. O
, O
2019b O
) O
89.94 O
% O
2 O
91.20 O
% O
1 O
89.72 O
% O
1 O
94.86 O
% O
1 O
89.66 O
% O
2 O
— O
91.85 O
% O
1 O
Saarland O
( O
Donatelli O
et O
al O
. O
, O
2019 O
) O
86.31 O
% O
4 O
90.61 O
% O
2 O
78.99 O
% O
3 O
86.55 O
% O
6 O
90.96 O
% O
1 O
— O
89.10 O
% O
4 O
( O
c O
) O
EDS O
framework O
System O
Tops O
Labels O
Properties O
Anchors O
Edges O
Attributes O
All O
Bugﬁx O
ST O
submission O
62.51 O
% O
9 O
— O
— O
95.44 O
% O
2 O
59.45 O
% O
4 O
39.35 O
% O
4 O
73.24 O
% O
4 O
99 O
% O
training O
data O
63.53 O
% O
9 O
— O
— O
95.80 O
% O
2 O
60.51 O
% O
4 O
39.81 O
% O
4 O
73.95 O
% O
4 O
No O
BERT B-MethodName
embeddings O
59.40 O
% O
10 O
— O
— O
94.11 O
% O
5 O
48.70 O
% O
8 O
26.52 O
% O
6 O
66.90 O
% O
7 O
Ensemble O
63.28 O
% O
9 O
— O
— O
96.19 O
% O
2 O
62.14 O
% O
4 O
41.39 O
% O
3 O
75.22 O
% O
4 O
HIT O
- O
SCIR O
( O
Che O
et O
al O
. O
, O
2019 O
) O
100.00 O
% O
1 O
— O
— O
95.36 O
% O
3 O
72.66 O
% O
1 O
61.98 O
% O
1 O
81.67 O
% O
1 O
SJTU O
– O
NICT O
( O
Li O
et O
al O
. O
, O
2019 O
) O
95.31 O
% O
5 O
— O
— O
96.36 O
% O
1 O
65.56 O
% O
3 O
47.00 O
% O
2 O
77.80 O
% O
3 O
SUDA O
– O
Alibaba O
( O
Zhang O
et O
al O
. O
, O
2019b O
) O
99.56 O
% O
3 O
— O
— O
95.02 O
% O
4 O
67.74 O
% O
2 O
40.80 O
% O
3 O
78.43 O
% O
2 O
Saarland O
( O
Donatelli O
et O
al O
. O
, O
2019 O
) O
80.95 O
% O
8 O
— O
— O
90.81 O
% O
6 O
52.66 O
% O
6 O
— O
67.55 O
% O
6 O
( O
d O
) O
UCCA O
framework O
System O
Tops O
Labels O
Properties O
Anchors O
Edges O
Attributes O
All O
Bugﬁx O
ST O
submission O
80.17 O
% O
6 O
82.09 O
% O
4 O
71.44 O
% O
5 O
— O
60.83 O
% O
6 O
— O
71.83 O
% O
4 O
99 O
% O
training O
data O
80.77 O
% O
6 O
81.69 O
% O
4 O
72.45 O
% O
4 O
— O
60.93 O
% O
6 O
— O
71.73 O
% O
5 O
No O
BERT B-MethodName
embeddings O
70.91 O
% O
8 O
77.67 O
% O
6 O
66.36 O
% O
6 O
— O
51.39 O
% O
8 O
— O
65.29 O
% O
7 O
Ensemble O
81.39 O
% O
6 O
82.40 O
% O
3 O
74.21 O
% O
4 O
— O
62.65 O
% O
3 O
— O
73.03 O
% O
2 O
HIT O
- O
SCIR O
( O
Che O
et O
al O
. O
, O
2019 O
) O
78.15 O
% O
7 O
82.51 O
% O
2 O
71.33 O
% O
5 O
— O
63.21 O
% O
2 O
— O
72.94 O
% O
2 O
SJTU O
– O
NICT O
( O
Li O
et O
al O
. O
, O
2019 O
) O
84.88 O
% O
4 O
78.78 O
% O
5 O
79.08 O
% O
1 O
— O
62.64 O
% O
3 O
— O
71.97 O
% O
3 O
SUDA O
– O
Alibaba O
( O
Zhang O
et O
al O
. O
, O
2019b O
) O
62.86 O
% O
9 O
81.53 O
% O
4 O
74.96 O
% O
3 O
— O
61.78 O
% O
5 O
— O
71.72 O
% O
5 O
Saarland O
( O
Donatelli O
et O
al O
. O
, O
2019 O
) O
86.89 O
% O
1 O
74.02 O
% O
6 O
40.79 O
% O
7 O
— O
62.16 O
% O
4 O
— O
66.72 O
% O
6 O
( O
e O
) O
AMR O
framework O
Table O
4 O
: O
Results O
on O
individual O
frameworks O
. O

4.2 O
, O
we O
conducted O
a O
mixed O
- O
effects O
logistic B-MethodName
regression I-MethodName
on O
trial O
- O
level O
responses O
( O
i.e. O
‘ O
correct’100 O
101 O
102 O
103 O
104 O
105 O
106 O
107 O
108 O
109 O
110 O
111 O
112 O
113 O
114 O
115 O
116 O
117 O
118 O
119 O
120 O
121 O
122 O
123 O
124 O
125 O
126 O
127 O
128 O
129 O
130 O
131 O
132 O
133 O
134 O
135 O
136 O
137 O
138 O
139 O
140 O
141 O
142 O
143 O
144 O
145 O
146 O
147 O
148 O
149150 O
151 O
152 O
153 O
154 O
155 O
156 O
157 O
158 O
159 O
160 O
161 O
162 O
163 O
164 O
165 O
166 O
167 O
168 O
169 O
170 O
171 O
172 O
173 O
174 O
175 O
176 O
177 O
178 O
179 O
180 O
181 O
182 O
183 O
184 O
185 O
186 O
187 O
188 O
189 O
190 O
191 O
192 O
193 O
194 O
195 O
196 O
197 O
198 O
There O
is O
snow O
on O
the O
ground O
 O
outside O
the O
windows O
view O
 O
there O
s O
snow O
on O
the O
ground O
snow O
snow O
snow O
snow O
1 O
2 O
3 O
4 O
5 O
6 O
A O
table O
full O
of O
cups O
tabletable O
full O
of O
cups O
 O
table O
tabletable O
of O
glasswareYou O
are O
looking O
out O
side O
 O
the O
window O
of O
a O
bus O
 O
Looking O
outside O
a O
bus O
bus O
bus O
busrain O
garageit O
is O
looking O
outside O
a O
 O
window O
pan O
where O
you O
can O
 O
clearly O
see O
the O
windows O
frame O
. O

5.2 O
using O
a O
mixed O
- O
effects O
logistic B-MethodName
regression I-MethodName
with O
ﬁxed O
effects O
of O
repetition O
number O
and O
model O
variant O
, O
as O
well O
as O
participant O
- O
level O
random O
intercepts O
and O
slopes O
( O
for O
repetition O
number O
) O
. O

Utterances O
were O
selected O
from O
the O
LSTM B-MethodName
decoder O
using O
beam O
search O
with O
a O
beam O
width O
of O
50 O
and O
standard O
length O
normalization O
to O
mitigate O
the O
default O
bias O
against O
long O
utterances O
( O
e.g. O
Wu O
et O
al O
. O
, O
2016 O
) O
. O

2.2 O
Recurrent O
, O
Hierarchical O
, O
and O
Segmental O
Speech O
Processing O
in O
Humans O
Artiﬁcial O
recurrent O
neural B-MethodName
networks I-MethodName
such O
as O
those O
employed O
here O
were O
initially O
proposed O
as O
algorithmic O
- O
level O
( O
Marr O
, O
1982 O
) O
models O
of O
activity O
in O
biological O
neural B-MethodName
networks I-MethodName
( O
Little O
, O
1974 O
; O
Hopﬁeld O
, O
1982 O
) O
, O
and O
subsequent O
studies O
support O
ubiquitous O
recurrence O
in O
the O
cortex O
( O
Harris O
and O
MrsicFlogel O
, O
2013 O
) O
. O

3.1 O
Encoder O
Our O
encoder O
closely O
follows O
a O
hierarchical O
multiscale O
extension O
( O
HM O
- O
LSTM B-MethodName
, O
Chung O
et O
al O
. O
, O
2017 O
) O
of O
long O
short O
- O
term O
memory O
( O
LSTM B-MethodName
) O
networks O
( O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
. O

The O
encoder O
consists O
of O
multiple O
LSTM B-MethodName
layers O
linked O
by O
discrete O
boundary O
neurons O
that O
govern O
memory O
retention O
and O
information O
ﬂow O
between O
layers O
. O

As O
argued O
in O
Chung O
et O
al O
. O
( O
2017 O
) O
, O
this O
design O
enforces O
a O
trade O
- O
off O
between O
recurrent O
information O
( O
which O
is O
erased O
by O
segmentation O
) O
and O
top O
- O
down O
information O
( O
which O
is O
made O
available O
by O
segmentation).1 O
Although O
the O
linguistic O
quality O
of O
discovered O
HMLSTM B-MethodName
segments O
is O
not O
systematically O
examined O
in O
the O
original O
proposal O
( O
Chung O
et O
al O
. O
, O
2017 O
) O
and O
recent O
analysis O
has O
called O
it O
into O
question O
( O
K O
´ O
ad´ar O
et O
al O
. O
, O
2018 O
) O
, O
our O
results O
indicate O
that O
HM O
- O
LSTMs B-MethodName
can O
discover O
segmental O
structure O
from O
speech O
, O
at O
least O
at O
the O
phonemic O
level O
. O

3.2 O
Decoder O
The O
decoder O
consists O
of O
two O
multi O
- O
layer O
attentional O
sequence O
- O
to O
- O
sequence O
( O
seq2seq O
) O
LSTMs B-MethodName
with O
L O
layers O
each O
, O
one O
backward O
- O
directional O
( O
memory O
) O
and O
one O
forward O
directional O
( O
prediction O
) O
. O

We O
use O
linear B-MethodName
regression I-MethodName
on O
the O
combined O
metrics O
to O
quantitatively O
evaluate O
the O
contribution O
of O
both O
memory O
and O
prediction O
pressures O
to O
phoneme O
acquisition O
. O

We O
tested O
this O
hypothesis O
using O
unidirectional O
transformer O
and O
long O
short O
- O
term O
memory O
network O
( O
LSTM B-MethodName
; O
Hochreiter O
and O
Schmidhuber O
, O
1997 O
) O
language O
models O
. O

We O
ﬁnd O
that O
LSTM B-MethodName
LMs O
fail O
to O
acquire O
a O
subject O
/ O
object O
- O
biased O
IC O
distinction O
that O
inﬂuences O
reference O
or O
RC O
attachment O
. O

Previous O
work O
at O
this O
granularity O
of O
coreference B-TaskName
resolution O
has O
shown O
LSTM B-MethodName
LMs O
strongly O
favor O
reference O
to O
male O
entities O
( O
Jumelet O
et O
al O
. O
, O
2019 O
) O
, O
for O
which O
the O
present O
study O
ﬁnds O
additional O
support O
. O

With O
regards O
to O
linguistic O
representations O
, O
a O
growing O
body O
of O
literature O
suggests O
that O
LSTM B-MethodName
LMs O
are O
able O
to O
acquire O
syntactic O
knowledge O
. O

3 O
Language O
Models O
We O
trained O
25 O
LSTM B-MethodName
LMs O
on O
the O
Wikitext-103 O
corpus O
( O
Merity O
et O
al O
. O
, O
2016 O
) O
with O
a O
vocabulary O
constrained O
to O
the O
most O
frequent O
50 O
K O
words.1We O
used O
1The O
models O
had O
two O
LSTM B-MethodName
layers O
with O
400 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName
each O
, O
400 O
- O
dimensional O
word O
embeddings O
, O
a O
dropout B-HyperparameterName
rate I-HyperparameterName
of O
0.2 B-HyperparameterValue
and O
batchsize O
20 O
, O
and O
were O
trained O
for O
40 O
epochs B-HyperparameterName
( O
with O
early O
stopping O
) O
using O
PyTorch O
. O

The O
LSTMs B-MethodName
and O
code O
for O
the O
experiments O
in O
this O
paper O
can O
be O
found O
at O
https://github.com/ O
forrestdavis O
/ O
ImplicitCausality O
.two O
pretrained O
unidirectional O
transformer O
LMs O
: O
TransformerXL O
( O
Dai O
et O
al O
. O
, O
2019 O
) O
and O
GPT-2 B-MethodName
XL O
( O
Radford O
et O
al O
. O
, O
2019).2 O
TransformerXL O
was O
trained O
on O
Wikitext-103 O
, O
like O
our O
LSTM B-MethodName
LMs O
, O
but O
has O
more O
parameters O
and O
a O
larger O
vocabulary O
. O

GPT-2 B-MethodName
XL O
differs O
from O
the O
other O
models O
in O
lacking O
recurrence O
( O
instead O
utilizing O
non O
- O
recurrent O
self O
- O
attention O
) O
and O
in O
amount O
and O
diversity O
of O
training O
data O
( O
1 O
billion O
words O
compared O
to O
the O
103 O
million O
in O
Wikitext-103 O
) O
. O

Instead O
, O
our O
work O
points O
to O
apparent O
differences O
between O
transformers O
and O
LSTMs B-MethodName
with O
regard O
to O
use O
and O
acquisition O
of O
discourse O
structure O
, O
leaving O
explanatory O
principles O
to O
further O
work O
. O

This O
methodology O
follows O
subject O
- O
verb O
agreement O
experiments O
, O
where O
verbs O
that O
agree O
in O
number O
with O
459 O
verbs O
were O
outside O
of O
our O
LSTM B-MethodName
LM O
vocabulary O
, O
so O
they O
were O
excluded O
. O

This O
is O
particularly O
stark O
for O
the O
syntactic O
stimuli O
where O
the O
embedding O
size O
for O
GPT-2 B-MethodName
XL O
is O
roughly O
13 O
times O
larger O
than O
the O
number O
of O
stimuli O
. O

These O
techniques O
may O
ultimately O
provide O
stronger O
evidence O
for O
representations O
of O
implicit O
causality O
in O
these O
language O
models O
, O
particularly O
for O
the O
LSTM B-MethodName
LMs O
where O
no O
representational O
trace O
of O
implicit O
causality O
was O
found O
. O

It O
is O
worth O
noting O
that O
the O
LSTM B-MethodName
behavior O
does O
not O
show O
an O
inﬂuence O
of O
implicity O
causality O
, O
so O
if O
we O
were O
to O
ﬁnd O
such O
a O
representation O
with O
a O
better O
measure O
of O
similarity O
it O
would O
further O
the O
disconnect O
between O
model O
representations O
and O
behavior O
we O
found O
for O
the O
transformers O
. O

9Given O
the O
BPE O
tokenizer O
for O
GPT-2 B-MethodName
XL O
, O
if O
a O
noun O
was O
broken O
into O
components O
, O
we O
used O
the O
hidden O
representation O
of O
the O
ﬁnal O
component O
. O

Results O
for O
each O
LM O
type O
( O
LSTMs B-MethodName
, O
TransformerXL O
, O
GPT-2 B-MethodName
XL O
) O
are O
given O
in O
Figure O
1 O
. O

Statistical O
analyses10 O
were O
conducted O
via O
linear O
- O
mixed O
effects O
models.11 O
Post O
- O
hoc O
t O
- O
tests O
were O
conducted O
to O
assess O
effects.12 O
As O
is O
visually O
apparent O
in O
Figure O
1 O
, O
all O
three O
models O
showed O
some O
gender O
bias O
( O
male O
for O
TransformerXL O
and O
LSTM B-MethodName
LMs O
and O
female O
for O
GPT-2 B-MethodName
XL O
) O
, O
in O
line O
with O
existing O
ﬁndings O
of O
gender O
preferences O
in O
LSTMs B-MethodName
( O
see O
Jumelet O
et O
al O
. O
, O
2019 O
) O
. O

For O
the O
LSTMs B-MethodName
, O
the O
inﬂuence O
of O
IC O
was O
marginal O
( O
p= O
0:02 O
) O
being O
driven O
by O
an O
extremely O
small O
dif10We O
used O
lmer O
( O
version O
1.1.23 O
; O
Bates O
et O
al O
. O
, O
2015 O
) O
and O
lmerTest O
( O
version O
3.1.2 O
; O
Kuznetsova O
et O
al O
. O
, O
2017 O
) O
in O
R. O

We O
include O
every O
third O
layer O
for O
GPT-2 B-MethodName
XL O
( O
48 O
layers O
total O
) O
. O

We O
concluded O
that O
the O
LSTM B-MethodName
IC O
effect O
was O
spurious O
and O
that O
LSTM B-MethodName
LMs O
acquired O
no O
IC O
- O
conditioned O
expectation O
about O
reference O
. O

The O
behavior O
of O
GPT-2 B-MethodName
XL O
was O
in O
line O
with O
the O
human O
ﬁndings O
from O
Ferstl O
et O
al O
. O
( O
2011 O
) O
. O

This O
suggests O
that O
GPT-2 B-MethodName
XL O
has O
acquired O
a O
robust O
IC O
representation O
that O
inﬂuences O
expectations O
for O
pronominal O
reference.4.4 O
Inﬂuence O
of O
IC O
on O
Referential O
Representation O
We O
turn O
now O
to O
the O
ability O
of O
the O
models O
to O
distinguish O
the O
correct O
referent O
when O
both O
the O
subject O
and O
object O
have O
the O
same O
gender O
. O

For O
LSTM B-MethodName
LMs O
, O
IC O
bias O
did O
not O
inﬂuence O
model O
representations O
, O
at O
least O
as O
measured O
in O
the O
present O
study O
. O

We O
include O
every O
third O
layer O
for O
GPT-2 B-MethodName
XL O
( O
48 O
layers O
total O
) O
. O

For O
GPT-2 B-MethodName
XL O
we O
found O
a O
small O
, O
yet O
signiﬁcant O
, O
difference O
in O
degree O
of O
similarity O
with O
the O
subject O
antecedent O
starting O
in O
layer O
15 O
and O
continuing O
through O
layer O
47 O
. O

The O
self O
- O
paced O
reading O
time O
study O
in O
Rohde O
et O
al O
. O
( O
2011 O
) O
consisted O
of O
20 O
pairs O
of O
sentences O
, O
as O
in O
: O
( O
6 O
) O
a. O
Anna O
scolded O
the O
chef O
of O
the O
aristocrats O
who O
was O
/ O
were O
routinely O
letting O
147 O
prompts O
were O
excluded O
because O
either O
the O
non O
- O
IC O
or O
the O
IC O
verb O
was O
not O
in O
the O
vocabulary O
of O
our O
LSTM B-MethodName
LMs O
. O

This O
resulted O
in O
192 O
test O
sentences O
generated O
from O
12 O
pairs.15 O
15We O
excluded O
pairs O
where O
either O
of O
the O
main O
verbs O
was O
not O
in O
the O
vocabulary O
of O
our O
LSTM B-MethodName
LMs O
. O

The O
LSTM B-MethodName
LMs O
favored O
attachment O
to O
the O
higher O
noun O
in O
0 O
% O
of O
stimuli O
( O
across O
both O
IC O
and O
non O
- O
IC O
stimuli O
) O
. O

The O
LSTM B-MethodName
LMs O
had O
no O
representational O
effect O
of O
IC O
on O
either O
who O
or O
the O
RC O
verb O
, O
similar O
to O
the O
lack O
of O
an O
effect O
in O
pronouns O
. O

Instead O
, O
the O
LSTM B-MethodName
LMs O
( O
i.e. O
where O
we O
do O
not O
expect O
attachment O
to O
the O
higher O
noun O
) O
. O

Finally O
, O
for O
GPT-2 B-MethodName
XL O
, O
for O
the O
stimuli O
with O
an O
object O
- O
biased O
IC O
verb O
attachment O
to O
the O
higher O
noun O
was O
preferred O
in O
23 O
% O
of O
the O
stimuli O
, O
and O
for O
stimuli O
without O
an O
object O
- O
biased O
IC O
verbs O
9 O
% O
of O
the O
time O
. O

For O
GPT-2 B-MethodName
, O
object O
- O
biased O
IC O
verbs O
increased O
the O
similarity O
between O
the O
higher O
noun O
and O
who O
, O
but O
only O
increased O
the O
similarity O
between O
the O
RC O
verb O
and O
the O
higher O
noun O
when O
they O
agreed O
in O
number O
( O
i.e. O
increased O
similarity O
between O
chef O
andwasnot O
chef O
andwere O
) O
. O

Similarly O
, O
GPT-2 B-MethodName
XL O
showed O
no O
preference O
for O
attachment O
location O
in O
the O
ﬁnal O
layers O
despite O
unambiguous O
agreement O
with O
only O
one O
of O
the O
nouns O
. O

These O
results O
suggest O
that O
a O
preference O
for O
local O
agreement O
is O
robust O
in O
both O
LSTMs B-MethodName
and O
transformer O
LMs O
. O

Moreover O
, O
unambiguous O
syntactic O
knowledge O
about O
RC O
attachment O
was O
discarded O
in O
the O
ﬁnal O
layers O
of O
TransformerXL O
and O
GPT-2 B-MethodName
. O

6 O
Discussion O
The O
present O
study O
examined O
the O
extent O
to O
which O
discourse O
structure O
, O
determined O
by O
implicit O
causality O
verbs O
, O
could O
be O
acquired O
by O
transformer O
and O
LSTM B-MethodName
language O
models O
( O
cf O
. O

We O
found O
that O
LSTM B-MethodName
LMs O
were O
unable O
to O
demonstrate O
knowledge O
of O
IC O
either O
in O
inﬂuencing O
reference O
or O
syntax O
. O

However O
, O
a O
transformer O
( O
TransformerXL O
) O
trained O
on O
the O
exact O
same O
data O
as O
the O
LSTM B-MethodName
LMs O
was O
able O
to O
partially O
represent O
an O
IC O
distinction O
, O
but O
model O
output O
was O
only O
inﬂuenced O
by O
IC O
bias O
when O
resolving O
reference O
, O
not O
syntactic O
attachment O
. O

In O
evaluating O
a O
transformer O
modeltrained O
on O
vastly O
more O
data O
( O
GPT-2 B-MethodName
XL O
) O
, O
we O
found O
a O
more O
robust O
, O
human O
- O
like O
sensitivity O
to O
IC O
bias O
when O
resolving O
reference O
: O
subject O
- O
biased O
IC O
verbs O
increased O
model O
preference O
for O
subject O
pronouns O
and O
object O
- O
biased O
IC O
verbs O
increased O
model O
preferences O
for O
object O
pronouns O
. O

In O
contrast O
to O
our O
results O
, O
Davis O
and O
van O
Schijndel O
( O
2020a O
) O
showed O
syntactic O
predictions O
for O
LSTM B-MethodName
LMs O
are O
inﬂuenced O
by O
some O
aspects O
of O
discourse O
structure O
. O

20Further O
cross O
- O
linguistic O
evidence O
bearing O
on O
the O
inability O
of O
LSTM B-MethodName
LMs O
, O
speciﬁcally O
, O
to O
learn O
relative O
clause O
attachment O
is O
given O
in O
Davis O
and O
van O
Schijndel O
( O
2020b O
) O
. O
