Understanding O
the O
Source O
of O
Semantic O
Regularities O
in O
Word O
Embeddings O
. O

Semantic O
relations O
are O
core O
to O
how O
humans O
understand O
and O
express O
concepts O
in O
the O
real O
world O
using O
language O
. O

Recently O
, O
there O
has O
been O
a O
thread O
of O
research O
aimed O
at O
modeling O
these O
relations O
by O
learning O
vector O
representations O
from O
text O
corpora O
. O

Most O
of O
these O
approaches O
focus O
strictly O
on O
leveraging O
the O
co O
- O
occurrences O
of O
relationship O
word O
pairs O
within O
sentences O
. O

In O
this O
paper O
, O
we O
investigate O
the O
hypothesis O
that O
examples O
of O
a O
lexical O
relation O
in O
a O
corpus O
are O
fundamental O
to O
a O
neural O
word O
embedding O
’s O
ability O
to O
complete O
analogies O
involving O
the O
relation O
. O

Our O
experiments O
, O
in O
which O
we O
remove O
all O
known O
examples O
of O
a O
relation O
from O
training O
corpora O
, O
show O
only O
marginal O
degradation O
in O
analogy O
completion O
performance O
involving O
the O
removed O
relation O
. O

This O
ﬁnding O
enhances O
our O
understanding O
of O
neural O
word O
embeddings O
, O
showing O
that O
co O
- O
occurrence O
information O
of O
a O
particular O
semantic O
relation O
is O
the O
not O
the O
main O
source O
of O
their O
structural O
regularity O
. O

1 O
Introduction O
The O
representation O
of O
words O
has O
been O
a O
longstanding O
task O
in O
natural O
language O
processing O
( O
NLP O
) O
. O

The O
main O
underlying O
principle O
is O
known O
for O
decades O
, O
as O
explained O
by O
Firth O
( O
1957 O
) O
. O

This O
principle O
was O
based O
on O
the O
idea O
that O
the O
meaning O
of O
a O
word O
can O
be O
understood O
by O
its O
surrounding O
company O
( O
i.e. O
, O
the O
words O
in O
its O
context O
) O
. O

Most O
modern O
representation O
learning O
theory O
in O
NLP O
is O
based O
on O
this O
assumption O
, O
with O
vector O
representation O
being O
the O
most O
successful O
area O
to O
date O
( O
Turney O
and O
Pantel O
, O
2010 O
) O
. O

More O
recently O
, O
low O
- O
dimensional O
word O
representations O
learned O
from O
text O
corpora O
using O
neural O
networks O
( O
i.e. O
, O
word O
embeddings O
) O
have O
emerged O
( O
Mikolov O
et O
al O
. O
, O
2013a O
; O
Pennington O
et O
al O
. O
, O
2014 O
; O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
stemming B-TaskName
from O
cognitive O
frameworks O
based O
on O
distributed O
representation O
( O
Hinton O
et O
al O
. O
, O
1986 O
; O
Feldman O
and O
Ballard,1982 O
) O
. O

Neural O
word O
embeddings O
have O
been O
proven O
to O
contain O
useful O
information O
about O
concepts O
and O
entities O
, O
and O
provide O
a O
generalization O
boost O
to O
many O
NLP O
applications O
( O
Goldberg O
, O
2017 O
) O
. O

Surprisingly O
, O
these O
representations O
have O
also O
been O
shown O
to O
exhibit O
linear O
relationships O
between O
words O
in O
the O
vector O
space O
, O
demonstrated O
by O
analogy O
. O

For O
example O
, O
Mikolov O
et O
al O
. O
( O
2013b O
) O
showed O
that O
a O
simple O
operation O
such O
as O
king O
- O
man+woman O
will O
result O
in O
a O
point O
near O
queen O
in O
the O
vector O
space1 O
. O

These O
word O
analogies O
have O
been O
extensively O
investigated O
in O
the O
literature O
, O
aiming O
to O
shed O
light O
on O
this O
surprising O
property O
. O

However O
, O
while O
there O
has O
been O
a O
body O
of O
research O
seeking O
to O
understand O
how O
these O
analogies O
work O
( O
Arora O
et O
al O
. O
, O
2016 O
; O
Gittens O
et O
al O
. O
, O
2017 O
; O
Ethayarajh O
et O
al O
. O
, O
2019 O
; O
Allen O
and O
Hospedales O
, O
2019 O
) O
, O
and O
noting O
issues O
about O
their O
methodology O
( O
Linzen O
, O
2016 O
; O
Gladkova O
et O
al O
. O
, O
2016 O
; O
Nissim O
et O
al O
. O
, O
2020 O
) O
, O
there O
has O
not O
been O
a O
speciﬁc O
analysis O
on O
the O
source O
of O
statistical O
cues O
that O
leads O
to O
their O
high O
performance O
on O
this O
task O
. O

Concurrently O
, O
a O
thread O
of O
research O
has O
focused O
on O
explicitly O
modeling O
lexical O
relationships O
of O
word O
pairs O
in O
text O
corpora O
( O
Jameel O
et O
al O
. O
, O
2018 O
; O
EspinosaAnke O
and O
Schockaert O
, O
2018 O
; O
Washio O
and O
Kato O
, O
2018 O
; O
Joshi O
et O
al O
. O
, O
2019 O
; O
Camacho O
- O
Collados O
et O
al O
. O
, O
2019 O
) O
. O

While O
these O
methods O
employ O
different O
means O
for O
learning O
relation O
vectors O
, O
they O
share O
a O
common O
initial O
premise O
: O
only O
co O
- O
occurring O
words O
in O
the O
corpus O
are O
considered2 O
. O

While O
this O
simpliﬁed O
assumption O
works O
well O
enough O
in O
practice O
, O
providing O
a O
useful O
signal O
even O
in O
downstream B-TaskName
NLP O
applications O
, O
in O
this O
paper O
we O
ﬁnd O
that O
valuable O
information O
is O
likely O
lost O
in O
the O
process O
. O

In O
fact O
, O
we O
1More O
information O
of O
how O
word O
analogies O
work O
can O
be O
found O
in O
Section O
2.1 O
. O

2Some O
of O
these O
methods O
also O
provide O
tools O
to O
learn O
representations O
for O
out O
- O
of O
- O
vocabulary O
pairs O
( O
Joshi O
et O
al O
. O
, O
2019 O
; O
Camacho O
- O
Collados O
et O
al O
. O
, O
2019 O
) O
. O

However O
, O
their O
initial O
vector O
spaces O
are O
based O
on O
co O
- O
occurring O
word O
pairs O
only.ﬁnd O
that O
a O
text O
corpus O
provides O
enough O
information O
to O
infer O
pairwise O
relations O
without O
training O
on O
any O
speciﬁc O
examples O
of O
a O
given O
relation O
. O

In O
our O
experiments O
, O
we O
focus O
on O
semantic O
relations O
in O
particular O
, O
which O
are O
the O
most O
suited O
for O
both O
word O
and O
relation O
embedding O
models O
. O

Neural O
word O
embeddings O
are O
used O
to O
learn O
representations O
from O
text O
corpora O
, O
with O
word O
analogies O
as O
the O
evaluation O
mechanism O
to O
test O
our O
hypothesis O
. O

We O
run O
an O
extensive O
set O
of O
control O
experiments O
where O
the O
co O
- O
occurrence O
information O
is O
completely O
removed O
from O
the O
reference O
corpora O
. O

The O
results O
show O
that O
, O
with O
relationship O
instance O
removal O
, O
analogy O
performance O
degrades O
to O
only O
a O
limited O
extent O
, O
overall.3 O
This O
ﬁnding O
suggests O
that O
neural O
embeddings O
do O
not O
learn O
lexical O
relation O
regularities O
from O
examples O
of O
the O
relation O
, O
but O
that O
they O
are O
still O
able O
to O
be O
inferred O
through O
the O
semantic O
featurization O
of O
individual O
words O
. O

2 O
Related O
Work O
2.1 O
Understanding O
analogies O
in O
word O
embeddings O
The O
surprising O
result O
of O
Mikolov O
et O
al O
. O
( O
2013b O
) O
, O
showing O
that O
word O
embedding O
can O
solve O
linear O
analogy O
problems O
, O
led O
to O
a O
careful O
investigation O
by O
researchers O
from O
different O
ﬁelds O
. O

A O
line O
of O
research O
proposed O
mathematical O
formalisms O
to O
try O
to O
understand O
the O
intrinsic O
properties O
of O
word O
embeddings O
. O

Arora O
et O
al O
. O
( O
2016 O
) O
was O
one O
of O
the O
ﬁrst O
to O
provide O
a O
rigorous O
theoretical O
explanation O
on O
the O
linear O
algebraic O
structure O
of O
word O
embeddings O
. O

Their O
formalism O
is O
based O
on O
a O
latent O
variable O
model O
that O
makes O
assumptions O
on O
the O
nature O
of O
the O
vector O
space O
. O

Later O
works O
rely O
on O
the O
notion O
of O
paraphrasing O
( O
Gittens O
et O
al O
. O
, O
2017 O
; O
Allen O
and O
Hospedales O
, O
2019 O
) O
, O
based O
on O
the O
observation O
that O
different O
words O
can O
be O
used O
in O
similar O
contexts O
interchangeably O
, O
dropping O
some O
of O
the O
previous O
assumptions O
made O
by O
Arora O
et O
al O
. O
( O
2016 O
) O
. O

Concurrently O
, O
other O
works O
have O
attempted O
to O
provide O
explanations O
of O
the O
compositional O
properties O
of O
distributional O
models O
through O
additions O
( O
Levy O
and O
Goldberg O
, O
2014a O
; O
Paperno O
and O
Baroni O
, O
2016 O
; O
Ethayarajh O
et O
al O
. O
, O
2019 O
) O
, O
which O
lie O
at O
the O
core O
of O
word O
analogy O
completion O
. O

While O
these O
works O
formalize O
word O
analogies O
and O
attempt O
to O
explain O
how O
they O
work O
mathematically O
, O
3In O
a O
few O
speciﬁc O
relations O
the O
degradation O
is O
more O
marked O
. O

Nonetheless O
, O
this O
degradation O
does O
not O
surpass O
50 O
% O
even O
in O
the O
most O
unfavorable O
case O
for O
accuracy O
, O
and O
for O
other O
metrics O
this O
degradation O
does O
not O
surpass O
20%.our O
empirical O
analysis O
is O
focused O
on O
understanding O
the O
source O
of O
signal O
in O
corpora O
that O
affect O
the O
performance O
of O
word O
analogy O
completion O
, O
without O
asserting O
any O
predeﬁned O
assumption O
. O

In O
particular O
, O
we O
are O
mostly O
interested O
in O
determining O
whether O
relationship O
pair O
co O
- O
occurrence O
in O
sentences O
is O
necessary O
in O
order O
for O
a O
word O
embedding O
to O
succeed O
at O
analogy O
completion O
. O

2.2 O
Issues O
in O
word O
analogies O
A O
number O
of O
publications O
have O
encountered O
methodological O
issues O
in O
the O
word O
analogy O
task O
through O
word O
embeddings O
. O

Levy O
and O
Goldberg O
( O
2014a O
) O
found O
that O
the O
addition O
operations O
may O
not O
be O
optimal O
, O
as O
they O
are O
reduced O
to O
three O
separate O
similarity O
problems O
that O
can O
be O
solved O
through O
more O
appropriate O
operations O
. O

Linzen O
( O
2016 O
) O
showed O
that O
simple O
baselines O
based O
on O
nearest O
neighbour O
searches O
are O
competitive O
in O
the O
analogy O
categories O
proposed O
by O
Mikolov O
et O
al O
. O
( O
2013b O
) O
. O

Because O
of O
this O
, O
Gladkova O
et O
al O
. O
( O
2016 O
) O
proposed O
a O
new O
dataset O
, O
partially O
addressing O
some O
of O
the O
previous O
shortcomings O
. O

Other O
works O
have O
shown O
that O
linear O
relationships O
, O
while O
being O
implicit O
, O
are O
not O
directly O
apparent O
in O
the O
word O
embedding O
space O
, O
and O
therefore O
word O
analogies O
may O
not O
be O
the O
best O
method O
to O
retrieve O
this O
information O
( O
Drozd O
et O
al O
. O
, O
2016 O
; O
Schluter O
, O
2018 O
; O
Bouraoui O
et O
al O
. O
, O
2018 O
) O
. O

Finally O
, O
Gonen O
and O
Goldberg O
( O
2019 O
) O
and O
Nissim O
et O
al O
. O
( O
2020 O
) O
cautioned O
against O
over O
- O
reliance O
on O
analogies O
as O
a O
means O
to O
uncover O
and O
correct O
for O
biases O
in O
word O
embeddings O
. O

These O
methodological O
observations O
challenge O
the O
supremacy O
of O
analogy O
evaluations O
as O
the O
optimal O
proxy O
for O
downstream B-TaskName
task O
performance O
of O
a O
word O
embedding O
. O

Nevertheless O
, O
they O
represent O
a O
valuable O
mechanism O
with O
which O
to O
compare O
the O
semantic O
regularities O
of O
two O
different O
neural O
embeddings O
. O

In O
particular O
, O
word O
analogies O
represent O
an O
ideal O
benchmark O
for O
our O
research O
questions O
, O
as O
the O
impact O
of O
co O
- O
occurrence O
statistics O
within O
word O
relations O
can O
be O
evaluated O
directly O
through O
analogy O
validation O
. O

This O
would O
not O
be O
the O
case O
for O
other O
more O
complicated O
tasks O
such O
as O
relation O
classiﬁcation O
or O
extraction O
, O
which O
may O
add O
additional O
confounds O
. O

3 O
Methodology O
In O
this O
section O
we O
explain O
the O
experimental O
methodology O
we O
follow O
to O
answer O
our O
main O
research O
question O
. O

First O
, O
we O
brieﬂy O
describe O
how O
to O
solve O
wordanalogies O
using O
word O
embeddings O
( O
Section O
3.1 O
) O
. O

We O
then O
explain O
our O
methodology O
to O
compile O
corpora O
to O
train O
word O
embeddings O
( O
Section O
3.2 O
) O
. O

3.1 O
Solving O
word O
analogies O
with O
neural O
word O
embeddings O
The O
ﬁrst O
step O
to O
solve O
word O
analogies O
using O
neural O
word O
embeddings O
is O
to O
ﬁrst O
learn O
word O
vectors O
from O
an O
unlabeled O
text O
corpora O
. O

To O
do O
so O
, O
standard O
word O
embedding O
models O
such O
as O
Word2Vec O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
, O
GloVe B-MethodName
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
or O
FastText O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
are O
often O
used O
. O

The O
output O
of O
these O
models O
is O
a O
vector O
space O
where O
each O
word O
is O
represented O
as O
a O
single O
point O
. O

With O
these O
vectors O
, O
mathematical O
operations O
can O
then O
be O
made O
to O
solve O
a O
given O
analogy O
. O

Formally O
, O
given O
three O
words O
( O
a O
, O
bandc O
) O
, O
the O
task O
of O
word O
analogy O
completion O
consists O
of O
predicting O
the O
most O
appropriate O
word O
dthat O
satisﬁes O
ais O
to O
b O
ascis O
to O
d. O
In O
this O
case O
, O
both O
a O
- O
bandc O
- O
dare O
part O
of O
the O
same O
relationship O
. O

For O
instance O
, O
in O
the O
case O
ofParis O
, O
France O
andBerlin O
, O
the O
word O
to O
retrieve O
would O
be O
Germany O
. O

In O
this O
case O
both O
Paris O
-France O
andBerlin O
-Germany O
belong O
to O
the O
capital O
- O
of O
relation O
. O

With O
word O
embeddings O
this O
can O
be O
solved O
with O
the O
simple O
vector O
operation4 O
~ O
b ~ O
a+~ O
c O
, O
retrieving O
the O
word O
whose O
vector O
is O
closest O
to O
that O
point O
in O
the O
space O
. O

Word O
analogy O
completion O
is O
used O
as O
the O
main O
evaluation O
for O
our O
experiments O
. O

3.2 O
Corpus O
preparation O
Our O
main O
research O
question O
is O
whether O
an O
explicit O
observation O
of O
a O
relationship O
in O
a O
corpus O
is O
necessary O
to O
complete O
an O
example O
of O
that O
relationship O
via O
analogy O
. O

To O
this O
end O
, O
given O
a O
reference O
unlabeled O
corpus O
, O
we O
devise O
the O
following O
methodology O
per O
lexical O
relation O
type O
. O

3.2.1 O
Sentence O
removal O
First O
, O
for O
each O
relation O
type O
( O
e.g. O

capital O
- O
of O
) O
in O
a O
dataset O
, O
we O
remove O
all O
sentences O
from O
the O
corpus O
that O
contain O
word O
pairs O
belonging O
to O
the O
relation O
. O

This O
results O
in O
a O
modiﬁed O
corpus O
for O
each O
relation O
type O
and O
a O
respective O
word O
embedding O
for O
each O
relation O
type O
trained O
on O
the O
modiﬁed O
corpus O
. O

For O
example O
, O
for O
the O
pair O
Lisbon O
-Portugal O
, O
we O
would O
remove O
all O
sentences O
from O
our O
reference O
corpus O
4Levy O
and O
Goldberg O
( O
2014a O
) O
showed O
that O
a O
multiplication O
operation O
can O
also O
be O
used O
to O
solve O
analogies O
. O

However O
, O
for O
this O
analysis O
we O
focused O
on O
the O
traditional O
solution O
of O
the O
problem O
proposed O
by O
Mikolov O
et O
al O
. O
( O
2013b).where O
Lisbon O
andPortugal O
co O
- O
occur O
, O
and O
this O
process O
would O
be O
repeated O
for O
all O
pairs O
of O
the O
capital O
- O
of O
relation O
. O

3.2.2 O
Sentence O
replacement O
This O
setting O
is O
similar O
to O
the O
sentence O
removal O
strategies O
with O
the O
added O
inclusion O
of O
new O
sentences O
to O
replace O
the O
removed O
ones O
. O

In O
particular O
, O
for O
each O
removed O
sentence O
containing O
a O
word O
pair O
of O
a O
relation O
type O
, O
two O
sentences O
from O
a O
similar O
corpus O
replace O
it O
, O
where O
each O
sentence O
contains O
one O
of O
the O
words O
in O
the O
pair O
. O

This O
is O
arguably O
the O
most O
fairly O
comparable O
setting O
to O
no O
removal O
, O
as O
the O
number O
of O
occurrences O
for O
each O
word O
in O
the O
relation O
would O
be O
approximately O
the O
same O
with O
respect O
to O
the O
default O
setting O
. O

The O
overall O
number O
of O
sentences O
would O
be O
slightly O
higher O
, O
though O
this O
is O
negligible O
in O
comparison O
to O
the O
full O
corpus O
( O
see O
Section O
4.1 O
for O
the O
speciﬁc O
details O
on O
the O
corpora O
used O
for O
the O
evaluation O
) O
. O

For O
both O
the O
sentence B-TaskName
replacement I-TaskName
and O
sentence B-TaskName
removal I-TaskName
strategies O
, O
we O
also O
experiment O
with O
a O
more O
aggressive O
setting O
. O

In O
this O
setting O
( O
referred O
to O
as O
removal+ O
orreplacement+ O
in O
our O
experiments O
) O
, O
all O
sentences O
containing O
any O
two O
words O
from O
the O
vocabulary O
of O
the O
relation5are O
removed O
, O
in O
the O
case O
of O
removal+ O
, O
or O
replaced O
, O
in O
the O
case O
of O
replacement+ O
. O

This O
is O
the O
most O
aggressive O
setting O
where O
no O
co O
- O
occurrence O
information O
of O
a O
given O
relation O
is O
preserved O
. O

4 O
Evaluation O
In O
this O
section O
we O
provide O
the O
details O
of O
our O
experimental O
setup O
( O
Section O
4.1 O
) O
and O
then O
, O
the O
main O
results O
of O
our O
evaluation O
( O
Section O
4.2 O
) O
. O

4.1 O
Experimental O
Setting O
In O
the O
following O
we O
describe O
the O
experimental O
setting O
for O
all O
our O
experiments O
. O

More O
details O
and O
code O
to O
reproduce O
our O
experiments O
can O
be O
found O
online6 O
. O

Text O
corpora O
. O

As O
our O
reference O
corpus O
we O
selected O
UMBC O
( O
Han O
et O
al O
. O
, O
2013 O
) O
, O
which O
is O
a O
diverse O
3 O
- O
billion O
- O
token O
corpus O
of O
paragraphs O
extracted O
from O
the O
web O
, O
amounting O
to O
a O
total O
of O
132 O
M O
sentences O
. O

In O
particular O
, O
we O
randomly O
select O
80 O
% O
of O
all O
sentences O
which O
would O
be O
the O
base O
corpus O
for O
the O
experiments O
. O

Then O
, O
we O
used O
the O
remaining O
5Recall B-MetricName
from O
the O
previous O
subsection O
( O
Section O
3.1 O
) O
that O
each O
analogy O
instance O
is O
formed O
by O
four O
words O
. O

6https://github.com/h-yuc/ O
Lexical O
- O
Relations O
- O
Analogies20 O
% O
to O
add O
replacement O
sentences O
when O
necessary O
( O
see O
Section O
3.2.2 O
for O
more O
details O
on O
the O
replacement O
setting O
) O
. O

To O
complement O
the O
main O
results O
and O
test O
the O
generalization O
of O
our O
ﬁndings O
, O
we O
also O
use O
Wikipedia7(2 O
billion O
tokens O
and O
104 O
M O
sentences O
) O
for O
the O
base O
removal O
experiments O
. O

Word O
embedding O
models O
. O

For O
word O
embedding O
models O
, O
we O
use O
both O
the O
CBOW O
and O
Skip O
- O
Gram O
variants O
of O
Word2Vec B-MethodName
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
. O

These O
neural O
representation O
learning O
approaches O
have O
shown O
to O
be O
amenable O
to O
analogy O
completion O
since O
their O
introduction O
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
. O

Unlike O
FastText O
( O
Bojanowski O
et O
al O
. O
, O
2017 O
) O
, O
they O
do O
not O
include O
character O
information O
and O
are O
therefore O
more O
suitable O
for O
our O
experiments O
as O
pure O
word O
- O
based O
models O
. O

We O
use O
standard O
hyperparameters O
for O
both O
CBOW B-MethodName
and O
Skip B-MethodName
- I-MethodName
Gram I-MethodName
, O
with O
300dimensions O
and O
a O
window O
size O
of O
10 O
in O
both O
cases O
. O

Given O
the O
difference O
in O
speed O
( O
CBOW O
being O
around O
ﬁve O
times O
faster O
to O
train O
) O
and O
the O
small O
performance O
difference O
, O
we O
considered O
CBOW O
for O
all O
our O
main O
experiments O
, O
but O
included O
Skip O
- O
Gram O
results O
in O
the O
appendix O
. O

Validation O
datasets O
. O

Google O
( O
Mikolov O
et O
al O
. O
, O
2013b O
) O
and O
BATS O
( O
Gladkova O
et O
al O
. O
, O
2016 O
) O
relation O
example O
datasets O
are O
used O
for O
analogy O
completion O
. O

BATS O
was O
introduced O
after O
the O
Google O
dataset O
to O
address O
some O
of O
its O
shortcomings O
in O
the O
number O
and O
type O
of O
relations O
, O
so O
the O
inclusion O
of O
both O
datasets O
in O
our O
experiments O
help O
give O
a O
more O
general O
overview O
. O

In O
particular O
, O
we O
focus O
on O
the O
semantic O
relations O
of O
each O
dataset O
. O

Table O
1 O
shows O
the O
statistics O
of O
the O
relations O
considered O
in O
each O
of O
the O
datasets O
. O

Evaluation O
protocol O
. O

For O
solving O
word O
analogies O
, O
we O
follow O
the O
original O
methodology O
of O
Mikolov O
et O
al O
. O
( O
2013b)8 O
, O
as O
explained O
in O
Section O
3.1 O
. O

For O
simplicity O
, O
we O
unify O
the O
evaluation O
setting O
for O
both O
datasets O
where O
we O
only O
consider O
a O
single O
solution O
. O

In O
the O
case O
of O
BATS O
, O
we O
consider O
the O
ﬁrst O
answer O
as O
provided O
in O
the O
dataset O
, O
which O
is O
generally O
the O
most O
speciﬁc O
. O

With O
the O
expectation O
that O
performance O
after O
co O
- O
occurrence O
removal O
may O
degrade O
substantially O
, O
we O
report O
analogy O
completion O
results O
using O
recall B-MetricName
at O
1 B-MetricValue
( O
accuracy O
) O
, O
10 O
, O
and O
50 O
. O

Recall@50 B-MetricName
, O
for O
example O
, O
reports O
the O
percentage O
of O
analogy O
completions O
in O
which O
the O
expected O
word O
was O
among O
the O
50 O
nearest B-MethodName
neighbors I-MethodName
to O
the O
three O
7We O
use O
the O
Wikipedia O
dump O
of O
November O
2016 O
. O

8As O
in O
the O
original O
protocol O
, O
words O
that O
are O
included O
in O
the O
instance O
are O
excluded O
from O
the O
nearest B-MethodName
neighbours I-MethodName
search O
. O
Google O
capital O
- O
world O
4,524 O
232 O
Muscat O
, O
Oman O
, O
Tokyo O
, O
Japan O
currency O
866 O
60 O
Europe O
, O
euro O
, O
Korea O
, O
won O
city O
- O
in O
- O
state O
2,467 O
94 O
Toledo O
, O
Ohio O
, O
Dallas O
, O
Texas O
family O
506 O
46 O
king O
, O
queen O
, O
man O
, O
woman O
nationality O
- O
adj O
. O

1,599 O
82 O
Greece O
, O
Greek O
, O
Spain O
, O
SpanishBATScountry O
- O
capital O
2450 O
100 O
Hanoi O
, O
Vietnam O
, O
Rome O
, O
Italy O
country O
- O
language O
2,450 O
100 O
Jordan O
, O
Arabic O
, O
USA O
, O
English O
UKcity O
- O
county O
2,450 O
100 O
Exeter O
, O
Devon O
, O
Wells O
, O
Somerset O
name O
- O
nationality O
2,450 O
100 O
Caesar O
, O
Roman O
, O
Plato O
, O
Greek O
name O
- O
occupation O
2,450 O
100 O
Plato O
, O
philosopher O
, O
Dante O
, O
poet O
animal O
- O
young O
2,450 O
100 O
bee O
, O
larva O
, O
ox O
, O
calf O
animal O
- O
sound O
2,450 O
100 O
bee O
buzz O
frog O
ribbit O
animal O
- O
shelter O
2,450 O
100 O
horse O
, O
stable O
, O
ant O
, O
anthill O
things O
- O
color O
2,450 O
100 O
coffee O
, O
black O
, O
cream O
, O
white O
male O
- O
female O
2,450 O
100 O
son O
, O
daughter O
, O
father O
, O
mother O
hypernym O
- O
animal O
2,450 O
100 O
human O
, O
primate O
, O
cat O
, O
feline O
hypernym O
- O
misc O
2,450 O
100 O
pastry O
, O
food O
, O
plum O
, O
fruit O
hyponym O
- O
misc O
2,450 O
100 O
bag O
, O
pouch O
, O
dessert O
, O
cake O
meronym O
- O
subst O
. O

2,450 O
100 O
bag O
, O
leather O
, O
penny O
, O
metal O
meronym O
- O
member O
2,450 O
100 O
bird O
, O
ﬂock O
, O
page O
, O
book O
meronym O
- O
part O
2,450 O
100 O
day O
, O
hour O
, O
dollar O
, O
cent O
synonym O
- O
intensity O
2,450 O
100 O
angry O
, O
furious O
, O
ask O
, O
beg O
synonym O
- O
exact O
2,450 O
100 O
help O
, O
aid O
, O
child O
, O
kid O
antonym O
- O
gradable O
2,450 O
100 O
aware O
, O
unaware O
, O
slow O
, O
fast O
antonym O
- O
binary O
2,450 O
100 O
after O
, O
before O
, O
below O
, O
above O
Table O
1 O
: O
Statistics O
of O
the O
word O
analogy O
datasets O
used O
in O
our O
experiments O
: O
Number O
of O
instances O
( O
# O
inst O
) O
and O
unique O
words O
( O
# O
word O
) O
. O

word O
subtraction O
and O
addition O
. O

The O
inclusion O
of O
recall B-MetricName
at O
different O
thresholds B-MetricName
allows O
for O
a O
more O
complete O
overview O
of O
the O
performance O
, O
as O
the O
standard O
accuracy B-MetricName
measure O
alone O
may O
not O
reﬂect O
the O
full O
picture O
( O
Gladkova O
et O
al O
. O
, O
2016 O
; O
Schluter O
, O
2018 O
) O
. O

Training O
. O

For O
each O
relation O
type O
, O
we O
utilized O
three O
different O
variants O
of O
each O
corpora O
that O
are O
used O
to O
train O
word O
embeddings O
( O
i.e. O
, O
the O
original O
corpus O
and O
two O
resulting O
from O
our O
removal O
and O
replacement O
strategies O
, O
as O
explained O
in O
Section O
3.2 O
) O
. O

To O
reduce O
the O
amount O
of O
training O
, O
we O
only O
considered O
the O
default O
and O
removal O
strategy O
for O
Wikipedia9 O
, O
while O
all O
experiments O
are O
performed O
on O
the O
main O
UMBC O
reference O
corpus O
. O

In O
total O
, O
we O
compiled O
156 O
different O
corpora O
, O
occupying O
2.2 O
TB O
of O
disk O
space O
, O
and O
learned O
184 O
different O
word O
embedding O
models O
, O
totalling O
around O
1,980 O
hours O
( O
around O
83 O
full O
days O
) O
of O
model O
training O
on O
a O
high O
performance O
single O
node O
( O
48 O
- O
core O
) O
system O
. O

4.2 O
Results O
As O
explained O
in O
the O
previous O
section O
, O
our O
experiments O
are O
aimed O
at O
understanding O
the O
role O
of O
co O
- O
occurring O
words O
in O
a O
given O
relation O
type O
( O
e.g. O

9Likewise O
, O
for O
Skip O
- O
Gram O
we O
only O
considered O
the O
Google O
analogy O
dataset O
and O
a O
single O
strategy O
per O
corpus O
( O
results O
in O
the O
appendix O
) O
. O

Rm O
/ O
Rp O
Rm+/Rp+ O
Def O
Rm O
Rp O
Rm+ O
Rp+ O
Def O
Rm O
Rp O
Rm+ O
Rp+ O
Def O
Rm O
Rp O
Rm+ O
Rp+Googlecap.-country O
49,248 O
245,677 O
61.7 O
53.0 O
52.4 O
51.4 O
51.6 O
89.7 O
81.6 O
82.6 O
83.4 O
83.4 O
98.2 O
90.3 O
90.5 O
91.3 O
92.3 O
cap.-world O
79,060 O
452,915 O
49.4 O
31.3 O
32.7 O
30.1 O
33.6 O
82.0 O
65.8 O
67.9 O
65.0 O
67.2 O
91.2 O
79.2 O
80.5 O
78.6 O
80.8 O
currency O
4,260 O
145,179 O
8.7 O
5.1 O
6.9 O
5.8 O
6.2 O
37.3 O
32.6 O
34.2 O
33.1 O
34.5 O
63.5 O
58.7 O
59.6 O
58.4 O
60.4 O
city O
- O
state O
96,666 O
247,238 O
14.1 O
7.7 O
7.7 O
8.6 O
9.1 O
40.4 O
28.0 O
28.0 O
28.9 O
31.5 O
62.1 O
50.1 O
50.0 O
49.9 O
53.8 O
family O
450,875 O
2,830,852 O
91.1 O
85.0 O
82.0 O
69.8 O
72.9 O
100.0 O
95.3 O
94.7 O
88.1 O
89.7 O
100.0 O
98.4 O
96.8 O
90.5 O
91.1 O
nation O
- O
adj O
145,064 O
492,671 O
86.4 O
80.2 O
81.7 O
81.3 O
83.1 O
96.0 O
96.1 O
95.6 O
96.4 O
95.9 O
97.9 O
97.8 O
97.9 O
97.7 O
97.4 O
A O
VERAGE O
137,529 O
735,755 O
51.9 O
43.7 O
43.9 O
41.2 O
42.7 O
74.2 O
66.5 O
67.1 O
65.8 O
67.0 O
85.5 O
79.1 O
79.2 O
77.7 O
79.3BATScountry O
- O
cap O
66,988 O
369,463 O
71.6 O
54.7 O
55.8 O
56.7 O
59.6 O
90.9 O
86.9 O
87.3 O
86.1 O
86.5 O
95.1 O
93.0 O
93.1 O
91.6 O
91.2 O
country O
- O
lang O
21,066 O
649,071 O
26.7 O
24.0 O
24.9 O
16.9 O
19.6 O
60.8 O
56.9 O
57.4 O
47.5 O
52.5 O
74.8 O
73.3 O
73.1 O
62.6 O
69.1 O
city O
- O
county O
2,599 O
56,719 O
1.8 O
1.2 O
1.2 O
0.2 O
0.7 O
10.4 O
7.8 O
8.3 O
2.5 O
5.7 O
27.7 O
25.3 O
25.0 O
6.3 O
17.8 O
name O
- O
nation O
. O

12,069 O
1,808,784 O
20.6 O
17.5 O
18.1 O
18.9 O
16.1 O
54.7 O
50.7 O
51.6 O
50.6 O
46.9 O
72.5 O
68.9 O
69.6 O
65.1 O
65.9 O
name O
- O
occup O
. O

10,934 O
1,030,106 O
45.1 O
41.8 O
42.1 O
12.0 O
34.9 O
72.2 O
70.6 O
69.8 O
30.4 O
65.1 O
80.6 O
79.4 O
79.0 O
44.9 O
76.0 O
animal O
- O
young O
3,238 O
221,647 O
3.6 O
2.7 O
3.0 O
1.2 O
3.4 O
15.5 O
13.6 O
12.4 O
5.8 O
10.2 O
28.4 O
26.4 O
25.6 O
10.7 O
21.4 O
animal O
- O
sound O
1,307 O
75,529 O
3.8 O
2.7 O
3.5 O
0.8 O
2.0 O
12.2 O
9.0 O
11.0 O
2.8 O
6.4 O
20.0 O
15.9 O
17.2 O
4.5 O
11.1 O
animal O
- O
shelter O
11,892 O
569,567 O
3.3 O
1.7 O
1.7 O
0.6 O
1.0 O
16.6 O
12.4 O
12.5 O
3.4 O
9.3 O
32.0 O
26.0 O
25.7 O
8.0 O
18.8 O
things O
- O
color O
52,219 O
1,179,341 O
11.2 O
11.5 O
11.8 O
12.5 O
10.9 O
41.6 O
39.1 O
41.0 O
41.1 O
33.7 O
59.4 O
55.2 O
56.7 O
54.7 O
49.5 O
male O
- O
female O
108,898 O
445,429 O
48.0 O
42.8 O
43.5 O
38.6 O
40.0 O
70.4 O
69.3 O
68.3 O
63.6 O
64.7 O
79.0 O
77.4 O
77.0 O
73.6 O
72.7 O
hyper O
- O
animal O
3,410 O
93,258 O
9.6 O
8.4 O
7.8 O
1.6 O
4.4 O
41.1 O
37.5 O
38.4 O
9.8 O
29.3 O
67.1 O
63.8 O
64.8 O
24.1 O
52.5 O
hyper O
- O
misc O
15,157 O
456,525 O
4.7 O
4.4 O
4.2 O
1.6 O
3.1 O
23.8 O
22.9 O
22.3 O
14.8 O
20.9 O
43.0 O
41.5 O
40.8 O
28.2 O
37.6 O
hypo O
- O
misc O
32,118 O
274,377 O
9.8 O
10.3 O
9.3 O
8.8 O
10.9 O
55.9 O
54.2 O
55.4 O
49.1 O
55.1 O
81.3 O
79.3 O
79.1 O
73.4 O
79.0 O
mero O
- O
subst O
. O

68,128 O
1,548,273 O
6.5 O
5.4 O
5.3 O
2.9 O
3.7 O
30.7 O
26.0 O
25.8 O
15.6 O
20.8 O
52.5 O
45.2 O
45.4 O
30.5 O
40.7 O
mero O
- O
member O
257,447 O
4,897,322 O
3.1 O
2.9 O
2.8 O
2.3 O
3.0 O
26.9 O
24.7 O
25.3 O
17.9 O
25.9 O
44.7 O
41.8 O
42.1 O
30.4 O
43.4 O
mero O
- O
part O
30,016 O
364,053 O
2.9 O
2.7 O
2.4 O
2.7 O
2.7 O
29.0 O
27.4 O
27.2 O
23.9 O
26.9 O
49.8 O
48.1 O
48.1 O
42.0 O
46.7 O
syn O
- O
intens O
. O

40,241 O
1,329,093 O
17.9 O
17.7 O
17.4 O
14.8 O
19.0 O
49.6 O
47.1 O
49.0 O
42.3 O
47.2 O
70.2 O
68.4 O
68.9 O
59.8 O
68.3 O
syn O
- O
exact O
49,292 O
882,075 O
27.8 O
26.0 O
25.8 O
26.6 O
27.5 O
69.0 O
66.2 O
65.4 O
66.9 O
68.6 O
85.8 O
84.2 O
83.8 O
84.4 O
84.9 O
auto O
- O
grad O
. O

237,221 O
2,397,131 O
21.3 O
20.0 O
19.8 O
17.1 O
22.1 O
46.2 O
43.3 O
43.6 O
40.4 O
46.9 O
65.1 O
61.5 O
61.6 O
59.1 O
65.4 O
auto O
- O
binary O
1,648,965 O
42,023,189 O
27.6 O
26.2 O
27.6 O
28.6 O
35.1 O
59.2 O
56.1 O
56.7 O
57.4 O
67.2 O
71.1 O
69.9 O
70.0 O
71.8 O
80.6 O
A O
VERAGE O
133,660 O
3,033,547 O
18.3 O
16.2 O
16.4 O
13.3 O
16.0 O
43.8 O
41.1 O
41.4 O
33.6 O
39.5 O
60.0 O
57.2 O
57.3 O
46.3 O
54.6 O
Table O
2 O
: O
UMBC O
corpus O
word O
analogy O
results O
using O
CBOW B-MethodName
with O
ﬁve O
different O
conﬁgurations O
: O
Default O
( O
Def O
) O
, O
Remove O
( O
Rm O
) O
, O
Replace O
( O
Rp O
) O
and O
their O
more O
aggressive O
counterparts O
removing O
all O
pairwise O
co O
- O
occurrences O
( O
Rm+ O
and O
Rp+ O
) O
. O

capital O
- O
of O
) O
as O
they O
pertain O
to O
analogy O
completion O
. O

Table O
2 O
shows O
the O
main O
set O
of O
results O
of O
the O
CBOW B-MethodName
model O
on O
the O
UMBC O
corpus O
. O

As O
can O
be O
observed O
, O
and O
as O
expected O
, O
the O
default O
model O
( O
i.e. O
, O
no O
cooccurrence O
removal O
) O
trained O
on O
the O
original O
corpus O
provides O
the O
highest O
analogy O
completion O
results O
, O
overall O
. O

However O
, O
less O
expected O
is O
the O
low O
magnitude O
decrease O
in O
performance O
of O
the O
experiments O
involving O
co O
- O
occurrence O
removal O
. O

The O
default O
experiments O
performed O
analogies O
with O
51.9 B-MetricValue
% O
accuracy B-MetricName
( O
R@1 O
) O
, O
on O
average B-MetricName
, O
compared O
to O
42.7 B-MetricValue
% O
with O
the O
most O
aggressive O
replace O
plus O
( O
Rp+ O
) O
strategy O
on O
the O
Google O
dataset O
, O
and O
18.1 B-MetricValue
% O
vs. O

16.0 O
% O
, O
respectively O
, O
on O
the O
BATS O
dataset O
. O

Figure O
1 O
shows O
the O
average B-MetricName
decrease O
in O
performance O
of O
the O
replace O
strategy O
( O
Rp O
) O
per O
relation O
type O
as O
a O
percentage O
of O
the O
default O
performance O
. O

For O
R@1 O
, O
the O
decrease O
in O
performance O
is O
lower O
than O
10 B-MetricValue
% O
for O
the O
majority O
of O
relations O
. O

When O
considering O
R@10 O
and O
R@50 O
, O
this O
decrease O
in O
performance O
is O
even O
less O
pronounced O
, O
which O
suggests O
that O
the O
main O
geometrical O
features O
of O
the O
space O
were O
largely O
preserved O
( O
a O
more O
visu O
- O
alization O
of O
the O
space O
is O
presented O
in O
Section O
5.2 O
) O
. O

Theanimal O
- O
shelter O
signiﬁcant O
decrease O
in O
performance O
is O
a O
special O
case O
as O
the O
performance O
of O
the O
default O
model O
was O
very O
low O
to O
start O
with O
( O
3.3 B-MetricValue
% O
) O
, O
which O
highlights O
the O
difﬁculty O
to O
model O
that O
particular O
relationship O
via O
word O
analogies O
. O

The O
same O
could O
be O
attributed O
to O
the O
city O
- O
state O
( O
Default O
accuracy B-MetricName
of O
14.1 B-MetricValue
% O
) O
, O
which O
is O
the O
relation O
with O
the O
second O
highest O
decrease O
in O
performance O
for O
R@1 O
. O

Finally O
, O
Table O
3 O
shows O
experimental O
results O
for O
the O
default O
, O
remove O
( O
Rm O
) O
and O
remove O
plus O
( O
Rm+ O
) O
strategies O
for O
models O
trained O
on O
the O
Wikipedia O
corpus O
. O

The O
results O
are O
slightly O
higher O
overall O
, O
given O
the O
clean O
, O
consistent O
, O
and O
topically O
comprehensive O
nature O
of O
the O
corpus O
. O

The O
overall O
difference O
between O
default O
and O
removal O
strategies O
was O
similar O
to O
that O
of O
UMBC O
( O
reminder O
that O
the O
remove O
plus O
strategy O
consisted O
of O
removing O
all O
sentences O
where O
any O
two O
words O
from O
a O
given O
relation O
co O
- O
occur O
) O
. O

No O
co O
- O
occurrence O
removal O
( O
Def O
) O
vs. O

removal O
plus O
( O
Rm+ O
) O
had O
analogy O
completion O
accuracies B-MetricName
of O
59 B-MetricValue
% O
vs. O

21.7 B-MetricValue
% O
vs. O

20.3 B-MetricValue
% O
on O
the O
BATS O
dataset O
. O

5 O
Analysis O
In O
this O
section O
we O
aim O
to O
better O
understand O
the O
results O
presented O
in O
the O
previous O
section O
. O

In O
particular O
, O
we O
analyze O
to O
what O
extent O
the O
performance O
drops O
that O
exist O
are O
correlated O
with O
frequencies O
of O
words O
in O
the O
relationship O
( O
Section O
5.1 O
) O
. O

We O
also O
compare O
, O
through O
principal B-MethodName
component I-MethodName
analysis I-MethodName
( O
PCA B-MethodName
) O
visualization O
, O
the O
structure O
of O
the O
relation O
with O
the O
highest O
point O
decrease O
, O
before O
and O
after O
co O
- O
occurrence O
removal O
( O
Section O
5.2 O
) O
. O

5.1 O
Correlation O
with O
word O
frequency O
A O
natural O
question O
that O
may O
arise O
when O
looking O
at O
the O
results O
is O
whether O
word O
frequency O
has O
any O
inﬂuence O
on O
the O
performance O
drop O
of O
co O
- O
occurring O
word O
pairs O
. O

For O
instance O
, O
one O
may O
wonder O
if O
getting O
a O
high O
- O
quality O
word O
embedding O
, O
which O
is O
generally O
achieved O
when O
word O
frequency O
is O
sufﬁciently O
high O
in O
the O
corpus O
, O
is O
enough O
to O
compensate O
for O
the O
lack O
of O
sentences O
with O
words O
forming O
a O
certain O
relation O
. O

Or O
, O
alternatively O
, O
if O
the O
relative O
frequency O
of O
co O
- O
occurring O
words O
in O
a O
relation O
has O
an O
effect O
on O
the O
ﬁnal O
embedding O
, O
as O
this O
would O
mean O
that O
frequency O
is O
a O
necessary O
condition O
for O
learning O
robust O
semantic O
regularities O
. O

To O
answer O
these O
questions O
, O
we O
computed O
the O
correlation B-MetricName
between O
word O
and O
pair O
frequencies O
in O
a O
word O
analogy O
instance O
and O
the O
performance O
drop O
. O

For O
the O
frequency O
indicator O
, O
we O
computed O
two O
numbers O
, O
HindandHpair O
: O
1.Harmonic O
mean B-MetricName
between B-MetricName
all O
individual O
10 O
We O
decided O
to O
use O
the O
harmonic B-MetricName
mean I-MetricName
because O
it O
is O
gener O
- O
words O
in O
the O
analogy O
instance O
( O
Hind O
) O
was O
computed O
as O
follows O
: O
Hind O
= O
nPn O
i=11 O
xi(1 O
) O
where O
xiis O
the O
frequency O
of O
a O
word O
in O
the O
given O
word O
analogy O
instance O
and O
nis O
the O
number O
of O
words O
, O
i.e. O
, O
four O
in O
the O
case O
of O
individual O
words O
in O
word O
analogies O
. O

For O
example O
, O
the O
harmonic B-MetricName
mean B-MetricName
of O
the O
four O
words O
, O
king O
( O
220,958 O
occurrences O
in O
UMBC O
) O
, O
queen O
( O
52,262 O
) O
, O
man O
( O
751,262 O
) O
, O
and O
woman O
( O
296,915 O
) O
would O
be O
141,048 O
. O

2.The O
relative O
pairwise O
frequency O
( O
Hpair O
) O
was O
computed O
as O
the O
previous O
number O
divided O
by O
the O
harmonic O
mean O
of O
the O
number O
of O
sentences O
where O
two O
words O
of O
a O
pair O
co O
- O
occur O
( O
i.e. O
, O
Hco O
): O
Hpair O
= O
Hco O
Hind=2p1p2 O
( O
p1+p2)1 O
Hind(2 O
) O
where O
picorresponds O
to O
the O
frequency O
of O
a O
relation O
pair O
in O
a O
word O
analogy O
instance O
. O

This O
number O
can O
give O
a O
better O
indication O
of O
how O
relevant O
the O
co O
- O
occurrence O
information O
of O
a O
given O
word O
pair O
is O
. O

Following O
the O
previous O
example O
, O
the O
relative O
pairwise O
frequency O
Hpairof O
the O
instance O
composed O
of O
king O
- O
queen O
( O
5,498 O
joint O
co O
- O
occurrences O
in O
UMBC O
) O
and O
man O
- O
woman O
( O
36,189 O
) O
is O
0.068 O
. O
ally O
more O
robust O
to O
outliers O
( O
e.g. O
, O
a O
highly O
frequent O
word O
) O
than O
the O
usual O
arithmetic B-MetricName
mean I-MetricName
. O
Googlecapital O
- O
country O
162,671 O
521,038 O
61.9 O
54.6 O
55.5 O
93.3 O
89.5 O
90.3 O
97.6 O
94.9 O
96.1 O
capital O
- O
world O
320,106 O
1,102,291 O
66.8 O
48.1 O
51.4 O
92.9 O
80.6 O
82.1 O
97.2 O
90.3 O
91.1 O
currency O
4,327 O
223,588 O
31.4 O
28.5 O
25.3 O
60.9 O
58.2 O
51.3 O
77.9 O
78.4 O
71.0 O
city O
- O
in O
- O
state O
344,196 O
659,539 O
19.7 O
12.6 O
15.2 O
54.0 O
34.8 O
39.4 O
72.3 O
55.7 O
58.5 O
family O
375,904 O
1,012,935 O
78.1 O
73.3 O
72.3 O
94.3 O
92.9 O
89.7 O
95.7 O
94.5 O
93.9 O
nationality O
- O
adjective O
437,316 O
1,381,200 O
95.9 O
94.9 O
95.5 O
98.5 O
98.6 O
97.6 O
99.8 O
99.7 O
98.9 O
A O
VERAGE O
274,087 O
816,765 O
59.0 O
52.0 O
52.5 O
82.3 O
75.8 O
75.1 O
90.1 O
85.6 O
84.9BATScountry O
- O
capital O
267,095 O
885,924 O
83.8 O
79.5 O
78.7 O
91.1 O
90.0 O
88.9 O
93.6 O
91.8 O
91.3 O
country O
- O
language O
82,842 O
527,339 O
28.6 O
25.3 O
23.9 O
62.0 O
58.5 O
58.9 O
72.7 O
70.9 O
71.2 O
UKcity O
- O
county O
39,780 O
198,005 O
17.0 O
9.1 O
9.7 O
43.5 O
34.0 O
34.3 O
59.0 O
52.3 O
51.8 O
name O
- O
nationality O
31,980 O
390,864 O
31.4 O
29.5 O
30.7 O
56.0 O
53.9 O
56.5 O
67.8 O
65.2 O
69.1 O
name O
- O
occupation O
24,798 O
136,447 O
39.9 O
35.9 O
34.7 O
63.0 O
59.5 O
58.3 O
72.1 O
68.8 O
67.8 O
animal O
- O
young O
7,021 O
135,815 O
5.4 O
2.4 O
2.4 O
20.0 O
12.5 O
10.5 O
33.1 O
24.4 O
22.3 O
animal O
- O
sound O
2,921 O
121,416 O
5.3 O
2.8 O
1.7 O
15.2 O
10.5 O
7.6 O
24.2 O
18.6 O
14.5 O
animal O
- O
shelter O
23,832 O
262,708 O
2.0 O
1.3 O
0.9 O
8.7 O
5.4 O
4.6 O
17.5 O
12.1 O
10.7 O
things O
- O
color O
47,298 O
445,312 O
14.5 O
15.0 O
15.2 O
39.9 O
38.6 O
42.8 O
55.6 O
52.7 O
56.2 O
male O
- O
female O
400,035 O
1,387,664 O
51.8 O
47.4 O
46.5 O
77.6 O
73.7 O
71.6 O
84.5 O
82.4 O
80.7 O
hypernyms O
- O
animals O
11,723 O
77,632 O
20.1 O
16.8 O
14.4 O
57.6 O
52.7 O
47.4 O
75.4 O
71.0 O
66.6 O
hypernyms O
- O
misc O
16,825 O
164,097 O
7.4 O
6.7 O
6.2 O
33.1 O
29.9 O
30.3 O
55.8 O
53.0 O
51.9 O
hyponyms O
- O
misc O
90,811 O
594,541 O
12.4 O
12.4 O
10.9 O
52.2 O
51.2 O
49.4 O
72.7 O
70.6 O
69.8 O
meronyms O
- O
substance O
70,896 O
510,291 O
7.8 O
6.7 O
6.9 O
28.9 O
24.2 O
24.9 O
48.7 O
43.4 O
41.4 O
meronyms O
- O
member O
774,473 O
4,259,786 O
10.4 O
9.1 O
12.0 O
33.3 O
28.2 O
35.6 O
49.9 O
44.7 O
53.0 O
meronyms O
- O
part O
91,041 O
539,371 O
6.7 O
6.1 O
5.5 O
34.0 O
31.5 O
31.7 O
56.5 O
52.0 O
51.4 O
synonyms O
- O
intensity O
44,149 O
624,662 O
15.6 O
14.9 O
14.4 O
42.9 O
40.4 O
40.7 O
61.3 O
60.7 O
58.7 O
synonyms O
- O
exact O
83,651 O
1,308,183 O
23.2 O
20.7 O
25.4 O
52.6 O
50.0 O
54.4 O
69.1 O
67.1 O
70.2 O
antonyms O
- O
gradable O
282,330 O
1,233,454 O
21.6 O
18.5 O
22.3 O
49.0 O
46.3 O
49.6 O
67.1 O
64.6 O
66.5 O
antonyms O
- O
binary O
1,726,724 O
21,329,952 O
29.4 O
26.1 O
44.6 O
57.7 O
55.4 O
77.2 O
72.2 O
70.7 O
85.6 O
A O
VERAGE O
206,011 O
1,756,673 O
21.7 O
19.3 O
20.3 O
45.9 O
42.3 O
43.8 O
60.4 O
56.8 O
57.5 O
Table O
3 O
: O
Wikipedia O
corpus O
word O
analogy O
results O
using O
CBOW O
with O
three O
different O
conﬁgurations O
: O
Default O
( O
Def O
) O
, O
Remove O
( O
Rm O
) O
, O
and O
its O
more O
aggressive O
setting O
removing O
all O
pairwise O
co O
- O
occurrences O
( O
Rm+ O
) O
With O
respect O
to O
performance O
drops O
, O
for O
each O
analogy O
completion O
instance O
we O
considered O
the O
ranks11 O
of O
the O
correct O
completion O
words O
in O
both O
the O
default O
and O
replace O
settings O
and O
computed O
the O
difference O
. O

Table O
4 O
shows O
the O
results O
of O
the O
correlation O
results O
in O
the O
Google O
analogy O
dataset O
. O

Not O
surprisingly O
, O
the O
correlation O
between O
the O
individual O
frequency O
of O
the O
words O
in O
an O
instance O
and O
the O
rank O
difference O
is O
negative O
in O
all O
relation O
types O
except O
for O
one O
( O
nationality O
- O
adjective O
) O
. O

However O
, O
the O
correlation O
is O
rather O
weak O
, O
as O
the O
addition O
of O
new O
sentences O
compensate O
for O
the O
initial O
removal O
, O
even O
if O
the O
sentences O
are O
of O
a O
different O
kind O
. O

As O
for O
the O
relative O
frequency O
of O
pairs O
in O
the O
instance O
, O
the O
correlation B-MetricName
is O
positive O
as O
expected O
. O

In O
this O
case O
, O
the O
signal O
is O
higher O
than O
with O
the O
individual O
frequency O
case O
, O
especially O
in O
thefamily O
relationship O
. O

Overall O
, O
this O
experiment O
shows O
a O
level O
of O
support O
for O
our O
initial O
premises O
on O
the O
effect O
of O
relative O
pair O
frequencies O
, O
but O
further O
research O
would O
be O
necessary O
to O
understand O
other O
11We O
only O
considered O
the O
position O
of O
the O
ﬁfty O
ﬁrst O
nearest B-MethodName
neighbours I-MethodName
. O

If O
the O
correct O
word O
was O
not O
among O
the O
ﬁfty O
nearest B-MethodName
neighbours I-MethodName
, O
51 O
was O
used O
as O
the O
position O
, O
which O
would O
be O
equivalent O
to O
a O
wrong O
answer.reasons O
behind O
the O
performance O
drop O
. O

Frequency O
Drop O
Correlation B-MetricName
Ind O
Pair O
Hind O
Hpair O
cap.-country O
26,111 O
2,002 O
15.1 O
-0.239 O
-0.114 O
cap.-world O
4,328 O
434 O
33.8 O
-0.128 O
+0.188 O
currency O
3,815 O
52 O
20.0 O
-0.092 O
+0.041 O
city O
- O
state O
14,226 O
1,121 O
45.3 O
-0.012 O
+0.192 O
family O
58,197 O
5,617 O
10.0 O
-0.065 O
+0.315 O
nation O
- O
adj O
30,076 O
2,238 O
5.4 O
+0.063 O
+0.021 O
A O
VERAGE O
22,792 O
1,911 O
22.9 O
-0.079 O
+0.107 O
Table O
4 O
: O
Pearson O
correlations O
between O
frequency O
( O
average O
among O
all O
instances O
in O
the O
dataset O
) O
and O
performance O
drop O
between O
the O
default O
and O
replace O
( O
Rp O
) O
corpora O
. O

5.2 O
Visualization O
In O
this O
section O
, O
we O
present O
visualizations O
of O
the O
word O
pairs O
from O
one O
lexical O
relation O
, O
before O
and O
after O
co O
- O
occurrence O
removal O
, O
in O
order O
to O
gain O
insight O
into O
the O
effect O
of O
removal O
on O
the O
learned O
structure O
of O
the O
space O
. O

All O
models O
trained O
on O
different O
subsets O
of O
UMBC O
: O
default O
( O
left O
) O
and O
remove O
( O
right O
) O
. O

Red O
marks O
( O
x O
) O
correspond O
to O
countries O
and O
blue O
dots O
, O
capitals O
. O

performance O
point O
drop12 O
. O

Figure O
2 O
shows O
the O
two O
principal O
components O
of O
word O
pairs O
in O
the O
capitalworld O
relation O
for O
both O
the O
default O
( O
Def O
) O
and O
remove O
( O
Rm O
) O
settings O
. O

A O
reminder O
from O
Section O
3.2.1 O
, O
the O
remove O
setting O
involves O
removing O
all O
sentences O
where O
both O
words O
from O
a O
relation O
co O
- O
occur O
, O
without O
any O
replacement O
. O

As O
can O
be O
seen O
, O
even O
in O
the O
case O
of O
a O
relation O
with O
a O
R@1 O
performance O
gap O
as O
high O
as O
36.6 O
% O
, O
the O
linear O
relations O
are O
largely O
preserved O
in O
the O
word O
embedding O
space O
. O

This O
can O
also O
be O
supported O
by O
the O
fact O
that O
the O
performance O
drop O
is O
much O
smaller O
for O
R@10 O
and O
R@50 O
( O
see O
Figure O
1 O
) O
, O
suggesting O
that O
the O
correct O
completion O
word O
is O
still O
somewhere O
near O
in O
the O
space O
. O

For O
example O
, O
for O
the O
instance O
Australia O
, O
Canberra O
, O
Spain O
, O
the O
correct O
word O
Madrid O
is O
found O
as O
the O
ﬁfth O
nearest B-MethodName
neighbours I-MethodName
in O
the O
Rm O
vector O
space O
, O
with O
the O
top O
two O
words O
being O
Barcelona O
andValencia O
( O
other O
large O
Spanish O
cities O
) O
. O

Intuitively O
, O
this O
error O
does O
not O
affect O
the O
overall O
representation O
of O
the O
relation O
in O
the O
vector O
space O
, O
as O
those O
words O
were O
already O
in O
a O
similar O
linear O
relationship O
in O
the O
default O
model O
. O

6 O
Discussion O
6.1 O
The O
source O
of O
semantic O
regularities O
In O
this O
paper O
, O
we O
ﬁnd O
that O
neural O
word O
embeddings O
( O
i.e. O
, O
Word2Vec O
models O
) O
do O
not O
require O
observation O
of O
instances O
of O
the O
relation O
( O
e.g. O
, O
Madrid O
is O
thecapital O
of O
Spain O
) O
in O
order O
to O
maintain O
nominal O
accuracy O
in O
relation O
completion O
tasks O
. O

We O
believe O
this O
is O
the O
ﬁrst O
time O
such O
an O
observation O
has O
been O
made O
, O
empirically O
, O
using O
natural O
language O
, O
though O
it O
has O
been O
observed O
in O
neural O
embeddings O
trained O
12In O
the O
appendix O
we O
also O
include O
the O
same O
visualizations O
for O
the O
relations O
with O
the O
second O
largest O
performance O
drop O
( O
city O
- O
in O
- O
state O
) O
and O
the O
smallest O
performance O
drop O
( O
nationalityadjective O
) O
, O
which O
largely O
share O
the O
same O
conclusions.on O
non O
- O
linguistic O
data O
( O
Pardos O
and O
Nam O
, O
2020 O
) O
. O

In O
Mikolov O
et O
al O
. O
( O
2013b O
) O
, O
where O
Word2Vec O
models O
were O
ﬁrst O
introduced O
, O
the O
phrase O
” O
Linguistic O
Regularities O
” O
was O
used O
. O

While O
it O
was O
not O
made O
explicit O
what O
the O
word O
regularities O
referred O
to O
, O
analogy O
completion O
was O
exclusively O
used O
for O
validation O
, O
leaving O
open O
the O
possibility O
that O
regularities O
referred O
to O
some O
pattern O
of O
structure O
allowing O
for O
lexical O
relationships O
to O
be O
expressed O
. O

If O
the O
regularities O
relevant O
to O
analogy O
completion O
are O
not O
formed O
from O
examples O
of O
the O
lexical O
relationship O
contained O
in O
the O
analogy O
, O
then O
how O
are O
they O
formed O
and O
how O
was O
the O
accuracy B-MetricName
of O
the O
completion O
mostly O
retained O
in O
our O
experiments O
in O
the O
absence O
of O
examples O
? O

Instead O
, O
it O
may O
leverage O
the O
robustness O
of O
regularities O
, O
or O
features O
, O
learned O
about O
individual O
words O
to O
lay O
the O
structural O
foundation O
for O
inferences O
to O
then O
be O
made O
about O
a O
lexical O
relation O
. O

Removing O
co O
- O
occurrences O
of O
capitals O
and O
countries O
, O
for O
example O
, O
would O
not O
completely O
remove O
the O
concept O
of O
capitals O
and O
countries O
from O
the O
corpus O
. O

The O
embedding O
of O
” O
Madrid O
” O
would O
likely O
still O
encode O
features O
associated O
with O
a O
busy O
city O
, O
government O
buildings O
, O
culture O
, O
and O
European O
regionality O
. O

This O
is O
also O
related O
to O
work O
that O
showed O
relations O
and O
relevant O
information O
from O
relations O
can O
be O
captured O
from O
word O
embeddings O
( O
Jadhav O
et O
al O
. O
, O
2020 O
) O
, O
even O
if O
the O
relation O
can O
not O
be O
retrieved O
explicitly O
from O
linear O
transformations O
( O
Drozd O
et O
al O
. O
, O
2016 O
; O
Bouraoui O
et O
al O
. O
, O
2018 O
) O
. O

Interestingly O
, O
however O
, O
our O
results O
indicate O
that O
the O
frequency O
of O
an O
individual O
word O
in O
a O
corpus O
is O
only O
weakly O
related O
to O
the O
robustness O
of O
features O
leveraged O
for O
successful O
analogy O
completion O
. O

Finally O
, O
even O
though O
co O
- O
occurrences O
of O
pairs O
from O
a O
speciﬁc O
relation O
are O
not O
necessary O
to O
learn O
the O
necessary O
features O
, O
word O
pairs O
still O
may O
play O
a O
critical O
role O
in O
regularity O
development O
. O

Mostword O
embedding O
models O
( O
including O
the O
Skip B-MethodName
- I-MethodName
Gram I-MethodName 
model O
of O
Word2Vec B-MethodName
) O
are O
trained O
in O
pairwise O
fashion O
after O
- O
all O
, O
making O
predictions O
and O
calculating O
loss O
based O
on O
each O
pair O
of O
input O
and O
context O
words O
. O

6.2 O
Cognitive O
perspective O
Neural O
embeddings O
come O
from O
a O
cognitive O
perspective O
on O
semantic O
representation O
. O

They O
stem O
from O
a O
hypothesized O
architecture O
of O
the O
mind O
called O
Connectionism O
( O
Feldman O
and O
Ballard O
, O
1982 O
) O
in O
which O
emergent O
concepts O
( O
Hopﬁeld O
, O
1982 O
; O
Hinton O
, O
1986 O
) O
are O
learned O
as O
distributed O
representations O
across O
the O
embedding O
space O
( O
Hinton O
et O
al O
. O
, O
1986 O
) O
. O

If O
neural O
word O
embeddings O
are O
a O
candidate O
model O
of O
a O
component O
of O
human O
cognition O
, O
then O
our O
results O
suggest O
that O
the O
faculties O
of O
the O
mind O
that O
understand O
relational O
concepts O
( O
e.g. O
, O
male O
and O
female O
) O
may O
establish O
these O
concepts O
primarily O
through O
induction O
and O
observations O
of O
behavior O
. O

For O
example O
, O
this O
would O
mean O
that O
we O
learn O
features O
of O
male O
and O
female O
separately O
, O
rather O
than O
through O
explicit O
declaration O
of O
representative O
pairs O
( O
i.e. O
, O
explicit O
cooccurrences O
) O
. O

It O
is O
perhaps O
a O
separate O
faculty O
of O
the O
mind O
that O
queries O
this O
conceptual O
representation O
framework O
for O
inferences O
to O
be O
made O
about O
relationships O
between O
new O
elements O
. O

These O
inferences O
, O
conducted O
by O
way O
of O
analogy O
, O
may O
indeed O
be O
key O
to O
innovation O
( O
Hope O
et O
al O
. O
, O
2017 O
) O
and O
a O
possible O
component O
of O
human O
creativity O
( O
Holyoak O
et O
al O
. O
, O
1996 O
) O
. O

7 O
Conclusion O
and O
Future O
Work O
In O
this O
paper O
we O
have O
presented O
a O
large O
- O
scale O
analysis O
on O
the O
role O
of O
co O
- O
occurring O
relational O
word O
pairs O
in O
completing O
analogies O
. O

In O
the O
analyses O
we O
have O
measured O
to O
what O
extent O
the O
loss O
of O
co O
- O
occurrence O
information O
within O
relation O
types O
affects O
analogy O
completion O
using O
neural O
word O
embeddings O
. O

Perhaps O
surprisingly O
, O
this O
effect O
is O
quite O
small O
, O
to O
the O
point O
that O
word O
embeddings O
can O
complete O
analogies O
of O
a O
relationship O
in O
the O
vector O
space O
even O
if O
the O
co O
- O
occurrence O
information O
from O
the O
reference O
corpus O
is O
totally O
removed O
. O

In O
order O
to O
complement O
this O
analysis O
, O
for O
future O
work O
it O
would O
be O
interesting O
to O
analyze O
to O
what O
extent O
the O
conclusions O
of O
this O
analysis O
apply O
to O
purely O
distributional O
models O
, O
e.g. O
, O
PMI O
- O
based O
, O
as O
they O
have O
shown O
to O
share O
similarity O
properties O
with O
word O
embeddings O
( O
Levy O
et O
al O
. O
, O
2015 O
) O
, O
to O
the O
point O
of O
SkipGram B-MethodName
being O
viewed O
as O
an O
implicit O
co O
- O
occurrence O
matrix O
factorization O
( O
Levy O
and O
Goldberg O
, O
2014b).Moreover O
, O
the O
analysis O
could O
be O
extended O
to O
other O
types O
of O
relations O
, O
not O
only O
semantic O
. O

Further O
investigation O
could O
then O
focus O
on O
how O
the O
main O
sources O
of O
concepts O
and O
linguistic O
regularities O
in O
word O
embeddings O
are O
learned O
, O
and O
how O
they O
can O
be O
leveraged O
to O
improve O
unsupervised O
relation O
models O
, O
e.g. O
, O
( O
Jameel O
et O
al O
. O
, O
2018 O
; O
Joshi O
et O
al O
. O
, O
2019 O
) O
. O

Finally O
, O
as O
a O
follow O
- O
up O
to O
recent O
work O
aiming O
at O
understanding O
how O
language O
models O
and O
contextualized O
embeddings O
capture O
relations O
( O
Petroni O
et O
al O
. O
, O
2019 O
; O
Bouraoui O
et O
al O
. O
, O
2020 O
; O
Jiang O
et O
al O
. O
, O
2020 O
) O
, O
further O
research O
could O
be O
devoted O
to O
analyze O
the O
performance O
of O
such O
models O
with O
and O
without O
pairwise O
co O
- O
occurrence O
information O
. O

Beyond O
Accuracy O
: O
Behavioral O
Testing O
of O
NLP O
Models O
with O
CheckList B-MethodName
Although O
measuring O
held O
- O
out O
accuracy O
has O
been O
the O
primary O
approach O
to O
evaluate O
generalization O
, O
it O
often O
overestimates O
the O
performance O
of O
NLP O
models O
, O
while O
alternative O
approaches O
for O
evaluating O
models O
either O
focus O
on O
individual O
tasks O
or O
on O
specific O
behaviors O
. O

Inspired O
by O
principles O
of O
behavioral O
testing O
in O
software O
engineering O
, O
we O
introduce O
CheckList B-MethodName
, O
a O
taskagnostic O
methodology O
for O
testing O
NLP O
models O
. O

CheckList B-MethodName
includes O
a O
matrix O
of O
general O
linguistic O
capabilities O
and O
test O
types O
that O
facilitate O
comprehensive O
test O
ideation O
, O
as O
well O
as O
a O
software O
tool O
to O
generate O
a O
large O
and O
diverse O
number O
of O
test O
cases O
quickly O
. O

We O
illustrate O
the O
utility O
of O
CheckList B-MethodName
with O
tests O
for O
three O
tasks O
, O
identifying B-TaskName
critical I-TaskName
failures I-TaskName
in O
both O
commercial O
and O
state O
- O
of O
- O
art O
models O
. O

In O
a O
user O
study O
, O
a O
team O
responsible O
for O
a O
commercial O
sentiment B-TaskName
analysis I-TaskName
model O
found O
new O
and O
actionable O
bugs O
in O
an O
extensively O
tested O
model O
. O

In O
another O
user O
study O
, O
NLP O
practitioners O
with O
CheckList B-MethodName
created O
twice O
as O
many O
tests O
, O
and O
found O
almost O
three O
times O
as O
many O
bugs O
as O
users O
without O
it O
. O

One O
of O
the O
primary O
goals O
of O
training O
NLP O
models O
is O
generalization O
. O

Since O
testing O
“ O
in O
the O
wild O
” O
is O
expensive O
and O
does O
not O
allow O
for O
fast O
iterations O
, O
the O
standard O
paradigm O
for O
evaluation O
is O
using O
trainvalidationtest O
splits O
to O
estimate O
the O
accuracy O
of O
the O
model O
, O
including O
the O
use O
of O
leader O
boards O
to O
track O
progress O
on O
a O
task O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
. O

While O
performance O
on O
held O
- O
out O
data O
is O
a O
useful O
indicator O
, O
held O
- O
out O
datasets O
are O
often O
not O
comprehensive O
, O
and O
contain O
the O
same O
biases O
as O
the O
training O
data O
( O
Rajpurkar O
et O
al O
. O
, O
2018 O
) O
, O
such O
that O
real O
- O
world O
performance O
may O
be O
overestimated O
( O
Patel O
et O
al O
. O
, O
2008 O
; O
Recht O
et O
al O
. O
, O
2019 O
) O
. O

Further O
, O
by O
summarizing O
the O
performance O
as O
a O
single O
aggregate O
statistic O
, O
it O
becomes O
dicult O
to O
figure O
out O
where O
the O
model O
is O
failing O
, O
and O
how O
to O
fix O
it O
( O
Wu O
et O
al O
. O
, O
2019 O
) O
. O

A O
number O
of O
additional O
evaluation O
approaches O
have O
been O
proposed O
, O
such O
as O
evaluating O
robustness B-MetricName
to I-MetricName
noise I-MetricName
( O
Belinkov O
and O
Bisk O
, O
2018 B-MetricValue
; O
Rychalska O
et O
al O
. O
, O
2019 B-MetricValue
) O
or O
adversarial B-MetricName
changes I-MetricName
( O
Ribeiro O
et O
al O
. O
, O
2018 O
; O
Iyyer O
et O
al O
. O
, O
2018 O
) O
, O
fairness B-MetricName
( O
Prabhakaran O
et O
al O
. O
, O
2019 O
) O
, O
logical B-MetricName
consistency I-MetricName
( O
Ribeiro O
et O
al O
. O
, O
2019 O
) O
, O
explanations O
( O
Ribeiro O
et O
al O
. O
, O
2016 O
) O
, O
diagnostic O
datasets O
( O
Wang O
et O
al O
. O
, O
2019b O
) O
, O
and O
interactive O
error O
analysis O
( O
Wu O
et O
al O
. O
, O
2019 O
) O
. O

However O
, O
these O
approaches O
focus O
either O
on O
individual O
tasks O
such O
as O
Question B-TaskName
Answering I-TaskName
or O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
, O
or O
on O
a O
few O
capabilities O
( O
e.g. O

robustness O
) O
, O
and O
thus O
do O
not O
provide O
comprehensive O
guidance O
on O
how O
to O
evaluate O
models O
. O

Software O
engineering O
research O
, O
on O
the O
other O
hand O
, O
has O
proposed O
a O
variety O
of O
paradigms O
and O
tools O
for O
testing O
complex O
software O
systems O
. O

In O
particular O
, O
“ O
behavioral O
testing O
” O
( O
also O
known O
as O
black O
- O
box O
testing O
) O
is O
concerned O
with O
testing O
di O
 O
erent O
capabilities O
of O
a O
system O
by O
validating O
the O
input O
- O
output O
behavior O
, O
without O
any O
knowledge O
of O
the O
internal O
structure O
( O
Beizer O
, O
1995 O
) O
. O

While O
there O
are O
clear O
similarities O
, O
many O
insights O
from O
software O
engineering O
are O
yet O
to O
be O
applied O
to O
NLP O
models O
. O

In O
this O
work O
, O
we O
propose O
CheckList B-MethodName
, O
a O
new O
evaluation O
methodology O
and O
accompanying O
tool1 O
for O
comprehensive O
behavioral O
testing O
of O
NLP O
models O
. O

CheckList B-MethodName
guides O
users O
in O
what O
to O
test O
, O
by O
providing O
a O
list O
of O
linguistic O
capabilities O
, O
which O
are O
applicable O
to O
most O
tasks O
. O

To O
break O
down O
potential O
capability O
failures O
into O
specific O
behaviors O
, O
CheckList B-MethodName
introduces O
di O
 O
erent O
test O
types O
, O
such O
as O
prediction O
invariance O
in O
the O
presence O
of O
certain O
perturbations O
, O
or O
performance O
on O
a O
set O
of O
“ O
sanity O
checks O
. O
” O
Finally O
, O
our O
implementation O
of O
CheckList B-MethodName
includes O
multiple O
abstractions O
that O
help O
users O
generate O
large O
numbers O
of O
test O
cases O
easily O
, O
such O
as O
templates O
, O
lexicons O
, O
general O
- O
purpose O
perturbations O
, O
visualizations O
, O
and O
context O
- O
aware O
suggestions O
. O

We O
demonstrate O
the O
usefulness O
and O
generality O
of O
CheckList B-MethodName
via O
instantiation O
on O
three O
NLP O
tasks O
: O
sentiment B-TaskName
analysis I-TaskName
( O
Sentiment O
) O
, O
duplicate B-TaskName
question I-TaskName
detection I-TaskName
( O
QQP B-DatasetName
; O
Wang O
et O
al O
. O
, O
2019b O
) O
, O
and O
machine B-TaskName
comprehension I-TaskName
( O
MC O
; O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
. O

While O
traditional O
benchmarks O
indicate O
that O
models O
on O
these O
tasks O
are O
as O
accurate O
as O
humans O
, O
CheckList B-MethodName
reveals O
a O
variety O
of O
severe O
bugs O
, O
where O
commercial O
and O
research O
models O
do O
not O
e O
 O
ectively O
handle O
basic O
linguistic O
phenomena O
such O
as O
negation O
, O
named O
entities O
, O
coreferences B-TaskName
, O
semantic O
role O
labeling O
, O
etc O
, O
as O
they O
pertain O
to O
each O
task O
. O

Further O
, O
CheckList B-MethodName
is O
easy O
to O
use O
and O
provides O
immediate O
value O
– O
in O
a O
user O
study O
, O
the O
team O
responsible O
for O
a O
commercial O
sentiment B-TaskName
analysis I-TaskName
model O
discovered O
many O
new O
and O
actionable O
bugs O
in O
their O
own O
model O
, O
even O
though O
it O
had O
been O
extensively O
tested O
and O
used O
by O
customers O
. O

In O
an O
additional O
user O
study O
, O
we O
found O
that O
NLP O
practitioners O
with O
CheckList B-MethodName
generated O
more O
than O
twice O
as O
many O
tests O
( O
each O
test O
containing O
an O
order O
of O
magnitude O
more O
examples O
) O
, O
and O
uncovered O
almost O
three O
times O
as O
many O
bugs O
, O
compared O
to O
users O
without O
CheckList B-MethodName
. O

Conceptually O
, O
users O
“ O
CheckList B-MethodName
” O
a O
model O
by O
filling O
out O
cells O
in O
a O
matrix O
( O
Figure O
1 O
) O
, O
each O
cell O
potentially O
containing O
multiple O
tests O
. O

In O
this O
section O
, O
we O
go O
into O
more O
detail O
on O
the O
rows O
( O
capabilities O
) O
, O
columns O
( O
test O
types O
) O
, O
and O
how O
to O
fill O
the O
cells O
( O
tests O
) O
. O

CheckList B-MethodName
applies O
the O
behavioral O
testing O
principle O
of O
“ O
decoupling O
testing O
from O
implementation O
” O
by O
treating O
the O
model O
as O
a O
black O
box O
, O
which O
allows O
for O
comparison O
of O
di O
 O
erent O
models O
trained O
on O
di O
 O
erent O
data O
, O
or O
third O
- O
party O
models O
where O
access O
to O
training O
data O
or O
model O
structure O
is O
not O
granted O
. O

While O
testing O
individual O
components O
is O
a O
common O
practice O
in O
software O
engineering O
, O
modern O
NLP O
models O
are O
rarely O
built O
one O
component O
at O
a O
time O
. O

Instead O
, O
CheckList B-MethodName
encourages O
users O
to O
consider O
how O
di O
 O
erent O
natural O
language O
capabilities O
are O
manifested O
on O
the O
task O
at O
hand O
, O
and O
to O
create O
tests O
to O
evaluate O
the O
model O
on O
each O
of O
these O
capabilities O
. O

For O
example O
, O
the O
Vocabulary+POS B-TaskName
capability O
pertains O
to O
whether O
a O
model O
has O
the O
necessary O
vocabulary O
, O
and O
whether O
it O
can O
appropriately O
handle O
the O
impact O
of O
words O
with O
di O
 O
erent O
parts O
of O
speech O
on O
the O
task O
. O

For O
Sentiment O
, O
we O
may O
want O
to O
check O
if O
the O
model O
is O
able O
to O
identify O
words O
that O
carry O
positive O
, O
negative O
, O
or O
neutral O
sentiment O
, O
by O
verifying O
how O
it O
behaves O
on O
examples O
like O
“ O
This O
was O
a O
good O
flight O
. O
” O
For O
QQP B-DatasetName
, O
we O
might O
want O
the O
model O
to O
4904 O
understand O
when O
modifiers O
di O
 O
erentiate O
questions O
, O
e.g. O

accredited O
in O
( O
“ O
Is O
John O
a O
teacher O
? O
” O
, O
“ O
Is O
John O
an O
accredited O
teacher O
? O
” O
) O
. O

For O
MC O
, O
the O
model O
should O
be O
able O
to O
relate O
comparatives O
and O
superlatives O
, O
e.g. O

( O
Context O
: O
“ O
Mary O
is O
smarter O
than O
John O
. O
” O
, O
Q O
: O
“ O
Who O
is O
the O
smartest O
kid O
? O
” O
, O
A O
: O
“ O
Mary O
” O
) O
. O

We O
suggest O
that O
users O
consider O
at O
least O
the O
following O
capabilities O
: O
Vocabulary+POS B-TaskName
( O
important O
words O
or O
word O
types O
for O
the O
task O
) O
, O
Taxonomy O
( O
synonyms O
, O
antonyms O
, O
etc O
) O
, O
Robustness O
( O
to O
typos O
, O
irrelevant O
changes O
, O
etc O
) O
, O
NER B-TaskName
( O
appropriately O
understanding O
named O
entities O
) O
, O
Fairness B-MetricName
, O
Temporal O
( O
understanding O
order O
of O
events O
) O
, O
Negation O
, O
Coreference B-TaskName
, O
Semantic B-TaskName
Role I-TaskName
Labeling I-TaskName
( O
understanding O
roles O
such O
as O
agent O
, O
object O
, O
etc O
) O
, O
and O
Logic O
( O
ability O
to O
handle O
symmetry O
, O
consistency O
, O
and O
conjunctions O
) O
. O

We O
will O
provide O
examples O
of O
how O
these O
capabilities O
can O
be O
tested O
in O
Section O
3 O
( O
Tables O
1 O
, O
2 O
, O
and O
3 O
) O
. O

This O
listing O
of O
capabilities O
is O
not O
exhaustive O
, O
but O
a O
starting O
point O
for O
users O
, O
who O
should O
also O
come O
up O
with O
additional O
capabilities O
that O
are O
specific O
to O
their O
task O
or O
domain O
. O

We O
prompt O
users O
to O
evaluate O
each O
capability O
with O
three O
di O
 O
erent O
test O
types O
( O
when O
possible O
): O
Minimum O
Functionality O
tests O
, O
Invariance O
, O
and O
Directional O
Expectation O
tests O
( O
the O
columns O
in O
the O
matrix O
) O
. O

A O
Minimum O
Functionality O
test O
( O
MFT O
) O
, O
inspired O
by O
unit O
tests O
in O
software O
engineering O
, O
is O
a O
collection O
of O
simple O
examples O
( O
and O
labels O
) O
to O
check O
a O
behavior O
within O
a O
capability O
. O

MFTs O
are O
similar O
to O
creating O
small O
and O
focused O
testing O
datasets O
, O
and O
are O
particularly O
useful O
for O
detecting O
when O
models O
use O
shortcuts O
to O
handle O
complex O
inputs O
without O
actually O
mastering O
the O
capability O
. O

The O
Vocabulary+POS B-TaskName
examples O
in O
the O
previous O
section O
are O
all O
MFTs O
. O

We O
also O
introduce O
two O
additional O
test O
types O
inspired O
by O
software O
metamorphic O
tests O
( O
Segura O
et O
al O
. O
, O
2016 O
) O
. O

An O
Invariance O
test O
( O
INV O
) O
is O
when O
we O
apply O
label O
- O
preserving O
perturbations O
to O
inputs O
and O
expect O
the O
model O
prediction O
to O
remain O
the O
same O
. O

Di O
 O
erent O
perturbation O
functions O
are O
needed O
for O
di O
 O
erent O
capabilities O
, O
e.g. O

changing O
location O
names O
for O
the O
NER B-TaskName
capability O
for O
Sentiment O
( O
Figure O
1B O
) O
, O
or O
introducing O
typos O
to O
test O
the O
Robustness O
capability O
. O

A O
Directional O
Expectation O
test O
( O
DIR O
) O
is O
similar O
, O
except O
that O
the O
label O
is O
expected O
to O
change O
in O
a O
certain O
way O
. O

For O
example O
, O
we O
expect O
that O
sentiment O
will O
not O
become O
more O
positive O
if O
we O
add O
“ O
You O
are O
lame O
. O
” O
to O
the O
end O
of O
tweets O
directed O
at O
an O
airline O
( O
Figure O
1C O
) O
. O

The O
expectation O
may O
also O
be O
a O
target O
label O
, O
e.g. O

replacing O
locations O
in O
only O
one O
of O
the O
questions O
in O
QQP B-DatasetName
, O
such O
as O
( O
“ O
How O
many O
people O
are O
there O
in O
England O
? O
” O
, O
“ O
What O
is O
the O
population O
of O
England O
) O
Turkey O
? O
” O
) O
, O
ensures O
that O
the O
questions O
are O
not O
duplicates O
. O

INVs O
and O
DIRs O
allow O
us O
to O
test O
models O
on O
unlabeled O
data O
– O
they O
test O
behaviors O
that O
do O
not O
rely O
on O
ground O
truth O
labels O
, O
but O
rather O
on O
relationships O
between O
predictions O
after O
perturbations O
are O
applied O
( O
invariance O
, O
monotonicity O
, O
etc O
) O
. O

We O
provide O
users O
with O
an O
abstraction O
where O
they O
mask O
part O
of O
a O
template O
and O
get O
masked O
language O
model O
( O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
in O
our O
case O
) O
suggestions O
for O
fill O
- O
ins O
, O
e.g. O

I O
really O
{ O
mask O
} O
the O
flight. O
yields O
{ O
enjoyed O
, O
liked O
, O
loved O
, O
regret O
, O
... O
} O
, O
which O
the O
user O
can O
filter O
into O
positive O
, O
negative O
, O
and O
neutral O
fill O
- O
in O
lists O
and O
later O
reuse O
across O
multiple O
tests O
( O
Figure O
2 O
) O
. O

Sometimes O
RoBERTa B-MethodName
suggestions O
can O
be O
used O
without O
filtering O
, O
e.g. O

This O
is O
a O
good O
{ O
mask} O
yields O
multiple O
nouns O
that O
do O
n’t O
need O
filtering O
. O

They O
can O
also O
be O
used O
in O
perturbations O
, O
e.g. O

replacing O
neutral O
words O
like O
that O
or O
the O
for O
other O
words O
in O
context O
( O
Vocabulary+POS B-TaskName
INV O
examples O
in O
Table O
1 O
) O
. O

RoBERTa B-MethodName
suggestions O
can O
be O
combined O
with O
WordNet B-DatasetName
categories O
( O
synonyms O
, O
antonyms O
, O
etc O
) O
, O
e.g. O

such O
that O
only O
contextappropriate O
synonyms O
get O
selected O
in O
a O
perturbation O
. O

We O
also O
provide O
additional O
common O
fill O
- O
ins O
for O
general O
- O
purpose O
categories O
, O
such O
as O
Named O
Entities O
( O
common O
male O
and O
female O
first O
/ O
last O
names O
, O
cities O
, O
countries O
) O
and O
protected O
group O
adjectives O
( O
nationalities O
, O
religions O
, O
gender O
and O
sexuality O
, O
etc O
) O
. O

We O
applied O
the O
same O
process O
to O
very O
di O
 O
erent O
tasks O
, O
and O
found O
that O
tests O
reveal O
interesting O
failures O
on O
a O
variety O
of O
task O
- O
relevant O
linguistic O
capabilities O
. O

While O
some O
tests O
are O
task O
specific O
( O
e.g. O

positive O
adjectives O
) O
, O
the O
capabilities O
and O
test O
types O
are O
general O
; O
many O
can O
be O
applied O
across O
tasks O
, O
as O
is O
( O
e.g. O

testing O
Robustness B-MetricName
with O
typos O
) O
or O
with O
minor O
variation O
( O
changing O
named O
entities O
yields O
di O
 O
erent O
expectations O
depending O
on O
the O
task O
) O
. O

This O
small O
selection O
of O
tests O
illustrates O
the O
benefits O
of O
systematic O
testing O
in O
addition O
to O
standard O
evaluation O
. O

These O
tasks O
may O
be O
considered O
“ O
solved O
” O
based O
on O
benchmark O
accuracy O
results O
, O
but O
the O
tests O
highlight O
various O
areas O
of O
improvement O
– O
in O
particular O
, O
failure O
to O
demonstrate O
basic O
skills O
that O
are O
de O
facto O
needs O
for O
the O
task O
at O
hand O
( O
e.g. O

basic O
negation O
, O
agent O
/ O
object O
distinction O
, O
etc O
) O
. O

Even O
though O
some O
of O
these O
failures O
have O
been O
observed O
by O
others O
, O
such O
as O
typos O
( O
Belinkov O
and O
Bisk O
, O
2018 O
; O
Rychalska O
et O
al O
. O
, O
2019 O
) O
and O
sensitivity O
to O
name O
changes O
( O
Prabhakaran O
et O
al O
. O
, O
2019 O
) O
, O
we O
believe O
the O
majority O
are O
not O
known O
to O
the O
community O
, O
and O
that O
comprehensive O
and O
structured O
testing O
will O
lead O
to O
avenues O
of O
improvement O
in O
these O
and O
other O
tasks O
. O

While O
useful O
, O
accuracy O
on O
benchmarks O
is O
not O
sucient O
for O
evaluating O
NLP O
models O
. O

Adopting O
principles O
from O
behavioral O
testing O
in O
software O
engineering O
, O
we O
propose O
CheckList B-MethodName
, O
a O
model O
- O
agnostic O
and O
task O
- O
agnostic O
testing O
methodology O
that O
tests O
individual O
capabilities O
of O
the O
model O
using O
three O
di O
 O
erent O
test O
types O
. O

To O
illustrate O
its O
utility O
, O
we O
highlight O
significant O
problems O
at O
multiple O
levels O
in O
the O
conceptual O
NLP O
pipeline O
for O
models O
that O
have O
“ O
solved O
” O
existing O
benchmarks O
on O
three O
di O
 O
erent O
tasks O
. O

Further O
, O
CheckList B-MethodName
reveals O
critical O
bugs O
in O
commercial O
systems O
developed O
by O
large O
software O
companies O
, O
indicating O
that O
it O
complements O
current O
practices O
well O
. O

Tests O
created O
with O
CheckList B-MethodName
can O
be O
applied O
to O
any O
model O
, O
making O
it O
easy O
to O
incorporate O
in O
current O
benchmarks O
or O
evaluation O
pipelines O
. O

Our O
user O
studies O
indicate O
that O
CheckList B-MethodName
is O
easy O
to O
learn O
and O
use O
, O
and O
helpful O
both O
for O
expert O
users O
who O
have O
tested O
their O
models O
at O
length O
as O
well O
as O
for O
practitioners O
with O
little O
experience O
in O
a O
task O
. O

The O
tests O
presented O
in O
this O
paper O
are O
part O
of O
CheckList B-MethodName
’s O
open O
source O
release O
, O
and O
can O
easily O
be O
incorporated O
into O
existing O
benchmarks O
. O

More O
importantly O
, O
the O
abstractions O
and O
tools O
in O
CheckList B-MethodName
can O
be O
used O
to O
collectively O
create O
more O
exhaustive O
test O
suites O
for O
a O
variety O
of O
tasks O
. O

Since O
many O
tests O
can O
be O
applied O
across O
tasks O
as O
is O
( O
e.g. O

typos O
) O
or O
with O
minor O
variations O
( O
e.g. O

changing O
names O
) O
, O
we O
expect O
that O
collaborative O
test O
creation O
will O
result O
in O
evaluation O
of O
NLP O
models O
that O
is O
much O
more O
robust O
and O
detailed O
, O
beyond O
just O
accuracy O
on O
held O
- O
out O
data O
. O

MNLI B-DatasetName
Multi B-DatasetName
- I-DatasetName
Genre I-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
is O
a O
large O
- O
scale, O
crowdsourced O
entailment B-TaskName
classification I-TaskName
task I-TaskName
( O
Williams O
et O
al. O
, O
2018 O
) O
. O
Given O
a O
pair O
of O
sentences O
, O
the O
goal O
is O
to O
predict O
whether O
the O
second O
sentence O
is O
an O
entailment O
, O
contradiction O
, O
or O
neutral O
with O
respect O
to O
the O
first O
one O
. O

F1 B-MetricName
scores O
are O
reported O
for O
QQP B-DatasetName
and O
MRPC B-DatasetName
, O
Spearman B-MetricName
correlations I-MetricName
are O
reported O
for O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
, O
and O
accuracy B-MetricName
scores O
are O
reported O
for O
the O
other O
tasks O
. O
We O
exclude O
entries O
that O
use O
BERT B-MethodName
as O
one O
of O
their O
components O
. O

In O
this O
work O
, O
we O
denote O
the O
number B-HyperparameterName
of I-HyperparameterName
layers I-HyperparameterName
( O
i.e. O
, O
Transformer O
blocks O
) O
as O
L B-HyperparameterName
, O
the O
hidden B-HyperparameterName
size I-HyperparameterName
as O
H B-HyperparameterName
, O
and O
the O
number B-HyperparameterName
of I-HyperparameterName
self I-HyperparameterName
- I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
as O
A B-HyperparameterName
. O
We O
primarily O
report O
results O
on O
two O
model O
sizes O
: O
BERTBASE B-MethodName
( O
L B-HyperparameterName
= O
12 B-HyperparameterValue
, O
H B-HyperparameterName
= O
768 B-HyperparameterValue
, O
A B-HyperparameterName
= O
12 B-HyperparameterValue
, O
Total O
Parameters O
= O
110M O
) O
and O
BERTLARGE B-MethodName
( O
L B-HyperparameterName
= O
24 B-HyperparameterValue
, O
H B-HyperparameterName
= O
1024 B-HyperparameterValue
, O
A B-HyperparameterName
= O
16 B-HyperparameterValue
, O
Total O
Parameters O
= O
340M O
) O
. O

In O
this O
paper O
, O
we O
improve O
the O
fine O
- O
tuning O
based O
approaches O
by O
proposing O
BERT B-MethodName
: O
Bidirectional B-MethodName
Encoder I-MethodName
Representations I-MethodName
from I-MethodName
Transformers I-MethodName
. O
BERT B-MethodName
alleviates O
the O
previously O
mentioned O
unidirectionality O
constraint O
by O
using O
a O
“ O
masked O
language O
model O
” O
( O
MLM O
) O
pre O
- O
training O
objective O
, O
inspired O
by O
the O
Cloze O
task O
( O
Taylor O
, O
1953 O
) O
. O

On O
the O
official O
GLUE B-DatasetName
leaderboard O
, O
BERTLARGE B-MethodName
obtains O
a O
score O
of O
80.5 B-MetricValue
, O
compared O
to O
OpenAI B-MethodName
GPT, I-MethodName
which O
obtains O
72.8 B-MetricValue
as O
of O
the O
date O
of O
writing O
. O